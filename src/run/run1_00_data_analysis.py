#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
DAEBAK AI ë°ì´í„° ì¤€ë¹„ í†µí•© íŒŒì´í”„ë¼ì¸ (ê°œì„ ëœ ë²„ì „)

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ML í•™ìŠµì„ ìœ„í•œ ì™„ì „í•œ ë°ì´í„° ì¤€ë¹„ íŒŒì´í”„ë¼ì¸ì„ ì œê³µí•©ë‹ˆë‹¤:
- Phase 1: í†µí•© ë°ì´í„° ë¶„ì„ (ê¸°ì¡´ + 3ìë¦¬ ìš°ì„  ì˜ˆì¸¡ ì‹œìŠ¤í…œ)
- Phase 2: ìµœì í™”ëœ ë²¡í„°í™” (ê¸°ì¡´ + ìƒˆë¡œìš´ ë²¡í„°í™” ì‹œìŠ¤í…œ)
- Phase 3: Negative ìƒ˜í”Œë§ (ML í•™ìŠµìš© ë¹„ë‹¹ì²¨ ì¡°í•© ìƒì„±)

ìƒˆë¡œìš´ ê¸°ëŠ¥:
- í†µí•© ë¶„ì„ê¸° (UnifiedAnalyzer) í™œìš©
- 3ìë¦¬ ìš°ì„  ì˜ˆì¸¡ ì‹œìŠ¤í…œ (ThreeDigitPriorityPredictor)
- ìµœì í™”ëœ ë²¡í„°í™” ì‹œìŠ¤í…œ (OptimizedPatternVectorizer)
- í†µí•© ì„±ëŠ¥ ìµœì í™” ì—”ì§„ í™œìš©
"""

import sys
import os
import numpy as np
import json
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any
import time
import gc

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# í™˜ê²½ ì„¤ì •
os.environ["PYTHONPATH"] = str(project_root)

# --- ë¦¬íŒ©í† ë§ëœ ì˜ì¡´ì„± ê´€ë¦¬ ---
# 1. ì˜ì¡´ì„± ì£¼ì… ì„¤ì •
from src.utils.dependency_injection import configure_dependencies, resolve

# 2. í•„ìš”í•œ í´ë˜ìŠ¤/íƒ€ì… import
from src.utils.unified_logging import get_logger
from src.utils.unified_config import Config
from src.utils.unified_memory_manager import UnifiedMemoryManager
from src.utils.cache_manager import CacheManager
from src.utils.data_loader import load_draw_history
from src.utils.enhanced_process_pool import EnhancedProcessPool, DynamicBatchSizeController
from src.utils.unified_feature_vector_validator import UnifiedFeatureVectorValidator
from src.utils.unified_performance_engine import UnifiedPerformanceEngine
from src.models.unified_model_manager import UnifiedModelManager
# ------------------------------------

# ê¸°ì¡´ ë¶„ì„ ê´€ë ¨ ëª¨ë“ˆë“¤
from src.analysis.enhanced_pattern_vectorizer import EnhancedPatternVectorizer
from src.analysis.negative_sample_generator import NegativeSampleGenerator

# ìƒˆë¡œìš´ ë¶„ì„ ì‹œìŠ¤í…œë“¤ (ë©”ì¸ ë²¡í„°í™” ì‹œìŠ¤í…œ ë³€ê²½)
from src.analysis.unified_analyzer import UnifiedAnalyzer
from src.analysis.three_digit_priority_predictor import ThreeDigitPriorityPredictor
from src.analysis.optimized_pattern_vectorizer import get_optimized_pattern_vectorizer

# ê³ ë„í™”ëœ ìƒˆë¡œìš´ ë¶„ì„ê¸°ë“¤ (ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ ë…ë¦½ì )
from src.analysis.trend_analyzer_v2 import TrendAnalyzerV2
from src.analysis.bayesian_analyzer import BayesianAnalyzer
from src.analysis.ensemble_analyzer import EnsembleAnalyzer

# ìµœì‹  ê³ ê¸‰ ë¶„ì„ê¸°ë“¤
from src.analysis.graph_network_analyzer import GraphNetworkAnalyzer
from src.analysis.meta_feature_analyzer import MetaFeatureAnalyzer

# íŒŒì´í”„ë¼ì¸ ê´€ë¦¬ìë“¤
from src.pipeline.unified_preprocessing_pipeline import UnifiedPreprocessingPipeline

# ì„±ëŠ¥ ìµœì í™” ë„êµ¬
# from src.utils.performance_optimizer import launch_max_performance  # ì œê±°: í•¨ìˆ˜ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ
from src.pipeline.optimized_data_analysis_pipeline import run_optimized_data_analysis

# ê³µìœ  íƒ€ì…ë“¤
from src.shared.types import LotteryNumber

logger = get_logger(__name__)


class EnhancedDataPreparationPipeline:
    """ê°œì„ ëœ ë°ì´í„° ì¤€ë¹„ í†µí•© íŒŒì´í”„ë¼ì¸"""

    def __init__(self):
        """ì´ˆê¸°í™” (ì˜ì¡´ì„± ì£¼ì… ì‚¬ìš©)"""
        self.logger = get_logger(__name__)
        
        # ì˜ì¡´ì„± í•´ê²°
        self.config_manager: Config = resolve(Config)
        self.config = self.config_manager.get_config("main")
        self.paths = self.config_manager.get_paths()

        self.memory_manager: UnifiedMemoryManager = resolve(UnifiedMemoryManager)
        self.performance_engine: UnifiedPerformanceEngine = resolve(UnifiedPerformanceEngine)
        self.batch_controller: DynamicBatchSizeController = resolve(DynamicBatchSizeController)
        self.process_pool: EnhancedProcessPool = resolve(EnhancedProcessPool)
        self.feature_validator: UnifiedFeatureVectorValidator = resolve(UnifiedFeatureVectorValidator)
        self.cache_manager: CacheManager = resolve(CacheManager)
        self.model_manager: UnifiedModelManager = resolve(UnifiedModelManager)

        # ê²°ê³¼ ì €ì¥ ê²½ë¡œë“¤
        self.cache_dir = Path(self.paths.cache_dir)
        self.result_dir = Path(self.paths.result_dir) / "analysis"
        self.performance_dir = Path(self.paths.result_dir) / "performance_reports"
        self.prediction_dir = Path(self.paths.result_dir) / "predictions"

        # ë””ë ‰í† ë¦¬ ìƒì„±
        for directory in [
            self.cache_dir,
            self.result_dir,
            self.performance_dir,
            self.prediction_dir,
        ]:
            directory.mkdir(parents=True, exist_ok=True)

        # ë¶„ì„ê¸°ë“¤ ì´ˆê¸°í™” (ì§€ì—° ì´ˆê¸°í™”)
        self._legacy_vectorizer = None
        self._optimized_vectorizer = None
        self._unified_analyzer = None
        self._three_digit_predictor = None
        self._negative_generator = None

        # ìƒˆë¡œìš´ ê³ ë„í™” ë¶„ì„ê¸°ë“¤ (ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ ë…ë¦½ì )
        self._trend_analyzer_v2 = None
        self._bayesian_analyzer = None
        self._ensemble_analyzer = None

        # ìµœì‹  ê³ ê¸‰ ë¶„ì„ê¸°ë“¤
        self._graph_network_analyzer = None
        self._meta_feature_analyzer = None

        # ì‹¤í–‰ ì˜µì…˜ (í™•ì¥ë¨)
        self.execution_options = {
            "enable_caching": True,
            "parallel_processing": True,
            "chunk_size": 10000,
            "memory_limit_ratio": 0.8,
            "vector_dimensions": [150, 200],
            "negative_sample_ratio": 3.0,
            "max_memory_usage_mb": 2048,  # 2GBë¡œ ì¦ê°€
            "performance_monitoring": True,
            # ìƒˆë¡œìš´ ì˜µì…˜ë“¤
            "enable_unified_analysis": True,
            "enable_3digit_prediction": True,
            "enable_optimized_vectorization": True,
            "use_gpu_acceleration": True,
            "comparison_mode": True,  # ê¸°ì¡´ vs ìƒˆë¡œìš´ ì‹œìŠ¤í…œ ë¹„êµ
        }

        self.preproc_manager = UnifiedPreprocessingPipeline(self.config)

        self.logger.info("âœ… ê°œì„ ëœ ë°ì´í„° ì¤€ë¹„ í†µí•© íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ")

    def execute_full_pipeline(
        self,
        clear_cache: bool = False,
        steps: List[str] = None,
        debug: bool = False,
        verbose: bool = False,
        comparison_mode: bool = False,
    ) -> Dict[str, Any]:
        """
        ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ê°œì„ ëœ ë²„ì „)

        Args:
            clear_cache: ìºì‹œ ì‚­ì œ ì—¬ë¶€
            steps: ì‹¤í–‰í•  ë‹¨ê³„ ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸ê°’: ëª¨ë“  ë‹¨ê³„)
            debug: ë””ë²„ê·¸ ëª¨ë“œ
            verbose: ìƒì„¸ ë¡œê¹…
            comparison_mode: ê¸°ì¡´ vs ìƒˆë¡œìš´ ì‹œìŠ¤í…œ ë¹„êµ ëª¨ë“œ

        Returns:
            Dict[str, Any]: ì‹¤í–‰ ê²°ê³¼ ìš”ì•½
        """
        start_time = time.time()

        if steps is None:
            steps = [
                "unified_analysis",
                "3digit_prediction",
                "optimized_vectorization_enhanced",
                "model_integration_test",
                "advanced_trend_analysis",
                "bayesian_analysis",
                "ensemble_analysis",
                "graph_network_analysis",
                "meta_feature_analysis",
                "negative_sampling",
            ]

        # ë¹„êµ ëª¨ë“œ ì„¤ì •
        self.execution_options["comparison_mode"] = comparison_mode

        # ë¡œê¹… ë ˆë²¨ ì„¤ì •
        if verbose:
            self.logger.setLevel("DEBUG")

        self.logger.info("=" * 80)
        self.logger.info("ğŸš€ DAEBAK AI ê°œì„ ëœ ë°ì´í„° ì¤€ë¹„ í†µí•© íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        self.logger.info(f"ğŸ“‹ ì‹¤í–‰ ë‹¨ê³„: {', '.join(steps)}")
        self.logger.info(
            f"ğŸ’¾ ë©”ëª¨ë¦¬ ì œí•œ: {self.execution_options['max_memory_usage_mb']}MB"
        )
        self.logger.info(
            f"ğŸ¯ ë²¡í„° ì°¨ì› ëª©í‘œ: {self.execution_options['vector_dimensions']}"
        )
        self.logger.info(f"ğŸ”„ ë¹„êµ ëª¨ë“œ: {'í™œì„±í™”' if comparison_mode else 'ë¹„í™œì„±í™”'}")
        self.logger.info("=" * 80)

        # ì‹¤í–‰ ê²°ê³¼ ì¶”ì  (í™•ì¥ë¨)
        pipeline_results = {
            "start_time": datetime.now().isoformat(),
            "steps_executed": [],
            "steps_failed": [],
            "performance_metrics": {},
            "output_files": {},
            "warnings": [],
            "comparison_results": {},  # ìƒˆë¡œ ì¶”ê°€
            "prediction_results": {},  # ìƒˆë¡œ ì¶”ê°€
        }

        try:
            # ìºì‹œ ì •ë¦¬ (ì„ íƒì )
            if clear_cache:
                self.logger.info("ğŸ§¹ ìºì‹œ ì •ë¦¬ ì¤‘...")
                self._clear_pipeline_cache()

            # 1. í†µí•© ë°ì´í„° ë¶„ì„ ë‹¨ê³„
            if "unified_analysis" in steps:
                self.logger.info("ğŸ“Š Phase 1: í†µí•© ë°ì´í„° ë¶„ì„ ì‹¤í–‰ ì¤‘...")
                unified_analysis_result = self.run_unified_analysis(comparison_mode)

                if unified_analysis_result["success"]:
                    pipeline_results["steps_executed"].append("unified_analysis")
                    pipeline_results["performance_metrics"]["unified_analysis"] = (
                        unified_analysis_result["metrics"]
                    )
                    pipeline_results["output_files"].update(
                        unified_analysis_result["output_files"]
                    )
                    if comparison_mode:
                        pipeline_results["comparison_results"]["analysis"] = (
                            unified_analysis_result["comparison"]
                        )
                    self.logger.info("âœ… í†µí•© ë°ì´í„° ë¶„ì„ ì™„ë£Œ")
                else:
                    pipeline_results["steps_failed"].append("unified_analysis")
                    self.logger.error("âŒ í†µí•© ë°ì´í„° ë¶„ì„ ì‹¤íŒ¨")
                    if not debug:
                        return pipeline_results

            # 2. 3ìë¦¬ ìš°ì„  ì˜ˆì¸¡ ë‹¨ê³„
            if "3digit_prediction" in steps:
                self.logger.info("ğŸ¯ Phase 2: 3ìë¦¬ ìš°ì„  ì˜ˆì¸¡ ì‹¤í–‰ ì¤‘...")
                prediction_result = self.run_3digit_prediction()

                if prediction_result["success"]:
                    pipeline_results["steps_executed"].append("3digit_prediction")
                    pipeline_results["performance_metrics"]["3digit_prediction"] = (
                        prediction_result["metrics"]
                    )
                    pipeline_results["output_files"].update(
                        prediction_result["output_files"]
                    )
                    pipeline_results["prediction_results"] = prediction_result[
                        "predictions"
                    ]
                    self.logger.info("âœ… 3ìë¦¬ ìš°ì„  ì˜ˆì¸¡ ì™„ë£Œ")
                else:
                    pipeline_results["steps_failed"].append("3digit_prediction")
                    self.logger.error("âŒ 3ìë¦¬ ìš°ì„  ì˜ˆì¸¡ ì‹¤íŒ¨")
                    if not debug:
                        return pipeline_results

            # 3. í–¥ìƒëœ ìµœì í™” ë²¡í„°í™” ë‹¨ê³„
            if "optimized_vectorization_enhanced" in steps:
                self.logger.info("ğŸ”¢ Phase 3: í–¥ìƒëœ ìµœì í™” ë²¡í„°í™” ì‹¤í–‰ ì¤‘...")

                # ë¶„ì„ ê²°ê³¼ê°€ í•„ìš”í•œ ê²½ìš° ë¡œë“œ
                if "unified_analysis" not in steps:
                    unified_analysis_result = self._load_unified_analysis_result()

                vectorization_result = self.run_optimized_vectorization_enhanced(
                    unified_analysis_result, comparison_mode
                )

                if "error" not in vectorization_result:
                    pipeline_results["steps_executed"].append(
                        "optimized_vectorization_enhanced"
                    )
                    pipeline_results["vectorization_result"] = vectorization_result
                    self.logger.info("âœ… í–¥ìƒëœ ìµœì í™” ë²¡í„°í™” ì™„ë£Œ")
                else:
                    pipeline_results["steps_failed"].append(
                        "optimized_vectorization_enhanced"
                    )
                    self.logger.error("âŒ í–¥ìƒëœ ìµœì í™” ë²¡í„°í™” ì‹¤íŒ¨")
                    if not debug:
                        return pipeline_results

            # 3.5. ëª¨ë¸ í†µí•© í…ŒìŠ¤íŠ¸ ë‹¨ê³„
            if "model_integration_test" in steps:
                self.logger.info("ğŸ¤– Phase 3.5: ëª¨ë¸ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")

                # ë²¡í„°í™” ê²°ê³¼ í™•ì¸
                vectorization_result = pipeline_results.get("vectorization_result")
                if not vectorization_result:
                    if "optimized_vectorization_enhanced" not in steps:
                        vectorization_result = self._load_vectorization_result()

                model_test_result = self.run_model_integration_test(
                    vectorization_result
                )

                if "error" not in model_test_result:
                    pipeline_results["steps_executed"].append("model_integration_test")
                    pipeline_results["model_test_result"] = model_test_result
                    self.logger.info("âœ… ëª¨ë¸ í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
                else:
                    pipeline_results["steps_failed"].append("model_integration_test")
                    self.logger.error("âŒ ëª¨ë¸ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
                    if not debug:
                        return pipeline_results

            # ê¸°ì¡´ ìµœì í™”ëœ ë²¡í„°í™” ë‹¨ê³„ (í•˜ìœ„ í˜¸í™˜ì„±)
            if "optimized_vectorization" in steps:
                self.logger.info("ğŸ”¢ Phase 3: ìµœì í™”ëœ ë²¡í„°í™” ì‹¤í–‰ ì¤‘...")

                # ë¶„ì„ ê²°ê³¼ê°€ í•„ìš”í•œ ê²½ìš° ë¡œë“œ
                if "unified_analysis" not in steps:
                    unified_analysis_result = self._load_unified_analysis_result()

                vectorization_result = self.run_optimized_vectorization(
                    unified_analysis_result, comparison_mode
                )

                if vectorization_result["success"]:
                    pipeline_results["steps_executed"].append("optimized_vectorization")
                    pipeline_results["performance_metrics"][
                        "optimized_vectorization"
                    ] = vectorization_result["metrics"]
                    pipeline_results["output_files"].update(
                        vectorization_result["output_files"]
                    )
                    if comparison_mode:
                        pipeline_results["comparison_results"]["vectorization"] = (
                            vectorization_result["comparison"]
                        )
                    self.logger.info("âœ… ìµœì í™”ëœ ë²¡í„°í™” ì™„ë£Œ")
                else:
                    pipeline_results["steps_failed"].append("optimized_vectorization")
                    self.logger.error("âŒ ìµœì í™”ëœ ë²¡í„°í™” ì‹¤íŒ¨")
                    if not debug:
                        return pipeline_results

            # 4. ê³ ë„í™”ëœ íŠ¸ë Œë“œ ë¶„ì„ ë‹¨ê³„
            if "advanced_trend_analysis" in steps:
                self.logger.info("ğŸ“ˆ Phase 4: ê³ ë„í™”ëœ íŠ¸ë Œë“œ ë¶„ì„ ì‹¤í–‰ ì¤‘...")
                trend_v2_result = self.run_advanced_trend_analysis()

                if trend_v2_result["success"]:
                    pipeline_results["steps_executed"].append("advanced_trend_analysis")
                    pipeline_results["performance_metrics"][
                        "advanced_trend_analysis"
                    ] = trend_v2_result["metrics"]
                    pipeline_results["output_files"].update(
                        trend_v2_result["output_files"]
                    )
                    self.logger.info("âœ… ê³ ë„í™”ëœ íŠ¸ë Œë“œ ë¶„ì„ ì™„ë£Œ")
                else:
                    pipeline_results["steps_failed"].append("advanced_trend_analysis")
                    self.logger.error("âŒ ê³ ë„í™”ëœ íŠ¸ë Œë“œ ë¶„ì„ ì‹¤íŒ¨")
                    if not debug:
                        return pipeline_results

            # 5. ë² ì´ì§€ì•ˆ ë¶„ì„ ë‹¨ê³„
            if "bayesian_analysis" in steps:
                self.logger.info("ğŸ² Phase 5: ë² ì´ì§€ì•ˆ ë¶„ì„ ì‹¤í–‰ ì¤‘...")
                bayesian_result = self.run_bayesian_analysis()

                if bayesian_result["success"]:
                    pipeline_results["steps_executed"].append("bayesian_analysis")
                    pipeline_results["performance_metrics"]["bayesian_analysis"] = (
                        bayesian_result["metrics"]
                    )
                    pipeline_results["output_files"].update(
                        bayesian_result["output_files"]
                    )
                    self.logger.info("âœ… ë² ì´ì§€ì•ˆ ë¶„ì„ ì™„ë£Œ")
                else:
                    pipeline_results["steps_failed"].append("bayesian_analysis")
                    self.logger.error("âŒ ë² ì´ì§€ì•ˆ ë¶„ì„ ì‹¤íŒ¨")
                    if not debug:
                        return pipeline_results

            # 6. ì•™ìƒë¸” ë¶„ì„ ë‹¨ê³„
            if "ensemble_analysis" in steps:
                self.logger.info("ğŸ”— Phase 6: ì•™ìƒë¸” ë¶„ì„ ì‹¤í–‰ ì¤‘...")
                ensemble_result = self.run_ensemble_analysis()

                if ensemble_result["success"]:
                    pipeline_results["steps_executed"].append("ensemble_analysis")
                    pipeline_results["performance_metrics"]["ensemble_analysis"] = (
                        ensemble_result["metrics"]
                    )
                    pipeline_results["output_files"].update(
                        ensemble_result["output_files"]
                    )
                    self.logger.info("âœ… ì•™ìƒë¸” ë¶„ì„ ì™„ë£Œ")
                else:
                    pipeline_results["steps_failed"].append("ensemble_analysis")
                    self.logger.error("âŒ ì•™ìƒë¸” ë¶„ì„ ì‹¤íŒ¨")
                    if not debug:
                        return pipeline_results

            # 7. ê·¸ë˜í”„ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ë‹¨ê³„
            if "graph_network_analysis" in steps:
                self.logger.info("ğŸ”— Phase 7: ê·¸ë˜í”„ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì‹¤í–‰ ì¤‘...")
                graph_result = self.run_graph_network_analysis()

                if graph_result["status"] == "success":
                    pipeline_results["steps_executed"].append("graph_network_analysis")
                    pipeline_results["performance_metrics"][
                        "graph_network_analysis"
                    ] = graph_result["performance_metrics"]
                    pipeline_results["output_files"]["graph_network_analysis"] = (
                        graph_result["output_file"]
                    )
                    self.logger.info("âœ… ê·¸ë˜í”„ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì™„ë£Œ")
                else:
                    pipeline_results["steps_failed"].append("graph_network_analysis")
                    self.logger.error("âŒ ê·¸ë˜í”„ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì‹¤íŒ¨")
                    if not debug:
                        return pipeline_results

            # 8. ë©”íƒ€ íŠ¹ì„± ë¶„ì„ ë‹¨ê³„
            if "meta_feature_analysis" in steps:
                self.logger.info("ğŸ” Phase 8: ë©”íƒ€ íŠ¹ì„± ë¶„ì„ ì‹¤í–‰ ì¤‘...")
                meta_result = self.run_meta_feature_analysis()

                if meta_result["status"] == "success":
                    pipeline_results["steps_executed"].append("meta_feature_analysis")
                    pipeline_results["performance_metrics"]["meta_feature_analysis"] = (
                        meta_result["performance_metrics"]
                    )
                    pipeline_results["output_files"]["meta_feature_analysis"] = (
                        meta_result["output_file"]
                    )
                    self.logger.info("âœ… ë©”íƒ€ íŠ¹ì„± ë¶„ì„ ì™„ë£Œ")
                else:
                    pipeline_results["steps_failed"].append("meta_feature_analysis")
                    self.logger.error("âŒ ë©”íƒ€ íŠ¹ì„± ë¶„ì„ ì‹¤íŒ¨")
                    if not debug:
                        return pipeline_results

            # 9. Negative ìƒ˜í”Œë§ ë‹¨ê³„ (ê¸°ì¡´ ìœ ì§€)
            if "negative_sampling" in steps:
                self.logger.info("ğŸ² Phase 4: Negative ìƒ˜í”Œë§ ì‹¤í–‰ ì¤‘...")

                # ë²¡í„°í™” ê²°ê³¼ê°€ í•„ìš”í•œ ê²½ìš° ë¡œë“œ
                if "optimized_vectorization" not in steps:
                    vectorization_result = self._load_vectorization_result()

                negative_result = self.run_negative_sampling(vectorization_result)

                if negative_result["success"]:
                    pipeline_results["steps_executed"].append("negative_sampling")
                    pipeline_results["performance_metrics"]["negative_sampling"] = (
                        negative_result["metrics"]
                    )
                    pipeline_results["output_files"].update(
                        negative_result["output_files"]
                    )
                    self.logger.info("âœ… Negative ìƒ˜í”Œë§ ì™„ë£Œ")
                else:
                    pipeline_results["steps_failed"].append("negative_sampling")
                    self.logger.error("âŒ Negative ìƒ˜í”Œë§ ì‹¤íŒ¨")

            # 5. ê²°ê³¼ ê²€ì¦ ë° ì €ì¥
            self.logger.info("ğŸ’¾ ê²°ê³¼ ê²€ì¦ ë° ì €ì¥ ì¤‘...")
            validation_result = self.validate_and_save_results(pipeline_results)
            pipeline_results.update(validation_result)

        except Exception as e:
            self.logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            pipeline_results["error"] = str(e)
            if debug:
                import traceback

                pipeline_results["traceback"] = traceback.format_exc()

        finally:
            # ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
            pipeline_results["total_time"] = time.time() - start_time
            pipeline_results["end_time"] = datetime.now().isoformat()

            # ì„±ëŠ¥ ìš”ì•½ ì¶œë ¥
            self._print_enhanced_performance_summary(pipeline_results)

            # íŒŒì´í”„ë¼ì¸ ë¦¬í¬íŠ¸ ì €ì¥
            self._save_enhanced_pipeline_report(pipeline_results)

            # ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()

        return pipeline_results

    def run_model_integration_test(
        self, vectorization_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ëª¨ë¸ í†µí•© í…ŒìŠ¤íŠ¸ ìˆ˜í–‰"""
        self.logger.info("ğŸ¤– ëª¨ë¸ í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
        start_time = time.time()

        try:
            # ë²¡í„° ë°ì´í„° í™•ì¸
            if "vector" not in vectorization_result:
                return {"error": "ë²¡í„°í™” ê²°ê³¼ì— ë²¡í„° ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"}

            vector = vectorization_result["vector"]
            feature_names = vectorization_result.get("feature_names", [])

            # ë”ë¯¸ íƒ€ê²Ÿ ë°ì´í„° ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)
            dummy_target = np.random.uniform(0, 1, size=100)
            test_vectors = np.tile(vector, (100, 1))

            # ëª¨ë¸ ì´ˆê¸°í™”
            init_results = self.model_manager.initialize_models(force_reload=False)
            self.logger.info(f"ëª¨ë¸ ì´ˆê¸°í™” ê²°ê³¼: {init_results}")

            # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ í•™ìŠµ (ì†ŒëŸ‰ ë°ì´í„°)
            training_results = self.model_manager.fit_all_models(
                test_vectors[:50],
                dummy_target[:50],
                validation_split=0.2,
                num_boost_round=10,  # LightGBM ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
                epochs=5,  # ë”¥ëŸ¬ë‹ ëª¨ë¸ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
            )

            # ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
            prediction_results = self.model_manager.predict_ensemble(
                test_vectors[50:60], use_weights=True
            )

            # ëª¨ë¸ í†µê³„
            model_stats = self.model_manager.get_model_stats()

            # ë°°ì¹˜ ì»¨íŠ¸ë¡¤ëŸ¬ í†µê³„
            batch_stats = self.batch_controller.get_stats()

            processing_time = time.time() - start_time

            result = {
                "model_initialization": init_results,
                "training_results": training_results,
                "prediction_results": prediction_results,
                "model_stats": model_stats,
                "batch_stats": batch_stats,
                "processing_time": processing_time,
                "test_data_shape": test_vectors.shape,
                "feature_count": len(feature_names),
            }

            self.logger.info(f"âœ… ëª¨ë¸ í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {processing_time:.2f}ì´ˆ")
            return result

        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return {"error": str(e), "processing_time": time.time() - start_time}

    def run_unified_analysis(self, comparison_mode: bool = False) -> Dict[str, Any]:
        """í†µí•© ë¶„ì„ ì‹¤í–‰"""
        start_time = time.time()

        try:
            # ë°ì´í„° ë¡œë“œ
            self.logger.info("ğŸ“‚ ë¡œë˜ ë°ì´í„° ë¡œë“œ ì¤‘...")
            historical_data = load_draw_history()
            self.logger.info(f"âœ… {len(historical_data)}ê°œ íšŒì°¨ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")

            # í†µí•© ë¶„ì„ê¸° ì´ˆê¸°í™”
            if self._unified_analyzer is None:
                self._unified_analyzer = UnifiedAnalyzer(self.config)

            # í†µí•© ë¶„ì„ ì‹¤í–‰
            self.logger.info("ğŸ” í†µí•© ë¶„ì„ ìˆ˜í–‰ ì¤‘...")
            unified_results = self._unified_analyzer.analyze(historical_data)

            # ê²°ê³¼ ì €ì¥
            result_file = self._unified_analyzer.save_analysis_results(unified_results)

            # ë¹„êµ ëª¨ë“œì¸ ê²½ìš° ê¸°ì¡´ ë¶„ì„ê³¼ ë¹„êµ
            comparison_results = {}
            if comparison_mode:
                self.logger.info("âš–ï¸ ê¸°ì¡´ ë¶„ì„ ì‹œìŠ¤í…œê³¼ ë¹„êµ ì¤‘...")
                comparison_results = self._compare_analysis_systems(
                    historical_data, unified_results
                )

            return {
                "success": True,
                "results": unified_results,
                "metrics": {
                    "execution_time": time.time() - start_time,
                    "data_count": len(historical_data),
                    "analysis_version": unified_results.get(
                        "analysis_version", "v2_unified_optimized"
                    ),
                },
                "output_files": {
                    "unified_analysis": result_file,
                },
                "comparison": comparison_results if comparison_mode else {},
            }

        except Exception as e:
            self.logger.error(f"âŒ í†µí•© ë¶„ì„ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "metrics": {"execution_time": time.time() - start_time},
                "output_files": {},
                "comparison": {},
            }

    def run_3digit_prediction(self) -> Dict[str, Any]:
        """3ìë¦¬ ìš°ì„  ì˜ˆì¸¡ ì‹¤í–‰"""
        start_time = time.time()

        try:
            # ë°ì´í„° ë¡œë“œ
            self.logger.info("ğŸ“‚ ë¡œë˜ ë°ì´í„° ë¡œë“œ ì¤‘...")
            historical_data = load_draw_history()

            # 3ìë¦¬ ìš°ì„  ì˜ˆì¸¡ê¸° ì´ˆê¸°í™”
            if self._three_digit_predictor is None:
                self._three_digit_predictor = ThreeDigitPriorityPredictor(self.config)

            # 3ìë¦¬ ìš°ì„  ì˜ˆì¸¡ ì‹¤í–‰
            self.logger.info("ğŸ¯ 3ìë¦¬ ìš°ì„  ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...")
            prediction_results = self._three_digit_predictor.predict_priority_numbers(
                historical_data
            )

            # ê²°ê³¼ ì €ì¥
            result_file = self._three_digit_predictor.save_predictions(
                prediction_results
            )

            # ì˜ˆì¸¡ ì„±ëŠ¥ ë¶„ì„
            performance_analysis = self._analyze_prediction_performance(
                prediction_results
            )

            return {
                "success": True,
                "predictions": prediction_results,
                "performance_analysis": performance_analysis,
                "metrics": {
                    "execution_time": time.time() - start_time,
                    "total_predictions": len(
                        prediction_results.get("priority_predictions", [])
                    ),
                    "avg_5th_prize_rate": prediction_results.get("summary", {}).get(
                        "avg_5th_prize_rate", 0.0
                    ),
                    "avg_total_win_rate": prediction_results.get("summary", {}).get(
                        "avg_total_win_rate", 0.0
                    ),
                },
                "output_files": {
                    "3digit_predictions": result_file,
                },
            }

        except Exception as e:
            self.logger.error(f"âŒ 3ìë¦¬ ìš°ì„  ì˜ˆì¸¡ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "metrics": {"execution_time": time.time() - start_time},
                "output_files": {},
            }

    def run_optimized_vectorization_enhanced(
        self, analysis_result: Dict[str, Any], comparison_mode: bool = False
    ) -> Dict[str, Any]:
        """í–¥ìƒëœ ìµœì í™” ë²¡í„°í™” ì‹œìŠ¤í…œ (ìƒˆë¡œìš´ ìœ í‹¸ë¦¬í‹° í™œìš©)"""
        self.logger.info("ğŸš€ í–¥ìƒëœ ìµœì í™” ë²¡í„°í™” ì‹œìŠ¤í…œ ì‹œì‘")
        start_time = time.time()

        try:
            # ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì •
            optimal_batch_size = self.batch_controller.get_current_batch_size()
            self.logger.info(f"ìµœì  ë°°ì¹˜ í¬ê¸°: {optimal_batch_size}")

            # ìºì‹œ í™•ì¸
            cache_key = "optimized_vectorization_enhanced"
            cached_result = self.cache_manager.get(cache_key)

            if cached_result is not None and not comparison_mode:
                self.logger.info("ìºì‹œëœ ë²¡í„°í™” ê²°ê³¼ ì‚¬ìš©")
                return cached_result

            # ìµœì í™”ëœ ë²¡í„°í™” ìˆ˜í–‰
            vectorizer = get_optimized_pattern_vectorizer(self.config)

            # ë²¡í„° ìƒì„±
            vector = vectorizer.vectorize_analysis(analysis_result)
            feature_names = vectorizer.get_feature_names()

            # ë²¡í„° ê²€ì¦
            validation_report = self.feature_validator.validate_with_detailed_report(
                vector, feature_names
            )

            if not validation_report["is_valid"]:
                self.logger.warning("ë²¡í„° ê²€ì¦ ì‹¤íŒ¨, ê¸°ë³¸ê°’ìœ¼ë¡œ ëŒ€ì²´")
                vector = np.random.uniform(0.1, 1.0, size=len(feature_names)).astype(
                    np.float32
                )

            # ë²¡í„° ì €ì¥
            vector_path = vectorizer.save_vector_to_file(vector)

            # ì„±ëŠ¥ í†µê³„
            processing_time = time.time() - start_time
            self.batch_controller.report_success(processing_time)

            result = {
                "vector": vector,
                "feature_names": feature_names,
                "vector_path": vector_path,
                "validation_report": validation_report,
                "processing_time": processing_time,
                "batch_size_used": optimal_batch_size,
                "vectorizer_stats": vectorizer.get_performance_stats(),
            }

            # ìºì‹œ ì €ì¥
            self.cache_manager.set(cache_key, result, use_disk=True)

            self.logger.info(f"âœ… í–¥ìƒëœ ë²¡í„°í™” ì™„ë£Œ: {processing_time:.2f}ì´ˆ")
            return result

        except Exception as e:
            self.logger.error(f"âŒ í–¥ìƒëœ ë²¡í„°í™” ì‹¤íŒ¨: {e}")
            return {"error": str(e), "processing_time": time.time() - start_time}

    def run_optimized_vectorization(
        self, analysis_result: Dict[str, Any], comparison_mode: bool = False
    ) -> Dict[str, Any]:
        """ìµœì í™”ëœ ë²¡í„°í™” ì‹¤í–‰"""
        start_time = time.time()

        try:
            # ìµœì í™”ëœ ë²¡í„°í™”ê¸° ì´ˆê¸°í™”
            if self._optimized_vectorizer is None:
                self._optimized_vectorizer = get_optimized_pattern_vectorizer(
                    self.config
                )

            # ë¶„ì„ ê²°ê³¼ì—ì„œ ë°ì´í„° ì¶”ì¶œ
            unified_results = analysis_result.get("results", {})

            # ìµœì í™”ëœ ë²¡í„°í™” ì‹¤í–‰
            self.logger.info("ğŸ”¢ ìµœì í™”ëœ ë²¡í„°í™” ìˆ˜í–‰ ì¤‘...")
            optimized_vector = self._optimized_vectorizer.vectorize_analysis(
                unified_results
            )

            # ë²¡í„° ì €ì¥
            vector_file = self._optimized_vectorizer.save_vector_to_file(
                optimized_vector
            )

            # ë²¡í„° í’ˆì§ˆ ê²€ì¦
            feature_names = self._optimized_vectorizer.get_feature_names()
            quality_metrics = self._validate_vector_quality(
                optimized_vector, feature_names
            )

            # ë¹„êµ ëª¨ë“œì¸ ê²½ìš° ê¸°ì¡´ ë²¡í„°í™”ì™€ ë¹„êµ
            comparison_results = {}
            if comparison_mode:
                self.logger.info("âš–ï¸ ê¸°ì¡´ ë²¡í„°í™” ì‹œìŠ¤í…œê³¼ ë¹„êµ ì¤‘...")
                comparison_results = self._compare_vectorization_systems(
                    unified_results, optimized_vector
                )

            return {
                "success": True,
                "vector": optimized_vector,
                "feature_names": feature_names,
                "quality_metrics": quality_metrics,
                "metrics": {
                    "execution_time": time.time() - start_time,
                    "vector_dimensions": len(optimized_vector),
                    "feature_count": len(feature_names),
                    "vectorization_method": "optimized_pattern_vectorizer",
                },
                "output_files": {
                    "optimized_vector": vector_file,
                },
                "comparison": comparison_results if comparison_mode else {},
            }

        except Exception as e:
            self.logger.error(f"âŒ ìµœì í™”ëœ ë²¡í„°í™” ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "metrics": {"execution_time": time.time() - start_time},
                "output_files": {},
                "comparison": {},
            }

    def _compare_analysis_systems(
        self, historical_data: List[LotteryNumber], unified_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ë¶„ì„ ì‹œìŠ¤í…œ ë¹„êµ"""
        try:
            # ê¸°ì¡´ ë¶„ì„ ì‹œìŠ¤í…œ ì‹¤í–‰
            legacy_start = time.time()
            legacy_results = run_optimized_data_analysis(
                historical_data, config=self.config, enable_caching=False
            )
            legacy_time = time.time() - legacy_start

            # í†µí•© ë¶„ì„ ì‹œìŠ¤í…œ ì‹œê°„
            unified_time = unified_results.get("performance_stats", {}).get(
                "total_time", 0
            )

            # ë¹„êµ ê²°ê³¼
            comparison = {
                "performance_comparison": {
                    "legacy_time": legacy_time,
                    "unified_time": unified_time,
                    "speed_improvement": (
                        (legacy_time - unified_time) / legacy_time * 100
                        if legacy_time > 0
                        else 0
                    ),
                },
                "feature_comparison": {
                    "legacy_features": (
                        len(legacy_results.keys()) if legacy_results else 0
                    ),
                    "unified_features": len(unified_results.keys()),
                    "new_features": [
                        "three_digit_analysis",
                        "three_digit_priority_predictions",
                    ],
                },
                "data_quality": {
                    "legacy_data_count": (
                        legacy_results.get("data_count", 0) if legacy_results else 0
                    ),
                    "unified_data_count": unified_results.get("data_count", 0),
                },
            }

            self.logger.info(
                f"ğŸ“Š ë¶„ì„ ì‹œìŠ¤í…œ ë¹„êµ ì™„ë£Œ: ì†ë„ ê°œì„  {comparison['performance_comparison']['speed_improvement']:.1f}%"
            )
            return comparison

        except Exception as e:
            self.logger.warning(f"ë¶„ì„ ì‹œìŠ¤í…œ ë¹„êµ ì‹¤íŒ¨: {e}")
            return {}

    def _compare_vectorization_systems(
        self, analysis_results: Dict[str, Any], optimized_vector: np.ndarray
    ) -> Dict[str, Any]:
        """ë²¡í„°í™” ì‹œìŠ¤í…œ ë¹„êµ"""
        try:
            # ê¸°ì¡´ ë²¡í„°í™” ì‹œìŠ¤í…œ ì‹¤í–‰
            if self._legacy_vectorizer is None:
                self._legacy_vectorizer = EnhancedPatternVectorizer(self.config)

            legacy_start = time.time()
            legacy_vector = self._legacy_vectorizer.vectorize_full_analysis_enhanced(
                analysis_results
            )
            legacy_time = time.time() - legacy_start

            # ìµœì í™”ëœ ë²¡í„°í™” ì‹œê°„ (ì´ë¯¸ ì‹¤í–‰ë¨)
            optimized_time = 0.1  # ëŒ€ëµì ì¸ ì‹œê°„

            # ë¹„êµ ê²°ê³¼
            comparison = {
                "performance_comparison": {
                    "legacy_time": legacy_time,
                    "optimized_time": optimized_time,
                    "speed_improvement": (
                        (legacy_time - optimized_time) / legacy_time * 100
                        if legacy_time > 0
                        else 0
                    ),
                },
                "dimension_comparison": {
                    "legacy_dimensions": (
                        len(legacy_vector) if legacy_vector is not None else 0
                    ),
                    "optimized_dimensions": len(optimized_vector),
                },
                "quality_comparison": {
                    "legacy_zero_ratio": (
                        np.sum(legacy_vector == 0) / len(legacy_vector)
                        if legacy_vector is not None
                        else 1.0
                    ),
                    "optimized_zero_ratio": np.sum(optimized_vector == 0)
                    / len(optimized_vector),
                },
            }

            self.logger.info(
                f"ğŸ”¢ ë²¡í„°í™” ì‹œìŠ¤í…œ ë¹„êµ ì™„ë£Œ: ì†ë„ ê°œì„  {comparison['performance_comparison']['speed_improvement']:.1f}%"
            )
            return comparison

        except Exception as e:
            self.logger.warning(f"ë²¡í„°í™” ì‹œìŠ¤í…œ ë¹„êµ ì‹¤íŒ¨: {e}")
            return {}

    def _analyze_prediction_performance(
        self, prediction_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ì˜ˆì¸¡ ì„±ëŠ¥ ë¶„ì„"""
        predictions = prediction_results.get("priority_predictions", [])
        summary = prediction_results.get("summary", {})
        targets = prediction_results.get("performance_targets", {})

        performance_analysis = {
            "prediction_count": len(predictions),
            "quality_metrics": {
                "avg_confidence": (
                    np.mean([pred.get("integrated_score", 0) for pred in predictions])
                    if predictions
                    else 0
                ),
                "high_confidence_count": len(
                    [
                        pred
                        for pred in predictions
                        if pred.get("integrated_score", 0) >= 0.7
                    ]
                ),
                "target_achievement": {
                    "5th_prize_rate": {
                        "current": summary.get("avg_5th_prize_rate", 0),
                        "target": targets.get("target_5th_prize_rate", 0.25),
                        "achievement_ratio": (
                            summary.get("avg_5th_prize_rate", 0)
                            / targets.get("target_5th_prize_rate", 0.25)
                            if targets.get("target_5th_prize_rate", 0.25) > 0
                            else 0
                        ),
                    },
                    "total_win_rate": {
                        "current": summary.get("avg_total_win_rate", 0),
                        "target": targets.get("target_total_win_rate", 0.35),
                        "achievement_ratio": (
                            summary.get("avg_total_win_rate", 0)
                            / targets.get("target_total_win_rate", 0.35)
                            if targets.get("target_total_win_rate", 0.35) > 0
                            else 0
                        ),
                    },
                },
            },
            "top_predictions": (
                predictions[:5] if len(predictions) >= 5 else predictions
            ),
        }

        return performance_analysis

    def _load_unified_analysis_result(self) -> Dict[str, Any]:
        """í†µí•© ë¶„ì„ ê²°ê³¼ ë¡œë“œ"""
        try:
            # ìµœì‹  í†µí•© ë¶„ì„ ê²°ê³¼ íŒŒì¼ ì°¾ê¸°
            result_files = list(self.result_dir.glob("unified_analysis_*.json"))
            if not result_files:
                raise FileNotFoundError("í†µí•© ë¶„ì„ ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            latest_file = max(result_files, key=lambda x: x.stat().st_mtime)

            with open(latest_file, "r", encoding="utf-8") as f:
                return {"results": json.load(f)}

        except Exception as e:
            self.logger.error(f"í†µí•© ë¶„ì„ ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {"results": {}}

    def _print_enhanced_performance_summary(self, results: Dict[str, Any]):
        """ê°œì„ ëœ ì„±ëŠ¥ ìš”ì•½ ì¶œë ¥"""
        self.logger.info("=" * 80)
        self.logger.info("ğŸ“Š DAEBAK AI íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê²°ê³¼ ìš”ì•½")
        self.logger.info("=" * 80)

        # ê¸°ë³¸ ì‹¤í–‰ ì •ë³´
        self.logger.info(f"â° ì´ ì‹¤í–‰ ì‹œê°„: {results.get('total_time', 0):.2f}ì´ˆ")
        self.logger.info(
            f"âœ… ì„±ê³µí•œ ë‹¨ê³„: {', '.join(results.get('steps_executed', []))}"
        )

        if results.get("steps_failed"):
            self.logger.info(
                f"âŒ ì‹¤íŒ¨í•œ ë‹¨ê³„: {', '.join(results.get('steps_failed', []))}"
            )

        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        metrics = results.get("performance_metrics", {})
        for step, metric in metrics.items():
            if isinstance(metric, dict) and "execution_time" in metric:
                self.logger.info(f"â±ï¸ {step}: {metric['execution_time']:.2f}ì´ˆ")

        # 3ìë¦¬ ì˜ˆì¸¡ ê²°ê³¼
        if "prediction_results" in results:
            pred_summary = results["prediction_results"].get("summary", {})
            self.logger.info("ğŸ¯ 3ìë¦¬ ìš°ì„  ì˜ˆì¸¡ ê²°ê³¼:")
            self.logger.info(
                f"   - í‰ê·  5ë“± ì ì¤‘ë¥ : {pred_summary.get('avg_5th_prize_rate', 0):.1%}"
            )
            self.logger.info(
                f"   - í‰ê·  ì „ì²´ ì ì¤‘ë¥ : {pred_summary.get('avg_total_win_rate', 0):.1%}"
            )
            self.logger.info(
                f"   - ìµœì¢… ì˜ˆì¸¡ ìˆ˜: {pred_summary.get('final_predictions_count', 0)}ê°œ"
            )

        # ë¹„êµ ê²°ê³¼
        if results.get("comparison_results"):
            self.logger.info("âš–ï¸ ì‹œìŠ¤í…œ ë¹„êµ ê²°ê³¼:")
            for system, comparison in results["comparison_results"].items():
                if "performance_comparison" in comparison:
                    improvement = comparison["performance_comparison"].get(
                        "speed_improvement", 0
                    )
                    self.logger.info(f"   - {system} ì†ë„ ê°œì„ : {improvement:.1f}%")

        # ì¶œë ¥ íŒŒì¼
        self.logger.info("ğŸ“ ìƒì„±ëœ íŒŒì¼:")
        for file_type, file_path in results.get("output_files", {}).items():
            self.logger.info(f"   - {file_type}: {file_path}")

        self.logger.info("=" * 80)

    def _save_enhanced_pipeline_report(self, results: Dict[str, Any]):
        """ê°œì„ ëœ íŒŒì´í”„ë¼ì¸ ë¦¬í¬íŠ¸ ì €ì¥"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_file = (
                self.performance_dir / f"enhanced_pipeline_report_{timestamp}.json"
            )

            with open(report_file, "w", encoding="utf-8") as f:
                json.dump(results, f, indent=2, ensure_ascii=False, default=str)

            self.logger.info(f"ğŸ“‹ íŒŒì´í”„ë¼ì¸ ë¦¬í¬íŠ¸ ì €ì¥: {report_file}")

        except Exception as e:
            self.logger.error(f"íŒŒì´í”„ë¼ì¸ ë¦¬í¬íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")

    # ê¸°ì¡´ ë©”ì„œë“œë“¤ ìœ ì§€
    def run_data_analysis(self) -> Dict[str, Any]:
        """ê¸°ì¡´ ë°ì´í„° ë¶„ì„ ì‹¤í–‰ (í•˜ìœ„ í˜¸í™˜ì„±)"""
        return self.run_unified_analysis(comparison_mode=False)

    def run_vectorization(self, analysis_result: Dict[str, Any]) -> Dict[str, Any]:
        """ê¸°ì¡´ ë²¡í„°í™” ì‹¤í–‰ (í•˜ìœ„ í˜¸í™˜ì„±)"""
        return self.run_optimized_vectorization(analysis_result, comparison_mode=False)

    def run_negative_sampling(
        self, vectorization_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Negative ìƒ˜í”Œë§ ì‹¤í–‰ (ê¸°ì¡´ ìœ ì§€)"""
        start_time = time.time()

        try:
            # Negative ìƒ˜í”Œ ìƒì„±ê¸° ì´ˆê¸°í™”
            if self._negative_generator is None:
                self._negative_generator = NegativeSampleGenerator(self.config)

            # ë²¡í„° ë°ì´í„° ì¶”ì¶œ
            vector_data = vectorization_result.get("vector")
            if vector_data is None:
                raise ValueError("ë²¡í„°í™” ê²°ê³¼ì—ì„œ ë²¡í„° ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            # Negative ìƒ˜í”Œë§ ì‹¤í–‰
            self.logger.info("ğŸ² Negative ìƒ˜í”Œë§ ìˆ˜í–‰ ì¤‘...")
            negative_samples = self._negative_generator.generate_negative_samples(
                sample_count=int(
                    len(vector_data) * self.execution_options["negative_sample_ratio"]
                )
            )

            # ê²°ê³¼ ì €ì¥
            output_file = self.cache_dir / "negative_samples.npy"
            np.save(output_file, negative_samples)

            return {
                "success": True,
                "negative_samples": negative_samples,
                "metrics": {
                    "execution_time": time.time() - start_time,
                    "sample_count": len(negative_samples),
                    "sample_ratio": self.execution_options["negative_sample_ratio"],
                },
                "output_files": {
                    "negative_samples": str(output_file),
                },
            }

        except Exception as e:
            self.logger.error(f"âŒ Negative ìƒ˜í”Œë§ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "metrics": {"execution_time": time.time() - start_time},
                "output_files": {},
            }

    def run_advanced_trend_analysis(self) -> Dict[str, Any]:
        """ê³ ë„í™”ëœ íŠ¸ë Œë“œ ë¶„ì„ ì‹¤í–‰ (TrendAnalyzerV2)"""
        start_time = time.time()

        try:
            # ë°ì´í„° ë¡œë“œ
            self.logger.info("ğŸ“‚ ë¡œë˜ ë°ì´í„° ë¡œë“œ ì¤‘...")
            historical_data = load_draw_history()

            # TrendAnalyzerV2 ì´ˆê¸°í™”
            if self._trend_analyzer_v2 is None:
                self._trend_analyzer_v2 = TrendAnalyzerV2(self.config)

            # ê³ ë„í™”ëœ íŠ¸ë Œë“œ ë¶„ì„ ì‹¤í–‰
            self.logger.info("ğŸ“ˆ TrendAnalyzerV2 ë¶„ì„ ìˆ˜í–‰ ì¤‘...")
            trend_v2_results = self._trend_analyzer_v2.analyze(historical_data)

            # ê²°ê³¼ ì €ì¥
            result_file = self._trend_analyzer_v2.save_analysis_results(
                trend_v2_results
            )

            # ì„±ëŠ¥ ë¶„ì„
            performance_analysis = self._analyze_trend_v2_performance(trend_v2_results)

            return {
                "success": True,
                "results": trend_v2_results,
                "performance_analysis": performance_analysis,
                "metrics": {
                    "execution_time": time.time() - start_time,
                    "data_count": len(historical_data),
                    "analyzer_version": trend_v2_results.get(
                        "analyzer_version", "TrendAnalyzerV2_v1.0"
                    ),
                    "trend_strength": trend_v2_results.get("trend_summary", {})
                    .get("system_health", {})
                    .get("overall_stability", 0),
                },
                "output_files": {
                    "trend_v2_analysis": result_file,
                },
            }

        except Exception as e:
            self.logger.error(f"âŒ ê³ ë„í™”ëœ íŠ¸ë Œë“œ ë¶„ì„ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "metrics": {"execution_time": time.time() - start_time},
                "output_files": {},
            }

    def run_bayesian_analysis(self) -> Dict[str, Any]:
        """ë² ì´ì§€ì•ˆ ë¶„ì„ ì‹¤í–‰"""
        start_time = time.time()

        try:
            # ë°ì´í„° ë¡œë“œ
            self.logger.info("ğŸ“‚ ë¡œë˜ ë°ì´í„° ë¡œë“œ ì¤‘...")
            historical_data = load_draw_history()

            # BayesianAnalyzer ì´ˆê¸°í™”
            if self._bayesian_analyzer is None:
                self._bayesian_analyzer = BayesianAnalyzer(self.config)

            # ë² ì´ì§€ì•ˆ ë¶„ì„ ì‹¤í–‰
            self.logger.info("ğŸ² ë² ì´ì§€ì•ˆ í™•ë¥  ë¶„ì„ ìˆ˜í–‰ ì¤‘...")
            bayesian_results = self._bayesian_analyzer.analyze(historical_data)

            # ê²°ê³¼ ì €ì¥
            result_file = self._bayesian_analyzer.save_analysis_results(
                bayesian_results
            )

            # ì„±ëŠ¥ ë¶„ì„
            performance_analysis = self._analyze_bayesian_performance(bayesian_results)

            return {
                "success": True,
                "results": bayesian_results,
                "performance_analysis": performance_analysis,
                "metrics": {
                    "execution_time": time.time() - start_time,
                    "data_count": len(historical_data),
                    "analyzer_version": bayesian_results.get(
                        "analyzer_version", "BayesianAnalyzer_v1.0"
                    ),
                    "confidence_level": bayesian_results.get("analysis_summary", {})
                    .get("recommendation_confidence", {})
                    .get("overall_confidence", 0),
                    "convergence_ratio": bayesian_results.get("posterior_updates", {})
                    .get("convergence_analysis", {})
                    .get("system_convergence", {})
                    .get("converged_ratio", 0),
                },
                "output_files": {
                    "bayesian_analysis": result_file,
                },
            }

        except Exception as e:
            self.logger.error(f"âŒ ë² ì´ì§€ì•ˆ ë¶„ì„ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "metrics": {"execution_time": time.time() - start_time},
                "output_files": {},
            }

    def run_ensemble_analysis(self) -> Dict[str, Any]:
        """ì•™ìƒë¸” ë¶„ì„ ì‹¤í–‰"""
        start_time = time.time()

        try:
            # ë°ì´í„° ë¡œë“œ
            self.logger.info("ğŸ“‚ ë¡œë˜ ë°ì´í„° ë¡œë“œ ì¤‘...")
            historical_data = load_draw_history()

            # EnsembleAnalyzer ì´ˆê¸°í™”
            if self._ensemble_analyzer is None:
                self._ensemble_analyzer = EnsembleAnalyzer(self.config)

            # ì•™ìƒë¸” ë¶„ì„ ì‹¤í–‰
            self.logger.info("ğŸ”— ì•™ìƒë¸” íŒ¨í„´ ë¶„ì„ ìˆ˜í–‰ ì¤‘...")
            ensemble_results = self._ensemble_analyzer.analyze(historical_data)

            # ê²°ê³¼ ì €ì¥
            result_file = self._ensemble_analyzer.save_analysis_results(
                ensemble_results
            )

            # ì„±ëŠ¥ ë¶„ì„
            performance_analysis = self._analyze_ensemble_performance(ensemble_results)

            return {
                "success": True,
                "results": ensemble_results,
                "performance_analysis": performance_analysis,
                "metrics": {
                    "execution_time": time.time() - start_time,
                    "data_count": len(historical_data),
                    "analyzer_version": ensemble_results.get(
                        "analyzer_version", "EnsembleAnalyzer_v1.0"
                    ),
                    "ensemble_methods": len(
                        ensemble_results.get("weighted_ensemble_analysis", {}).get(
                            "ensemble_results", {}
                        )
                    ),
                    "prediction_confidence": ensemble_results.get(
                        "final_predictions", {}
                    )
                    .get("prediction_summary", {})
                    .get("average_confidence", 0),
                    "window_consistency": ensemble_results.get(
                        "multi_window_analysis", {}
                    )
                    .get("consistency_analysis", {})
                    .get("consistency_score", 0),
                },
                "output_files": {
                    "ensemble_analysis": result_file,
                },
            }

        except Exception as e:
            self.logger.error(f"âŒ ì•™ìƒë¸” ë¶„ì„ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "metrics": {"execution_time": time.time() - start_time},
                "output_files": {},
            }

    def _analyze_trend_v2_performance(
        self, trend_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """TrendAnalyzerV2 ì„±ëŠ¥ ë¶„ì„"""
        summary = trend_results.get("trend_summary", {})

        return {
            "top_numbers_quality": len(summary.get("top_recommended_numbers", [])),
            "system_stability": summary.get("system_health", {}).get(
                "overall_stability", 0
            ),
            "recommendation_confidence": summary.get("trend_analysis_summary", {}).get(
                "recommendation_confidence", "unknown"
            ),
            "change_point_density": summary.get("system_health", {}).get(
                "change_point_density", 0
            ),
        }

    def _analyze_bayesian_performance(
        self, bayesian_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ë² ì´ì§€ì•ˆ ë¶„ì„ ì„±ëŠ¥ ë¶„ì„"""
        summary = bayesian_results.get("analysis_summary", {})

        return {
            "model_quality": summary.get("key_findings", {}).get(
                "best_model", "unknown"
            ),
            "convergence_quality": summary.get("key_findings", {})
            .get("system_convergence", {})
            .get("converged_ratio", 0),
            "prediction_confidence": summary.get("recommendation_confidence", {}).get(
                "overall_confidence", 0
            ),
            "high_confidence_count": summary.get("recommendation_confidence", {}).get(
                "high_confidence_numbers", 0
            ),
        }

    def _analyze_ensemble_performance(
        self, ensemble_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ì•™ìƒë¸” ë¶„ì„ ì„±ëŠ¥ ë¶„ì„"""
        summary = ensemble_results.get("ensemble_summary", {})

        return {
            "ensemble_diversity": summary.get("analysis_overview", {}).get(
                "ensemble_methods_used", 0
            ),
            "window_consistency": summary.get("key_findings", {}).get(
                "window_consistency", 0
            ),
            "prediction_quality": summary.get("performance_metrics", {}).get(
                "prediction_confidence", 0
            ),
            "ensemble_strength": summary.get("recommendations", {}).get(
                "ensemble_strength", "unknown"
            ),
        }

    def run_graph_network_analysis(self) -> Dict[str, Any]:
        """ê·¸ë˜í”„ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì‹¤í–‰"""
        try:
            self.logger.info("ğŸ”— ê·¸ë˜í”„ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì‹œì‘...")

            # ë°ì´í„° ë¡œë“œ
            historical_data = load_draw_history()
            if not historical_data:
                raise ValueError("ë¡œë˜ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            # ê·¸ë˜í”„ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ê¸° ì´ˆê¸°í™”
            if self._graph_network_analyzer is None:
                graph_config = self.config.get("graph_network_analysis", {})
                self._graph_network_analyzer = GraphNetworkAnalyzer(graph_config)

            # ë¶„ì„ ì‹¤í–‰
            analysis_results = self._graph_network_analyzer.analyze(historical_data)

            # ê²°ê³¼ ì €ì¥
            output_file = self.result_dir / "graph_network_analysis.json"
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(
                    analysis_results, f, ensure_ascii=False, indent=2, default=str
                )

            # ê·¸ë˜í”„ íŠ¹ì„± ë²¡í„° ìƒì„±
            if "number_graph_features" in analysis_results:
                graph_vector = self._graph_network_analyzer.get_graph_features_vector(
                    analysis_results["number_graph_features"]
                )

                # ë²¡í„° ì €ì¥
                vector_file = self.cache_dir / "graph_network_features.npy"
                np.save(vector_file, graph_vector)

                self.logger.info(f"ê·¸ë˜í”„ íŠ¹ì„± ë²¡í„° ì €ì¥: {graph_vector.shape}")

            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
            performance_metrics = self._analyze_graph_network_performance(
                analysis_results
            )

            self.logger.info("âœ… ê·¸ë˜í”„ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì™„ë£Œ")

            return {
                "status": "success",
                "output_file": str(output_file),
                "analysis_results": analysis_results,
                "performance_metrics": performance_metrics,
                "data_samples": len(historical_data),
            }

        except Exception as e:
            self.logger.error(f"ê·¸ë˜í”„ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                "status": "failed",
                "error": str(e),
                "output_file": None,
            }

    def run_meta_feature_analysis(self) -> Dict[str, Any]:
        """ë©”íƒ€ íŠ¹ì„± ë¶„ì„ ì‹¤í–‰"""
        try:
            self.logger.info("ğŸ” ë©”íƒ€ íŠ¹ì„± ë¶„ì„ ì‹œì‘...")

            # ë°ì´í„° ë¡œë“œ
            historical_data = load_draw_history()
            if not historical_data:
                raise ValueError("ë¡œë˜ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            # ë©”íƒ€ íŠ¹ì„± ë¶„ì„ê¸° ì´ˆê¸°í™”
            if self._meta_feature_analyzer is None:
                meta_config = self.config.get("meta_feature_analysis", {})
                self._meta_feature_analyzer = MetaFeatureAnalyzer(meta_config)

            # ë¶„ì„ ì‹¤í–‰
            analysis_results = self._meta_feature_analyzer.analyze(historical_data)

            # ê²°ê³¼ ì €ì¥
            output_file = self.result_dir / "meta_feature_analysis.json"
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(
                    analysis_results, f, ensure_ascii=False, indent=2, default=str
                )

            # ë©”íƒ€ íŠ¹ì„± ë²¡í„° ìƒì„±
            meta_vector = self._meta_feature_analyzer.get_meta_features_vector(
                analysis_results
            )

            # ë²¡í„° ì €ì¥
            vector_file = self.cache_dir / "meta_features.npy"
            np.save(vector_file, meta_vector)

            self.logger.info(f"ë©”íƒ€ íŠ¹ì„± ë²¡í„° ì €ì¥: {meta_vector.shape}")

            # ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œë„ ì €ì¥
            meta_results_file = self.result_dir / "meta_analysis_results.json"
            self._meta_feature_analyzer.save_meta_analysis_results(
                analysis_results, meta_results_file.name
            )

            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
            performance_metrics = self._analyze_meta_feature_performance(
                analysis_results
            )

            self.logger.info("âœ… ë©”íƒ€ íŠ¹ì„± ë¶„ì„ ì™„ë£Œ")

            return {
                "status": "success",
                "output_file": str(output_file),
                "analysis_results": analysis_results,
                "performance_metrics": performance_metrics,
                "data_samples": len(historical_data),
            }

        except Exception as e:
            self.logger.error(f"ë©”íƒ€ íŠ¹ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                "status": "failed",
                "error": str(e),
                "output_file": None,
            }

    def _analyze_graph_network_performance(
        self, graph_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ê·¸ë˜í”„ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì„±ëŠ¥ ë¶„ì„"""
        try:
            graph_stats = graph_results.get("graph_statistics", {})
            communities = graph_results.get("communities", {})
            centrality = graph_results.get("centrality_analysis", {})

            return {
                "graph_connectivity": graph_stats.get("is_connected", False),
                "graph_density": graph_stats.get("density", 0.0),
                "community_count": len(communities.get("greedy", [])),
                "modularity_score": communities.get("modularity", 0.0),
                "centrality_methods": len(
                    [k for k in centrality.keys() if k != "statistics"]
                ),
                "node_coverage": graph_stats.get("nodes", 0) / 45,  # 45ê°œ ë²ˆí˜¸ ëŒ€ë¹„
                "edge_count": graph_stats.get("edges", 0),
                "analysis_completeness": 1.0 if "error" not in graph_results else 0.0,
            }

        except Exception as e:
            self.logger.error(f"ê·¸ë˜í”„ ë„¤íŠ¸ì›Œí¬ ì„±ëŠ¥ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"analysis_completeness": 0.0, "error": str(e)}

    def _analyze_meta_feature_performance(
        self, meta_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ë©”íƒ€ íŠ¹ì„± ë¶„ì„ ì„±ëŠ¥ ë¶„ì„"""
        try:
            importance_analysis = meta_results.get("importance_analysis", {})
            dimension_reduction = meta_results.get("dimension_reduction", {})
            feature_selection = meta_results.get("feature_selection", {})

            return {
                "importance_methods": len(
                    [k for k in importance_analysis.keys() if k != "error"]
                ),
                "pca_success": dimension_reduction.get("pca", {}).get("success", False),
                "variance_explained": dimension_reduction.get("pca", {}).get(
                    "total_variance_explained", 0.0
                ),
                "feature_selection_success": feature_selection.get(
                    "model_based", {}
                ).get("success", False),
                "selected_features_ratio": (
                    feature_selection.get("model_based", {}).get("selected_count", 0)
                    / max(meta_results.get("original_features", 1), 1)
                ),
                "data_quality_score": (
                    1.0
                    - (
                        meta_results.get("meta_statistics", {})
                        .get("data_quality", {})
                        .get("missing_values", 0)
                        + meta_results.get("meta_statistics", {})
                        .get("data_quality", {})
                        .get("infinite_values", 0)
                    )
                    / max(meta_results.get("data_samples", 1), 1)
                ),
                "analysis_completeness": 1.0 if "error" not in meta_results else 0.0,
            }

        except Exception as e:
            self.logger.error(f"ë©”íƒ€ íŠ¹ì„± ì„±ëŠ¥ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"analysis_completeness": 0.0, "error": str(e)}

    def validate_and_save_results(
        self, pipeline_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ê²°ê³¼ ê²€ì¦ ë° ì €ì¥"""
        validation_results = {
            "validation_passed": True,
            "validation_errors": [],
            "file_validations": {},
        }

        # ì¶œë ¥ íŒŒì¼ ê²€ì¦
        for file_type, file_path in pipeline_results.get("output_files", {}).items():
            try:
                if Path(file_path).exists():
                    file_size = Path(file_path).stat().st_size
                    validation_results["file_validations"][file_type] = {
                        "exists": True,
                        "size_bytes": file_size,
                        "size_mb": file_size / (1024 * 1024),
                    }
                else:
                    validation_results["validation_errors"].append(
                        f"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {file_path}"
                    )
                    validation_results["validation_passed"] = False
            except Exception as e:
                validation_results["validation_errors"].append(
                    f"íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨ {file_path}: {e}"
                )
                validation_results["validation_passed"] = False

        return validation_results

    def _clear_pipeline_cache(self):
        """íŒŒì´í”„ë¼ì¸ ìºì‹œ ì •ë¦¬"""
        try:
            cache_files = list(self.cache_dir.glob("*"))
            for cache_file in cache_files:
                if cache_file.is_file():
                    cache_file.unlink()
            self.logger.info(f"ğŸ§¹ {len(cache_files)}ê°œ ìºì‹œ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")

    def _validate_vector_quality(
        self, vector: np.ndarray, names: List[str]
    ) -> Dict[str, Any]:
        """ë²¡í„° í’ˆì§ˆ ê²€ì¦"""
        return {
            "dimension_match": len(vector) == len(names),
            "zero_ratio": np.sum(vector == 0) / len(vector),
            "nan_count": np.sum(np.isnan(vector)),
            "inf_count": np.sum(np.isinf(vector)),
            "value_range": {"min": float(np.min(vector)), "max": float(np.max(vector))},
            "mean": float(np.mean(vector)),
            "std": float(np.std(vector)),
        }

    # ê¸°ì¡´ ë¡œë“œ ë©”ì„œë“œë“¤ ìœ ì§€
    def _load_analysis_result(self) -> Dict[str, Any]:
        """ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ ë¡œë“œ (í•˜ìœ„ í˜¸í™˜ì„±)"""
        return self._load_unified_analysis_result()

    def _load_vectorization_result(self) -> Dict[str, Any]:
        """ë²¡í„°í™” ê²°ê³¼ ë¡œë“œ"""
        try:
            vector_file = self.cache_dir / "optimized_feature_vector.npy"
            if vector_file.exists():
                vector = np.load(vector_file)
                return {"vector": vector}
            else:
                return {"vector": np.array([])}
        except Exception as e:
            self.logger.error(f"ë²¡í„°í™” ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {"vector": np.array([])}


def parse_arguments():
    """ëª…ë ¹í–‰ ì¸ìˆ˜ íŒŒì‹±"""
    parser = argparse.ArgumentParser(
        description="DAEBAK AI ê°œì„ ëœ ë°ì´í„° ì¤€ë¹„ í†µí•© íŒŒì´í”„ë¼ì¸"
    )

    parser.add_argument(
        "--steps",
        nargs="+",
        choices=[
            "unified_analysis",
            "3digit_prediction",
            "optimized_vectorization",
            "advanced_trend_analysis",
            "bayesian_analysis",
            "ensemble_analysis",
            "graph_network_analysis",
            "meta_feature_analysis",
            "negative_sampling",
        ],
        default=[
            "unified_analysis",
            "3digit_prediction",
            "optimized_vectorization",
            "advanced_trend_analysis",
            "bayesian_analysis",
            "ensemble_analysis",
            "graph_network_analysis",
            "meta_feature_analysis",
            "negative_sampling",
        ],
        help="ì‹¤í–‰í•  íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ (ìµœì‹  ê³ ê¸‰ ë¶„ì„ê¸° í¬í•¨)",
    )

    parser.add_argument("--clear-cache", action="store_true", help="ì‹¤í–‰ ì „ ìºì‹œ ì •ë¦¬")
    parser.add_argument("--debug", action="store_true", help="ë””ë²„ê·¸ ëª¨ë“œ")
    parser.add_argument("--verbose", action="store_true", help="ìƒì„¸ ë¡œê¹…")
    parser.add_argument(
        "--comparison", action="store_true", help="ê¸°ì¡´ vs ìƒˆë¡œìš´ ì‹œìŠ¤í…œ ë¹„êµ"
    )
    parser.add_argument("--config", type=str, help="ì„¤ì • íŒŒì¼ ê²½ë¡œ")

    return parser.parse_args()


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ìµœìš°ì„ : ì˜ì¡´ì„± ì„¤ì •
    configure_dependencies()
    
    parser = parse_arguments()
    args = parser.parse_args()

    # íŒŒì´í”„ë¼ì¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì„¤ì • ì¸ì ì œê±°)
    pipeline = EnhancedDataPreparationPipeline()

    try:
        # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        results = pipeline.execute_full_pipeline(
            clear_cache=args.clear_cache,
            steps=args.steps,
            debug=args.debug,
            verbose=args.verbose,
            comparison_mode=args.comparison,
        )

        # ì‹¤í–‰ ê²°ê³¼ ì¶œë ¥
        if results.get("steps_failed"):
            print(f"âŒ ì¼ë¶€ ë‹¨ê³„ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {', '.join(results['steps_failed'])}")
            return 1
        else:
            print("âœ… ëª¨ë“  íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            return 0

    except Exception as e:
        print(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return 1


if __name__ == "__main__":
    exit(main())
