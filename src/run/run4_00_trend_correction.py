#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
íŠ¸ë Œë“œ ë³´ì • ì‹œìŠ¤í…œ (4ë‹¨ê³„) - TCN ëª¨ë¸ ê¸°ë°˜ ì‹œê³„ì—´ íŠ¸ë Œë“œ ë³´ì •

ê¸°ì¡´ TCN ëª¨ë¸ê³¼ TrendAnalyzerV2ë¥¼ ì™„ì „íˆ í†µí•©í•˜ì—¬ ì‹œê³„ì—´ íŠ¸ë Œë“œ ë³´ì •ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
ì´ì „ 3ë‹¨ê³„(run1, run2, run3)ì˜ ëª¨ë“  ê²°ê³¼ë¥¼ ì‹œê³„ì—´ í˜•íƒœë¡œ í†µí•©í•˜ê³ ,
ìµœê·¼ íšŒì°¨ ë³€í™”ë¥¼ ë°˜ì˜í•œ íŠ¸ë Œë“œ ë³´ì •ì„ ì ìš©í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- ì´ì „ ë‹¨ê³„ ê²°ê³¼ ì™„ì „ í†µí•© (run1 ë¶„ì„, run2 ì˜ˆì¸¡, run3 ì´ìƒê°ì§€)
- TrendAnalyzerV2 ê¸°ë°˜ íšŒì°¨ë³„ íŠ¸ë Œë“œ ë¶„ì„
- TCN ëª¨ë¸ì„ í†µí•œ ì‹œê³„ì—´ íŒ¨í„´ í•™ìŠµ ë° ì˜ˆì¸¡
- GPU ìµœì í™” ë° ë©”ëª¨ë¦¬ í’€ ê´€ë¦¬
- ì ì‘ì  íŠ¸ë Œë“œ ë³´ì • ê°€ì¤‘ì¹˜ ì ìš©
"""

# 1. í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import json
import time
from typing import Dict, List, Any, Tuple
from pathlib import Path
from datetime import datetime
import warnings

warnings.filterwarnings("ignore")

# 2. ì„œë“œíŒŒí‹°
import numpy as np
import torch
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# 3. í”„ë¡œì íŠ¸ ë‚´ë¶€ (ë¦¬íŒ©í† ë§ëœ ì˜ì¡´ì„± ê´€ë¦¬)
from ..utils.dependency_injection import configure_dependencies, resolve
from ..shared.types import LotteryNumber
from ..utils.unified_logging import get_logger
from ..utils.unified_performance_engine import UnifiedPerformanceEngine, AutoPerformanceMonitor
from ..utils.cuda_optimizers import CudaOptimizer, CudaConfig
from ..utils.unified_memory_manager import UnifiedMemoryManager
from ..utils.unified_config import Config
from ..utils.data_loader import load_draw_history
from ..models.dl.tcn_model import TCNModel
from ..analysis.trend_analyzer_v2 import TrendAnalyzerV2
from ..pipeline.optimized_tcn_preprocessor import OptimizedTCNPreprocessor
from ..utils.cache_manager import UnifiedCachePathManager, CacheManager

logger = get_logger(__name__)


class TrendCorrectionEngine:
    """
    íŠ¸ë Œë“œ ë³´ì • ì—”ì§„
    
    TCN ëª¨ë¸ê³¼ TrendAnalyzerV2ë¥¼ ì™„ì „íˆ í†µí•©í•œ ì‹œê³„ì—´ íŠ¸ë Œë“œ ë³´ì • ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
    GPU ìµœì í™”, ë©”ëª¨ë¦¬ ê´€ë¦¬, ì—ëŸ¬ ì²˜ë¦¬ë¥¼ í¬í•¨í•œ ê³ ì„±ëŠ¥ ì²˜ë¦¬ê°€ íŠ¹ì§•ì…ë‹ˆë‹¤.
    """

    def __init__(self):
        """
        íŠ¸ë Œë“œ ë³´ì • ì—”ì§„ ì´ˆê¸°í™” (ì˜ì¡´ì„± ì£¼ì… ì‚¬ìš©)
        """
        self.logger = get_logger(__name__)

        # --- ì˜ì¡´ì„± í•´ê²° ---
        self.config_manager: Config = resolve(Config)
        self.config = self.config_manager.get_config("main")
        self.paths = self.config_manager.get_paths()
        self.performance_engine: UnifiedPerformanceEngine = resolve(UnifiedPerformanceEngine)
        self.performance_monitor: AutoPerformanceMonitor = resolve(AutoPerformanceMonitor)
        self.memory_manager: UnifiedMemoryManager = resolve(UnifiedMemoryManager)
        self.cuda_optimizer: CudaOptimizer = resolve(CudaOptimizer)
        self.cache_path_manager: UnifiedCachePathManager = resolve(UnifiedCachePathManager)
        # --------------------

        # GPU ìµœì í™” ì„¤ì •
        cuda_config_data = self.config.get("cuda_config", {})
        self.cuda_config = CudaConfig(
            use_amp=cuda_config_data.get("use_amp", True),
            use_cudnn_benchmark=cuda_config_data.get("use_cudnn_benchmark", True),
            use_tensorrt=cuda_config_data.get("use_tensorrt", False),
            tensorrt_precision="fp16",
        )
        self.cuda_optimizer.configure(self.cuda_config)

        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.logger.info(f"ë””ë°”ì´ìŠ¤ ì„¤ì •: {self.device}")
        
        # TCN ëª¨ë¸ ì„¤ì • ë° ì´ˆê¸°í™”
        self.tcn_config = self._prepare_tcn_config()
        tcn_cache_manager = CacheManager(
            path_manager=self.cache_path_manager,
            cache_type="tcn_model",
            config=self.tcn_config.get("cache_config")
        )
        self.tcn_model = TCNModel(config=self.tcn_config, cache_manager=tcn_cache_manager)
        
        # TrendAnalyzerV2 ì´ˆê¸°í™”
        self.trend_analyzer = TrendAnalyzerV2(self.config.get("trend_analyzer_v2", {}))
        
        # TCN ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        self.tcn_preprocessor = OptimizedTCNPreprocessor(
            self.config.get("tcn_preprocessor", {})
        )
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ì´ˆê¸°í™”
        self.feature_scaler = StandardScaler()
        self.target_scaler = MinMaxScaler()
        
        # ê²½ë¡œ ì„¤ì •
        self._setup_directories()
        
        # íŠ¸ë Œë“œ ë³´ì • ì„¤ì •
        self.trend_config = self._setup_trend_config()
        
        self.logger.info("âœ… TrendCorrectionEngine ì´ˆê¸°í™” ì™„ë£Œ")

    def _prepare_tcn_config(self) -> Dict[str, Any]:
        """TCN ëª¨ë¸ ì„¤ì • ì¤€ë¹„"""
        base_config = self.config.get("tcn_model", {})
        return {
            **base_config,
            "use_gpu": torch.cuda.is_available(),
            "tcn": {
                "input_dim": base_config.get("input_dim", 135),  # 45*3 (unified+trend+ml)
                "num_channels": base_config.get("num_channels", [64, 128, 256, 128, 64]),
                "kernel_size": base_config.get("kernel_size", 3),
                "dropout": base_config.get("dropout", 0.2),
                "sequence_length": base_config.get("sequence_length", 50),
                "output_dim": 45,  # ë¡œë˜ ë²ˆí˜¸ 45ê°œ
                "learning_rate": base_config.get("learning_rate", 0.001),
                "batch_size": base_config.get("batch_size", 32),
            }
        }

    def _setup_directories(self):
        """í•„ìš”í•œ ë””ë ‰í† ë¦¬ ì„¤ì •"""
        self.output_dir = Path(self.paths.result_dir) / "trend_correction"
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.model_dir = Path(self.paths.models_dir) / "tcn"
        self.model_dir.mkdir(parents=True, exist_ok=True)
        
        self.cache_dir = Path(self.paths.cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)

    def _setup_trend_config(self) -> Dict[str, Any]:
        """íŠ¸ë Œë“œ ë³´ì • ì„¤ì •"""
        return {
            "correction_weight": self.config.get("trend_correction", {}).get("weight", 0.3),
            "sequence_length": self.config.get("tcn_model", {}).get("sequence_length", 50),
            "prediction_horizon": self.config.get("trend_correction", {}).get("prediction_horizon", 5),
            "min_data_points": 100,  # ìµœì†Œ ë°ì´í„° í¬ì¸íŠ¸
            "trend_threshold": 0.1,  # íŠ¸ë Œë“œ ì„ê³„ê°’
            "adaptive_weight": True,  # ì ì‘ì  ê°€ì¤‘ì¹˜ ì‚¬ìš©
        }

    def load_all_previous_results(self) -> Dict[str, Any]:
        """
        ì´ì „ ë‹¨ê³„(run1, run2, run3)ì˜ ëª¨ë“  ê²°ê³¼ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
        
        Returns:
            Dict[str, Any]: í†µí•©ëœ ì´ì „ ê²°ê³¼
        """
        try:
            self.logger.info("ì´ì „ ë‹¨ê³„ ê²°ê³¼ ë¡œë“œ ì‹œì‘...")
            
            with self.performance_monitor.track("load_previous_results"):
                results = {
                    "run1_analysis": {},
                    "run2_predictions": {},
                    "run3_anomaly": {},
                    "metadata": {}
                }
                
                # run1 ë¶„ì„ ê²°ê³¼ ë¡œë“œ
                analysis_dir = self.paths.get_analysis_path()
                if analysis_dir.exists():
                    for file_path in analysis_dir.glob("*.json"):
                        try:
                            with open(file_path, 'r', encoding='utf-8') as f:
                                data = json.load(f)
                                results["run1_analysis"][file_path.stem] = data
                        except Exception as e:
                            self.logger.warning(f"íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ {file_path}: {e}")
                
                # run2 ì˜ˆì¸¡ ê²°ê³¼ ë¡œë“œ
                predictions_dir = self.paths.get_ml_predictions_path()
                if predictions_dir.exists():
                    for file_path in predictions_dir.glob("*.json"):
                        try:
                            with open(file_path, 'r', encoding='utf-8') as f:
                                data = json.load(f)
                                results["run2_predictions"][file_path.stem] = data
                        except Exception as e:
                            self.logger.warning(f"íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ {file_path}: {e}")
                
                # run3 ì´ìƒê°ì§€ ê²°ê³¼ ë¡œë“œ
                anomaly_dir = self.paths.get_anomaly_detection_path()
                if anomaly_dir.exists():
                    for file_path in anomaly_dir.glob("*.json"):
                        try:
                            with open(file_path, 'r', encoding='utf-8') as f:
                                data = json.load(f)
                                results["run3_anomaly"][file_path.stem] = data
                        except Exception as e:
                            self.logger.warning(f"íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ {file_path}: {e}")
                
                # ë©”íƒ€ë°ì´í„° ì¶”ê°€
                results["metadata"] = {
                    "load_timestamp": datetime.now().isoformat(),
                    "run1_files": len(results["run1_analysis"]),
                    "run2_files": len(results["run2_predictions"]),
                    "run3_files": len(results["run3_anomaly"]),
                }
            
            self.logger.info(f"âœ… ì´ì „ ê²°ê³¼ ë¡œë“œ ì™„ë£Œ: "
                           f"run1({len(results['run1_analysis'])}), "
                           f"run2({len(results['run2_predictions'])}), "
                           f"run3({len(results['run3_anomaly'])})")
            
            return results
            
        except Exception as e:
            self.logger.error(f"ì´ì „ ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise

    def load_historical_data(self) -> List[LotteryNumber]:
        """
        ê³¼ê±° ë¡œë˜ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
        
        Returns:
            List[LotteryNumber]: ê³¼ê±° ë¡œë˜ ë‹¹ì²¨ ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸
        """
        try:
            self.logger.info("ê³¼ê±° ë¡œë˜ ë°ì´í„° ë¡œë“œ ì‹œì‘...")
            
            with self.performance_monitor.track("load_historical_data"):
                # ì„¤ì •ì—ì„œ ë°ì´í„° ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
                config = self.config_manager.get_config("main")
                data_path = config.get("data", {}).get("historical_data_path", "data/raw/lottery.csv")
                
                # ë°ì´í„° ë¡œë“œ
                historical_data = load_draw_history(data_path)
                
                if not historical_data:
                    self.logger.warning("ê³¼ê±° ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë”ë¯¸ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
                    historical_data = self._create_dummy_historical_data()
                
                # íšŒì°¨ ìˆœìœ¼ë¡œ ì •ë ¬
                historical_data.sort(key=lambda x: x.draw_no)
            
            self.logger.info(f"âœ… ê³¼ê±° ë¡œë˜ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(historical_data)}ê°œ íšŒì°¨")
            return historical_data
            
        except Exception as e:
            self.logger.error(f"ê³¼ê±° ë¡œë˜ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            # ë¹„ìƒìš© ë”ë¯¸ ë°ì´í„° ë°˜í™˜
            return self._create_dummy_historical_data()

    def _create_dummy_historical_data(self) -> List[LotteryNumber]:
        """ë”ë¯¸ ê³¼ê±° ë¡œë˜ ë°ì´í„° ìƒì„±"""
        dummy_data = []
        for i in range(1, 201):  # 200íšŒì°¨ ë”ë¯¸ ë°ì´í„°
            numbers = sorted(np.random.choice(range(1, 46), 6, replace=False).tolist())
            dummy_data.append(LotteryNumber(
                draw_no=i,
                numbers=numbers,
                date=f"2023-{(i % 12) + 1:02d}-{(i % 28) + 1:02d}"
            ))
        return dummy_data

    def prepare_time_series_data(self, all_results: Dict[str, Any]) -> Tuple[np.ndarray, List[str]]:
        """
        ì´ì „ ë‹¨ê³„ ê²°ê³¼ë“¤ì„ ì‹œê³„ì—´ ë°ì´í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        
        Args:
            all_results: ì´ì „ ë‹¨ê³„ ê²°ê³¼ë“¤
            
        Returns:
            Tuple[np.ndarray, List[str]]: ì‹œê³„ì—´ ë°ì´í„°ì™€ íŠ¹ì„±ëª… ë¦¬ìŠ¤íŠ¸
        """
        try:
            self.logger.info("ì‹œê³„ì—´ ë°ì´í„° ì¤€ë¹„ ì‹œì‘...")
            
            with self.performance_monitor.track("prepare_time_series"):
                features_list = []
                feature_names = []
                
                # run1 ë¶„ì„ ê²°ê³¼ì—ì„œ íŠ¹ì„± ì¶”ì¶œ
                for name, data in all_results["run1_analysis"].items():
                    if "unified_analysis" in name and "comprehensive_scores" in data:
                        scores = data["comprehensive_scores"]
                        for num in range(1, 46):
                            features_list.append(scores.get(str(num), 0.0))
                            feature_names.append(f"unified_score_{num}")
                    
                    elif "trend_analyzer_v2_results" in name and "comprehensive_trend_score" in data:
                        trend_scores = data["comprehensive_trend_score"]
                        for num in range(1, 46):
                            features_list.append(trend_scores.get(str(num), 0.0))
                            feature_names.append(f"trend_score_{num}")
                
                # run2 ì˜ˆì¸¡ ê²°ê³¼ì—ì„œ íŠ¹ì„± ì¶”ì¶œ
                for name, data in all_results["run2_predictions"].items():
                    if "ml_predictions" in name and "predictions" in data:
                        predictions = data["predictions"]
                        for num in range(1, 46):
                            features_list.append(predictions.get(str(num), 0.0))
                            feature_names.append(f"ml_prediction_{num}")
                
                # run3 ì´ìƒê°ì§€ ê²°ê³¼ì—ì„œ íŠ¹ì„± ì¶”ì¶œ
                for name, data in all_results["run3_anomaly"].items():
                    if "anomaly_detection" in name and "anomaly_scores" in data:
                        anomaly_scores = data["anomaly_scores"]
                        for num in range(1, 46):
                            features_list.append(anomaly_scores.get(str(num), 0.0))
                            feature_names.append(f"anomaly_score_{num}")
                
                # íŠ¹ì„±ì´ ì—†ëŠ” ê²½ìš° ë”ë¯¸ ë°ì´í„° ìƒì„±
                if not features_list:
                    self.logger.warning("ì´ì „ ê²°ê³¼ê°€ ì—†ì–´ ë”ë¯¸ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
                    features_list = np.random.random(135).tolist()  # 45*3
                    feature_names = [f"dummy_feature_{i}" for i in range(135)]
                
                # ì‹œê³„ì—´ ë°ì´í„° êµ¬ì„±
                features_array = np.array(features_list)
                if features_array.ndim == 1:
                    # 1ì°¨ì› ë°°ì—´ì„ 2ì°¨ì›ìœ¼ë¡œ ë³€í™˜ (1 x features)
                    time_series_data = features_array.reshape(1, -1)
                else:
                    time_series_data = features_array
                
                # ë°ì´í„° ì •ê·œí™”
                if time_series_data.shape[0] > 1:
                    time_series_data = self.feature_scaler.fit_transform(time_series_data)
                else:
                    # ë‹¨ì¼ ìƒ˜í”Œì¸ ê²½ìš° ì •ê·œí™” ìŠ¤í‚µ
                    pass
                
                self.logger.info(f"âœ… ì‹œê³„ì—´ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {time_series_data.shape}")
                return time_series_data, feature_names
                
        except Exception as e:
            self.logger.error(f"ì‹œê³„ì—´ ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            # ë¹„ìƒìš© ë”ë¯¸ ë°ì´í„° ë°˜í™˜
            dummy_data = np.random.random((10, 135))
            dummy_names = [f"feature_{i}" for i in range(135)]
            return dummy_data, dummy_names

    def analyze_trends_v2(self, historical_data: List[LotteryNumber]) -> Dict[str, Any]:
        """
        TrendAnalyzerV2ë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¸ë Œë“œë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
        
        Args:
            historical_data: ê³¼ê±° ë¡œë˜ ë°ì´í„°
            
        Returns:
            Dict[str, Any]: íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼
        """
        try:
            self.logger.info("TrendAnalyzerV2 íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘...")
            
            with self.performance_monitor.track("trend_analysis_v2"):
                if not historical_data:
                    self.logger.warning("ê³¼ê±° ë°ì´í„°ê°€ ì—†ì–´ ë¹ˆ íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
                    return {"error": "no_historical_data"}
                
                trend_results = self.trend_analyzer.analyze(historical_data)
            
            self.logger.info("âœ… TrendAnalyzerV2 ë¶„ì„ ì™„ë£Œ")
            return trend_results
            
        except Exception as e:
            self.logger.error(f"TrendAnalyzerV2 ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    def create_tcn_sequences(self, time_series_data: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        TCNì„ ìœ„í•œ ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            time_series_data: ì‹œê³„ì—´ ë°ì´í„°
            
        Returns:
            Tuple[np.ndarray, np.ndarray]: (X_sequences, y_targets)
        """
        try:
            sequence_length = self.trend_config["sequence_length"]
            
            if len(time_series_data) < sequence_length:
                self.logger.warning(f"ë°ì´í„° ê¸¸ì´({len(time_series_data)})ê°€ ì‹œí€€ìŠ¤ ê¸¸ì´({sequence_length})ë³´ë‹¤ ì‘ìŠµë‹ˆë‹¤.")
                # íŒ¨ë”©ìœ¼ë¡œ ë°ì´í„° í™•ì¥
                padded_data = np.pad(
                    time_series_data,
                    ((sequence_length - len(time_series_data), 0), (0, 0)),
                    mode='edge'
                )
                time_series_data = padded_data
            
            X_sequences = []
            y_targets = []
            
            for i in range(len(time_series_data) - sequence_length):
                # ì…ë ¥ ì‹œí€€ìŠ¤
                X_sequences.append(time_series_data[i:i+sequence_length])
                
                # íƒ€ê²Ÿ: ë‹¤ìŒ ì‹œì ì˜ ì²« 45ê°œ íŠ¹ì„± (ë¡œë˜ ë²ˆí˜¸ ì ìˆ˜)
                y_targets.append(time_series_data[i+sequence_length, :45])
            
            if not X_sequences:
                # ì‹œí€€ìŠ¤ê°€ ìƒì„±ë˜ì§€ ì•Šì€ ê²½ìš° ë”ë¯¸ ë°ì´í„°
                X_sequences = [time_series_data[-sequence_length:]]
                y_targets = [np.random.random(45)]
            
            return np.array(X_sequences), np.array(y_targets)
            
        except Exception as e:
            self.logger.error(f"TCN ì‹œí€€ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            # ë¹„ìƒìš© ë”ë¯¸ ì‹œí€€ìŠ¤
            dummy_X = np.random.random((1, sequence_length, time_series_data.shape[1]))
            dummy_y = np.random.random((1, 45))
            return dummy_X, dummy_y

    def train_tcn_model(self, X_sequences: np.ndarray, y_targets: np.ndarray) -> Dict[str, Any]:
        """
        TCN ëª¨ë¸ì„ í›ˆë ¨í•©ë‹ˆë‹¤.
        
        Args:
            X_sequences: ì…ë ¥ ì‹œí€€ìŠ¤
            y_targets: íƒ€ê²Ÿ ë°ì´í„°
            
        Returns:
            Dict[str, Any]: í›ˆë ¨ ê²°ê³¼
        """
        try:
            self.logger.info("TCN ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
            
            training_results = {}
            
            if self.cuda_optimizer.is_cuda_available():
                with self.cuda_optimizer.gpu_memory_scope(size_mb=1024, device_id=0):
                    with self.performance_monitor.track("tcn_training"):
                        training_results = self._execute_tcn_training(X_sequences, y_targets)
            else:
                with self.performance_monitor.track("tcn_training"):
                    training_results = self._execute_tcn_training(X_sequences, y_targets)
            
            # ëª¨ë¸ ì €ì¥
            model_path = self.model_dir / "trend_correction_tcn.pt"
            save_success = self.tcn_model.save(str(model_path))
            training_results["model_saved"] = save_success
            training_results["model_path"] = str(model_path)
            
            self.logger.info("âœ… TCN ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ")
            return training_results
            
        except Exception as e:
            self.logger.error(f"TCN ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨: {e}")
            return {"error": str(e), "training_completed": False}

    def _execute_tcn_training(self, X_sequences: np.ndarray, y_targets: np.ndarray) -> Dict[str, Any]:
        """TCN í›ˆë ¨ ì‹¤í–‰"""
        try:
            # TCN ì „ì²˜ë¦¬ ì ìš©
            X_processed, preprocess_metadata = self.tcn_preprocessor.preprocess_for_tcn(X_sequences)
            
            # íƒ€ê²Ÿ ë°ì´í„° ì •ê·œí™”
            y_normalized = self.target_scaler.fit_transform(y_targets)
            
            # TCN ëª¨ë¸ í›ˆë ¨
            training_results = self.tcn_model.fit(X_processed, y_normalized)
            
            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            training_results["preprocess_metadata"] = preprocess_metadata
            training_results["target_scaler_params"] = {
                "scale_": self.target_scaler.scale_.tolist() if hasattr(self.target_scaler, 'scale_') else None,
                "min_": self.target_scaler.min_.tolist() if hasattr(self.target_scaler, 'min_') else None,
            }
            
            return training_results
            
        except Exception as e:
            self.logger.error(f"TCN í›ˆë ¨ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    def predict_trend_corrections(self, recent_data: np.ndarray) -> np.ndarray:
        """
        ìµœê·¼ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ íŠ¸ë Œë“œ ë³´ì •ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
        
        Args:
            recent_data: ìµœê·¼ ì‹œê³„ì—´ ë°ì´í„°
            
        Returns:
            np.ndarray: íŠ¸ë Œë“œ ë³´ì • ì˜ˆì¸¡ê°’
        """
        try:
            self.logger.info("íŠ¸ë Œë“œ ë³´ì • ì˜ˆì¸¡ ì‹œì‘...")
            
            with self.performance_monitor.track("trend_prediction"):
                # ì‹œí€€ìŠ¤ ê¸¸ì´ì— ë§ê²Œ ë°ì´í„° ì¤€ë¹„
                sequence_length = self.trend_config["sequence_length"]
                
                if len(recent_data) < sequence_length:
                    # íŒ¨ë”©ìœ¼ë¡œ ì‹œí€€ìŠ¤ ê¸¸ì´ ë§ì¶¤
                    padded_data = np.pad(
                        recent_data,
                        ((sequence_length - len(recent_data), 0), (0, 0)),
                        mode='edge'
                    )
                    prediction_input = padded_data[-sequence_length:].reshape(1, sequence_length, -1)
                else:
                    prediction_input = recent_data[-sequence_length:].reshape(1, sequence_length, -1)
                
                # TCN ì „ì²˜ë¦¬ ì ìš©
                X_processed = self.tcn_preprocessor.transform_new_data(prediction_input)
                
                # TCN ì˜ˆì¸¡
                normalized_predictions = self.tcn_model.predict(X_processed)
                
                # ì •ê·œí™” í•´ì œ
                if hasattr(self.target_scaler, 'scale_') and normalized_predictions.ndim == 2:
                    trend_corrections = self.target_scaler.inverse_transform(normalized_predictions)
                else:
                    trend_corrections = normalized_predictions
                
                # 1ì°¨ì›ìœ¼ë¡œ ë³€í™˜
                if trend_corrections.ndim > 1:
                    trend_corrections = trend_corrections.flatten()[:45]  # 45ê°œ ë²ˆí˜¸ë§Œ
                
                # ë²”ìœ„ ì¡°ì • (-1 ~ 1)
                trend_corrections = np.clip(trend_corrections, -1, 1)
            
            self.logger.info("âœ… íŠ¸ë Œë“œ ë³´ì • ì˜ˆì¸¡ ì™„ë£Œ")
            return trend_corrections
            
        except Exception as e:
            self.logger.error(f"íŠ¸ë Œë“œ ë³´ì • ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            # ë¹„ìƒìš© ì¤‘ë¦½ ë³´ì •ê°’ ë°˜í™˜
            return np.zeros(45)

    def apply_trend_corrections(self, base_scores: np.ndarray, trend_corrections: np.ndarray) -> np.ndarray:
        """
        ê¸°ë³¸ ì ìˆ˜ì— íŠ¸ë Œë“œ ë³´ì •ì„ ì ìš©í•©ë‹ˆë‹¤.
        
        Args:
            base_scores: ê¸°ë³¸ ì ìˆ˜ (45ê°œ ë²ˆí˜¸)
            trend_corrections: íŠ¸ë Œë“œ ë³´ì •ê°’ (45ê°œ ë²ˆí˜¸)
            
        Returns:
            np.ndarray: ë³´ì •ëœ ì ìˆ˜
        """
        try:
            self.logger.info("íŠ¸ë Œë“œ ë³´ì • ì ìš© ì‹œì‘...")
            
            # ì ì‘ì  ë³´ì • ê°€ì¤‘ì¹˜ ê³„ì‚°
            if self.trend_config["adaptive_weight"]:
                correction_weight = self._calculate_adaptive_weight(trend_corrections)
            else:
                correction_weight = self.trend_config["correction_weight"]
            
            # íŠ¸ë Œë“œ ë³´ì • ì ìš©
            corrected_scores = base_scores + (trend_corrections * correction_weight)
            
            # ì ìˆ˜ ì •ê·œí™” (0-1 ë²”ìœ„)
            corrected_scores = np.clip(corrected_scores, 0, 1)
            
            # í™•ë¥  ë¶„í¬ë¡œ ì •ê·œí™”
            score_sum = np.sum(corrected_scores)
            if score_sum > 0:
                corrected_scores = corrected_scores / score_sum
            else:
                # ëª¨ë“  ì ìˆ˜ê°€ 0ì¸ ê²½ìš° ê· ë“± ë¶„í¬
                corrected_scores = np.ones(45) / 45
            
            self.logger.info(f"âœ… íŠ¸ë Œë“œ ë³´ì • ì ìš© ì™„ë£Œ (ë³´ì • ê°€ì¤‘ì¹˜: {correction_weight:.3f})")
            return corrected_scores
            
        except Exception as e:
            self.logger.error(f"íŠ¸ë Œë“œ ë³´ì • ì ìš© ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ì ìˆ˜ ë°˜í™˜
            return base_scores

    def _calculate_adaptive_weight(self, trend_corrections: np.ndarray) -> float:
        """ì ì‘ì  ë³´ì • ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        try:
            # íŠ¸ë Œë“œ ê°•ë„ ì¸¡ì •
            trend_strength = np.std(trend_corrections)
            
            # ê¸°ë³¸ ê°€ì¤‘ì¹˜
            base_weight = self.trend_config["correction_weight"]
            
            # ê°•ë„ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ ì¡°ì •
            if trend_strength > 0.5:
                adaptive_weight = base_weight * 1.5  # ê°•í•œ íŠ¸ë Œë“œ
            elif trend_strength > 0.2:
                adaptive_weight = base_weight * 1.0  # ë³´í†µ íŠ¸ë Œë“œ
            else:
                adaptive_weight = base_weight * 0.5  # ì•½í•œ íŠ¸ë Œë“œ
            
            # ë²”ìœ„ ì œí•œ
            adaptive_weight = np.clip(adaptive_weight, 0.1, 0.8)
            
            return adaptive_weight
            
        except Exception:
            return self.trend_config["correction_weight"]

    def generate_base_scores(self, all_results: Dict[str, Any]) -> np.ndarray:
        """ì´ì „ ë‹¨ê³„ ê²°ê³¼ë¡œë¶€í„° ê¸°ë³¸ ì ìˆ˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            base_scores = np.zeros(45)
            weight_sum = 0
            
            # run1 í†µí•© ë¶„ì„ ì ìˆ˜ (ê°€ì¤‘ì¹˜: 0.4)
            for name, data in all_results["run1_analysis"].items():
                if "unified_analysis" in name and "comprehensive_scores" in data:
                    scores = data["comprehensive_scores"]
                    for i in range(45):
                        base_scores[i] += scores.get(str(i+1), 0.0) * 0.4
                    weight_sum += 0.4
                    break
            
            # run2 ML ì˜ˆì¸¡ ì ìˆ˜ (ê°€ì¤‘ì¹˜: 0.4)
            for name, data in all_results["run2_predictions"].items():
                if "ml_predictions" in name and "predictions" in data:
                    predictions = data["predictions"]
                    for i in range(45):
                        base_scores[i] += predictions.get(str(i+1), 0.0) * 0.4
                    weight_sum += 0.4
                    break
            
            # run3 ì´ìƒê°ì§€ ì ìˆ˜ (ê°€ì¤‘ì¹˜: 0.2)
            for name, data in all_results["run3_anomaly"].items():
                if "anomaly_detection" in name and "anomaly_scores" in data:
                    anomaly_scores = data["anomaly_scores"]
                    for i in range(45):
                        # ì´ìƒ ì ìˆ˜ê°€ ë‚®ì„ìˆ˜ë¡ ì •ìƒì´ë¯€ë¡œ ì—­ìˆ˜ ì‚¬ìš©
                        anomaly_score = anomaly_scores.get(str(i+1), 0.5)
                        base_scores[i] += (1.0 - anomaly_score) * 0.2
                    weight_sum += 0.2
                    break
            
            # ê°€ì¤‘ì¹˜ ì •ê·œí™”
            if weight_sum > 0:
                base_scores = base_scores / weight_sum
            else:
                # ê¸°ë³¸ ì ìˆ˜ê°€ ì—†ëŠ” ê²½ìš° ê· ë“± ë¶„í¬
                base_scores = np.ones(45) / 45
            
            # í™•ë¥  ë¶„í¬ë¡œ ì •ê·œí™”
            base_scores = np.clip(base_scores, 0, 1)
            score_sum = np.sum(base_scores)
            if score_sum > 0:
                base_scores = base_scores / score_sum
            
            return base_scores
            
        except Exception as e:
            self.logger.error(f"ê¸°ë³¸ ì ìˆ˜ ìƒì„± ì‹¤íŒ¨: {e}")
            return np.ones(45) / 45  # ê· ë“± ë¶„í¬

    def save_trend_results(self, results: Dict[str, Any]) -> str:
        """
        íŠ¸ë Œë“œ ë³´ì • ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
        
        Args:
            results: ì €ì¥í•  ê²°ê³¼
            
        Returns:
            str: ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"trend_correction_results_{timestamp}.json"
            filepath = self.output_dir / filename
            
            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            results["metadata"] = {
                "generated_at": datetime.now().isoformat(),
                "version": "TrendCorrectionEngine_v1.0",
                "config": self.config,
                "trend_config": self.trend_config,
                "device": str(self.device),
                "performance_stats": self.performance_monitor.get_performance_summary(),
            }
            
            # GPU ë©”ëª¨ë¦¬ í†µê³„ ì¶”ê°€
            if self.cuda_optimizer.is_cuda_available():
                results["metadata"]["gpu_memory_stats"] = self.cuda_optimizer.get_gpu_memory_pool().get_stats()
            
            # ê²°ê³¼ ì €ì¥
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"âœ… íŠ¸ë Œë“œ ë³´ì • ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {filepath}")
            return str(filepath)
            
        except Exception as e:
            self.logger.error(f"íŠ¸ë Œë“œ ë³´ì • ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise

    def run_full_trend_correction(self) -> Dict[str, Any]:
        """
        ì „ì²´ íŠ¸ë Œë“œ ë³´ì • í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
        
        Returns:
            Dict[str, Any]: íŠ¸ë Œë“œ ë³´ì • ê²°ê³¼
        """
        try:
            self.logger.info("ğŸš€ íŠ¸ë Œë“œ ë³´ì • ì‹œìŠ¤í…œ ì‹œì‘")
            start_time = time.time()
            
            # 1. ì´ì „ ë‹¨ê³„ ê²°ê³¼ ë¡œë“œ
            all_results = self.load_all_previous_results()
            
            # 2. ê³¼ê±° ë¡œë˜ ë°ì´í„° ë¡œë“œ
            historical_data = self.load_historical_data()
            
            # 3. ì‹œê³„ì—´ ë°ì´í„° ì¤€ë¹„
            time_series_data, feature_names = self.prepare_time_series_data(all_results)
            
            # 4. TrendAnalyzerV2 íŠ¸ë Œë“œ ë¶„ì„
            trend_analysis = self.analyze_trends_v2(historical_data)
            
            # 5. TCN ì‹œí€€ìŠ¤ ìƒì„±
            X_sequences, y_targets = self.create_tcn_sequences(time_series_data)
            
            # 6. TCN ëª¨ë¸ í›ˆë ¨
            training_results = self.train_tcn_model(X_sequences, y_targets)
            
            # 7. íŠ¸ë Œë“œ ë³´ì • ì˜ˆì¸¡
            trend_corrections = self.predict_trend_corrections(time_series_data)
            
            # 8. ê¸°ë³¸ ì ìˆ˜ ìƒì„±
            base_scores = self.generate_base_scores(all_results)
            
            # 9. íŠ¸ë Œë“œ ë³´ì • ì ìš©
            corrected_scores = self.apply_trend_corrections(base_scores, trend_corrections)
            
            # 10. ê²°ê³¼ í†µí•©
            results = {
                "trend_correction_scores": {
                    str(i+1): float(corrected_scores[i]) for i in range(45)
                },
                "base_scores": {
                    str(i+1): float(base_scores[i]) for i in range(45)
                },
                "trend_corrections": {
                    str(i+1): float(trend_corrections[i]) for i in range(45)
                },
                "trend_analysis": trend_analysis,
                "training_results": training_results,
                "feature_names": feature_names,
                "data_summary": {
                    "time_series_shape": time_series_data.shape,
                    "sequence_count": len(X_sequences),
                    "historical_data_count": len(historical_data),
                },
                "correction_config": self.trend_config,
            }
            
            # 11. ê²°ê³¼ ì €ì¥
            saved_path = self.save_trend_results(results)
            results["saved_path"] = saved_path
            
            # 12. ì„±ëŠ¥ í†µê³„
            end_time = time.time()
            execution_time = end_time - start_time
            
            performance_summary = {
                "execution_time_seconds": execution_time,
                "performance_monitor": self.performance_monitor.get_performance_summary(),
                "memory_manager_stats": self.memory_manager.get_stats(),
            }
            
            if self.cuda_optimizer.is_cuda_available():
                performance_summary["gpu_memory_stats"] = self.cuda_optimizer.get_gpu_memory_pool().get_stats()
            
            results["performance_summary"] = performance_summary
            
            self.logger.info(f"âœ… íŠ¸ë Œë“œ ë³´ì • ì‹œìŠ¤í…œ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {execution_time:.2f}ì´ˆ)")
            
            return results
            
        except Exception as e:
            self.logger.error(f"íŠ¸ë Œë“œ ë³´ì • ì‹œìŠ¤í…œ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            raise


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # 1. ì˜ì¡´ì„± ì„¤ì •
    configure_dependencies()

    logger.info("=" * 80)
    logger.info("ğŸš€ 4ë‹¨ê³„: íŠ¸ë Œë“œ ë³´ì • ì‹œìŠ¤í…œ ì‹œì‘")
    logger.info("=" * 80)
    
    start_time = time.time()

    try:
        # TrendCorrectionEngine ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì„¤ì • ì œê±°)
        engine = TrendCorrectionEngine()
        
        # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        final_results = engine.run_full_trend_correction()

        total_time = time.time() - start_time
        logger.info(f"âœ… íŠ¸ë Œë“œ ë³´ì • ì™„ë£Œ! ì´ ì‹¤í–‰ ì‹œê°„: {total_time:.2f}ì´ˆ")
        logger.info(f"ğŸ“ ìµœì¢… ê²°ê³¼ íŒŒì¼: {final_results.get('saved_path')}")

    except Exception as e:
        logger.error(f"âŒ íŠ¸ë Œë“œ ë³´ì • ì‹¤íŒ¨: {e}", exc_info=True)
        return 1

    return 0


if __name__ == "__main__":
    main()
