"""
DAEBAK AI ë¡œë˜ ì‹œìŠ¤í…œ - ì´ìƒê°ì§€ ë‹¨ê³„ (Phase 3)

ì´ ëª¨ë“ˆì€ Phase 1(ë°ì´í„° ë¶„ì„)ê³¼ Phase 2(ML ì˜ˆì¸¡)ì˜ ê²°ê³¼ë¥¼ ì…ë ¥ë°›ì•„
AutoEncoder ëª¨ë¸ì„ ì‚¬ìš©í•´ ì´ìƒ íŒ¨í„´ì„ ê°ì§€í•˜ê³  ê°ì í•˜ëŠ” ì´ìƒê°ì§€ íŒŒì´í”„ë¼ì¸ì„ êµ¬í˜„í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- Phase 1, 2 ê²°ê³¼ ë¡œë“œ ë° í†µí•©
- AutoEncoder ëª¨ë¸ GPU ìµœì í™” í•™ìŠµ
- ì •ìƒ íŒ¨í„´ í•™ìŠµ ë° ì´ìƒì¹˜ ê°ì§€
- ì¬êµ¬ì„± ì˜¤ë¥˜ ê¸°ë°˜ ê°ì  ì‹œìŠ¤í…œ
- ë™ì  ì„ê³„ê°’ ì¡°ì • ë° ì„±ëŠ¥ ìµœì í™”
"""

# 1. í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import json
import time
import argparse
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
from dataclasses import dataclass, asdict
import warnings

# 2. ì„œë“œíŒŒí‹°
import numpy as np
import torch
from sklearn.preprocessing import StandardScaler

# 3. í”„ë¡œì íŠ¸ ë‚´ë¶€ (ë¦¬íŒ©í† ë§ëœ ì˜ì¡´ì„± ê´€ë¦¬)
from ..utils.dependency_injection import configure_dependencies, resolve
from ..utils.unified_logging import get_logger
from ..utils.data_loader import DataLoader
from ..utils.unified_performance_engine import UnifiedPerformanceEngine, AutoPerformanceMonitor
from ..utils.cuda_optimizers import CudaOptimizer, CudaConfig
from ..utils.unified_config import Config
from ..models.dl.autoencoder_model import AutoencoderModel
from ..pipeline.optimized_autoencoder_preprocessor import OptimizedAutoEncoderPreprocessor
from ..utils.unified_memory_manager import UnifiedMemoryManager
from ..utils.auto_recovery_system import AutoRecoverySystem

# ê²½ê³  ë©”ì‹œì§€ í•„í„°ë§
warnings.filterwarnings("ignore", category=UserWarning)


@dataclass
class AnomalyDetectionConfig:
    """ì´ìƒê°ì§€ ì„¤ì •"""

    # ì…ë ¥ ê²½ë¡œ
    analysis_result_dir: str = "data/result/analysis"
    ml_predictions_dir: str = "data/result/ml_predictions"
    cache_dir: str = "data/cache"

    # ì¶œë ¥ ê²½ë¡œ
    output_dir: str = "data/result/anomaly_detection"
    model_save_dir: str = "data/models/autoencoder"

    # ëª¨ë¸ ì„¤ì •
    use_gpu: bool = True
    latent_dim: int = 32
    hidden_dims: List[int] = None
    learning_rate: float = 0.001
    batch_size: int = 256
    epochs: int = 100

    # ì´ìƒê°ì§€ ì„¤ì •
    zscore_threshold: float = 2.5
    anomaly_penalty_factor: float = 0.3
    min_penalty: float = 0.1
    max_penalty: float = 0.8

    # ì„±ëŠ¥ ì„¤ì •
    max_memory_usage_mb: int = 2048
    target_execution_time_minutes: int = 5
    early_stopping_patience: int = 10

    # ê²€ì¦ ì„¤ì •
    validation_split: float = 0.2
    contamination_rate: float = 0.1

    def __post_init__(self):
        if self.hidden_dims is None:
            self.hidden_dims = [128, 64]


class AnomalyDetectionEngine:
    """ì´ìƒê°ì§€ ì—”ì§„"""

    def __init__(self, cli_config: Optional[Dict[str, Any]] = None):
        """
        ì´ìƒê°ì§€ ì—”ì§„ ì´ˆê¸°í™” (ì˜ì¡´ì„± ì£¼ì… ì‚¬ìš©)
        """
        # ë¡œê±° ì´ˆê¸°í™”
        self.logger = get_logger(__name__)

        # --- ì˜ì¡´ì„± í•´ê²° ---
        self.config_manager: Config = resolve(Config)
        self.system_config = self.config_manager.get_config("main")
        self.performance_engine: UnifiedPerformanceEngine = resolve(UnifiedPerformanceEngine)
        self.performance_monitor: AutoPerformanceMonitor = resolve(AutoPerformanceMonitor)
        self.data_loader: DataLoader = resolve(DataLoader)
        self.memory_manager: UnifiedMemoryManager = resolve(UnifiedMemoryManager)
        self.error_handler: AutoRecoverySystem = resolve(AutoRecoverySystem)
        # --------------------

        # ì„¤ì • ë¡œë“œ ë° ë³‘í•©
        self.config = AnomalyDetectionConfig()
        if cli_config:
            for key, value in cli_config.items():
                if hasattr(self.config, key):
                    setattr(self.config, key, value)

        # ê²½ë¡œ ì„¤ì •
        self.analysis_result_dir = Path(self.config.analysis_result_dir)
        self.ml_predictions_dir = Path(self.config.ml_predictions_dir)
        self.cache_dir = Path(self.config.cache_dir)
        self.output_dir = Path(self.config.output_dir)
        self.model_save_dir = Path(self.config.model_save_dir)

        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.model_save_dir.mkdir(parents=True, exist_ok=True)

        # GPU ìµœì í™” ì„¤ì •
        self.device = torch.device(
            "cuda" if self.config.use_gpu and torch.cuda.is_available() else "cpu"
        )

        if self.config.use_gpu and torch.cuda.is_available():
            cuda_config_data = self.system_config.get("cuda_config", {})
            cuda_config = CudaConfig(
                use_amp=cuda_config_data.get("use_amp", True),
                use_tensorrt=cuda_config_data.get("use_tensorrt", False),
                use_cudnn_benchmark=cuda_config_data.get("use_cudnn_benchmark", True),
            )
            self.cuda_optimizer: Optional[CudaOptimizer] = resolve(CudaOptimizer)
            if self.cuda_optimizer:
                self.cuda_optimizer.configure(cuda_config)
        else:
            self.cuda_optimizer = None

        # ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        preprocessor_config = {
            "input_dim": 200,  # ê¸°ë³¸ê°’, ì‹¤ì œ ë°ì´í„°ì— ë”°ë¼ ì¡°ì •
            "latent_dim": self.config.latent_dim,
            "hidden_dims": self.config.hidden_dims,
            "learning_rate": self.config.learning_rate,
            "batch_size": self.config.batch_size,
            "epochs": self.config.epochs,
            "use_gpu": self.config.use_gpu,
        }
        self.preprocessor = OptimizedAutoEncoderPreprocessor(preprocessor_config)

        # AutoEncoder ëª¨ë¸ ì„¤ì •
        autoencoder_config = {
            "use_gpu": self.config.use_gpu,
            "autoencoder": {
                "input_dim": 200,  # ì‹¤ì œ ë°ì´í„°ì— ë”°ë¼ ì¡°ì •
                "hidden_dims": self.config.hidden_dims,
                "latent_dim": self.config.latent_dim,
                "learning_rate": self.config.learning_rate,
                "dropout_rate": 0.2,
                "batch_norm": True,
                "zscore_threshold": self.config.zscore_threshold,
            },
        }
        self.autoencoder_model = AutoencoderModel(autoencoder_config)

        # ìŠ¤ì¼€ì¼ëŸ¬
        self.scaler = StandardScaler()

        # ì‹¤í–‰ í†µê³„
        self.execution_stats = {
            "start_time": None,
            "end_time": None,
            "total_time": 0,
            "data_loading_time": 0,
            "preprocessing_time": 0,
            "training_time": 0,
            "detection_time": 0,
            "memory_usage": {},
            "gpu_usage": {},
            "error_count": 0,
            "warnings": [],
        }

        # ì´ìƒê°ì§€ í†µê³„
        self.anomaly_stats = {
            "total_samples": 0,
            "anomaly_count": 0,
            "anomaly_rate": 0.0,
            "mean_reconstruction_error": 0.0,
            "std_reconstruction_error": 0.0,
            "threshold_value": 0.0,
            "penalty_applied": 0,
        }

        self.logger.info("âœ… ì´ìƒê°ì§€ ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info(f"ğŸ“ ë¶„ì„ ê²°ê³¼ ë””ë ‰í† ë¦¬: {self.analysis_result_dir}")
        self.logger.info(f"ğŸ“ ML ì˜ˆì¸¡ ë””ë ‰í† ë¦¬: {self.ml_predictions_dir}")
        self.logger.info(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {self.output_dir}")
        self.logger.info(
            f"ğŸ¯ GPU ì‚¬ìš©: {'í™œì„±í™”' if self.config.use_gpu else 'ë¹„í™œì„±í™”'}"
        )
        self.logger.info(f"ğŸ§  ì ì¬ ì°¨ì›: {self.config.latent_dim}")
        self.logger.info(f"ğŸ“Š Z-Score ì„ê³„ê°’: {self.config.zscore_threshold}")

    def load_previous_results(self) -> Dict[str, Any]:
        """
        ì´ì „ ë‹¨ê³„ ê²°ê³¼ ë¡œë“œ (run1, run2)

        Returns:
            í†µí•©ëœ ì´ì „ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        self.logger.info("ğŸ“Š ì´ì „ ë‹¨ê³„ ê²°ê³¼ ë¡œë“œ ì‹œì‘...")

        with self.performance_monitor.track("load_previous_results"):
            start_time = time.time()

            try:
                # Phase 1 ê²°ê³¼ ë¡œë“œ
                unified_analysis_files = list(
                    self.analysis_result_dir.glob("unified_analysis_*.json")
                )
                if not unified_analysis_files:
                    raise FileNotFoundError(
                        f"Phase 1 ë¶„ì„ ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.analysis_result_dir}"
                    )

                latest_analysis_file = max(
                    unified_analysis_files, key=lambda x: x.stat().st_mtime
                )
                self.logger.info(f"ğŸ“„ ìµœì‹  ë¶„ì„ ê²°ê³¼ íŒŒì¼: {latest_analysis_file}")

                with open(latest_analysis_file, "r", encoding="utf-8") as f:
                    analysis_results = json.load(f)

                # Phase 2 ê²°ê³¼ ë¡œë“œ
                ml_prediction_files = list(
                    self.ml_predictions_dir.glob("ml_predictions_*.json")
                )
                if not ml_prediction_files:
                    raise FileNotFoundError(
                        f"Phase 2 ML ì˜ˆì¸¡ ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.ml_predictions_dir}"
                    )

                latest_ml_file = max(
                    ml_prediction_files, key=lambda x: x.stat().st_mtime
                )
                self.logger.info(f"ğŸ“„ ìµœì‹  ML ì˜ˆì¸¡ íŒŒì¼: {latest_ml_file}")

                with open(latest_ml_file, "r", encoding="utf-8") as f:
                    ml_results = json.load(f)

                # íŠ¹ì„± ë²¡í„° ë¡œë“œ
                feature_vectors = None
                vector_file = self.cache_dir / "optimized_feature_vector.npy"
                if vector_file.exists():
                    feature_vectors = np.load(vector_file)
                    self.logger.info(f"ğŸ”¢ íŠ¹ì„± ë²¡í„° ë¡œë“œ: {feature_vectors.shape}")

                # íŒ¨í„´ ì ìˆ˜ ë¡œë“œ
                pattern_scores = None
                score_files = list(self.ml_predictions_dir.glob("pattern_scores_*.npy"))
                if score_files:
                    latest_score_file = max(
                        score_files, key=lambda x: x.stat().st_mtime
                    )
                    pattern_scores = np.load(latest_score_file)
                    self.logger.info(f"ğŸ¯ íŒ¨í„´ ì ìˆ˜ ë¡œë“œ: {pattern_scores.shape}")

                # ì¶”ê°€ íŠ¹ì„± ë¡œë“œ
                additional_features = {}

                # ê·¸ë˜í”„ ë„¤íŠ¸ì›Œí¬ íŠ¹ì„±
                graph_vector_file = self.cache_dir / "graph_network_features.npy"
                if graph_vector_file.exists():
                    additional_features["graph_features"] = np.load(graph_vector_file)
                    self.logger.info(
                        f"ğŸ•¸ï¸ ê·¸ë˜í”„ íŠ¹ì„± ë¡œë“œ: {additional_features['graph_features'].shape}"
                    )

                # ë©”íƒ€ íŠ¹ì„±
                meta_vector_file = self.cache_dir / "meta_features.npy"
                if meta_vector_file.exists():
                    additional_features["meta_features"] = np.load(meta_vector_file)
                    self.logger.info(
                        f"ğŸ§  ë©”íƒ€ íŠ¹ì„± ë¡œë“œ: {additional_features['meta_features'].shape}"
                    )

                # ê²°ê³¼ í†µí•©
                combined_results = {
                    "analysis_results": analysis_results,
                    "ml_results": ml_results,
                    "feature_vectors": feature_vectors,
                    "pattern_scores": pattern_scores,
                    "additional_features": additional_features,
                    "load_time": time.time() - start_time,
                    "source_files": {
                        "analysis": str(latest_analysis_file),
                        "ml_predictions": str(latest_ml_file),
                        "feature_vectors": (
                            str(vector_file) if vector_file.exists() else None
                        ),
                        "pattern_scores": (
                            str(latest_score_file) if score_files else None
                        ),
                    },
                }

                self.execution_stats["data_loading_time"] = time.time() - start_time
                self.logger.info(
                    f"âœ… ì´ì „ ë‹¨ê³„ ê²°ê³¼ ë¡œë“œ ì™„ë£Œ ({self.execution_stats['data_loading_time']:.2f}ì´ˆ)"
                )

                return combined_results

            except Exception as e:
                self.execution_stats["error_count"] += 1
                self.logger.error(f"âŒ ì´ì „ ë‹¨ê³„ ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
                raise

    def prepare_autoencoder_data(
        self, all_results: Dict[str, Any]
    ) -> Tuple[np.ndarray, List[str]]:
        """
        AutoEncoder í•™ìŠµìš© ë°ì´í„° ì¤€ë¹„

        Args:
            all_results: ëª¨ë“  ì´ì „ ê²°ê³¼

        Returns:
            (X, feature_names) íŠœí”Œ - íŠ¹ì„± ë²¡í„°ì™€ íŠ¹ì„± ì´ë¦„
        """
        self.logger.info("ğŸ”„ AutoEncoder í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ì‹œì‘...")

        with self.performance_monitor.track("prepare_autoencoder_data"):
            start_time = time.time()

            try:
                # ê¸°ë³¸ íŠ¹ì„± ë²¡í„°
                X_base = all_results["feature_vectors"]
                if X_base is None:
                    raise ValueError("ê¸°ë³¸ íŠ¹ì„± ë²¡í„°ê°€ ì—†ìŠµë‹ˆë‹¤")

                # íŠ¹ì„± ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
                feature_matrices = [X_base]
                feature_names = [f"base_feature_{i}" for i in range(X_base.shape[1])]

                # ML ì˜ˆì¸¡ ì ìˆ˜ ì¶”ê°€
                if all_results["pattern_scores"] is not None:
                    pattern_scores = all_results["pattern_scores"]
                    if pattern_scores.ndim == 1:
                        pattern_scores = pattern_scores.reshape(-1, 1)

                    # ìƒ˜í”Œ ìˆ˜ ë§ì¶¤
                    if pattern_scores.shape[0] != X_base.shape[0]:
                        if pattern_scores.shape[0] > X_base.shape[0]:
                            pattern_scores = pattern_scores[: X_base.shape[0]]
                        else:
                            # íŒ¨í„´ ì ìˆ˜ê°€ ì ìœ¼ë©´ ë°˜ë³µ
                            repeat_count = (
                                X_base.shape[0] + pattern_scores.shape[0] - 1
                            ) // pattern_scores.shape[0]
                            pattern_scores = np.tile(pattern_scores, (repeat_count, 1))[
                                : X_base.shape[0]
                            ]

                    feature_matrices.append(pattern_scores)
                    feature_names.extend(
                        [f"ml_score_{i}" for i in range(pattern_scores.shape[1])]
                    )
                    self.logger.info(f"â• ML ì˜ˆì¸¡ ì ìˆ˜ ì¶”ê°€: {pattern_scores.shape}")

                # ì¶”ê°€ íŠ¹ì„±ë“¤ ê²°í•©
                additional_features = all_results.get("additional_features", {})

                for feature_name, feature_data in additional_features.items():
                    if (
                        feature_data is not None
                        and feature_data.shape[0] == X_base.shape[0]
                    ):
                        feature_matrices.append(feature_data)
                        feature_names.extend(
                            [
                                f"{feature_name}_{i}"
                                for i in range(feature_data.shape[1])
                            ]
                        )
                        self.logger.info(
                            f"â• {feature_name} ì¶”ê°€: {feature_data.shape}"
                        )

                # ëª¨ë“  íŠ¹ì„± ê²°í•©
                X_combined = np.concatenate(feature_matrices, axis=1)

                # ë°ì´í„° ê²€ì¦
                if np.any(np.isnan(X_combined)) or np.any(np.isinf(X_combined)):
                    self.logger.warning(
                        "âš ï¸ ë°ì´í„°ì— NaN ë˜ëŠ” Inf ê°’ì´ ìˆìŠµë‹ˆë‹¤. 0ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤."
                    )
                    X_combined = np.nan_to_num(
                        X_combined, nan=0.0, posinf=0.0, neginf=0.0
                    )

                # ìŠ¤ì¼€ì¼ë§
                X_scaled = self.scaler.fit_transform(X_combined)

                # AutoEncoder ì…ë ¥ ì°¨ì› ì—…ë°ì´íŠ¸
                self.config.input_dim = X_scaled.shape[1]
                self.autoencoder_model.config["autoencoder"]["input_dim"] = (
                    X_scaled.shape[1]
                )

                self.execution_stats["preprocessing_time"] = time.time() - start_time
                self.logger.info(
                    f"âœ… AutoEncoder ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {X_scaled.shape} ({self.execution_stats['preprocessing_time']:.2f}ì´ˆ)"
                )

                return X_scaled, feature_names

            except Exception as e:
                self.execution_stats["error_count"] += 1
                self.logger.error(f"âŒ AutoEncoder ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨: {e}")
                raise

    def train_autoencoder_model(self, X: np.ndarray) -> Dict[str, Any]:
        """
        AutoEncoder ëª¨ë¸ í•™ìŠµ

        Args:
            X: í•™ìŠµ ë°ì´í„°

        Returns:
            í•™ìŠµ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        self.logger.info("ğŸš€ AutoEncoder ëª¨ë¸ í•™ìŠµ ì‹œì‘...")

        with self.performance_monitor.track(
            "train_autoencoder_model", track_gpu=self.config.use_gpu
        ):
            start_time = time.time()

            try:
                # ë°ì´í„° ë¶„í• 
                from sklearn.model_selection import train_test_split

                X_train, X_val = train_test_split(
                    X, test_size=self.config.validation_split, random_state=42
                )

                self.logger.info(
                    f"ğŸ“Š ë°ì´í„° ë¶„í•  ì™„ë£Œ: Train={X_train.shape}, Val={X_val.shape}"
                )

                # GPU ë©”ëª¨ë¦¬ ìµœì í™”
                if self.config.use_gpu and self.cuda_optimizer:
                    self.cuda_optimizer.clear_cache()

                # ëª¨ë¸ í•™ìŠµ (ì •ìƒ íŒ¨í„´ í•™ìŠµ)
                # yëŠ” Noneìœ¼ë¡œ ì„¤ì • (ë¹„ì§€ë„ í•™ìŠµ)
                train_result = self.autoencoder_model.fit(
                    X_train,
                    y=None,
                    validation_data=(X_val, None),
                    epochs=self.config.epochs,
                    batch_size=self.config.batch_size,
                    early_stopping_patience=self.config.early_stopping_patience,
                    verbose=True,
                )

                # ì¬êµ¬ì„± ì˜¤ë¥˜ í†µê³„ ê³„ì‚°
                reconstruction_errors = self._compute_reconstruction_errors(X_train)

                # ì„ê³„ê°’ ê³„ì‚° (Z-score ê¸°ë°˜)
                mean_error = np.mean(reconstruction_errors)
                std_error = np.std(reconstruction_errors)
                threshold = mean_error + self.config.zscore_threshold * std_error

                # í†µê³„ ì—…ë°ì´íŠ¸
                self.anomaly_stats.update(
                    {
                        "mean_reconstruction_error": float(mean_error),
                        "std_reconstruction_error": float(std_error),
                        "threshold_value": float(threshold),
                    }
                )

                training_time = time.time() - start_time
                self.execution_stats["training_time"] = training_time

                # ê²°ê³¼ ì •ë¦¬
                training_results = {
                    "success": True,
                    "training_time": training_time,
                    "model_metadata": train_result,
                    "reconstruction_stats": {
                        "mean_error": float(mean_error),
                        "std_error": float(std_error),
                        "threshold": float(threshold),
                        "zscore_threshold": self.config.zscore_threshold,
                    },
                    "data_splits": {
                        "train_size": X_train.shape[0],
                        "val_size": X_val.shape[0],
                        "feature_count": X.shape[1],
                    },
                    "model_config": {
                        "input_dim": X.shape[1],
                        "latent_dim": self.config.latent_dim,
                        "hidden_dims": self.config.hidden_dims,
                    },
                    "gpu_used": self.config.use_gpu,
                }

                self.logger.info(f"âœ… AutoEncoder ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
                self.logger.info(
                    f"ğŸ“ˆ ì¬êµ¬ì„± ì˜¤ë¥˜ í†µê³„: í‰ê· ={mean_error:.4f}, í‘œì¤€í¸ì°¨={std_error:.4f}"
                )
                self.logger.info(f"ğŸ¯ ì´ìƒì¹˜ ì„ê³„ê°’: {threshold:.4f}")
                self.logger.info(f"â±ï¸ í•™ìŠµ ì‹œê°„: {training_time:.2f}ì´ˆ")

                return training_results

            except Exception as e:
                self.execution_stats["error_count"] += 1
                self.logger.error(f"âŒ AutoEncoder ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {e}")
                return {
                    "success": False,
                    "error": str(e),
                    "training_time": time.time() - start_time,
                }

    def _compute_reconstruction_errors(self, X: np.ndarray) -> np.ndarray:
        """
        ì¬êµ¬ì„± ì˜¤ë¥˜ ê³„ì‚°

        Args:
            X: ì…ë ¥ ë°ì´í„°

        Returns:
            ì¬êµ¬ì„± ì˜¤ë¥˜ ë°°ì—´
        """
        try:
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì¬êµ¬ì„± ì˜¤ë¥˜ ê³„ì‚°
            batch_size = self.config.batch_size
            errors = []

            for i in range(0, X.shape[0], batch_size):
                batch_end = min(i + batch_size, X.shape[0])
                batch_X = X[i:batch_end]

                # ì¬êµ¬ì„±
                reconstructed = self.autoencoder_model.predict(batch_X)

                # MSE ê³„ì‚°
                batch_errors = np.mean((batch_X - reconstructed) ** 2, axis=1)
                errors.extend(batch_errors)

            return np.array(errors)

        except Exception as e:
            self.logger.error(f"âŒ ì¬êµ¬ì„± ì˜¤ë¥˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return np.zeros(X.shape[0])

    def detect_anomalies(self, feature_vectors: np.ndarray) -> np.ndarray:
        """
        ì´ìƒì¹˜ ê°ì§€

        Args:
            feature_vectors: íŠ¹ì„± ë²¡í„°

        Returns:
            ì´ìƒì¹˜ ì ìˆ˜ ë°°ì—´
        """
        self.logger.info("ğŸ” ì´ìƒì¹˜ ê°ì§€ ì‹œì‘...")

        with self.performance_monitor.track(
            "detect_anomalies", track_gpu=self.config.use_gpu
        ):
            start_time = time.time()

            try:
                if not self.autoencoder_model.is_trained:
                    raise ValueError("AutoEncoder ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

                # ë°ì´í„° ìŠ¤ì¼€ì¼ë§
                X_scaled = self.scaler.transform(feature_vectors)

                # ì¬êµ¬ì„± ì˜¤ë¥˜ ê³„ì‚°
                reconstruction_errors = self._compute_reconstruction_errors(X_scaled)

                # ì´ìƒì¹˜ ì ìˆ˜ ì •ê·œí™” (0~1 ë²”ìœ„)
                min_error = np.min(reconstruction_errors)
                max_error = np.max(reconstruction_errors)

                if max_error > min_error:
                    anomaly_scores = (reconstruction_errors - min_error) / (
                        max_error - min_error
                    )
                else:
                    anomaly_scores = np.zeros_like(reconstruction_errors)

                # ì´ìƒì¹˜ í†µê³„ ì—…ë°ì´íŠ¸
                threshold = self.anomaly_stats["threshold_value"]
                anomaly_count = np.sum(reconstruction_errors > threshold)

                self.anomaly_stats.update(
                    {
                        "total_samples": len(reconstruction_errors),
                        "anomaly_count": int(anomaly_count),
                        "anomaly_rate": float(
                            anomaly_count / len(reconstruction_errors)
                        ),
                    }
                )

                detection_time = time.time() - start_time
                self.execution_stats["detection_time"] = detection_time

                self.logger.info(
                    f"âœ… ì´ìƒì¹˜ ê°ì§€ ì™„ë£Œ: {anomaly_count}/{len(reconstruction_errors)} ({self.anomaly_stats['anomaly_rate']:.1%})"
                )
                self.logger.info(f"â±ï¸ ê°ì§€ ì‹œê°„: {detection_time:.2f}ì´ˆ")

                return anomaly_scores

            except Exception as e:
                self.execution_stats["error_count"] += 1
                self.logger.error(f"âŒ ì´ìƒì¹˜ ê°ì§€ ì‹¤íŒ¨: {e}")
                raise

    def apply_anomaly_penalties(
        self, base_scores: np.ndarray, anomaly_scores: np.ndarray
    ) -> np.ndarray:
        """
        ì´ìƒì¹˜ ì ìˆ˜ì— ë”°ë¥¸ ê°ì  ì ìš©

        Args:
            base_scores: ê¸°ë³¸ ì ìˆ˜
            anomaly_scores: ì´ìƒì¹˜ ì ìˆ˜

        Returns:
            ê°ì  ì ìš©ëœ ì ìˆ˜
        """
        self.logger.info("âš–ï¸ ì´ìƒì¹˜ ê°ì  ì ìš© ì‹œì‘...")

        try:
            # ê°ì  ê³„ìˆ˜ ê³„ì‚° (ì´ìƒì¹˜ ì ìˆ˜ì— ë¹„ë¡€)
            penalty_factors = anomaly_scores * self.config.anomaly_penalty_factor

            # ê°ì  ë²”ìœ„ ì œí•œ
            penalty_factors = np.clip(
                penalty_factors, self.config.min_penalty, self.config.max_penalty
            )

            # ê°ì  ì ìš©
            penalized_scores = base_scores * (1 - penalty_factors)

            # ê°ì  í†µê³„
            penalty_applied = np.sum(penalty_factors > 0)
            avg_penalty = (
                np.mean(penalty_factors[penalty_factors > 0])
                if penalty_applied > 0
                else 0
            )

            self.anomaly_stats["penalty_applied"] = int(penalty_applied)

            self.logger.info(
                f"âœ… ê°ì  ì ìš© ì™„ë£Œ: {penalty_applied}/{len(base_scores)} ìƒ˜í”Œì— ê°ì "
            )
            self.logger.info(f"ğŸ“Š í‰ê·  ê°ì ë¥ : {avg_penalty:.1%}")

            return penalized_scores

        except Exception as e:
            self.logger.error(f"âŒ ê°ì  ì ìš© ì‹¤íŒ¨: {e}")
            return base_scores

    def save_anomaly_results(self, results: Dict[str, Any]) -> str:
        """
        ì´ìƒê°ì§€ ê²°ê³¼ ì €ì¥

        Args:
            results: ì´ìƒê°ì§€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬

        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        self.logger.info("ğŸ’¾ ì´ìƒê°ì§€ ê²°ê³¼ ì €ì¥ ì‹œì‘...")

        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

            # ë©”ì¸ ê²°ê³¼ íŒŒì¼
            result_file = self.output_dir / f"anomaly_detection_{timestamp}.json"

            # ì €ì¥í•  ë°ì´í„° ì¤€ë¹„
            save_data = {
                "timestamp": timestamp,
                "config": asdict(self.config),
                "execution_stats": self.execution_stats,
                "anomaly_stats": self.anomaly_stats,
                "results": results,
                "model_info": {
                    "model_type": "AutoEncoder",
                    "is_trained": self.autoencoder_model.is_trained,
                    "input_dim": self.autoencoder_model.input_dim,
                    "latent_dim": self.config.latent_dim,
                    "hidden_dims": self.config.hidden_dims,
                },
            }

            # JSON íŒŒì¼ ì €ì¥
            with open(result_file, "w", encoding="utf-8") as f:
                json.dump(save_data, f, indent=2, ensure_ascii=False, default=str)

            # ëª¨ë¸ ì €ì¥
            model_file = self.model_save_dir / f"autoencoder_model_{timestamp}.pt"
            model_saved = self.autoencoder_model.save(str(model_file))

            if model_saved:
                self.logger.info(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_file}")
                save_data["model_file"] = str(model_file)

            # ì´ìƒì¹˜ ì ìˆ˜ë§Œ ë³„ë„ ì €ì¥ (NumPy í˜•ì‹)
            if "anomaly_scores" in results:
                scores_file = self.output_dir / f"anomaly_scores_{timestamp}.npy"
                np.save(scores_file, results["anomaly_scores"])
                self.logger.info(f"ğŸ’¾ ì´ìƒì¹˜ ì ìˆ˜ ì €ì¥ ì™„ë£Œ: {scores_file}")

            # ê°ì  ì ìš©ëœ ì ìˆ˜ ì €ì¥
            if "penalized_scores" in results:
                penalized_file = self.output_dir / f"penalized_scores_{timestamp}.npy"
                np.save(penalized_file, results["penalized_scores"])
                self.logger.info(f"ğŸ’¾ ê°ì  ì ìˆ˜ ì €ì¥ ì™„ë£Œ: {penalized_file}")

            self.logger.info(f"âœ… ì´ìƒê°ì§€ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {result_file}")

            return str(result_file)

        except Exception as e:
            self.execution_stats["error_count"] += 1
            self.logger.error(f"âŒ ì´ìƒê°ì§€ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise

    def run_full_anomaly_detection(self) -> Dict[str, Any]:
        """
        ì „ì²´ ì´ìƒê°ì§€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

        Returns:
            ì‹¤í–‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        self.logger.info("ğŸš€ ì „ì²´ ì´ìƒê°ì§€ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        self.logger.info("=" * 80)

        # ì‹¤í–‰ ì‹œì‘ ì‹œê°„
        self.execution_stats["start_time"] = datetime.now()

        try:
            # 1. ì´ì „ ê²°ê³¼ ë¡œë“œ
            self.logger.info("ğŸ“Š Step 1: ì´ì „ ë‹¨ê³„ ê²°ê³¼ ë¡œë“œ")
            previous_results = self.load_previous_results()

            # 2. AutoEncoder í•™ìŠµ ë°ì´í„° ì¤€ë¹„
            self.logger.info("ğŸ”„ Step 2: AutoEncoder í•™ìŠµ ë°ì´í„° ì¤€ë¹„")
            X, feature_names = self.prepare_autoencoder_data(previous_results)

            # 3. AutoEncoder ëª¨ë¸ í•™ìŠµ
            self.logger.info("ğŸš€ Step 3: AutoEncoder ëª¨ë¸ í•™ìŠµ")
            training_results = self.train_autoencoder_model(X)

            if not training_results["success"]:
                raise RuntimeError(
                    f"AutoEncoder í•™ìŠµ ì‹¤íŒ¨: {training_results.get('error', 'Unknown error')}"
                )

            # 4. ì´ìƒì¹˜ ê°ì§€
            self.logger.info("ğŸ” Step 4: ì´ìƒì¹˜ ê°ì§€")
            anomaly_scores = self.detect_anomalies(X)

            # 5. ê°ì  ì ìš©
            self.logger.info("âš–ï¸ Step 5: ê°ì  ì ìš©")
            base_scores = previous_results["pattern_scores"]
            if base_scores is None:
                # ê¸°ë³¸ ì ìˆ˜ ìƒì„±
                base_scores = np.random.rand(len(anomaly_scores)) * 0.5 + 0.5
                self.logger.warning("âš ï¸ ê¸°ë³¸ ì ìˆ˜ê°€ ì—†ì–´ ì„ì‹œ ì ìˆ˜ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")

            # ìƒ˜í”Œ ìˆ˜ ë§ì¶¤
            if len(base_scores) != len(anomaly_scores):
                if len(base_scores) > len(anomaly_scores):
                    base_scores = base_scores[: len(anomaly_scores)]
                else:
                    repeat_count = (len(anomaly_scores) + len(base_scores) - 1) // len(
                        base_scores
                    )
                    base_scores = np.tile(base_scores, repeat_count)[
                        : len(anomaly_scores)
                    ]

            penalized_scores = self.apply_anomaly_penalties(base_scores, anomaly_scores)

            # 6. ê²°ê³¼ ì •ë¦¬
            detection_results = {
                "anomaly_scores": anomaly_scores,
                "penalized_scores": penalized_scores,
                "base_scores": base_scores,
                "training_results": training_results,
                "feature_names": feature_names,
                "previous_results_metadata": {
                    "source_files": previous_results["source_files"],
                    "data_loading_time": previous_results["load_time"],
                },
            }

            # 7. ê²°ê³¼ ì €ì¥
            self.logger.info("ğŸ’¾ Step 6: ì´ìƒê°ì§€ ê²°ê³¼ ì €ì¥")
            result_file = self.save_anomaly_results(detection_results)

            # ì‹¤í–‰ ì™„ë£Œ
            self.execution_stats["end_time"] = datetime.now()
            self.execution_stats["total_time"] = (
                self.execution_stats["end_time"] - self.execution_stats["start_time"]
            ).total_seconds()

            # ì„±ëŠ¥ í†µê³„ ìˆ˜ì§‘
            performance_stats = self.performance_monitor.get_performance_summary()

            # ìµœì¢… ê²°ê³¼
            final_results = {
                "success": True,
                "result_file": result_file,
                "execution_stats": self.execution_stats,
                "anomaly_stats": self.anomaly_stats,
                "performance_stats": performance_stats,
                "detection_summary": {
                    "total_samples": self.anomaly_stats["total_samples"],
                    "anomaly_count": self.anomaly_stats["anomaly_count"],
                    "anomaly_rate": self.anomaly_stats["anomaly_rate"],
                    "penalty_applied": self.anomaly_stats["penalty_applied"],
                    "mean_penalty_reduction": float(
                        np.mean(base_scores - penalized_scores)
                    ),
                },
                "model_performance": training_results["reconstruction_stats"],
                "warnings": self.execution_stats["warnings"],
            }

            self.logger.info("=" * 80)
            self.logger.info("âœ… ì´ìƒê°ì§€ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")
            self.logger.info(
                f"â±ï¸ ì´ ì‹¤í–‰ ì‹œê°„: {self.execution_stats['total_time']:.2f}ì´ˆ"
            )
            self.logger.info(
                f"ğŸ” ì´ìƒì¹˜ ê°ì§€: {self.anomaly_stats['anomaly_count']}/{self.anomaly_stats['total_samples']} ({self.anomaly_stats['anomaly_rate']:.1%})"
            )
            self.logger.info(
                f"âš–ï¸ ê°ì  ì ìš©: {self.anomaly_stats['penalty_applied']}ê°œ ìƒ˜í”Œ"
            )
            self.logger.info(
                f"ğŸ“Š í‰ê·  ì ìˆ˜ ê°ì†Œ: {final_results['detection_summary']['mean_penalty_reduction']:.4f}"
            )
            self.logger.info(f"ğŸ’¾ ê²°ê³¼ íŒŒì¼: {result_file}")
            self.logger.info("=" * 80)

            return final_results

        except Exception as e:
            self.execution_stats["error_count"] += 1
            self.execution_stats["end_time"] = datetime.now()
            self.execution_stats["total_time"] = (
                self.execution_stats["end_time"] - self.execution_stats["start_time"]
            ).total_seconds()

            self.logger.error("=" * 80)
            self.logger.error(f"âŒ ì´ìƒê°ì§€ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {e}")
            self.logger.error(
                f"â±ï¸ ì‹¤í–‰ ì‹œê°„: {self.execution_stats['total_time']:.2f}ì´ˆ"
            )
            self.logger.error(f"ğŸš¨ ì˜¤ë¥˜ ê°œìˆ˜: {self.execution_stats['error_count']}")
            self.logger.error("=" * 80)

            return {
                "success": False,
                "error": str(e),
                "execution_stats": self.execution_stats,
                "anomaly_stats": self.anomaly_stats,
                "warnings": self.execution_stats["warnings"],
            }


def parse_arguments():
    """ëª…ë ¹í–‰ ì¸ìˆ˜ íŒŒì‹±"""
    parser = argparse.ArgumentParser(
        description="DAEBAK AI ë¡œë˜ ì‹œìŠ¤í…œ - ì´ìƒê°ì§€ ë‹¨ê³„ (Phase 3)"
    )

    parser.add_argument("--config", type=str, help="ì„¤ì • íŒŒì¼ ê²½ë¡œ (JSON í˜•ì‹)")

    parser.add_argument("--no-gpu", action="store_true", help="GPU ì‚¬ìš© ë¹„í™œì„±í™”")

    parser.add_argument(
        "--latent-dim", type=int, default=32, help="AutoEncoder ì ì¬ ì°¨ì›"
    )

    parser.add_argument("--batch-size", type=int, default=256, help="ë°°ì¹˜ í¬ê¸°")

    parser.add_argument("--epochs", type=int, default=100, help="í•™ìŠµ ì—í¬í¬ ìˆ˜")

    parser.add_argument(
        "--zscore-threshold", type=float, default=2.5, help="Z-score ì„ê³„ê°’"
    )

    parser.add_argument(
        "--penalty-factor", type=float, default=0.3, help="ì´ìƒì¹˜ ê°ì  ê³„ìˆ˜"
    )

    parser.add_argument(
        "--output-dir",
        type=str,
        default="data/result/anomaly_detection",
        help="ì¶œë ¥ ë””ë ‰í† ë¦¬",
    )

    parser.add_argument("--verbose", action="store_true", help="ìƒì„¸ ë¡œê·¸ ì¶œë ¥")

    return parser.parse_args()


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # 1. ì˜ì¡´ì„± ì„¤ì •
    configure_dependencies()

    # 2. ëª…ë ¹í–‰ ì¸ìˆ˜ íŒŒì‹±
    args = parse_arguments()
    cli_config = {}
    if args.config:
        with open(args.config, "r", encoding="utf-8") as f:
            cli_config = json.load(f)

    # ëª…ë ¹í–‰ ì¸ìˆ˜ ë°˜ì˜
    if args.no_gpu:
        cli_config["use_gpu"] = False
    if args.latent_dim:
        cli_config["latent_dim"] = args.latent_dim
    if args.batch_size:
        cli_config["batch_size"] = args.batch_size
    if args.epochs:
        cli_config["epochs"] = args.epochs
    if args.zscore_threshold:
        cli_config["zscore_threshold"] = args.zscore_threshold
    if args.penalty_factor:
        cli_config["anomaly_penalty_factor"] = args.penalty_factor
    if args.output_dir:
        cli_config["output_dir"] = args.output_dir

    # ë¡œê±° ì„¤ì •
    logger = get_logger(__name__)
    if args.verbose:
        logger.setLevel("DEBUG")

    try:
        # 3. ì´ìƒê°ì§€ ì—”ì§„ ìƒì„± ë° ì‹¤í–‰
        engine = AnomalyDetectionEngine(cli_config)
        final_results = engine.run_full_anomaly_detection()

        # 4. ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        if final_results["success"]:
            print(f"\nâœ… ì´ìƒê°ì§€ ì™„ë£Œ!")
            print(f"ğŸ“ ê²°ê³¼ íŒŒì¼: {final_results['result_file']}")
            print(f"â±ï¸ ì‹¤í–‰ ì‹œê°„: {final_results['execution_stats']['total_time']:.2f}ì´ˆ")
            print(f"ğŸ” ì´ìƒì¹˜ ê°ì§€ìœ¨: {final_results['anomaly_stats']['anomaly_rate']:.1%}")
            return 0
        else:
            print(f"\nâŒ ì´ìƒê°ì§€ ì‹¤íŒ¨: {final_results['error']}")
            return 1

    except Exception as e:
        logger.error(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return 1


if __name__ == "__main__":
    exit(main())
