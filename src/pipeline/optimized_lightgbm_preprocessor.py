"""
Optimized LightGBM Preprocessor
LightGBM ìµœì í™” ì „ì²˜ë¦¬ê¸° - GPU ê°€ì†, ì¹´í…Œê³ ë¦¬ ì„ë² ë”©, í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
"""

import numpy as np
from typing import Dict, List, Optional, Tuple, Any
import lightgbm as lgb
import optuna
from sklearn.preprocessing import LabelEncoder, TargetEncoder
from sklearn.feature_selection import SelectKBest, mutual_info_classif
import itertools
import warnings

warnings.filterwarnings("ignore")

from ..utils.unified_logging import get_logger
from ..utils.unified_memory_manager import get_unified_memory_manager
from ..shared.types import PreprocessedData

logger = get_logger(__name__)


class CategoricalEmbedder:
    """ì¹´í…Œê³ ë¦¬ íŠ¹ì„± ì„ë² ë”©"""

    def __init__(self, embedding_dim: int = 32, method: str = "target_guided"):
        self.embedding_dim = embedding_dim
        self.method = method
        self.encoders = {}
        self.target_encoders = {}

    def fit(
        self,
        X: np.ndarray,
        y: Optional[np.ndarray] = None,
        categorical_features: List[int] = None,
    ):
        """ì„ë² ë”© í•™ìŠµ"""

        if categorical_features is None:
            # ìë™ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ íŠ¹ì„± ê°ì§€
            categorical_features = self._detect_categorical_features(X)

        for feat_idx in categorical_features:
            feature_data = X[:, feat_idx]

            # ë¼ë²¨ ì¸ì½”ë”©
            le = LabelEncoder()
            encoded_data = le.fit_transform(feature_data.astype(str))
            self.encoders[feat_idx] = le

            # íƒ€ê²Ÿ ê¸°ë°˜ ì¸ì½”ë”©
            if y is not None and self.method == "target_guided":
                te = TargetEncoder(smooth="auto")
                te.fit(encoded_data.reshape(-1, 1), y)
                self.target_encoders[feat_idx] = te

    def transform(self, X: np.ndarray) -> np.ndarray:
        """ì„ë² ë”© ë³€í™˜"""

        X_embedded = X.copy()

        for feat_idx, encoder in self.encoders.items():
            feature_data = X[:, feat_idx]

            # ë¼ë²¨ ì¸ì½”ë”©
            try:
                encoded_data = encoder.transform(feature_data.astype(str))
            except ValueError:
                # ìƒˆë¡œìš´ ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬
                encoded_data = np.zeros(len(feature_data))

            # íƒ€ê²Ÿ ì¸ì½”ë”© ì ìš©
            if feat_idx in self.target_encoders:
                target_encoded = self.target_encoders[feat_idx].transform(
                    encoded_data.reshape(-1, 1)
                )
                X_embedded[:, feat_idx] = target_encoded.flatten()
            else:
                X_embedded[:, feat_idx] = encoded_data

        return X_embedded

    def _detect_categorical_features(self, X: np.ndarray) -> List[int]:
        """ì¹´í…Œê³ ë¦¬ íŠ¹ì„± ìë™ ê°ì§€"""
        categorical_features = []

        for i in range(X.shape[1]):
            unique_values = len(np.unique(X[:, i]))
            total_values = len(X[:, i])

            # ê³ ìœ ê°’ ë¹„ìœ¨ì´ 10% ì´í•˜ì´ë©´ ì¹´í…Œê³ ë¦¬ë¡œ ê°„ì£¼
            if unique_values / total_values <= 0.1 and unique_values <= 50:
                categorical_features.append(i)

        return categorical_features


class FeatureInteractionGenerator:
    """íŠ¹ì„± ìƒí˜¸ì‘ìš© ìƒì„±ê¸°"""

    def __init__(
        self, max_interactions: int = 50, selection_method: str = "lightgbm_importance"
    ):
        self.max_interactions = max_interactions
        self.selection_method = selection_method
        self.interaction_features = []
        self.feature_importance = None

    def fit(self, X: np.ndarray, y: np.ndarray):
        """ìƒí˜¸ì‘ìš© íŠ¹ì„± í•™ìŠµ"""

        # ê¸°ë³¸ íŠ¹ì„± ì¤‘ìš”ë„ ê³„ì‚°
        self.feature_importance = self._calculate_feature_importance(X, y)

        # ìƒìœ„ íŠ¹ì„±ë“¤ ì„ íƒ
        n_top_features = min(20, X.shape[1])
        top_features = np.argsort(self.feature_importance)[-n_top_features:]

        # 2ì°¨ ìƒí˜¸ì‘ìš© ìƒì„±
        interaction_candidates = []
        for i, j in itertools.combinations(top_features, 2):
            interaction_candidates.append((i, j))

        # ìƒí˜¸ì‘ìš© ì¤‘ìš”ë„ í‰ê°€
        interaction_scores = []
        for i, j in interaction_candidates:
            interaction_feature = X[:, i] * X[:, j]
            score = self._evaluate_interaction(interaction_feature, y)
            interaction_scores.append((score, i, j))

        # ìƒìœ„ ìƒí˜¸ì‘ìš© ì„ íƒ
        interaction_scores.sort(reverse=True)
        self.interaction_features = [
            (i, j) for _, i, j in interaction_scores[: self.max_interactions]
        ]

        logger.info(f"ìƒì„±ëœ ìƒí˜¸ì‘ìš© íŠ¹ì„±: {len(self.interaction_features)}ê°œ")

    def transform(self, X: np.ndarray) -> np.ndarray:
        """ìƒí˜¸ì‘ìš© íŠ¹ì„± ë³€í™˜"""

        if not self.interaction_features:
            return X

        # ìƒí˜¸ì‘ìš© íŠ¹ì„± ìƒì„±
        interaction_matrix = []
        for i, j in self.interaction_features:
            interaction_feature = X[:, i] * X[:, j]
            interaction_matrix.append(interaction_feature)

        interaction_matrix = np.array(interaction_matrix).T

        # ì›ë³¸ íŠ¹ì„±ê³¼ ê²°í•©
        X_enhanced = np.hstack([X, interaction_matrix])

        return X_enhanced

    def _calculate_feature_importance(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:
        """íŠ¹ì„± ì¤‘ìš”ë„ ê³„ì‚°"""

        if self.selection_method == "lightgbm_importance":
            # LightGBMìœ¼ë¡œ íŠ¹ì„± ì¤‘ìš”ë„ ê³„ì‚°
            train_data = lgb.Dataset(X, label=y)
            params = {
                "objective": "multiclass" if len(np.unique(y)) > 2 else "binary",
                "num_class": len(np.unique(y)) if len(np.unique(y)) > 2 else 1,
                "metric": (
                    "multi_logloss" if len(np.unique(y)) > 2 else "binary_logloss"
                ),
                "boosting_type": "gbdt",
                "num_leaves": 31,
                "learning_rate": 0.1,
                "feature_fraction": 0.9,
                "verbose": -1,
            }

            model = lgb.train(params, train_data, num_boost_round=100)
            return model.feature_importance()

        else:
            # ìƒí˜¸ì •ë³´ëŸ‰ ê¸°ë°˜ ì¤‘ìš”ë„
            selector = SelectKBest(score_func=mutual_info_classif, k="all")
            selector.fit(X, y)
            return selector.scores_

    def _evaluate_interaction(
        self, interaction_feature: np.ndarray, y: np.ndarray
    ) -> float:
        """ìƒí˜¸ì‘ìš© íŠ¹ì„± í‰ê°€"""

        # ìƒí˜¸ì •ë³´ëŸ‰ ê³„ì‚°
        from sklearn.feature_selection import mutual_info_classif

        score = mutual_info_classif(interaction_feature.reshape(-1, 1), y)[0]
        return score


class OptimizedLightGBMPreprocessor:
    """ìµœì í™”ëœ LightGBM ì „ì²˜ë¦¬ê¸°"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = get_logger(__name__)
        self.memory_manager = get_unified_memory_manager()

        # GPU ì„¤ì •
        self.gpu_params = {
            "device_type": "gpu",
            "gpu_platform_id": 0,
            "gpu_use_dp": True,
            "max_bin": 255,
            "num_gpu": 1,
        }

        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.cat_embedder = CategoricalEmbedder(
            embedding_dim=32, method="target_guided"
        )

        self.interaction_generator = FeatureInteractionGenerator(
            max_interactions=50, selection_method="lightgbm_importance"
        )

        self.feature_selector = None
        self.best_params = None

    def preprocess_for_lightgbm(
        self, X: np.ndarray, y: np.ndarray, feature_names: Optional[List[str]] = None
    ) -> Tuple[np.ndarray, Dict[str, Any]]:
        """
        LightGBM ìµœì í™” ì „ì²˜ë¦¬

        Args:
            X: ì…ë ¥ íŠ¹ì„±
            y: íƒ€ê²Ÿ ë³€ìˆ˜
            feature_names: íŠ¹ì„± ì´ë¦„ ëª©ë¡

        Returns:
            Tuple[np.ndarray, Dict]: ì „ì²˜ë¦¬ëœ ë°ì´í„°ì™€ ìµœì  íŒŒë¼ë¯¸í„°
        """

        try:
            self.logger.info("LightGBM ìµœì í™” ì „ì²˜ë¦¬ ì‹œì‘")

            # 1. ì¹´í…Œê³ ë¦¬ ì„ë² ë”©
            self.logger.info("ì¹´í…Œê³ ë¦¬ ì„ë² ë”© ì ìš©...")
            self.cat_embedder.fit(X, y)
            X_embedded = self.cat_embedder.transform(X)

            # 2. íŠ¹ì„± ìƒí˜¸ì‘ìš© ìƒì„±
            self.logger.info("íŠ¹ì„± ìƒí˜¸ì‘ìš© ìƒì„±...")
            self.interaction_generator.fit(X_embedded, y)
            X_enhanced = self.interaction_generator.transform(X_embedded)

            # 3. íŠ¹ì„± ì„ íƒ
            self.logger.info("íŠ¹ì„± ì„ íƒ...")
            X_selected = self._select_features(X_enhanced, y, k_best=100)

            # 4. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
            self.logger.info("í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”...")
            self.best_params = self._optimize_hyperparameters(
                X_selected, y, n_trials=100
            )

            self.logger.info("ì „ì²˜ë¦¬ ì™„ë£Œ")

            return X_selected, self.best_params

        except Exception as e:
            self.logger.error(f"LightGBM ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì›ë³¸ ë°ì´í„°ì™€ ê¸°ë³¸ íŒŒë¼ë¯¸í„° ë°˜í™˜
            return X, {}

    def _select_features(
        self, X: np.ndarray, y: np.ndarray, k_best: int = 100
    ) -> np.ndarray:
        """íŠ¹ì„± ì„ íƒ"""

        if X.shape[1] <= k_best:
            return X

        # ìƒí˜¸ì •ë³´ëŸ‰ ê¸°ë°˜ íŠ¹ì„± ì„ íƒ
        self.feature_selector = SelectKBest(
            score_func=mutual_info_classif, k=min(k_best, X.shape[1])
        )

        X_selected = self.feature_selector.fit_transform(X, y)

        self.logger.info(f"íŠ¹ì„± ì„ íƒ ì™„ë£Œ: {X.shape[1]} -> {X_selected.shape[1]}")
        return X_selected

    def _optimize_hyperparameters(
        self, X: np.ndarray, y: np.ndarray, n_trials: int = 100
    ) -> Dict[str, Any]:
        """Optunaë¥¼ ì´ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”"""

        def objective(trial):
            # í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ê³µê°„ ì •ì˜
            params = {
                "objective": "multiclass" if len(np.unique(y)) > 2 else "binary",
                "num_class": len(np.unique(y)) if len(np.unique(y)) > 2 else 1,
                "metric": (
                    "multi_logloss" if len(np.unique(y)) > 2 else "binary_logloss"
                ),
                "boosting_type": "gbdt",
                "num_leaves": trial.suggest_int("num_leaves", 10, 300),
                "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3),
                "feature_fraction": trial.suggest_float("feature_fraction", 0.4, 1.0),
                "bagging_fraction": trial.suggest_float("bagging_fraction", 0.4, 1.0),
                "bagging_freq": trial.suggest_int("bagging_freq", 1, 7),
                "min_child_samples": trial.suggest_int("min_child_samples", 5, 100),
                "reg_alpha": trial.suggest_float("reg_alpha", 0, 10),
                "reg_lambda": trial.suggest_float("reg_lambda", 0, 10),
                "verbose": -1,
            }

            # GPU ì„¤ì • ì¶”ê°€
            if self.gpu_params["device_type"] == "gpu":
                params.update(self.gpu_params)

            # êµì°¨ ê²€ì¦ìœ¼ë¡œ ì„±ëŠ¥ í‰ê°€
            train_data = lgb.Dataset(X, label=y)
            cv_results = lgb.cv(
                params,
                train_data,
                num_boost_round=200,
                nfold=5,
                stratified=True,
                shuffle=True,
                seed=42,
                return_cvbooster=False,
                eval_train_metric=False,
            )

            # ìµœì í™” ëŒ€ìƒ ë©”íŠ¸ë¦­ ë°˜í™˜
            if len(np.unique(y)) > 2:
                return min(cv_results["valid multi_logloss-mean"])
            else:
                return min(cv_results["valid binary_logloss-mean"])

        # Optuna ìµœì í™” ì‹¤í–‰
        study = optuna.create_study(direction="minimize")
        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)

        # ìµœì  íŒŒë¼ë¯¸í„° ë°˜í™˜
        best_params = study.best_params
        best_params.update(
            {
                "objective": "multiclass" if len(np.unique(y)) > 2 else "binary",
                "num_class": len(np.unique(y)) if len(np.unique(y)) > 2 else 1,
                "metric": (
                    "multi_logloss" if len(np.unique(y)) > 2 else "binary_logloss"
                ),
                "boosting_type": "gbdt",
                "verbose": -1,
            }
        )

        # GPU ì„¤ì • ì¶”ê°€
        if self.gpu_params["device_type"] == "gpu":
            best_params.update(self.gpu_params)

        self.logger.info(
            f"í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì™„ë£Œ - ìµœì  ì ìˆ˜: {study.best_value:.4f}"
        )

        return best_params

    def create_optimized_model(
        self, X: np.ndarray, y: np.ndarray
    ) -> lgb.LGBMClassifier:
        """ìµœì í™”ëœ LightGBM ëª¨ë¸ ìƒì„±"""

        if self.best_params is None:
            raise ValueError("ë¨¼ì € ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ì—¬ ìµœì  íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤.")

        # LightGBM ë¶„ë¥˜ê¸° ìƒì„±
        model = lgb.LGBMClassifier(**self.best_params)

        # ëª¨ë¸ í•™ìŠµ
        model.fit(X, y)

        return model

    def transform_new_data(self, X: np.ndarray) -> np.ndarray:
        """ìƒˆë¡œìš´ ë°ì´í„° ë³€í™˜"""

        # í•™ìŠµëœ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì ìš©
        X_embedded = self.cat_embedder.transform(X)
        X_enhanced = self.interaction_generator.transform(X_embedded)

        if self.feature_selector is not None:
            X_selected = self.feature_selector.transform(X_enhanced)
        else:
            X_selected = X_enhanced

        return X_selected

    def get_feature_importance_analysis(
        self, model: lgb.LGBMClassifier, feature_names: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„"""

        importance_gain = model.feature_importances_
        importance_split = model.booster_.feature_importance(importance_type="split")

        # íŠ¹ì„± ì´ë¦„ ìƒì„±
        if feature_names is None:
            feature_names = [f"feature_{i}" for i in range(len(importance_gain))]

        # ì¤‘ìš”ë„ ë¶„ì„ ê²°ê³¼
        analysis = {
            "feature_names": feature_names,
            "importance_gain": importance_gain,
            "importance_split": importance_split,
            "top_features_gain": [],
            "top_features_split": [],
        }

        # ìƒìœ„ íŠ¹ì„± ì¶”ì¶œ
        top_indices_gain = np.argsort(importance_gain)[-20:][::-1]
        top_indices_split = np.argsort(importance_split)[-20:][::-1]

        for idx in top_indices_gain:
            analysis["top_features_gain"].append(
                {"feature": feature_names[idx], "importance": importance_gain[idx]}
            )

        for idx in top_indices_split:
            analysis["top_features_split"].append(
                {"feature": feature_names[idx], "importance": importance_split[idx]}
            )

        return analysis

    def print_optimization_summary(self):
        """ìµœì í™” ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""

        print("=" * 60)
        print("ğŸš€ LightGBM ìµœì í™” ê²°ê³¼")
        print("=" * 60)

        if self.best_params:
            print("ğŸ“Š ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
            for param, value in self.best_params.items():
                if param not in ["objective", "metric", "boosting_type", "verbose"]:
                    print(f"  â€¢ {param}: {value}")

        print(f"\nğŸ”§ ì ìš©ëœ ìµœì í™” ê¸°ë²•:")
        print(f"  â€¢ ì¹´í…Œê³ ë¦¬ ì„ë² ë”©: Target-guided encoding")
        print(
            f"  â€¢ íŠ¹ì„± ìƒí˜¸ì‘ìš©: ìƒìœ„ {self.interaction_generator.max_interactions}ê°œ ìƒì„±"
        )
        print(f"  â€¢ íŠ¹ì„± ì„ íƒ: ìƒí˜¸ì •ë³´ëŸ‰ ê¸°ë°˜")
        print(
            f"  â€¢ GPU ê°€ì†: {'í™œì„±í™”' if self.gpu_params['device_type'] == 'gpu' else 'ë¹„í™œì„±í™”'}"
        )

        print("=" * 60)

    def preprocess(self, data: np.ndarray) -> PreprocessedData:
        """
        ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.
        """
        # ... existing code ...
