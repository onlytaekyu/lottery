"""
Model Performance Benchmark System
ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ ì‹œìŠ¤í…œ - A/B í…ŒìŠ¤íŠ¸, ì„±ëŠ¥ ì¶”ì , ì¢…í•© ë¶„ì„
"""

import numpy as np
from typing import Dict, List, Any
from dataclasses import dataclass, asdict
import time
import psutil
import threading
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
)
from sklearn.model_selection import cross_val_score, StratifiedKFold
from datetime import datetime
import json
import warnings
from pathlib import Path

warnings.filterwarnings("ignore")

from ..utils.unified_logging import get_logger
from ..utils.unified_config import load_config
from ..shared.types import ModelPerformanceMetrics

logger = get_logger(__name__)


@dataclass
class PerformanceMetrics:
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­"""

    accuracy: float = 0.0
    precision: float = 0.0
    recall: float = 0.0
    f1_score: float = 0.0
    auc_score: float = 0.0
    processing_time: float = 0.0
    memory_usage: float = 0.0
    gpu_utilization: float = 0.0
    throughput: float = 0.0  # samples/second

    def to_dict(self) -> Dict[str, float]:
        return asdict(self)


@dataclass
class BenchmarkResult:
    """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼"""

    model_name: str
    dataset_name: str
    timestamp: str
    metrics: PerformanceMetrics
    configuration: Dict[str, Any]

    def to_dict(self) -> Dict[str, Any]:
        return {
            "model_name": self.model_name,
            "dataset_name": self.dataset_name,
            "timestamp": self.timestamp,
            "metrics": self.metrics.to_dict(),
            "configuration": self.configuration,
        }


class SystemResourceMonitor:
    """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°"""

    def __init__(self):
        self.monitoring = False
        self.cpu_usage = []
        self.memory_usage = []
        self.gpu_usage = []
        self.monitor_thread = None

    def start_monitoring(self, interval: float = 0.1):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        self.monitoring = True
        self.cpu_usage = []
        self.memory_usage = []
        self.gpu_usage = []

        self.monitor_thread = threading.Thread(
            target=self._monitor_resources, args=(interval,)
        )
        self.monitor_thread.daemon = True
        self.monitor_thread.start()

    def stop_monitoring(self) -> Dict[str, float]:
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€ ë° ê²°ê³¼ ë°˜í™˜"""
        self.monitoring = False

        if self.monitor_thread:
            self.monitor_thread.join(timeout=1.0)

        return {
            "avg_cpu_usage": np.mean(self.cpu_usage) if self.cpu_usage else 0.0,
            "max_cpu_usage": np.max(self.cpu_usage) if self.cpu_usage else 0.0,
            "avg_memory_usage": (
                np.mean(self.memory_usage) if self.memory_usage else 0.0
            ),
            "max_memory_usage": np.max(self.memory_usage) if self.memory_usage else 0.0,
            "avg_gpu_usage": np.mean(self.gpu_usage) if self.gpu_usage else 0.0,
            "max_gpu_usage": np.max(self.gpu_usage) if self.gpu_usage else 0.0,
        }

    def _monitor_resources(self, interval: float):
        """ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
        while self.monitoring:
            try:
                # CPU ì‚¬ìš©ë¥ 
                cpu_percent = psutil.cpu_percent()
                self.cpu_usage.append(cpu_percent)

                # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
                memory_info = psutil.virtual_memory()
                self.memory_usage.append(memory_info.percent)

                # GPU ì‚¬ìš©ë¥  (NVIDIA GPUì¸ ê²½ìš°)
                try:
                    import GPUtil

                    gpus = GPUtil.getGPUs()
                    if gpus:
                        gpu_load = gpus[0].load * 100
                        self.gpu_usage.append(gpu_load)
                except ImportError:
                    self.gpu_usage.append(0.0)

                time.sleep(interval)

            except Exception as e:
                logger.warning(f"ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì¤‘ ì˜¤ë¥˜: {e}")
                time.sleep(interval)


class ABTester:
    """A/B í…ŒìŠ¤í„°"""

    def __init__(self, confidence_level: float = 0.95):
        self.confidence_level = confidence_level
        self.test_results = []

    def run_ab_test(
        self,
        model_a: Any,
        model_b: Any,
        X_test: np.ndarray,
        y_test: np.ndarray,
        model_a_name: str = "Model A",
        model_b_name: str = "Model B",
    ) -> Dict[str, Any]:
        """
        A/B í…ŒìŠ¤íŠ¸ ì‹¤í–‰

        Args:
            model_a: ëª¨ë¸ A
            model_b: ëª¨ë¸ B
            X_test: í…ŒìŠ¤íŠ¸ ë°ì´í„°
            y_test: í…ŒìŠ¤íŠ¸ ë¼ë²¨
            model_a_name: ëª¨ë¸ A ì´ë¦„
            model_b_name: ëª¨ë¸ B ì´ë¦„

        Returns:
            Dict: A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼
        """

        logger.info(f"A/B í…ŒìŠ¤íŠ¸ ì‹œì‘: {model_a_name} vs {model_b_name}")

        # ëª¨ë¸ A ì„±ëŠ¥ ì¸¡ì •
        metrics_a = self._measure_model_performance(model_a, X_test, y_test)

        # ëª¨ë¸ B ì„±ëŠ¥ ì¸¡ì •
        metrics_b = self._measure_model_performance(model_b, X_test, y_test)

        # í†µê³„ì  ìœ ì˜ì„± ê²€ì •
        significance_test = self._statistical_significance_test(
            metrics_a, metrics_b, len(X_test)
        )

        # ê²°ê³¼ ì •ë¦¬
        ab_result = {
            "model_a_name": model_a_name,
            "model_b_name": model_b_name,
            "model_a_metrics": metrics_a.to_dict(),
            "model_b_metrics": metrics_b.to_dict(),
            "winner": self._determine_winner(metrics_a, metrics_b),
            "performance_improvement": self._calculate_improvement(
                metrics_a, metrics_b
            ),
            "statistical_significance": significance_test,
            "recommendation": self._generate_recommendation(
                metrics_a, metrics_b, significance_test
            ),
        }

        self.test_results.append(ab_result)
        logger.info(f"A/B í…ŒìŠ¤íŠ¸ ì™„ë£Œ: ìŠ¹ì = {ab_result['winner']}")

        return ab_result

    def _measure_model_performance(
        self, model: Any, X: np.ndarray, y: np.ndarray
    ) -> PerformanceMetrics:
        """ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •"""

        # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘
        monitor = SystemResourceMonitor()
        monitor.start_monitoring()

        # ì„±ëŠ¥ ì¸¡ì • ì‹œì‘
        start_time = time.time()

        try:
            # ì˜ˆì¸¡ ìˆ˜í–‰
            if hasattr(model, "predict_proba"):
                y_pred_proba = model.predict_proba(X)
                y_pred = np.argmax(y_pred_proba, axis=1)
            else:
                y_pred = model.predict(X)
                y_pred_proba = None

            # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            processing_time = time.time() - start_time

            # ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ì¸¡ì •
            resource_usage = monitor.stop_monitoring()

            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
            accuracy = accuracy_score(y, y_pred)
            precision = precision_score(y, y_pred, average="weighted", zero_division=0)
            recall = recall_score(y, y_pred, average="weighted", zero_division=0)
            f1 = f1_score(y, y_pred, average="weighted", zero_division=0)

            # AUC ê³„ì‚° (ì´ì§„ ë¶„ë¥˜ ë˜ëŠ” í™•ë¥  ì˜ˆì¸¡ì´ ê°€ëŠ¥í•œ ê²½ìš°)
            auc = 0.0
            if y_pred_proba is not None:
                try:
                    if len(np.unique(y)) == 2:
                        auc = roc_auc_score(y, y_pred_proba[:, 1])
                    else:
                        auc = roc_auc_score(
                            y, y_pred_proba, multi_class="ovr", average="weighted"
                        )
                except Exception:
                    auc = 0.0

            # ì²˜ë¦¬ëŸ‰ ê³„ì‚°
            throughput = len(X) / processing_time if processing_time > 0 else 0

            metrics = PerformanceMetrics(
                accuracy=accuracy,
                precision=precision,
                recall=recall,
                f1_score=f1,
                auc_score=auc,
                processing_time=processing_time,
                memory_usage=resource_usage["max_memory_usage"],
                gpu_utilization=resource_usage["max_gpu_usage"],
                throughput=throughput,
            )

        except Exception as e:
            logger.error(f"ì„±ëŠ¥ ì¸¡ì • ì¤‘ ì˜¤ë¥˜: {e}")
            monitor.stop_monitoring()
            metrics = PerformanceMetrics()

        return metrics

    def _statistical_significance_test(
        self,
        metrics_a: PerformanceMetrics,
        metrics_b: PerformanceMetrics,
        sample_size: int,
    ) -> Dict[str, Any]:
        """í†µê³„ì  ìœ ì˜ì„± ê²€ì •"""

        # ê°„ë‹¨í•œ z-test ê·¼ì‚¬ (ì •í™•í•œ ê²€ì •ì„ ìœ„í•´ì„œëŠ” ë” ì •êµí•œ ë°©ë²• í•„ìš”)
        accuracy_diff = metrics_b.accuracy - metrics_a.accuracy

        # í‘œì¤€ ì˜¤ì°¨ ì¶”ì •
        p_pooled = (metrics_a.accuracy + metrics_b.accuracy) / 2
        se = np.sqrt(2 * p_pooled * (1 - p_pooled) / sample_size)

        # Z ì ìˆ˜ ê³„ì‚°
        z_score = accuracy_diff / se if se > 0 else 0

        # P-value ê·¼ì‚¬ (ì–‘ì¸¡ ê²€ì •)
        from scipy import stats

        p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))

        is_significant = p_value < (1 - self.confidence_level)

        return {
            "z_score": z_score,
            "p_value": p_value,
            "is_significant": is_significant,
            "confidence_level": self.confidence_level,
        }

    def _determine_winner(
        self, metrics_a: PerformanceMetrics, metrics_b: PerformanceMetrics
    ) -> str:
        """ìŠ¹ì ê²°ì •"""

        # ì¢…í•© ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ í‰ê· )
        score_a = (
            0.4 * metrics_a.accuracy
            + 0.2 * metrics_a.f1_score
            + 0.2 * (1 / (1 + metrics_a.processing_time))  # ì²˜ë¦¬ ì‹œê°„ ì—­ìˆ˜
            + 0.2 * (metrics_a.throughput / 1000)  # ì²˜ë¦¬ëŸ‰ ì •ê·œí™”
        )

        score_b = (
            0.4 * metrics_b.accuracy
            + 0.2 * metrics_b.f1_score
            + 0.2 * (1 / (1 + metrics_b.processing_time))
            + 0.2 * (metrics_b.throughput / 1000)
        )

        if score_b > score_a:
            return "Model B"
        elif score_a > score_b:
            return "Model A"
        else:
            return "Tie"

    def _calculate_improvement(
        self, metrics_a: PerformanceMetrics, metrics_b: PerformanceMetrics
    ) -> Dict[str, float]:
        """ì„±ëŠ¥ ê°œì„ ë¥  ê³„ì‚°"""

        def safe_improvement(old_val, new_val):
            if old_val == 0:
                return 0.0
            return ((new_val - old_val) / old_val) * 100

        return {
            "accuracy_improvement": safe_improvement(
                metrics_a.accuracy, metrics_b.accuracy
            ),
            "f1_improvement": safe_improvement(metrics_a.f1_score, metrics_b.f1_score),
            "speed_improvement": safe_improvement(
                1 / metrics_a.processing_time, 1 / metrics_b.processing_time
            ),
            "throughput_improvement": safe_improvement(
                metrics_a.throughput, metrics_b.throughput
            ),
        }

    def _generate_recommendation(
        self,
        metrics_a: PerformanceMetrics,
        metrics_b: PerformanceMetrics,
        significance_test: Dict[str, Any],
    ) -> str:
        """ê¶Œì¥ì‚¬í•­ ìƒì„±"""

        if not significance_test["is_significant"]:
            return "í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ì°¨ì´ê°€ ì—†ìŠµë‹ˆë‹¤. ì¶”ê°€ ë°ì´í„°ë¡œ ì¬í…ŒìŠ¤íŠ¸ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤."

        winner = self._determine_winner(metrics_a, metrics_b)

        if winner == "Model B":
            return "Model Bê°€ ë” ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. Model B ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤."
        elif winner == "Model A":
            return "Model Aê°€ ë” ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. Model A ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤."
        else:
            return "ë‘ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ë¹„ìŠ·í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ìš”ì†Œ(ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰, ì²˜ë¦¬ ì‹œê°„ ë“±)ë¥¼ ê³ ë ¤í•˜ì—¬ ì„ íƒí•˜ì„¸ìš”."


class ModelPerformanceBenchmark:
    """ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ ì‹œìŠ¤í…œ"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config if config else load_config()
        self.output_dir = Path(
            self.config.get_nested("data.result.performance_reports", "data/result/performance_reports")
        )
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.logger = get_logger(__name__)

        # ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥
        self.benchmark_results = []
        self.comparison_results = []

        # A/B í…ŒìŠ¤í„° ì´ˆê¸°í™”
        self.ab_tester = ABTester(confidence_level=self.config.get("confidence_level", 0.95))

        # ì„±ëŠ¥ ì„ê³„ê°’ ì„¤ì •
        self.performance_thresholds = {
            "accuracy_threshold": self.config.get("accuracy_threshold", 0.7),
            "processing_time_threshold": self.config.get("processing_time_threshold", 30.0),
            "memory_threshold": self.config.get("memory_threshold", 80.0),
            "throughput_threshold": self.config.get("throughput_threshold", 100.0),
        }

    def benchmark_preprocessing(
        self, X: np.ndarray, y: np.ndarray, model_type: str, preprocessor: Any
    ) -> Dict[str, Any]:
        """
        ì „ì²˜ë¦¬ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

        Args:
            X: ì…ë ¥ ë°ì´í„°
            y: íƒ€ê²Ÿ ë°ì´í„°
            model_type: ëª¨ë¸ íƒ€ì…
            preprocessor: ì „ì²˜ë¦¬ê¸°

        Returns:
            Dict: ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼
        """

        self.logger.info(f"{model_type} ì „ì²˜ë¦¬ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")

        # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§
        monitor = SystemResourceMonitor()
        monitor.start_monitoring()

        # ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •
        start_time = time.time()

        try:
            # ì „ì²˜ë¦¬ ìˆ˜í–‰
            if (
                hasattr(preprocessor, "preprocess_for_lightgbm")
                and model_type == "lightgbm"
            ):
                X_processed, metadata = preprocessor.preprocess_for_lightgbm(X, y)
            elif (
                hasattr(preprocessor, "preprocess_for_autoencoder")
                and model_type == "autoencoder"
            ):
                X_processed, metadata = preprocessor.preprocess_for_autoencoder(X, y)
            elif hasattr(preprocessor, "preprocess_for_tcn") and model_type == "tcn":
                X_processed, metadata = preprocessor.preprocess_for_tcn(X, y)
            else:
                X_processed = X
                metadata = {}

            processing_time = time.time() - start_time

            # ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ì¸¡ì •
            resource_usage = monitor.stop_monitoring()

            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
            throughput = len(X) / processing_time if processing_time > 0 else 0

            benchmark_result = {
                "model_type": model_type,
                "processing_speed": throughput,
                "memory_usage": resource_usage["max_memory_usage"],
                "gpu_utilization": resource_usage["max_gpu_usage"],
                "processing_time": processing_time,
                "input_shape": X.shape,
                "output_shape": (
                    X_processed.shape if hasattr(X_processed, "shape") else None
                ),
                "metadata": metadata,
                "performance_gain": self._calculate_preprocessing_gain(
                    X, X_processed, metadata
                ),
            }

            self.logger.info(f"{model_type} ì „ì²˜ë¦¬ ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ")

        except Exception as e:
            self.logger.error(f"ì „ì²˜ë¦¬ ë²¤ì¹˜ë§ˆí¬ ì¤‘ ì˜¤ë¥˜: {e}")
            monitor.stop_monitoring()
            benchmark_result = {
                "model_type": model_type,
                "error": str(e),
                "processing_speed": 0,
                "memory_usage": 0,
                "gpu_utilization": 0,
                "processing_time": 0,
            }

        return benchmark_result

    def _calculate_preprocessing_gain(
        self, X_original: np.ndarray, X_processed: Any, metadata: Dict[str, Any]
    ) -> Dict[str, float]:
        """ì „ì²˜ë¦¬ ì„±ëŠ¥ í–¥ìƒ ê³„ì‚°"""

        gain = {}

        # ì°¨ì› ì¶•ì†Œ íš¨ê³¼
        if hasattr(X_processed, "shape"):
            if len(X_original.shape) == len(X_processed.shape):
                compression_ratio = X_original.shape[1] / X_processed.shape[1]
                gain["compression_ratio"] = compression_ratio
            else:
                gain["compression_ratio"] = 1.0

        # ë©”íƒ€ë°ì´í„°ì—ì„œ ì„±ëŠ¥ í–¥ìƒ ì •ë³´ ì¶”ì¶œ
        if "compression_ratio" in metadata:
            gain["metadata_compression"] = metadata["compression_ratio"]

        if "reconstruction_loss" in metadata:
            gain["reconstruction_quality"] = 1.0 / (
                1.0 + metadata["reconstruction_loss"]
            )

        return gain

    def run_comprehensive_benchmark(
        self,
        models: Dict[str, Any],
        X_test: np.ndarray,
        y_test: np.ndarray,
        cross_validation: bool = True,
    ) -> Dict[str, Any]:
        """
        ì¢…í•© ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰

        Args:
            models: ëª¨ë¸ ë”•ì…”ë„ˆë¦¬
            X_test: í…ŒìŠ¤íŠ¸ ë°ì´í„°
            y_test: í…ŒìŠ¤íŠ¸ ë¼ë²¨
            cross_validation: êµì°¨ ê²€ì¦ ì—¬ë¶€

        Returns:
            Dict: ì¢…í•© ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼
        """

        self.logger.info("ì¢…í•© ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")

        benchmark_results = {}

        # ê° ëª¨ë¸ë³„ ë²¤ì¹˜ë§ˆí¬
        for model_name, model in models.items():
            self.logger.info(f"{model_name} ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰")

            # ë‹¨ì¼ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬
            model_result = self._benchmark_single_model(
                model, X_test, y_test, model_name
            )

            # êµì°¨ ê²€ì¦ (ì˜µì…˜)
            if cross_validation:
                cv_result = self._cross_validation_benchmark(model, X_test, y_test)
                model_result["cross_validation"] = cv_result

            benchmark_results[model_name] = model_result

        # ëª¨ë¸ ê°„ ë¹„êµ
        comparison_result = self._compare_models(benchmark_results)

        # ì¢…í•© ê²°ê³¼
        comprehensive_result = {
            "individual_results": benchmark_results,
            "comparison": comparison_result,
            "recommendations": self._generate_model_recommendations(benchmark_results),
            "performance_summary": self._generate_performance_summary(
                benchmark_results
            ),
        }

        self.logger.info("ì¢…í•© ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ")

        return comprehensive_result

    def _benchmark_single_model(
        self, model: Any, X: np.ndarray, y: np.ndarray, model_name: str
    ) -> Dict[str, Any]:
        """ë‹¨ì¼ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬"""

        # ì„±ëŠ¥ ì¸¡ì •
        metrics = self.ab_tester._measure_model_performance(model, X, y)

        # ì„±ëŠ¥ ì„ê³„ê°’ ê²€ì‚¬
        performance_check = self._check_performance_thresholds(metrics)

        # ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìƒì„±
        result = BenchmarkResult(
            model_name=model_name,
            dataset_name="test_dataset",
            timestamp=datetime.now().isoformat(),
            metrics=metrics,
            configuration={},
        )

        self.benchmark_results.append(result)

        return {
            "metrics": metrics.to_dict(),
            "performance_check": performance_check,
            "benchmark_result": result.to_dict(),
        }

    def _cross_validation_benchmark(
        self, model: Any, X: np.ndarray, y: np.ndarray, cv_folds: int = 5
    ) -> Dict[str, Any]:
        """êµì°¨ ê²€ì¦ ë²¤ì¹˜ë§ˆí¬"""

        try:
            # êµì°¨ ê²€ì¦ ìˆ˜í–‰
            cv_scores = cross_val_score(
                model,
                X,
                y,
                cv=StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42),
                scoring="accuracy",
            )

            cv_result = {
                "cv_scores": cv_scores.tolist(),
                "mean_score": np.mean(cv_scores),
                "std_score": np.std(cv_scores),
                "confidence_interval": [
                    np.mean(cv_scores) - 1.96 * np.std(cv_scores),
                    np.mean(cv_scores) + 1.96 * np.std(cv_scores),
                ],
            }

        except Exception as e:
            self.logger.warning(f"êµì°¨ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            cv_result = {
                "error": str(e),
                "cv_scores": [],
                "mean_score": 0.0,
                "std_score": 0.0,
            }

        return cv_result

    def _check_performance_thresholds(
        self, metrics: PerformanceMetrics
    ) -> Dict[str, bool]:
        """ì„±ëŠ¥ ì„ê³„ê°’ ê²€ì‚¬"""

        return {
            "accuracy_pass": metrics.accuracy
            >= self.performance_thresholds["accuracy_threshold"],
            "speed_pass": metrics.processing_time
            <= self.performance_thresholds["processing_time_threshold"],
            "memory_pass": metrics.memory_usage
            <= self.performance_thresholds["memory_threshold"],
            "throughput_pass": metrics.throughput
            >= self.performance_thresholds["throughput_threshold"],
        }

    def _compare_models(self, benchmark_results: Dict[str, Any]) -> Dict[str, Any]:
        """ëª¨ë¸ ê°„ ë¹„êµ"""

        comparison = {
            "best_accuracy": "",
            "best_speed": "",
            "best_memory_efficiency": "",
            "best_overall": "",
        }

        best_accuracy = 0
        best_speed = float("inf")
        best_memory = float("inf")
        best_overall_score = 0

        for model_name, result in benchmark_results.items():
            metrics = result["metrics"]

            # ìµœê³  ì •í™•ë„
            if metrics["accuracy"] > best_accuracy:
                best_accuracy = metrics["accuracy"]
                comparison["best_accuracy"] = model_name

            # ìµœê³  ì†ë„
            if metrics["processing_time"] < best_speed:
                best_speed = metrics["processing_time"]
                comparison["best_speed"] = model_name

            # ìµœê³  ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
            if metrics["memory_usage"] < best_memory:
                best_memory = metrics["memory_usage"]
                comparison["best_memory_efficiency"] = model_name

            # ì¢…í•© ì ìˆ˜
            overall_score = (
                0.4 * metrics["accuracy"]
                + 0.2 * metrics["f1_score"]
                + 0.2 * (1 / (1 + metrics["processing_time"]))
                + 0.2 * (metrics["throughput"] / 1000)
            )

            if overall_score > best_overall_score:
                best_overall_score = overall_score
                comparison["best_overall"] = model_name

        return comparison

    def _generate_model_recommendations(
        self, benchmark_results: Dict[str, Any]
    ) -> List[str]:
        """ëª¨ë¸ ê¶Œì¥ì‚¬í•­ ìƒì„±"""

        recommendations = []

        for model_name, result in benchmark_results.items():
            performance_check = result["performance_check"]

            if not performance_check["accuracy_pass"]:
                recommendations.append(
                    f"{model_name}: ì •í™•ë„ ê°œì„  í•„ìš” (í˜„ì¬: {result['metrics']['accuracy']:.3f})"
                )

            if not performance_check["speed_pass"]:
                recommendations.append(
                    f"{model_name}: ì²˜ë¦¬ ì†ë„ ê°œì„  í•„ìš” (í˜„ì¬: {result['metrics']['processing_time']:.2f}ì´ˆ)"
                )

            if not performance_check["memory_pass"]:
                recommendations.append(
                    f"{model_name}: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™” í•„ìš” (í˜„ì¬: {result['metrics']['memory_usage']:.1f}%)"
                )

            if not performance_check["throughput_pass"]:
                recommendations.append(
                    f"{model_name}: ì²˜ë¦¬ëŸ‰ ê°œì„  í•„ìš” (í˜„ì¬: {result['metrics']['throughput']:.1f} samples/sec)"
                )

        if not recommendations:
            recommendations.append("ëª¨ë“  ëª¨ë¸ì´ ì„±ëŠ¥ ì„ê³„ê°’ì„ ë§Œì¡±í•©ë‹ˆë‹¤.")

        return recommendations

    def _generate_performance_summary(
        self, benchmark_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ì„±ëŠ¥ ìš”ì•½ ìƒì„±"""

        all_accuracies = [
            result["metrics"]["accuracy"] for result in benchmark_results.values()
        ]
        all_times = [
            result["metrics"]["processing_time"]
            for result in benchmark_results.values()
        ]
        all_memories = [
            result["metrics"]["memory_usage"] for result in benchmark_results.values()
        ]

        return {
            "average_accuracy": np.mean(all_accuracies),
            "accuracy_std": np.std(all_accuracies),
            "average_processing_time": np.mean(all_times),
            "time_std": np.std(all_times),
            "average_memory_usage": np.mean(all_memories),
            "memory_std": np.std(all_memories),
            "total_models_tested": len(benchmark_results),
        }

    def save_benchmark_results(self, filepath: str):
        """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥"""

        results_data = {
            "benchmark_results": [
                result.to_dict() for result in self.benchmark_results
            ],
            "ab_test_results": self.ab_tester.test_results,
            "timestamp": datetime.now().isoformat(),
        }

        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)

        self.logger.info(f"ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {filepath}")

    def load_benchmark_results(self, filepath: str):
        """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë¡œë“œ"""

        with open(filepath, "r", encoding="utf-8") as f:
            results_data = json.load(f)

        # ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë³µì›
        self.benchmark_results = []
        for result_dict in results_data["benchmark_results"]:
            metrics = PerformanceMetrics(**result_dict["metrics"])
            result = BenchmarkResult(
                model_name=result_dict["model_name"],
                dataset_name=result_dict["dataset_name"],
                timestamp=result_dict["timestamp"],
                metrics=metrics,
                configuration=result_dict["configuration"],
            )
            self.benchmark_results.append(result)

        # A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë³µì›
        self.ab_tester.test_results = results_data["ab_test_results"]

        self.logger.info(f"ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë¡œë“œ ì™„ë£Œ: {filepath}")

    def print_benchmark_summary(self, comprehensive_result: Dict[str, Any]):
        """ë²¤ì¹˜ë§ˆí¬ ìš”ì•½ ì¶œë ¥"""

        print("=" * 80)
        print("ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼")
        print("=" * 80)

        # ê°œë³„ ëª¨ë¸ ê²°ê³¼
        print("\nğŸ” ê°œë³„ ëª¨ë¸ ì„±ëŠ¥:")
        for model_name, result in comprehensive_result["individual_results"].items():
            metrics = result["metrics"]
            print(f"\n  ğŸ“ˆ {model_name}:")
            print(f"    â€¢ ì •í™•ë„: {metrics['accuracy']:.4f}")
            print(f"    â€¢ F1 ì ìˆ˜: {metrics['f1_score']:.4f}")
            print(f"    â€¢ ì²˜ë¦¬ ì‹œê°„: {metrics['processing_time']:.2f}ì´ˆ")
            print(f"    â€¢ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {metrics['memory_usage']:.1f}%")
            print(f"    â€¢ ì²˜ë¦¬ëŸ‰: {metrics['throughput']:.1f} samples/sec")

        # ëª¨ë¸ ë¹„êµ
        print(f"\nğŸ† ëª¨ë¸ ë¹„êµ:")
        comparison = comprehensive_result["comparison"]
        print(f"  â€¢ ìµœê³  ì •í™•ë„: {comparison['best_accuracy']}")
        print(f"  â€¢ ìµœê³  ì†ë„: {comparison['best_speed']}")
        print(f"  â€¢ ìµœê³  ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±: {comparison['best_memory_efficiency']}")
        print(f"  â€¢ ì¢…í•© ìµœê³  ì„±ëŠ¥: {comparison['best_overall']}")

        # ì„±ëŠ¥ ìš”ì•½
        print(f"\nğŸ“Š ì„±ëŠ¥ ìš”ì•½:")
        summary = comprehensive_result["performance_summary"]
        print(
            f"  â€¢ í‰ê·  ì •í™•ë„: {summary['average_accuracy']:.4f} (Â±{summary['accuracy_std']:.4f})"
        )
        print(
            f"  â€¢ í‰ê·  ì²˜ë¦¬ ì‹œê°„: {summary['average_processing_time']:.2f}ì´ˆ (Â±{summary['time_std']:.2f})"
        )
        print(
            f"  â€¢ í‰ê·  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {summary['average_memory_usage']:.1f}% (Â±{summary['memory_std']:.1f})"
        )

        # ê¶Œì¥ì‚¬í•­
        print(f"\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
        for i, recommendation in enumerate(comprehensive_result["recommendations"], 1):
            print(f"  {i}. {recommendation}")

        print("=" * 80)

    def run_all_benchmarks(self) -> Dict[str, ModelPerformanceMetrics]:
        """ëª¨ë“  ëª¨ë¸ì— ëŒ€í•œ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        # ... existing code ...
