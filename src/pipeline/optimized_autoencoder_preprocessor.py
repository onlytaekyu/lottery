"""
Optimized AutoEncoder Preprocessor
AutoEncoder ìµœì í™” ì „ì²˜ë¦¬ê¸° - VAE, Progressive Training, Denoising ê°•í™”
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from sklearn.preprocessing import StandardScaler
import warnings

warnings.filterwarnings("ignore")

from ..utils.unified_logging import get_logger
from ..shared.types import PreprocessedData

logger = get_logger(__name__)


@dataclass
class VAEConfig:
    """VAE ì„¤ì •"""

    input_dim: int = 168
    latent_dim: int = 64
    hidden_dims: List[int] = None
    beta: float = 1.0  # KL divergence weight
    learning_rate: float = 0.001
    batch_size: int = 128
    epochs: int = 100
    device: str = "cuda"


class VariationalAutoEncoder(nn.Module):
    """Variational AutoEncoder"""

    def __init__(self, config: VAEConfig):
        super().__init__()
        self.config = config
        self.input_dim = config.input_dim
        self.latent_dim = config.latent_dim
        self.hidden_dims = config.hidden_dims or [128, 100]

        # Encoder
        encoder_layers = []
        prev_dim = self.input_dim

        for hidden_dim in self.hidden_dims:
            encoder_layers.extend(
                [
                    nn.Linear(prev_dim, hidden_dim),
                    nn.BatchNorm1d(hidden_dim),
                    nn.ReLU(),
                    nn.Dropout(0.2),
                ]
            )
            prev_dim = hidden_dim

        self.encoder = nn.Sequential(*encoder_layers)

        # Latent space
        self.fc_mu = nn.Linear(prev_dim, self.latent_dim)
        self.fc_var = nn.Linear(prev_dim, self.latent_dim)

        # Decoder
        decoder_layers = []
        prev_dim = self.latent_dim

        for hidden_dim in reversed(self.hidden_dims):
            decoder_layers.extend(
                [
                    nn.Linear(prev_dim, hidden_dim),
                    nn.BatchNorm1d(hidden_dim),
                    nn.ReLU(),
                    nn.Dropout(0.2),
                ]
            )
            prev_dim = hidden_dim

        decoder_layers.append(nn.Linear(prev_dim, self.input_dim))
        self.decoder = nn.Sequential(*decoder_layers)

    def encode(self, x):
        """ì¸ì½”ë”©"""
        h = self.encoder(x)
        mu = self.fc_mu(h)
        log_var = self.fc_var(h)
        return mu, log_var

    def reparameterize(self, mu, log_var):
        """ì¬ë§¤ê°œí™” íŠ¸ë¦­"""
        std = torch.exp(0.5 * log_var)
        eps = torch.randn_like(std)
        return mu + eps * std

    def decode(self, z):
        """ë””ì½”ë”©"""
        return self.decoder(z)

    def forward(self, x):
        """ìˆœì „íŒŒ"""
        mu, log_var = self.encode(x)
        z = self.reparameterize(mu, log_var)
        recon_x = self.decode(z)
        return recon_x, mu, log_var

    def loss_function(self, recon_x, x, mu, log_var, beta=1.0):
        """VAE ì†ì‹¤ í•¨ìˆ˜"""
        # Reconstruction loss
        recon_loss = nn.functional.mse_loss(recon_x, x, reduction="sum")

        # KL divergence loss
        kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())

        return recon_loss + beta * kl_loss, recon_loss, kl_loss


class ProgressiveTrainingScheduler:
    """Progressive Training ìŠ¤ì¼€ì¤„ëŸ¬"""

    def __init__(self, stages: List[int] = None):
        self.stages = stages or [168, 128, 100, 64]
        self.current_stage = 0
        self.stage_epochs = []

    def get_current_stage_config(self, total_epochs: int) -> Tuple[int, int]:
        """í˜„ì¬ ë‹¨ê³„ ì„¤ì • ë°˜í™˜"""
        epochs_per_stage = total_epochs // len(self.stages)
        current_dim = self.stages[self.current_stage]
        return current_dim, epochs_per_stage

    def next_stage(self):
        """ë‹¤ìŒ ë‹¨ê³„ë¡œ ì´ë™"""
        if self.current_stage < len(self.stages) - 1:
            self.current_stage += 1
            return True
        return False

    def is_final_stage(self) -> bool:
        """ìµœì¢… ë‹¨ê³„ ì—¬ë¶€"""
        return self.current_stage == len(self.stages) - 1


class AdaptiveNoiseInjection:
    """ì ì‘í˜• ë…¸ì´ì¦ˆ ì£¼ì…"""

    def __init__(self, noise_ratio: float = 0.1, adaptive_scaling: bool = True):
        self.noise_ratio = noise_ratio
        self.adaptive_scaling = adaptive_scaling
        self.current_noise_level = noise_ratio

    def inject_noise(
        self, x: torch.Tensor, epoch: int = 0, max_epochs: int = 100
    ) -> torch.Tensor:
        """ë…¸ì´ì¦ˆ ì£¼ì…"""
        if self.adaptive_scaling:
            # í•™ìŠµ ì§„í–‰ì— ë”°ë¼ ë…¸ì´ì¦ˆ ë ˆë²¨ ê°ì†Œ
            decay_factor = 1 - (epoch / max_epochs)
            self.current_noise_level = self.noise_ratio * decay_factor

        noise = torch.randn_like(x) * self.current_noise_level
        return x + noise

    def get_current_noise_level(self) -> float:
        """í˜„ì¬ ë…¸ì´ì¦ˆ ë ˆë²¨ ë°˜í™˜"""
        return self.current_noise_level


class OptimizedAutoEncoderPreprocessor:
    """ìµœì í™”ëœ AutoEncoder ì „ì²˜ë¦¬ê¸°"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = get_logger(__name__)

        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = torch.device(
            "cuda" if torch.cuda.is_available() else "cpu"
        )

        # VAE ì„¤ì •
        self.vae_config = VAEConfig(
            input_dim=config.get("input_dim", 168),
            latent_dim=config.get("latent_dim", 64),
            hidden_dims=config.get("hidden_dims", [128, 100]),
            beta=config.get("beta", 1.0),
            learning_rate=config.get("learning_rate", 0.001),
            batch_size=config.get("batch_size", 128),
            epochs=config.get("epochs", 100),
            device=str(self.device),
        )

        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.vae = None
        self.progressive_scheduler = ProgressiveTrainingScheduler(
            stages=config.get("progressive_stages", [168, 128, 100, 64])
        )
        self.noise_injection = AdaptiveNoiseInjection(
            noise_ratio=config.get("noise_ratio", 0.1),
            adaptive_scaling=config.get("adaptive_scaling", True),
        )

        # ìŠ¤ì¼€ì¼ëŸ¬
        self.scaler = StandardScaler()
        self.fitted = False

    def preprocess_for_autoencoder(
        self, X: np.ndarray, y: Optional[np.ndarray] = None
    ) -> Tuple[np.ndarray, Dict[str, Any]]:
        """
        AutoEncoder ìµœì í™” ì „ì²˜ë¦¬

        Args:
            X: ì…ë ¥ ë°ì´í„°
            y: íƒ€ê²Ÿ ë°ì´í„° (ì„ íƒì‚¬í•­)

        Returns:
            Tuple[np.ndarray, Dict]: ì „ì²˜ë¦¬ëœ ë°ì´í„°ì™€ ë©”íƒ€ë°ì´í„°
        """

        try:
            self.logger.info("AutoEncoder ìµœì í™” ì „ì²˜ë¦¬ ì‹œì‘")

            with self.memory_manager.get_context("autoencoder_preprocessing"):
                # 1. ë°ì´í„° ì •ê·œí™”
                self.logger.info("ë°ì´í„° ì •ê·œí™”...")
                X_scaled = self.scaler.fit_transform(X)

                # 2. VAE í•™ìŠµ
                self.logger.info("VAE í•™ìŠµ...")
                self.vae = self._train_vae(X_scaled)

                # 3. ì ì¬ í‘œí˜„ ì¶”ì¶œ
                self.logger.info("ì ì¬ í‘œí˜„ ì¶”ì¶œ...")
                X_encoded = self._extract_latent_representation(X_scaled)

                # 4. ì¬êµ¬ì„± í’ˆì§ˆ í‰ê°€
                reconstruction_quality = self._evaluate_reconstruction_quality(X_scaled)

                # ë©”íƒ€ë°ì´í„° ìƒì„±
                metadata = {
                    "latent_dim": self.vae_config.latent_dim,
                    "reconstruction_loss": reconstruction_quality[
                        "reconstruction_loss"
                    ],
                    "kl_divergence": reconstruction_quality["kl_divergence"],
                    "total_loss": reconstruction_quality["total_loss"],
                    "compression_ratio": X.shape[1] / X_encoded.shape[1],
                }

                self.fitted = True
                self.logger.info(
                    f"AutoEncoder ì „ì²˜ë¦¬ ì™„ë£Œ - ì••ì¶•ë¥ : {metadata['compression_ratio']:.2f}x"
                )

                return X_encoded, metadata

        except Exception as e:
            self.logger.error(f"AutoEncoder ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            raise

    def _train_vae(self, X: np.ndarray) -> VariationalAutoEncoder:
        """VAE í•™ìŠµ"""

        # VAE ëª¨ë¸ ì´ˆê¸°í™”
        vae = VariationalAutoEncoder(self.vae_config).to(self.device)
        optimizer = optim.Adam(vae.parameters(), lr=self.vae_config.learning_rate)

        # ë°ì´í„° ë¡œë” ìƒì„±
        X_tensor = torch.FloatTensor(X).to(self.device)
        dataset = TensorDataset(X_tensor)
        dataloader = DataLoader(
            dataset, batch_size=self.vae_config.batch_size, shuffle=True
        )

        # Progressive Training
        if self.progressive_scheduler:
            return self._progressive_train(vae, optimizer, dataloader)
        else:
            return self._standard_train(vae, optimizer, dataloader)

    def _progressive_train(
        self, vae: VariationalAutoEncoder, optimizer, dataloader
    ) -> VariationalAutoEncoder:
        """Progressive Training ì‹¤í–‰"""

        self.logger.info("Progressive Training ì‹œì‘")

        # ê° ë‹¨ê³„ë³„ í•™ìŠµ
        for stage in range(len(self.progressive_scheduler.stages)):
            current_dim, stage_epochs = (
                self.progressive_scheduler.get_current_stage_config(
                    self.vae_config.epochs
                )
            )

            self.logger.info(
                f"Stage {stage + 1}: ì°¨ì› {current_dim}, ì—í¬í¬ {stage_epochs}"
            )

            # ë‹¨ê³„ë³„ VAE ì„¤ì • ì¡°ì •
            if stage > 0:
                vae = self._adjust_vae_for_stage(vae, current_dim)

            # ë‹¨ê³„ë³„ í•™ìŠµ
            for epoch in range(stage_epochs):
                total_loss = 0
                vae.train()

                for batch_idx, (data,) in enumerate(dataloader):
                    optimizer.zero_grad()

                    # ë…¸ì´ì¦ˆ ì£¼ì… (Denoising)
                    noisy_data = self.noise_injection.inject_noise(
                        data, epoch, stage_epochs
                    )

                    # ìˆœì „íŒŒ
                    recon_batch, mu, log_var = vae(noisy_data)

                    # ì†ì‹¤ ê³„ì‚°
                    loss, recon_loss, kl_loss = vae.loss_function(
                        recon_batch, data, mu, log_var, self.vae_config.beta
                    )

                    # ì—­ì „íŒŒ
                    loss.backward()
                    optimizer.step()

                    total_loss += loss.item()

                # ì—í¬í¬ë³„ ë¡œê·¸
                if epoch % 10 == 0:
                    avg_loss = total_loss / len(dataloader)
                    self.logger.info(
                        f"Stage {stage + 1}, Epoch {epoch}: Loss = {avg_loss:.4f}"
                    )

            # ë‹¤ìŒ ë‹¨ê³„ë¡œ ì´ë™
            self.progressive_scheduler.next_stage()

        return vae

    def _standard_train(
        self, vae: VariationalAutoEncoder, optimizer, dataloader
    ) -> VariationalAutoEncoder:
        """í‘œì¤€ í•™ìŠµ ì‹¤í–‰"""

        self.logger.info("í‘œì¤€ VAE í•™ìŠµ ì‹œì‘")

        for epoch in range(self.vae_config.epochs):
            total_loss = 0
            vae.train()

            for batch_idx, (data,) in enumerate(dataloader):
                optimizer.zero_grad()

                # ë…¸ì´ì¦ˆ ì£¼ì…
                noisy_data = self.noise_injection.inject_noise(
                    data, epoch, self.vae_config.epochs
                )

                # ìˆœì „íŒŒ
                recon_batch, mu, log_var = vae(noisy_data)

                # ì†ì‹¤ ê³„ì‚°
                loss, recon_loss, kl_loss = vae.loss_function(
                    recon_batch, data, mu, log_var, self.vae_config.beta
                )

                # ì—­ì „íŒŒ
                loss.backward()
                optimizer.step()

                total_loss += loss.item()

            # ì—í¬í¬ë³„ ë¡œê·¸
            if epoch % 20 == 0:
                avg_loss = total_loss / len(dataloader)
                self.logger.info(f"Epoch {epoch}: Loss = {avg_loss:.4f}")

        return vae

    def _adjust_vae_for_stage(
        self, vae: VariationalAutoEncoder, target_dim: int
    ) -> VariationalAutoEncoder:
        """ë‹¨ê³„ë³„ VAE ì¡°ì •"""

        # í˜„ì¬ ë‹¨ê³„ì—ì„œëŠ” ê¸°ë³¸ VAE êµ¬ì¡° ìœ ì§€
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì •í•  ìˆ˜ ìˆìŒ
        return vae

    def _extract_latent_representation(self, X: np.ndarray) -> np.ndarray:
        """ì ì¬ í‘œí˜„ ì¶”ì¶œ"""
        
        if self.vae is None:
            raise ValueError("VAE ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
        self.vae.eval()
        X_tensor = torch.FloatTensor(X).to(self.device)
        
        with torch.no_grad():
            mu, log_var = self.vae.encode(X_tensor)
            # í‰ê· ê°’ì„ ì ì¬ í‘œí˜„ìœ¼ë¡œ ì‚¬ìš©
            latent_representation = mu.cpu().numpy()
        
        return latent_representation

    def _evaluate_reconstruction_quality(self, X: np.ndarray) -> Dict[str, float]:
        """ì¬êµ¬ì„± í’ˆì§ˆ í‰ê°€"""

        if self.vae is None:
            raise ValueError("VAE ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
        self.vae.eval()
        X_tensor = torch.FloatTensor(X).to(self.device)

        with torch.no_grad():
            recon_x, mu, log_var = self.vae(X_tensor)
            loss, recon_loss, kl_loss = self.vae.loss_function(
                recon_x, X_tensor, mu, log_var, self.vae_config.beta
            )

        return {
            "total_loss": loss.item() / len(X),
            "reconstruction_loss": recon_loss.item() / len(X),
            "kl_divergence": kl_loss.item() / len(X),
        }

    def transform_new_data(self, X: np.ndarray) -> np.ndarray:
        """ìƒˆë¡œìš´ ë°ì´í„° ë³€í™˜"""

        if not self.fitted:
            raise ValueError("ë¨¼ì € ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤.")

        # ì •ê·œí™”
        X_scaled = self.scaler.transform(X)

        # ì ì¬ í‘œí˜„ ì¶”ì¶œ
        X_encoded = self._extract_latent_representation(X_scaled)

        return X_encoded

    def reconstruct_data(self, X_encoded: np.ndarray) -> np.ndarray:
        """ë°ì´í„° ì¬êµ¬ì„±"""

        if not self.fitted:
            raise ValueError("ë¨¼ì € ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤.")

        self.vae.eval()
        X_tensor = torch.FloatTensor(X_encoded).to(self.device)

        with torch.no_grad():
            reconstructed = self.vae.decode(X_tensor)
            reconstructed = reconstructed.cpu().numpy()

        # ì—­ì •ê·œí™”
        reconstructed = self.scaler.inverse_transform(reconstructed)

        return reconstructed

    def get_anomaly_scores(self, X: np.ndarray) -> np.ndarray:
        """ì´ìƒê°’ ì ìˆ˜ ê³„ì‚°"""

        if not self.fitted:
            raise ValueError("ë¨¼ì € ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤.")

        # ì •ê·œí™”
        X_scaled = self.scaler.transform(X)

        # ì¬êµ¬ì„±
        X_encoded = self._extract_latent_representation(X_scaled)
        X_reconstructed = self.reconstruct_data(X_encoded)
        X_reconstructed_scaled = self.scaler.transform(X_reconstructed)

        # ì¬êµ¬ì„± ì˜¤ë¥˜ ê³„ì‚°
        reconstruction_errors = np.mean(
            (X_scaled - X_reconstructed_scaled) ** 2, axis=1
        )

        return reconstruction_errors

    def generate_synthetic_data(self, n_samples: int = 1000) -> np.ndarray:
        """í•©ì„± ë°ì´í„° ìƒì„±"""

        if not self.fitted:
            raise ValueError("ë¨¼ì € ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤.")

        self.vae.eval()

        # ì ì¬ ê³µê°„ì—ì„œ ìƒ˜í”Œë§
        z = torch.randn(n_samples, self.vae_config.latent_dim).to(self.device)

        with torch.no_grad():
            synthetic_data = self.vae.decode(z)
            synthetic_data = synthetic_data.cpu().numpy()

        # ì—­ì •ê·œí™”
        synthetic_data = self.scaler.inverse_transform(synthetic_data)

        return synthetic_data

    def print_optimization_summary(self, metadata: Dict[str, Any]):
        """ìµœì í™” ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""

        print("=" * 60)
        print("ğŸ§  AutoEncoder ìµœì í™” ê²°ê³¼")
        print("=" * 60)

        print(f"ğŸ“Š ëª¨ë¸ êµ¬ì„±:")
        print(f"  â€¢ ì…ë ¥ ì°¨ì›: {self.vae_config.input_dim}")
        print(f"  â€¢ ì ì¬ ì°¨ì›: {self.vae_config.latent_dim}")
        print(f"  â€¢ ì€ë‹‰ì¸µ: {self.vae_config.hidden_dims}")
        print(f"  â€¢ ì••ì¶•ë¥ : {metadata['compression_ratio']:.2f}x")

        print(f"\nğŸ“ˆ í•™ìŠµ ê²°ê³¼:")
        print(f"  â€¢ ì´ ì†ì‹¤: {metadata['total_loss']:.4f}")
        print(f"  â€¢ ì¬êµ¬ì„± ì†ì‹¤: {metadata['reconstruction_loss']:.4f}")
        print(f"  â€¢ KL ë°œì‚°: {metadata['kl_divergence']:.4f}")

        print(f"\nğŸ”§ ì ìš©ëœ ìµœì í™” ê¸°ë²•:")
        print(f"  â€¢ Variational AutoEncoder: í™•ë¥ ì  ì ì¬ í‘œí˜„")
        print(f"  â€¢ Progressive Training: {len(self.progressive_scheduler.stages)}ë‹¨ê³„")
        print(f"  â€¢ Denoising: ì ì‘í˜• ë…¸ì´ì¦ˆ ì£¼ì…")
        print(f"  â€¢ GPU ê°€ì†: {'í™œì„±í™”' if self.device.type == 'cuda' else 'ë¹„í™œì„±í™”'}")

        print("=" * 60)

    def preprocess(self, data: np.ndarray) -> PreprocessedData:
        """
        ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.

        Args:
            data: ì…ë ¥ ë°ì´í„°

        Returns:
            PreprocessedData: ì „ì²˜ë¦¬ëœ ë°ì´í„°ì™€ ë©”íƒ€ë°ì´í„°
        """

        try:
            self.logger.info("AutoEncoder ìµœì í™” ì „ì²˜ë¦¬ ì‹œì‘")

            with self.memory_manager.get_context("autoencoder_preprocessing"):
                # 1. ë°ì´í„° ì •ê·œí™”
                self.logger.info("ë°ì´í„° ì •ê·œí™”...")
                X_scaled = self.scaler.fit_transform(data)

                # 2. VAE í•™ìŠµ
                self.logger.info("VAE í•™ìŠµ...")
                self.vae = self._train_vae(X_scaled)

                # 3. ì ì¬ í‘œí˜„ ì¶”ì¶œ
                self.logger.info("ì ì¬ í‘œí˜„ ì¶”ì¶œ...")
                X_encoded = self._extract_latent_representation(X_scaled)

                # 4. ì¬êµ¬ì„± í’ˆì§ˆ í‰ê°€
                reconstruction_quality = self._evaluate_reconstruction_quality(X_scaled)

                # ë©”íƒ€ë°ì´í„° ìƒì„±
                metadata = {
                    "latent_dim": self.vae_config.latent_dim,
                    "reconstruction_loss": reconstruction_quality[
                        "reconstruction_loss"
                    ],
                    "kl_divergence": reconstruction_quality["kl_divergence"],
                    "total_loss": reconstruction_quality["total_loss"],
                    "compression_ratio": data.shape[1] / X_encoded.shape[1],
                }

                self.fitted = True
                self.logger.info(
                    f"AutoEncoder ì „ì²˜ë¦¬ ì™„ë£Œ - ì••ì¶•ë¥ : {metadata['compression_ratio']:.2f}x"
                )

                return PreprocessedData(
                    data=X_encoded,
                    metadata=metadata
                )

        except Exception as e:
            self.logger.error(f"AutoEncoder ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            raise
