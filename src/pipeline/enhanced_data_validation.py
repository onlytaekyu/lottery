"""
Enhanced Data Validation System
ê°•í™”ëœ ë°ì´í„° ê²€ì¦ ì‹œìŠ¤í…œ - ë°ì´í„° í’ˆì§ˆ ë° ëª¨ë¸ ì…ë ¥ ê²€ì¦
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from scipy import stats
import warnings
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.impute import SimpleImputer, KNNImputer

from ..utils.unified_logging import get_logger
from ..utils.memory_manager import MemoryManager
from ..shared.types import LotteryNumber

logger = get_logger(__name__)


@dataclass
class DataQualityReport:
    """ë°ì´í„° í’ˆì§ˆ ë³´ê³ ì„œ"""

    nan_ratio: float
    inf_ratio: float
    zero_ratio: float
    scale_imbalance: float
    outlier_ratio: float
    class_imbalance_ratio: Optional[float] = None
    feature_correlation_issues: List[str] = None
    data_drift_score: float = 0.0
    quality_score: float = 0.0
    recommendations: List[str] = None


class EnhancedDataValidator:
    """ê°•í™”ëœ ë°ì´í„° ê²€ì¦ê¸°"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = get_logger(__name__)
        self.memory_manager = MemoryManager()

        # ê²€ì¦ ì„ê³„ê°’ ì„¤ì •
        self.thresholds = {
            "nan_ratio_max": 0.05,  # NaN ë¹„ìœ¨ 5% ì´í•˜
            "inf_ratio_max": 0.001,  # Inf ë¹„ìœ¨ 0.1% ì´í•˜
            "scale_imbalance_max": 100.0,  # ìŠ¤ì¼€ì¼ ë¶ˆê· í˜• 100ë°° ì´í•˜
            "outlier_ratio_max": 0.1,  # ì´ìƒê°’ ë¹„ìœ¨ 10% ì´í•˜
            "class_imbalance_max": 10.0,  # í´ë˜ìŠ¤ ë¶ˆê· í˜• 10ë°° ì´í•˜
            "correlation_threshold": 0.95,  # ìƒê´€ê´€ê³„ ì„ê³„ê°’
            "drift_threshold": 0.3,  # ë°ì´í„° ë“œë¦¬í”„íŠ¸ ì„ê³„ê°’
        }

        # ìŠ¤ì¼€ì¼ëŸ¬ ì´ˆê¸°í™”
        self.scalers = {"standard": StandardScaler(), "robust": RobustScaler()}

        # ê²°ì¸¡ê°’ ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        self.imputers = {
            "simple": SimpleImputer(strategy="median"),
            "knn": KNNImputer(n_neighbors=5),
        }

    def enhanced_data_validation(
        self,
        X: np.ndarray,
        y: Optional[np.ndarray] = None,
        feature_names: Optional[List[str]] = None,
    ) -> DataQualityReport:
        """
        ê°•í™”ëœ ë°ì´í„° ê²€ì¦ ìˆ˜í–‰

        Args:
            X: ì…ë ¥ ë°ì´í„°
            y: íƒ€ê²Ÿ ë°ì´í„° (ì„ íƒì‚¬í•­)
            feature_names: í”¼ì²˜ ì´ë¦„ ëª©ë¡ (ì„ íƒì‚¬í•­)

        Returns:
            DataQualityReport: ë°ì´í„° í’ˆì§ˆ ë³´ê³ ì„œ
        """
        try:
            self.logger.info("ê°•í™”ëœ ë°ì´í„° ê²€ì¦ ì‹œì‘")

            with self.memory_manager.get_context("data_validation"):
                # 1. ê¸°ë³¸ ë°ì´í„° í’ˆì§ˆ ì²´í¬
                nan_ratio = self._check_nan_values(X)
                inf_ratio = self._check_inf_values(X)
                zero_ratio = self._check_zero_values(X)

                # 2. í”¼ì²˜ ìŠ¤ì¼€ì¼ ë¶ˆê· í˜• ì²´í¬
                scale_imbalance = self._check_scale_imbalance(X)

                # 3. ì´ìƒê°’ ê²€ì¶œ
                outlier_ratio = self._detect_outliers(X)

                # 4. íƒ€ê²Ÿ ë¶„í¬ ë¶„ì„
                class_imbalance_ratio = None
                if y is not None:
                    class_imbalance_ratio = self._analyze_target_distribution(y)

                # 5. í”¼ì²˜ ìƒê´€ê´€ê³„ ë¶„ì„
                correlation_issues = self._check_feature_correlation(X, feature_names)

                # 6. ë°ì´í„° ë“œë¦¬í”„íŠ¸ ê²€ì¶œ
                drift_score = self._detect_data_drift(X)

                # 7. ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
                quality_score = self._calculate_quality_score(
                    nan_ratio,
                    inf_ratio,
                    scale_imbalance,
                    outlier_ratio,
                    class_imbalance_ratio,
                    len(correlation_issues),
                    drift_score,
                )

                # 8. ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±
                recommendations = self._generate_recommendations(
                    nan_ratio,
                    inf_ratio,
                    scale_imbalance,
                    outlier_ratio,
                    class_imbalance_ratio,
                    correlation_issues,
                    drift_score,
                )

                # ë³´ê³ ì„œ ìƒì„±
                report = DataQualityReport(
                    nan_ratio=nan_ratio,
                    inf_ratio=inf_ratio,
                    zero_ratio=zero_ratio,
                    scale_imbalance=scale_imbalance,
                    outlier_ratio=outlier_ratio,
                    class_imbalance_ratio=class_imbalance_ratio,
                    feature_correlation_issues=correlation_issues,
                    data_drift_score=drift_score,
                    quality_score=quality_score,
                    recommendations=recommendations,
                )

                self.logger.info(f"ë°ì´í„° ê²€ì¦ ì™„ë£Œ - í’ˆì§ˆ ì ìˆ˜: {quality_score:.3f}")
                return report

        except Exception as e:
            self.logger.error(f"ë°ì´í„° ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            raise

    def _check_nan_values(self, X: np.ndarray) -> float:
        """NaN ê°’ ë¹„ìœ¨ ì²´í¬"""
        if X.size == 0:
            return 0.0
        nan_count = np.isnan(X).sum()
        return nan_count / X.size

    def _check_inf_values(self, X: np.ndarray) -> float:
        """Inf ê°’ ë¹„ìœ¨ ì²´í¬"""
        if X.size == 0:
            return 0.0
        inf_count = np.isinf(X).sum()
        return inf_count / X.size

    def _check_zero_values(self, X: np.ndarray) -> float:
        """0 ê°’ ë¹„ìœ¨ ì²´í¬"""
        if X.size == 0:
            return 0.0
        zero_count = (X == 0).sum()
        return zero_count / X.size

    def _check_scale_imbalance(self, X: np.ndarray) -> float:
        """í”¼ì²˜ ìŠ¤ì¼€ì¼ ë¶ˆê· í˜• ì²´í¬"""
        if X.shape[1] < 2:
            return 1.0

        # ê° í”¼ì²˜ì˜ í‘œì¤€í¸ì°¨ ê³„ì‚°
        feature_stds = np.std(X, axis=0)

        # 0ì¸ í‘œì¤€í¸ì°¨ ì œì™¸
        non_zero_stds = feature_stds[feature_stds > 1e-10]

        if len(non_zero_stds) < 2:
            return 1.0

        # ìµœëŒ€/ìµœì†Œ ë¹„ìœ¨ ê³„ì‚°
        scale_imbalance = np.max(non_zero_stds) / np.min(non_zero_stds)
        return scale_imbalance

    def _detect_outliers(self, X: np.ndarray) -> float:
        """ì´ìƒê°’ ê²€ì¶œ (IQR ë°©ë²•)"""
        outlier_count = 0
        total_count = X.size

        for col in range(X.shape[1]):
            col_data = X[:, col]

            # IQR ê³„ì‚°
            Q1 = np.percentile(col_data, 25)
            Q3 = np.percentile(col_data, 75)
            IQR = Q3 - Q1

            # ì´ìƒê°’ ë²”ìœ„ ì„¤ì •
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR

            # ì´ìƒê°’ ì¹´ìš´íŠ¸
            outliers = (col_data < lower_bound) | (col_data > upper_bound)
            outlier_count += outliers.sum()

        return outlier_count / total_count

    def _analyze_target_distribution(self, y: np.ndarray) -> float:
        """íƒ€ê²Ÿ ë¶„í¬ ë¶„ì„ (í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²´í¬)"""
        if y is None or len(y) == 0:
            return 1.0

        # ì—°ì†í˜• íƒ€ê²Ÿì¸ ê²½ìš° êµ¬ê°„í™”
        if len(np.unique(y)) > 10:
            y_binned = pd.cut(y, bins=5, labels=False)
            unique_values, counts = np.unique(y_binned, return_counts=True)
        else:
            unique_values, counts = np.unique(y, return_counts=True)

        if len(counts) < 2:
            return 1.0

        # í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¹„ìœ¨ ê³„ì‚°
        class_distribution = counts / len(y)
        imbalance_ratio = np.max(class_distribution) / np.min(class_distribution)

        return imbalance_ratio

    def _check_feature_correlation(
        self, X: np.ndarray, feature_names: Optional[List[str]] = None
    ) -> List[str]:
        """í”¼ì²˜ ê°„ ìƒê´€ê´€ê³„ ë¶„ì„"""
        if X.shape[1] < 2:
            return []

        correlation_issues = []

        try:
            # ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°
            corr_matrix = np.corrcoef(X.T)

            # ëŒ€ê°ì„  ì œì™¸í•˜ê³  ë†’ì€ ìƒê´€ê´€ê³„ ì°¾ê¸°
            n_features = X.shape[1]
            for i in range(n_features):
                for j in range(i + 1, n_features):
                    if (
                        abs(corr_matrix[i, j])
                        > self.thresholds["correlation_threshold"]
                    ):
                        feature_i = (
                            feature_names[i] if feature_names else f"feature_{i}"
                        )
                        feature_j = (
                            feature_names[j] if feature_names else f"feature_{j}"
                        )
                        correlation_issues.append(
                            f"{feature_i} - {feature_j}: {corr_matrix[i, j]:.3f}"
                        )

        except Exception as e:
            self.logger.warning(f"ìƒê´€ê´€ê³„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")

        return correlation_issues

    def _detect_data_drift(self, X: np.ndarray) -> float:
        """ë°ì´í„° ë“œë¦¬í”„íŠ¸ ê²€ì¶œ (í†µê³„ì  ë°©ë²•)"""
        if X.shape[0] < 100:  # ìƒ˜í”Œ ìˆ˜ê°€ ì ìœ¼ë©´ ë“œë¦¬í”„íŠ¸ ê²€ì¶œ ë¶ˆê°€
            return 0.0

        try:
            # ë°ì´í„°ë¥¼ ë‘ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ë¶„í¬ ë¹„êµ
            mid_point = X.shape[0] // 2
            X_first_half = X[:mid_point]
            X_second_half = X[mid_point:]

            drift_scores = []

            for col in range(X.shape[1]):
                # Kolmogorov-Smirnov í…ŒìŠ¤íŠ¸
                ks_stat, p_value = stats.ks_2samp(
                    X_first_half[:, col], X_second_half[:, col]
                )
                drift_scores.append(ks_stat)

            # í‰ê·  ë“œë¦¬í”„íŠ¸ ì ìˆ˜ ë°˜í™˜
            return np.mean(drift_scores)

        except Exception as e:
            self.logger.warning(f"ë°ì´í„° ë“œë¦¬í”„íŠ¸ ê²€ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return 0.0

    def _calculate_quality_score(
        self,
        nan_ratio: float,
        inf_ratio: float,
        scale_imbalance: float,
        outlier_ratio: float,
        class_imbalance_ratio: Optional[float],
        correlation_issues_count: int,
        drift_score: float,
    ) -> float:
        """ì „ì²´ ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0-1 ìŠ¤ì¼€ì¼)"""

        # ê° ë©”íŠ¸ë¦­ë³„ ì ìˆ˜ ê³„ì‚° (1ì´ ìµœê³ , 0ì´ ìµœì•…)
        nan_score = max(0, 1 - nan_ratio / self.thresholds["nan_ratio_max"])
        inf_score = max(0, 1 - inf_ratio / self.thresholds["inf_ratio_max"])

        scale_score = max(
            0, 1 - min(scale_imbalance / self.thresholds["scale_imbalance_max"], 1)
        )
        outlier_score = max(0, 1 - outlier_ratio / self.thresholds["outlier_ratio_max"])

        class_score = 1.0
        if class_imbalance_ratio is not None:
            class_score = max(
                0,
                1
                - min(
                    class_imbalance_ratio / self.thresholds["class_imbalance_max"], 1
                ),
            )

        correlation_score = max(0, 1 - min(correlation_issues_count / 10, 1))
        drift_score_normalized = max(
            0, 1 - drift_score / self.thresholds["drift_threshold"]
        )

        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        weights = [0.2, 0.1, 0.15, 0.15, 0.15, 0.15, 0.1]  # ê° ë©”íŠ¸ë¦­ ê°€ì¤‘ì¹˜
        scores = [
            nan_score,
            inf_score,
            scale_score,
            outlier_score,
            class_score,
            correlation_score,
            drift_score_normalized,
        ]

        quality_score = np.average(scores, weights=weights)
        return quality_score

    def _generate_recommendations(
        self,
        nan_ratio: float,
        inf_ratio: float,
        scale_imbalance: float,
        outlier_ratio: float,
        class_imbalance_ratio: Optional[float],
        correlation_issues: List[str],
        drift_score: float,
    ) -> List[str]:
        """ë°ì´í„° í’ˆì§ˆ ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""

        recommendations = []

        # NaN ê°’ ì²˜ë¦¬ ê¶Œì¥ì‚¬í•­
        if nan_ratio > self.thresholds["nan_ratio_max"]:
            recommendations.append(
                f"NaN ë¹„ìœ¨ì´ {nan_ratio:.3f}ë¡œ ë†’ìŠµë‹ˆë‹¤. KNN Imputer ë˜ëŠ” ê³ ê¸‰ ê²°ì¸¡ê°’ ì²˜ë¦¬ ê¸°ë²• ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤."
            )

        # Inf ê°’ ì²˜ë¦¬ ê¶Œì¥ì‚¬í•­
        if inf_ratio > self.thresholds["inf_ratio_max"]:
            recommendations.append(
                f"Inf ê°’ì´ {inf_ratio:.3f} ë¹„ìœ¨ë¡œ ì¡´ì¬í•©ë‹ˆë‹¤. ìˆ˜ì¹˜ ì•ˆì •ì„±ì„ ìœ„í•´ í´ë¦¬í•‘ ë˜ëŠ” ë³€í™˜ì´ í•„ìš”í•©ë‹ˆë‹¤."
            )

        # ìŠ¤ì¼€ì¼ ë¶ˆê· í˜• ì²˜ë¦¬ ê¶Œì¥ì‚¬í•­
        if scale_imbalance > self.thresholds["scale_imbalance_max"]:
            recommendations.append(
                f"í”¼ì²˜ ìŠ¤ì¼€ì¼ ë¶ˆê· í˜•ì´ {scale_imbalance:.1f}ë°°ì…ë‹ˆë‹¤. RobustScaler ë˜ëŠ” StandardScaler ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤."
            )

        # ì´ìƒê°’ ì²˜ë¦¬ ê¶Œì¥ì‚¬í•­
        if outlier_ratio > self.thresholds["outlier_ratio_max"]:
            recommendations.append(
                f"ì´ìƒê°’ ë¹„ìœ¨ì´ {outlier_ratio:.3f}ë¡œ ë†’ìŠµë‹ˆë‹¤. ì´ìƒê°’ ì œê±° ë˜ëŠ” ë³€í™˜ì„ ê³ ë ¤í•˜ì„¸ìš”."
            )

        # í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬ ê¶Œì¥ì‚¬í•­
        if (
            class_imbalance_ratio
            and class_imbalance_ratio > self.thresholds["class_imbalance_max"]
        ):
            recommendations.append(
                f"í´ë˜ìŠ¤ ë¶ˆê· í˜•ì´ {class_imbalance_ratio:.1f}ë°°ì…ë‹ˆë‹¤. SMOTE, ê°€ì¤‘ì¹˜ ì¡°ì • ë˜ëŠ” ì–¸ë”ìƒ˜í”Œë§ì„ ê³ ë ¤í•˜ì„¸ìš”."
            )

        # ìƒê´€ê´€ê³„ ë¬¸ì œ ê¶Œì¥ì‚¬í•­
        if len(correlation_issues) > 5:
            recommendations.append(
                f"{len(correlation_issues)}ê°œì˜ ë†’ì€ ìƒê´€ê´€ê³„ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ì°¨ì› ì¶•ì†Œ ë˜ëŠ” í”¼ì²˜ ì„ íƒì„ ê³ ë ¤í•˜ì„¸ìš”."
            )

        # ë°ì´í„° ë“œë¦¬í”„íŠ¸ ê¶Œì¥ì‚¬í•­
        if drift_score > self.thresholds["drift_threshold"]:
            recommendations.append(
                f"ë°ì´í„° ë“œë¦¬í”„íŠ¸ ì ìˆ˜ê°€ {drift_score:.3f}ì…ë‹ˆë‹¤. ë°ì´í„° ë¶„í¬ ë³€í™”ë¥¼ ëª¨ë‹ˆí„°ë§í•˜ê³  ëª¨ë¸ ì¬í•™ìŠµì„ ê³ ë ¤í•˜ì„¸ìš”."
            )

        if not recommendations:
            recommendations.append(
                "ë°ì´í„° í’ˆì§ˆì´ ì–‘í˜¸í•©ë‹ˆë‹¤. ì¶”ê°€ ì „ì²˜ë¦¬ ì—†ì´ ëª¨ë¸ í•™ìŠµì„ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            )

        return recommendations

    def auto_fix_data_quality(
        self, X: np.ndarray, y: Optional[np.ndarray] = None
    ) -> Tuple[np.ndarray, np.ndarray]:
        """ìë™ ë°ì´í„° í’ˆì§ˆ ê°œì„ """

        self.logger.info("ìë™ ë°ì´í„° í’ˆì§ˆ ê°œì„  ì‹œì‘")
        X_fixed = X.copy()
        y_fixed = y.copy() if y is not None else None

        try:
            # 1. Inf ê°’ ì²˜ë¦¬
            X_fixed = np.where(np.isinf(X_fixed), np.nan, X_fixed)

            # 2. NaN ê°’ ì²˜ë¦¬ (KNN Imputer ì‚¬ìš©)
            if np.isnan(X_fixed).any():
                self.logger.info("NaN ê°’ ì²˜ë¦¬ ì¤‘...")
                X_fixed = self.imputers["knn"].fit_transform(X_fixed)

            # 3. ì´ìƒê°’ ì²˜ë¦¬ (IQR ê¸°ë°˜ í´ë¦¬í•‘)
            for col in range(X_fixed.shape[1]):
                Q1 = np.percentile(X_fixed[:, col], 25)
                Q3 = np.percentile(X_fixed[:, col], 75)
                IQR = Q3 - Q1

                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR

                X_fixed[:, col] = np.clip(X_fixed[:, col], lower_bound, upper_bound)

            # 4. ìŠ¤ì¼€ì¼ ì •ê·œí™”
            X_fixed = self.scalers["robust"].fit_transform(X_fixed)

            self.logger.info("ìë™ ë°ì´í„° í’ˆì§ˆ ê°œì„  ì™„ë£Œ")
            return X_fixed, y_fixed

        except Exception as e:
            self.logger.error(f"ìë™ ë°ì´í„° í’ˆì§ˆ ê°œì„  ì¤‘ ì˜¤ë¥˜: {e}")
            return X, y

    def print_quality_report(self, report: DataQualityReport):
        """ë°ì´í„° í’ˆì§ˆ ë³´ê³ ì„œ ì¶œë ¥"""

        print("=" * 60)
        print("ğŸ“Š ë°ì´í„° í’ˆì§ˆ ë³´ê³ ì„œ")
        print("=" * 60)

        print(f"ğŸ¯ ì „ì²´ í’ˆì§ˆ ì ìˆ˜: {report.quality_score:.3f}/1.000")
        print()

        print("ğŸ“ˆ ì„¸ë¶€ ë©”íŠ¸ë¦­:")
        print(f"  â€¢ NaN ë¹„ìœ¨: {report.nan_ratio:.4f} ({report.nan_ratio*100:.2f}%)")
        print(f"  â€¢ Inf ë¹„ìœ¨: {report.inf_ratio:.4f} ({report.inf_ratio*100:.2f}%)")
        print(f"  â€¢ Zero ë¹„ìœ¨: {report.zero_ratio:.4f} ({report.zero_ratio*100:.2f}%)")
        print(f"  â€¢ ìŠ¤ì¼€ì¼ ë¶ˆê· í˜•: {report.scale_imbalance:.2f}ë°°")
        print(
            f"  â€¢ ì´ìƒê°’ ë¹„ìœ¨: {report.outlier_ratio:.4f} ({report.outlier_ratio*100:.2f}%)"
        )

        if report.class_imbalance_ratio:
            print(f"  â€¢ í´ë˜ìŠ¤ ë¶ˆê· í˜•: {report.class_imbalance_ratio:.2f}ë°°")

        print(f"  â€¢ ìƒê´€ê´€ê³„ ë¬¸ì œ: {len(report.feature_correlation_issues)}ê°œ")
        print(f"  â€¢ ë°ì´í„° ë“œë¦¬í”„íŠ¸: {report.data_drift_score:.3f}")
        print()

        if report.recommendations:
            print("ğŸ’¡ ê°œì„  ê¶Œì¥ì‚¬í•­:")
            for i, rec in enumerate(report.recommendations, 1):
                print(f"  {i}. {rec}")

        print("=" * 60)
