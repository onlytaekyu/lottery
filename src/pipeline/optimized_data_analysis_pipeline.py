#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
ìµœì í™”ëœ ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸

ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ë‹¤ìŒ ê¸°ëŠ¥ë“¤ì„ í¬í•¨í•©ë‹ˆë‹¤:
- ì¤‘ë³µ í•¨ìˆ˜ í†µí•© ë° ì œê±°
- ì„¸ë¶„í™”ëœ ìºì‹± ì‹œìŠ¤í…œ
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²­í¬ ì²˜ë¦¬
- ë³‘ë ¬ ì²˜ë¦¬ ì§€ì› (ProcessPool í†µí•©)
- í•˜ì´ë¸Œë¦¬ë“œ ìµœì í™” ì‹œìŠ¤í…œ
- ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ìµœì í™”
"""

import json
import logging
import os
import sys
import time
import random
import traceback
from pathlib import Path
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple, Union
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
import hashlib

import numpy as np
import psutil

from src.utils.error_handler_refactored import (
    get_logger,
    log_exception_with_trace,
    StrictErrorHandler,
    strict_error_handler,
    validate_and_fail_fast,
)
from src.utils.state_vector_cache import get_cache
from src.utils.data_loader import load_draw_history, LotteryJSONEncoder
from src.shared.types import LotteryNumber
from src.utils.unified_config import load_config
from src.utils.unified_performance import get_profiler

# ìµœì í™” ì‹œìŠ¤í…œ import
from src.utils.process_pool_manager import get_process_pool_manager
from src.utils.hybrid_optimizer import get_hybrid_optimizer, optimize
from src.utils.memory_manager import get_memory_manager

from src.analysis.pattern_analyzer import PatternAnalyzer
from src.analysis.pattern_vectorizer import PatternVectorizer
from src.utils.unified_report import safe_convert, save_physical_performance_report
from src.analysis.pair_analyzer import PairAnalyzer

# from src.utils.feature_vector_validator import (
#     validate_feature_vector_with_config,
#     check_vector_dimensions,
# )

# ìµœì í™”ëœ í•¨ìˆ˜ë“¤ import
from src.shared.graph_utils import (
    calculate_pair_frequency,
    calculate_segment_entropy,
    calculate_number_gaps,
    calculate_cluster_distribution,
    clear_cache,
    get_cache_stats,
)

# ë¡œê±° ì„¤ì •
logger = get_logger(__name__)

# ì„±ëŠ¥ ìµœì í™” ì„¤ì •
CHUNK_PROCESSING_CONFIG = {
    "historical_data": 100,  # 100íšŒì°¨ì”© ì²˜ë¦¬
    "vector_generation": 50,  # 50ê°œì”© ë²¡í„° ìƒì„±
    "cache_flush_interval": 200,  # 200íšŒì°¨ë§ˆë‹¤ ìºì‹œ í”ŒëŸ¬ì‹œ
    "parallel_workers": min(4, psutil.cpu_count()),  # CPU ì½”ì–´ ìˆ˜ì— ë”°ë¥¸ ì›Œì»¤ ìˆ˜
    "memory_threshold": 0.8,  # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 80% ì„ê³„ì 
}

CACHE_STRATEGY = {
    "data_load": "data_size_hash",
    "pattern_analysis": "data_hash + config_hash",
    "vectorization": "analysis_hash + vector_config_hash",
    "additional_analysis": "pattern_hash + addon_config_hash",
}

# ì „ì—­ ì—„ê²©í•œ ì—ëŸ¬ í•¸ë“¤ëŸ¬
strict_handler = StrictErrorHandler()

# ì „ì—­ ìµœì í™” ì‹œìŠ¤í…œë“¤
process_pool_manager = None
hybrid_optimizer = None
memory_manager = None


def initialize_optimization_systems(config: Dict[str, Any]):
    """ìµœì í™” ì‹œìŠ¤í…œë“¤ ì´ˆê¸°í™”"""
    global process_pool_manager, hybrid_optimizer, memory_manager

    try:
        # ìµœì í™” ì„¤ì • ë¡œë“œ
        optimization_config = config.get("optimization", {})

        # ProcessPool ê´€ë¦¬ì ì´ˆê¸°í™”
        try:
            process_pool_config = optimization_config.get(
                "process_pool",
                {
                    "max_workers": min(4, psutil.cpu_count()),
                    "chunk_size": 100,
                    "timeout": 300,
                },
            )
            process_pool_manager = get_process_pool_manager(process_pool_config)
            logger.info("ProcessPool ê´€ë¦¬ì ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"ProcessPool ê´€ë¦¬ì ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            process_pool_manager = None

        # ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™”
        try:
            from src.utils.memory_manager import MemoryConfig

            memory_config_dict = optimization_config.get("memory", {})
            memory_config = MemoryConfig(
                max_memory_usage=memory_config_dict.get("max_memory_usage", 0.85),
                cache_size=memory_config_dict.get("cache_size", 256 * 1024 * 1024),
                use_memory_pooling=memory_config_dict.get("use_memory_pooling", True),
                auto_cleanup_interval=memory_config_dict.get(
                    "auto_cleanup_interval", 60.0
                ),
            )
            memory_manager = get_memory_manager(memory_config)
            logger.info("ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            memory_manager = None

        # í•˜ì´ë¸Œë¦¬ë“œ ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        try:
            hybrid_config = optimization_config.get(
                "hybrid",
                {
                    "auto_optimization": True,
                    "memory_threshold": 0.8,
                    "cpu_threshold": 75.0,
                },
            )
            hybrid_optimizer = get_hybrid_optimizer(hybrid_config)
            logger.info("í•˜ì´ë¸Œë¦¬ë“œ ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"í•˜ì´ë¸Œë¦¬ë“œ ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            hybrid_optimizer = None

        # ìµœì í™” ì‹œìŠ¤í…œ ìƒíƒœ ë¡œê¹…
        initialized_systems = []
        if process_pool_manager is not None:
            initialized_systems.append("ProcessPool")
        if memory_manager is not None:
            initialized_systems.append("MemoryManager")
        if hybrid_optimizer is not None:
            initialized_systems.append("HybridOptimizer")

        logger.info(f"ì´ˆê¸°í™”ëœ ìµœì í™” ì‹œìŠ¤í…œ: {', '.join(initialized_systems)}")

    except Exception as e:
        logger.error(f"ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        # ìµœì í™” ì‹œìŠ¤í…œ ì—†ì´ë„ ë™ì‘í•  ìˆ˜ ìˆë„ë¡ Noneìœ¼ë¡œ ì„¤ì •
        process_pool_manager = None
        hybrid_optimizer = None
        memory_manager = None


@optimize(
    task_info={
        "function_type": "analysis",
        "parallelizable": True,
        "gpu_compatible": False,
    }
)
def optimized_pattern_analysis(
    data_chunk: List, analyzer: PatternAnalyzer
) -> Dict[str, Any]:
    """ìµœì í™”ëœ íŒ¨í„´ ë¶„ì„"""
    try:
        # ë©”ëª¨ë¦¬ ê´€ë¦¬ ìŠ¤ì½”í”„ ë‚´ì—ì„œ ì‹¤í–‰
        if memory_manager:
            with memory_manager.allocation_scope():
                return analyzer.analyze(data_chunk)
        else:
            return analyzer.analyze(data_chunk)
    except Exception as e:
        logger.error(f"íŒ¨í„´ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return {}


@optimize(
    task_info={
        "function_type": "vectorize",
        "parallelizable": True,
        "gpu_compatible": False,
    }
)
def optimized_vectorization(
    patterns: List, vectorizer: PatternVectorizer
) -> Tuple[np.ndarray, List[str]]:
    """ìµœì í™”ëœ ë²¡í„°í™”"""
    try:
        if memory_manager:
            with memory_manager.allocation_scope():
                return vectorizer.vectorize(patterns)
        else:
            return vectorizer.vectorize(patterns)
    except Exception as e:
        logger.error(f"ë²¡í„°í™” ì‹¤íŒ¨: {e}")
        return np.array([]), []


def generate_cache_key(data_info: str, operation: str, **kwargs) -> str:
    """ìµœì í™”ëœ ìºì‹œ í‚¤ ìƒì„±"""
    try:
        # ë°ì´í„° ì •ë³´ì™€ ë§¤ê°œë³€ìˆ˜ë¥¼ ê²°í•©í•˜ì—¬ í•´ì‹œ ìƒì„±
        combined_info = f"{data_info}_{operation}_{str(sorted(kwargs.items()))}"
        return hashlib.md5(combined_info.encode()).hexdigest()[:16]
    except Exception:
        # í•´ì‹œ ìƒì„± ì‹¤íŒ¨ì‹œ ê¸°ë³¸ í‚¤
        return f"{operation}_{hash(str(kwargs)) % 100000}"


def check_memory_usage() -> bool:
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸"""
    memory_info = psutil.virtual_memory()
    return memory_info.percent < (CHUNK_PROCESSING_CONFIG["memory_threshold"] * 100)


def process_data_chunks_optimized(data: List, chunk_size: int, process_func, **kwargs):
    """ìµœì í™”ëœ ì²­í¬ ë‹¨ìœ„ ë°ì´í„° ì²˜ë¦¬"""
    global process_pool_manager

    # ProcessPoolì´ ì‚¬ìš© ê°€ëŠ¥í•˜ê³  ë°ì´í„°ê°€ ì¶©ë¶„íˆ í° ê²½ìš° ë³‘ë ¬ ì²˜ë¦¬
    if process_pool_manager and len(data) > 200:
        try:
            chunks = process_pool_manager.chunk_and_split(data, chunk_size)
            results = process_pool_manager.parallel_analyze(
                chunks, process_func, **kwargs
            )
            return process_pool_manager.merge_results(results)
        except Exception as e:
            logger.warning(f"ë³‘ë ¬ ì²˜ë¦¬ ì‹¤íŒ¨, ìˆœì°¨ ì²˜ë¦¬ë¡œ ì „í™˜: {e}")

    # ìˆœì°¨ ì²˜ë¦¬ (ê¸°ì¡´ ë°©ì‹)
    results = []
    total_chunks = len(data) // chunk_size + (1 if len(data) % chunk_size else 0)

    for i in range(0, len(data), chunk_size):
        if not check_memory_usage():
            logger.warning(
                "ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì„ê³„ì ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."
            )
            import gc

            gc.collect()

        chunk = data[i : min(i + chunk_size, len(data))]
        chunk_result = process_func(chunk, **kwargs)
        results.append(chunk_result)

        if (i // chunk_size + 1) % 10 == 0:
            logger.info(f"ì²­í¬ ì²˜ë¦¬ ì§„í–‰ë¥ : {i // chunk_size + 1}/{total_chunks}")

    return results


def safe_analysis_step(step_name: str, func, *args, **kwargs):
    """íŠ¸ëœì­ì…˜ ë˜í¼ - ì•ˆì „í•œ ë¶„ì„ ë‹¨ê³„ ì‹¤í–‰"""
    try:
        logger.info(f"{step_name} ì‹œì‘")
        start_time = time.time()

        result = func(*args, **kwargs)

        duration = time.time() - start_time
        logger.info(f"{step_name} ì™„ë£Œ ({duration:.2f}ì´ˆ)")
        return result

    except Exception as e:
        logger.error(f"{step_name} ì‹¤íŒ¨: {str(e)}")
        # ë¡¤ë°± ë¡œì§ (í•„ìš”ì‹œ ì¶”ê°€)
        raise RuntimeError(f"{step_name} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


@strict_error_handler("ìµœì í™”ëœ ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸", exit_on_error=True)
def run_optimized_data_analysis() -> bool:
    """
    ìµœì í™”ëœ ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

    Returns:
        bool: ì‹¤í–‰ ì„±ê³µ ì—¬ë¶€
    """
    start_time = time.time()

    try:
        logger.info("ğŸš€ ìµœì í™”ëœ ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹œì‘")

        # ì„¤ì • ë¡œë“œ
        config = load_config()

        # ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        initialize_optimization_systems(config)

        # 1ë‹¨ê³„: ë°ì´í„° ë¡œë“œ ë° ê³ ê¸‰ ê²€ì¦
        logger.info("ğŸ“Š 1ë‹¨ê³„: ë°ì´í„° ë¡œë“œ ë° ê³ ê¸‰ ê²€ì¦")
        historical_data = load_draw_history()

        if not historical_data:
            logger.error("ë¡œë˜ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False

        logger.info(f"ë¡œë˜ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(historical_data)}íšŒì°¨")

        # ğŸ” ê³ ê¸‰ ë°ì´í„° ê²€ì¦ ì‹œìŠ¤í…œ ì ìš©
        from src.pipeline.data_validation import DataValidator

        validator = DataValidator(config)
        validation_result = validator.validate_lottery_data(historical_data)

        if not validation_result.is_valid:
            logger.error(f"ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {len(validation_result.errors)}ê°œ ì˜¤ë¥˜")
            for error in validation_result.errors[:5]:  # ì²˜ìŒ 5ê°œ ì˜¤ë¥˜ë§Œ í‘œì‹œ
                logger.error(f"  - {error}")
            return False

        logger.info(
            f"âœ… ë°ì´í„° ê²€ì¦ ì™„ë£Œ (í’ˆì§ˆ ì ìˆ˜: {validation_result.quality_score})"
        )

        # ê²½ê³  ì‚¬í•­ ë¡œê¹…
        if validation_result.warnings:
            logger.warning(f"ë°ì´í„° í’ˆì§ˆ ê²½ê³  {len(validation_result.warnings)}ê°œ:")
            for warning in validation_result.warnings[:3]:
                logger.warning(f"  - {warning}")

        # ì´ìƒì¹˜ ì •ë³´ ë¡œê¹…
        if validation_result.anomalies:
            logger.info(f"ê°ì§€ëœ ì´ìƒì¹˜: {len(validation_result.anomalies)}ê°œ")

        # í’ˆì§ˆ ë³´ê³ ì„œ ìƒì„± ë° ì €ì¥
        quality_report = validator.generate_quality_report(historical_data)
        validator.save_quality_report(quality_report)

        # 2ë‹¨ê³„: ë¶„ì„ê¸° ì´ˆê¸°í™”
        logger.info("ğŸ”§ 2ë‹¨ê³„: ë¶„ì„ê¸° ì´ˆê¸°í™”")

        def init_analyzer(analyzer_type: str):
            """ë¶„ì„ê¸° ì´ˆê¸°í™” í—¬í¼ í•¨ìˆ˜"""
            try:
                # ConfigProxyë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜
                if hasattr(config, "_config"):
                    config_dict = config._config
                elif hasattr(config, "to_dict"):
                    config_dict = config.to_dict()
                elif isinstance(config, dict):
                    config_dict = config
                else:
                    # ConfigProxy ê°ì²´ì˜ ì†ì„±ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
                    config_dict = {}
                    if hasattr(config, "__dict__"):
                        for key, value in config.__dict__.items():
                            if not key.startswith("_"):
                                config_dict[key] = value

                    # ê¸°ë³¸ ì„¤ì • ì¶”ê°€
                    if not config_dict:
                        config_dict = {
                            "analysis": {},
                            "paths": {
                                "cache_dir": "data/cache",
                                "result_dir": "data/result",
                            },
                            "vectorizer": {"use_cache": True, "normalize_output": True},
                            "filtering": {
                                "remove_low_variance_features": True,
                                "variance_threshold": 0.01,
                            },
                            "caching": {
                                "enable_feature_cache": True,
                                "max_cache_size": 10000,
                            },
                        }

                if analyzer_type == "pattern":
                    return PatternAnalyzer(config_dict)
                elif analyzer_type == "distribution":
                    from src.analysis.distribution_analyzer import DistributionAnalyzer

                    return DistributionAnalyzer(config_dict)
                elif analyzer_type == "roi":
                    from src.analysis.roi_analyzer import ROIAnalyzer

                    return ROIAnalyzer(config_dict)
                elif analyzer_type == "pair":
                    return PairAnalyzer(config_dict)
                elif analyzer_type == "vectorizer":
                    return PatternVectorizer(config_dict)
                else:
                    raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ë¶„ì„ê¸° íƒ€ì…: {analyzer_type}")
            except Exception as e:
                logger.error(f"{analyzer_type} ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì¬ì‹œë„
                try:
                    # ê¸°ë³¸ ì„¤ì • ë”•ì…”ë„ˆë¦¬ ìƒì„±
                    default_config = {
                        "analysis": {},
                        "paths": {
                            "cache_dir": "data/cache",
                            "result_dir": "data/result",
                        },
                        "vectorizer": {"use_cache": True, "normalize_output": True},
                        "filtering": {
                            "remove_low_variance_features": True,
                            "variance_threshold": 0.01,
                        },
                        "caching": {
                            "enable_feature_cache": True,
                            "max_cache_size": 10000,
                        },
                    }

                    if analyzer_type == "pattern":
                        return PatternAnalyzer(default_config)
                    elif analyzer_type == "distribution":
                        from src.analysis.distribution_analyzer import (
                            DistributionAnalyzer,
                        )

                        return DistributionAnalyzer(default_config)
                    elif analyzer_type == "roi":
                        from src.analysis.roi_analyzer import ROIAnalyzer

                        return ROIAnalyzer(default_config)
                    elif analyzer_type == "pair":
                        return PairAnalyzer(default_config)
                    elif analyzer_type == "vectorizer":
                        return PatternVectorizer(default_config)
                except Exception as e2:
                    logger.error(f"{analyzer_type} ë¶„ì„ê¸° ê¸°ë³¸ ì´ˆê¸°í™”ë„ ì‹¤íŒ¨: {e2}")
                    return None

        # ë¶„ì„ê¸°ë“¤ ì´ˆê¸°í™”
        pattern_analyzer = init_analyzer("pattern")
        distribution_analyzer = init_analyzer("distribution")
        roi_analyzer = init_analyzer("roi")
        pair_analyzer = init_analyzer("pair")
        vectorizer = init_analyzer("vectorizer")

        # ì´ˆê¸°í™” ì‹¤íŒ¨ ì²´í¬
        analyzers = {
            "pattern": pattern_analyzer,
            "distribution": distribution_analyzer,
            "roi": roi_analyzer,
            "pair": pair_analyzer,
            "vectorizer": vectorizer,
        }

        failed_analyzers = [
            name for name, analyzer in analyzers.items() if analyzer is None
        ]
        if failed_analyzers:
            logger.error(f"ë‹¤ìŒ ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {failed_analyzers}")
            return False

        logger.info("ëª¨ë“  ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")

        # 3ë‹¨ê³„: ë³‘ë ¬ ë¶„ì„ ì‹¤í–‰
        logger.info("âš¡ 3ë‹¨ê³„: ë³‘ë ¬ ë¶„ì„ ì‹¤í–‰")
        analysis_results = {}

        # ë©”ëª¨ë¦¬ ê´€ë¦¬ ìŠ¤ì½”í”„ ë‚´ì—ì„œ ì‹¤í–‰
        if memory_manager:
            with memory_manager.allocation_scope():
                analysis_results = run_parallel_analysis(
                    historical_data, analyzers, config
                )
        else:
            analysis_results = run_parallel_analysis(historical_data, analyzers, config)

        if not analysis_results:
            logger.error("ë¶„ì„ ì‹¤í–‰ ì‹¤íŒ¨")
            return False

        logger.info(f"ë¶„ì„ ì™„ë£Œ: {len(analysis_results)}ê°œ ê²°ê³¼")

        # 4ë‹¨ê³„: ê³ ê¸‰ íŠ¹ì„± ì¶”ì¶œ ë° ë²¡í„° ìƒì„±
        logger.info("ğŸ”¢ 4ë‹¨ê³„: ê³ ê¸‰ íŠ¹ì„± ì¶”ì¶œ ë° ë²¡í„° ìƒì„±")

        try:
            # í†µí•© ë¶„ì„ ê²°ê³¼ ìƒì„±
            unified_analysis = merge_analysis_results(analysis_results)

            # ğŸ§  ê³ ê¸‰ íŠ¹ì„± ì¶”ì¶œ ì—”ì§„ ì ìš©
            from src.analysis.feature_extractor import FeatureExtractor

            feature_extractor = FeatureExtractor(config)
            extraction_result = feature_extractor.extract_all_features(unified_analysis)

            logger.info(
                f"íŠ¹ì„± ì¶”ì¶œ ì™„ë£Œ: {len(extraction_result.feature_names)}ê°œ íŠ¹ì„±"
            )
            logger.info(f"íŠ¹ì„± ê·¸ë£¹: {list(extraction_result.feature_groups.keys())}")

            # íŠ¹ì„± ìµœì í™” (ì„ íƒì )
            if extraction_result.feature_matrix.size > 0:
                optimized_features, optimized_names, importance_scores = (
                    feature_extractor.optimize_feature_selection(
                        extraction_result.feature_matrix,
                        extraction_result.feature_names,
                    )
                )
                logger.info(f"íŠ¹ì„± ìµœì í™” ì™„ë£Œ: {len(optimized_names)}ê°œ íŠ¹ì„± ì„ íƒ")
            else:
                optimized_features = extraction_result.feature_matrix
                optimized_names = extraction_result.feature_names

            # ë²¡í„°í™” ì‹¤í–‰ (ê¸°ì¡´ ë²¡í„°í™” ì‹œìŠ¤í…œê³¼ ë³‘í–‰)
            feature_vector = None
            feature_names = []

            if vectorizer is not None:
                try:
                    feature_vector, feature_names = (
                        vectorizer.vectorize_extended_features(unified_analysis)
                    )
                    if feature_vector is not None and len(feature_vector) > 0:
                        logger.info(f"ê¸°ì¡´ ë²¡í„°í™” ì‹œìŠ¤í…œ: {len(feature_vector)}ì°¨ì›")
                except Exception as e:
                    logger.warning(f"ê¸°ì¡´ ë²¡í„°í™” ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}")
                    feature_vector = None
                    feature_names = []

            # ë‘ ë²¡í„° ì‹œìŠ¤í…œ ê²°í•©
            if feature_vector is not None and len(feature_vector) > 0:
                if optimized_features.size > 0:
                    # ì°¨ì›ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ê²°í•©
                    combined_vector = np.concatenate(
                        [feature_vector.flatten(), optimized_features.flatten()]
                    )
                    combined_names = feature_names + optimized_names
                    logger.info(f"ë²¡í„° ì‹œìŠ¤í…œ ê²°í•© ì™„ë£Œ: {len(combined_vector)}ì°¨ì›")
                else:
                    combined_vector = feature_vector
                    combined_names = feature_names
            else:
                # ê¸°ì¡´ ë²¡í„°í™” ì‹¤íŒ¨ì‹œ ìƒˆë¡œìš´ íŠ¹ì„± ì¶”ì¶œ ê²°ê³¼ ì‚¬ìš©
                combined_vector = (
                    optimized_features.flatten()
                    if optimized_features.size > 0
                    else np.array([])
                )
                combined_names = optimized_names

            if len(combined_vector) == 0:
                logger.error("íŠ¹ì„± ë²¡í„° ìƒì„± ì‹¤íŒ¨")
                return False

            logger.info(f"ìµœì¢… íŠ¹ì„± ë²¡í„° ìƒì„± ì™„ë£Œ: {len(combined_vector)}ì°¨ì›")

            # ë²¡í„° í’ˆì§ˆ ê²€ì¦
            if not validate_feature_vector(combined_vector, combined_names, config):
                logger.error("íŠ¹ì„± ë²¡í„° í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨")
                return False

        except Exception as e:
            logger.error(f"íŠ¹ì„± ë²¡í„° ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            log_exception_with_trace(e)
            return False

        # 5ë‹¨ê³„: ê²°ê³¼ ì €ì¥
        logger.info("ğŸ’¾ 5ë‹¨ê³„: ê²°ê³¼ ì €ì¥")

        try:
            # ë¶„ì„ ê²°ê³¼ ì €ì¥
            save_analysis_results(unified_analysis, config)

            # íŠ¹ì„± ë²¡í„° ì €ì¥
            if vectorizer is not None:
                vector_path = vectorizer.save_vector_to_file(combined_vector)
                names_path = vectorizer.save_names_to_file(combined_names)
            else:
                # vectorizerê°€ Noneì¸ ê²½ìš° ì§ì ‘ ì €ì¥
                from pathlib import Path
                import json

                cache_dir = Path("data/cache")
                cache_dir.mkdir(parents=True, exist_ok=True)

                vector_path = cache_dir / "feature_vector_full.npy"
                names_path = cache_dir / "feature_vector_full.names.json"

                np.save(vector_path, combined_vector)
                with open(names_path, "w", encoding="utf-8") as f:
                    json.dump(combined_names, f, ensure_ascii=False, indent=2)

                vector_path = str(vector_path)
                names_path = str(names_path)

            logger.info(f"ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
            logger.info(f"  - ë²¡í„° íŒŒì¼: {vector_path}")
            logger.info(f"  - ì´ë¦„ íŒŒì¼: {names_path}")

        except Exception as e:
            logger.error(f"ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

        # 6ë‹¨ê³„: ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±
        logger.info("ğŸ“ˆ 6ë‹¨ê³„: ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±")

        try:
            execution_time = time.time() - start_time

            # ì„±ëŠ¥ ë³´ê³ ì„œ ë°ì´í„° ìˆ˜ì§‘
            performance_data = {
                "execution_time": execution_time,
                "data_size": len(historical_data),
                "vector_dimensions": len(combined_vector),
                "analysis_results_count": len(analysis_results),
                "memory_usage": get_memory_usage(),
                "data_quality_score": validation_result.quality_score,
                "feature_extraction_quality": extraction_result.quality_metrics,
                "optimization_used": {
                    "process_pool": process_pool_manager is not None,
                    "memory_manager": memory_manager is not None,
                    "hybrid_optimizer": hybrid_optimizer is not None,
                },
            }

            # ì„±ëŠ¥ ë³´ê³ ì„œ ì €ì¥
            save_performance_report(performance_data, "optimized_data_analysis")

            logger.info(f"ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ (ì‹¤í–‰ì‹œê°„: {execution_time:.2f}ì´ˆ)")

        except Exception as e:
            logger.warning(f"ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")

        # ìµœì í™” ì‹œìŠ¤í…œ ì •ë¦¬
        cleanup_optimization_systems()

        logger.info(
            f"âœ… ìµœì í™”ëœ ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ (ì´ {time.time() - start_time:.2f}ì´ˆ)"
        )
        return True

    except Exception as e:
        logger.error(f"âŒ ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return False


def validate_lottery_data(historical_data: List[LotteryNumber]) -> bool:
    """ë¡œë˜ ë°ì´í„° ê²€ì¦"""
    try:
        if not historical_data:
            logger.error("ë¹ˆ ë°ì´í„°ì…‹")
            return False

        for i, draw in enumerate(historical_data):
            # ë²ˆí˜¸ ê°œìˆ˜ ê²€ì¦
            if len(draw.numbers) != 6:
                logger.error(
                    f"íšŒì°¨ {draw.draw_no}: ë²ˆí˜¸ ê°œìˆ˜ ì˜¤ë¥˜ ({len(draw.numbers)}ê°œ)"
                )
                return False

            # ë²ˆí˜¸ ë²”ìœ„ ê²€ì¦
            for number in draw.numbers:
                if not (1 <= number <= 45):
                    logger.error(f"íšŒì°¨ {draw.draw_no}: ë²ˆí˜¸ ë²”ìœ„ ì˜¤ë¥˜ ({number})")
                    return False

            # ì¤‘ë³µ ë²ˆí˜¸ ê²€ì¦
            if len(set(draw.numbers)) != 6:
                logger.error(f"íšŒì°¨ {draw.draw_no}: ì¤‘ë³µ ë²ˆí˜¸ ì¡´ì¬")
                return False

        logger.info(f"ë°ì´í„° ê²€ì¦ ì™„ë£Œ: {len(historical_data)}íšŒì°¨ ëª¨ë‘ ì •ìƒ")
        return True

    except Exception as e:
        logger.error(f"ë°ì´í„° ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
        return False


def run_parallel_analysis(
    historical_data: List[LotteryNumber],
    analyzers: Dict[str, Any],
    config: Dict[str, Any],
) -> Dict[str, Any]:
    """ë³‘ë ¬ ë¶„ì„ ì‹¤í–‰"""
    analysis_results = {}

    try:
        logger.info("ë³‘ë ¬ ë¶„ì„ ì‹œì‘")

        # ë¶„ì„ ì‘ì—… ì •ì˜
        analysis_tasks = [
            ("pattern", analyzers["pattern"]),
            ("distribution", analyzers["distribution"]),
            ("roi", analyzers["roi"]),
            ("pair", analyzers["pair"]),
        ]

        # í”„ë¡œì„¸ìŠ¤ í’€ì´ ìˆìœ¼ë©´ ë³‘ë ¬ ì‹¤í–‰, ì—†ìœ¼ë©´ ìˆœì°¨ ì‹¤í–‰
        if process_pool_manager and len(analysis_tasks) > 1:
            logger.info("í”„ë¡œì„¸ìŠ¤ í’€ì„ ì‚¬ìš©í•œ ë³‘ë ¬ ë¶„ì„")

            with ThreadPoolExecutor(max_workers=len(analysis_tasks)) as executor:
                future_to_name = {}

                for name, analyzer in analysis_tasks:
                    if analyzer:
                        future = executor.submit(
                            safe_analysis_execution, name, analyzer, historical_data
                        )
                        future_to_name[future] = name

                # ê²°ê³¼ ìˆ˜ì§‘
                for future in as_completed(future_to_name):
                    name = future_to_name[future]
                    try:
                        result = future.result(timeout=300)  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
                        if result:
                            analysis_results[name] = result
                            logger.info(f"{name} ë¶„ì„ ì™„ë£Œ")
                        else:
                            logger.warning(f"{name} ë¶„ì„ ê²°ê³¼ ì—†ìŒ")
                    except Exception as e:
                        logger.error(f"{name} ë¶„ì„ ì‹¤íŒ¨: {e}")
        else:
            logger.info("ìˆœì°¨ ë¶„ì„ ì‹¤í–‰")

            for name, analyzer in analysis_tasks:
                if analyzer:
                    try:
                        result = safe_analysis_execution(
                            name, analyzer, historical_data
                        )
                        if result:
                            analysis_results[name] = result
                            logger.info(f"{name} ë¶„ì„ ì™„ë£Œ")
                    except Exception as e:
                        logger.error(f"{name} ë¶„ì„ ì‹¤íŒ¨: {e}")

        logger.info(f"ë³‘ë ¬ ë¶„ì„ ì™„ë£Œ: {len(analysis_results)}ê°œ ê²°ê³¼")
        return analysis_results

    except Exception as e:
        logger.error(f"ë³‘ë ¬ ë¶„ì„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return {}


def safe_analysis_execution(
    name: str, analyzer: Any, historical_data: List[LotteryNumber]
) -> Optional[Dict[str, Any]]:
    """ì•ˆì „í•œ ë¶„ì„ ì‹¤í–‰"""
    try:
        logger.info(f"{name} ë¶„ì„ ì‹œì‘")
        result = analyzer.analyze(historical_data)
        logger.info(f"{name} ë¶„ì„ ì„±ê³µ")
        return result
    except Exception as e:
        logger.error(f"{name} ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        return None


def merge_analysis_results(analysis_results: Dict[str, Any]) -> Dict[str, Any]:
    """ë¶„ì„ ê²°ê³¼ë“¤ì„ í†µí•©"""
    try:
        unified_analysis = {
            "metadata": {
                "analysis_count": len(analysis_results),
                "timestamp": datetime.now().isoformat(),
                "version": "1.0.0",
            }
        }

        # ê° ë¶„ì„ ê²°ê³¼ë¥¼ í†µí•©
        for analysis_type, result in analysis_results.items():
            if result:
                unified_analysis[analysis_type] = result

        logger.info(f"ë¶„ì„ ê²°ê³¼ í†µí•© ì™„ë£Œ: {len(unified_analysis)}ê°œ í•­ëª©")
        return unified_analysis

    except Exception as e:
        logger.error(f"ë¶„ì„ ê²°ê³¼ í†µí•© ì¤‘ ì˜¤ë¥˜: {e}")
        return {}


def validate_feature_vector(
    feature_vector: np.ndarray, feature_names: List[str], config: Dict[str, Any]
) -> bool:
    """íŠ¹ì„± ë²¡í„° í’ˆì§ˆ ê²€ì¦"""
    try:
        # ê¸°ë³¸ ê²€ì¦
        if feature_vector is None or len(feature_vector) == 0:
            logger.error("ë¹ˆ íŠ¹ì„± ë²¡í„°")
            return False

        if feature_names is None or len(feature_names) == 0:
            logger.error("ë¹ˆ íŠ¹ì„± ì´ë¦„ ë¦¬ìŠ¤íŠ¸")
            return False

        # ì°¨ì› ì¼ì¹˜ ê²€ì¦
        if len(feature_vector) != len(feature_names):
            logger.error(
                f"ë²¡í„° ì°¨ì› ë¶ˆì¼ì¹˜: ë²¡í„°={len(feature_vector)}, ì´ë¦„={len(feature_names)}"
            )
            return False

        # ìµœì†Œ ì°¨ì› ê²€ì¦
        try:
            min_dimensions = config["vector"]["min_required_dimension"]
            if len(feature_vector) < min_dimensions:
                logger.error(
                    f"ë²¡í„° ì°¨ì› ë¶€ì¡±: {len(feature_vector)} < {min_dimensions}"
                )
                return False
        except (KeyError, TypeError):
            logger.warning("ìµœì†Œ ì°¨ì› ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")

        # NaN/Inf ê²€ì¦
        if np.any(np.isnan(feature_vector)) or np.any(np.isinf(feature_vector)):
            logger.error("ë²¡í„°ì— NaN ë˜ëŠ” Inf ê°’ í¬í•¨")
            return False

        logger.info(f"íŠ¹ì„± ë²¡í„° ê²€ì¦ ì™„ë£Œ: {len(feature_vector)}ì°¨ì›")
        return True

    except Exception as e:
        logger.error(f"íŠ¹ì„± ë²¡í„° ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
        return False


def save_analysis_results(
    unified_analysis: Dict[str, Any], config: Dict[str, Any]
) -> None:
    """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
    try:
        # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
        result_dir = Path("data/result/analysis")
        result_dir.mkdir(parents=True, exist_ok=True)

        # JSON íŒŒì¼ë¡œ ì €ì¥
        result_file = result_dir / "optimized_analysis_result.json"

        with open(result_file, "w", encoding="utf-8") as f:
            json.dump(
                unified_analysis,
                f,
                ensure_ascii=False,
                indent=2,
                cls=LotteryJSONEncoder,
            )

        logger.info(f"ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {result_file}")

    except Exception as e:
        logger.error(f"ë¶„ì„ ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
        raise


def save_performance_report(performance_data: Dict[str, Any], module_name: str) -> None:
    """ì„±ëŠ¥ ë³´ê³ ì„œ ì €ì¥"""
    try:
        # ë³´ê³ ì„œ ë””ë ‰í† ë¦¬ ìƒì„±
        report_dir = Path("data/result/performance_reports")
        report_dir.mkdir(parents=True, exist_ok=True)

        # ë³´ê³ ì„œ íŒŒì¼ ì €ì¥
        report_file = report_dir / f"{module_name}_performance_report.json"

        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(performance_data, f, ensure_ascii=False, indent=2)

        logger.info(f"ì„±ëŠ¥ ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ: {report_file}")

    except Exception as e:
        logger.error(f"ì„±ëŠ¥ ë³´ê³ ì„œ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")


def get_memory_usage() -> Dict[str, float]:
    """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
    try:
        process = psutil.Process()
        memory_info = process.memory_info()

        return {
            "rss_mb": memory_info.rss / 1024 / 1024,  # MB
            "vms_mb": memory_info.vms / 1024 / 1024,  # MB
            "percent": process.memory_percent(),
        }
    except Exception:
        return {"rss_mb": 0, "vms_mb": 0, "percent": 0}


def cleanup_optimization_systems() -> None:
    """ìµœì í™” ì‹œìŠ¤í…œ ì •ë¦¬"""
    global process_pool_manager, hybrid_optimizer, memory_manager

    try:
        if process_pool_manager:
            process_pool_manager.shutdown()
            logger.info("í”„ë¡œì„¸ìŠ¤ í’€ ì •ë¦¬ ì™„ë£Œ")

        if memory_manager:
            memory_manager.cleanup()
            logger.info("ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì •ë¦¬ ì™„ë£Œ")

        if hybrid_optimizer:
            hybrid_optimizer.cleanup()
            logger.info("í•˜ì´ë¸Œë¦¬ë“œ ìµœì í™” ì‹œìŠ¤í…œ ì •ë¦¬ ì™„ë£Œ")

    except Exception as e:
        logger.warning(f"ìµœì í™” ì‹œìŠ¤í…œ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    finally:
        # ì „ì—­ ë³€ìˆ˜ ì´ˆê¸°í™”
        process_pool_manager = None
        hybrid_optimizer = None
        memory_manager = None


def run_data_analysis() -> bool:
    """
    ê¸°ë³¸ ë°ì´í„° ë¶„ì„ í•¨ìˆ˜ (í•˜ìœ„ í˜¸í™˜ì„±)
    """
    return run_optimized_data_analysis()


def run_fully_optimized_analysis():
    """ì™„ì „ ìµœì í™”ëœ ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸"""

    # ğŸš€ ì „ì—­ ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    from src.utils.memory_manager import get_memory_manager, MemoryConfig
    from src.utils.cuda_optimizers import CudaConfig
    from src.utils.unified_performance import get_profiler

    logger.info("ğŸ‰ ì™„ì „ ìµœì í™”ëœ ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹œì‘")

    # ì „ì—­ ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    memory_config = MemoryConfig(
        max_memory_usage=0.85,
        use_memory_pooling=True,
        pool_size=32,
        auto_cleanup_interval=60.0,
    )
    memory_manager = get_memory_manager(memory_config)

    cuda_config = CudaConfig(
        use_amp=True,
        batch_size=128,
        use_cudnn=True,
    )

    # í”„ë¡œíŒŒì¼ëŸ¬ ì´ˆê¸°í™”
    profiler = get_profiler()

    # ğŸ§  ì „ì—­ ë©”ëª¨ë¦¬ ê´€ë¦¬ ì»¨í…ìŠ¤íŠ¸
    with memory_manager.allocation_scope():
        # ğŸ“ˆ ì „ì²´ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        with profiler.profile("ì™„ì „_ìµœì í™”_ë¶„ì„"):
            # ê¸°ë³¸ ìµœì í™”ëœ ë¶„ì„ ì‹¤í–‰
            return run_optimized_data_analysis()


def create_optimized_analyzer(
    memory_manager, cuda_optimizer, process_pool_manager, hybrid_optimizer
):
    """ìµœì í™”ëœ ë¶„ì„ê¸° ìƒì„±"""
    from src.analysis.unified_analyzer import UnifiedAnalyzer

    config = load_config()
    analyzer = UnifiedAnalyzer(config)

    # ìµœì í™” ì‹œìŠ¤í…œ ì£¼ì…
    if hasattr(analyzer, "set_optimizers"):
        analyzer.set_optimizers(
            memory_manager=memory_manager,
            cuda_optimizer=cuda_optimizer,
            process_pool_manager=process_pool_manager,
            hybrid_optimizer=hybrid_optimizer,
        )

    return analyzer


def create_optimized_vectorizer(memory_manager, cuda_optimizer):
    """ìµœì í™”ëœ ë²¡í„°ë¼ì´ì € ìƒì„±"""
    from src.analysis.pattern_vectorizer import PatternVectorizer

    config = load_config()
    vectorizer = PatternVectorizer(config)

    # ìµœì í™” ì‹œìŠ¤í…œ ì£¼ì…
    if hasattr(vectorizer, "set_optimizers"):
        vectorizer.set_optimizers(
            memory_manager=memory_manager, cuda_optimizer=cuda_optimizer
        )

    return vectorizer


def load_draw_history_optimized(memory_manager=None):
    """ìµœì í™”ëœ ë°ì´í„° ë¡œë“œ"""
    from src.utils.data_loader import load_draw_history

    if memory_manager:
        with memory_manager.allocation_scope():
            return load_draw_history()
    else:
        return load_draw_history()


def save_results_optimized(analysis_result, vectors, memory_manager=None):
    """ìµœì í™”ëœ ê²°ê³¼ ì €ì¥"""
    try:
        if memory_manager:
            with memory_manager.allocation_scope():
                # ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì €ì¥
                _save_with_memory_optimization(analysis_result, vectors)
        else:
            _save_standard(analysis_result, vectors)

        logger.info("âœ… ìµœì í™”ëœ ê²°ê³¼ ì €ì¥ ì™„ë£Œ")

    except Exception as e:
        logger.error(f"ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")


def _save_with_memory_optimization(analysis_result, vectors):
    """ë©”ëª¨ë¦¬ ìµœì í™”ëœ ì €ì¥"""
    import json
    import numpy as np
    from pathlib import Path

    # ë¶„ì„ ê²°ê³¼ ì €ì¥
    analysis_path = Path("data/result/analysis/lottery_data_analysis.json")
    analysis_path.parent.mkdir(parents=True, exist_ok=True)

    with open(analysis_path, "w", encoding="utf-8") as f:
        json.dump(analysis_result, f, ensure_ascii=False, indent=2)

    # ë²¡í„° ì €ì¥
    if vectors is not None:
        vector_path = Path("data/cache/feature_vectors_full.npy")
        vector_path.parent.mkdir(parents=True, exist_ok=True)
        np.save(vector_path, vectors)


def _save_standard(analysis_result, vectors):
    """í‘œì¤€ ì €ì¥"""
    _save_with_memory_optimization(analysis_result, vectors)


def cleanup_optimized_resources(*optimizers):
    """ìµœì í™” ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
    for optimizer in optimizers:
        if optimizer and hasattr(optimizer, "cleanup"):
            try:
                optimizer.cleanup()
            except Exception as e:
                logger.warning(f"ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    logger.info("âœ… ìµœì í™” ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")


def benchmark_optimization_performance():
    """ìµœì í™” ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    logger.info("ìµœì í™” ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")

    try:
        # ê¸°ë³¸ ë¶„ì„ ì‹¤í–‰
        start_time = time.time()
        result = run_optimized_data_analysis()
        duration = time.time() - start_time

        logger.info(f"ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ: {duration:.2f}ì´ˆ, ì„±ê³µ: {result}")
        return {"duration": duration, "success": result}

    except Exception as e:
        logger.error(f"ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return {"duration": 0, "success": False, "error": str(e)}


if __name__ == "__main__":
    success = run_optimized_data_analysis()
    sys.exit(0 if success else 1)
