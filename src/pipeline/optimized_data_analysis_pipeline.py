#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
ìµœì í™”ëœ ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸

ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ë‹¤ìŒ ê¸°ëŠ¥ë“¤ì„ í¬í•¨í•©ë‹ˆë‹¤:
- ì¤‘ë³µ í•¨ìˆ˜ í†µí•© ë° ì œê±°
- ì„¸ë¶„í™”ëœ ìºì‹± ì‹œìŠ¤í…œ
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²­í¬ ì²˜ë¦¬
- ë³‘ë ¬ ì²˜ë¦¬ ì§€ì› (ProcessPool í†µí•©)
- í•˜ì´ë¸Œë¦¬ë“œ ìµœì í™” ì‹œìŠ¤í…œ
- ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ìµœì í™”
"""

import json
import logging
import os
import sys
import time
import random
import traceback
from pathlib import Path
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple, Union
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
import hashlib

import numpy as np
import scipy.stats
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import torch
import psutil

from src.utils.error_handler import (
    get_logger,
    log_exception_with_trace,
    StrictErrorHandler,
    strict_error_handler,
    validate_and_fail_fast,
)
from src.utils.state_vector_cache import get_cache
from src.utils.data_loader import load_draw_history, LotteryJSONEncoder
from src.utils.config_loader import load_config
from src.utils.profiler import get_profiler

# ìµœì í™” ì‹œìŠ¤í…œ import
from src.utils.process_pool_manager import get_process_pool_manager
from src.utils.hybrid_optimizer import get_hybrid_optimizer, optimize
from src.utils.memory_manager import get_memory_manager

from src.analysis.pattern_analyzer import PatternAnalyzer
from src.analysis.pattern_vectorizer import PatternVectorizer
from src.utils.report_writer import safe_convert, save_physical_performance_report
from src.analysis.pair_analyzer import PairAnalyzer
from src.utils.feature_vector_validator import (
    validate_feature_vector_with_config,
    check_vector_dimensions,
)

# ìµœì í™”ëœ í•¨ìˆ˜ë“¤ import
from src.shared.graph_utils import (
    calculate_pair_frequency,
    calculate_segment_entropy,
    calculate_number_gaps,
    calculate_cluster_distribution,
    clear_cache,
    get_cache_stats,
)

# ë¡œê±° ì„¤ì •
logger = get_logger(__name__)

# ì„±ëŠ¥ ìµœì í™” ì„¤ì •
CHUNK_PROCESSING_CONFIG = {
    "historical_data": 100,  # 100íšŒì°¨ì”© ì²˜ë¦¬
    "vector_generation": 50,  # 50ê°œì”© ë²¡í„° ìƒì„±
    "cache_flush_interval": 200,  # 200íšŒì°¨ë§ˆë‹¤ ìºì‹œ í”ŒëŸ¬ì‹œ
    "parallel_workers": min(4, psutil.cpu_count()),  # CPU ì½”ì–´ ìˆ˜ì— ë”°ë¥¸ ì›Œì»¤ ìˆ˜
    "memory_threshold": 0.8,  # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 80% ì„ê³„ì 
}

CACHE_STRATEGY = {
    "data_load": "data_size_hash",
    "pattern_analysis": "data_hash + config_hash",
    "vectorization": "analysis_hash + vector_config_hash",
    "additional_analysis": "pattern_hash + addon_config_hash",
}

# ì „ì—­ ì—„ê²©í•œ ì—ëŸ¬ í•¸ë“¤ëŸ¬
strict_handler = StrictErrorHandler()

# ì „ì—­ ìµœì í™” ì‹œìŠ¤í…œë“¤
process_pool_manager = None
hybrid_optimizer = None
memory_manager = None


def initialize_optimization_systems(config: Dict[str, Any]):
    """ìµœì í™” ì‹œìŠ¤í…œë“¤ ì´ˆê¸°í™”"""
    global process_pool_manager, hybrid_optimizer, memory_manager

    try:
        # ìµœì í™” ì„¤ì • ë¡œë“œ
        optimization_config = config.get("optimization", {})

        # ProcessPool ê´€ë¦¬ì ì´ˆê¸°í™”
        process_pool_config = optimization_config.get("process_pool", {})
        process_pool_manager = get_process_pool_manager(process_pool_config)
        logger.info("ProcessPool ê´€ë¦¬ì ì´ˆê¸°í™” ì™„ë£Œ")

        # ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™”
        memory_config = optimization_config.get("memory_pool", {})
        memory_manager = get_memory_manager(memory_config)
        logger.info("ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™” ì™„ë£Œ")

        # í•˜ì´ë¸Œë¦¬ë“œ ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        hybrid_config = optimization_config.get("hybrid", {})
        hybrid_optimizer = get_hybrid_optimizer(hybrid_config)
        logger.info("í•˜ì´ë¸Œë¦¬ë“œ ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")

    except Exception as e:
        logger.error(f"ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        # ìµœì í™” ì‹œìŠ¤í…œ ì—†ì´ë„ ë™ì‘í•  ìˆ˜ ìˆë„ë¡ Noneìœ¼ë¡œ ì„¤ì •
        process_pool_manager = None
        hybrid_optimizer = None
        memory_manager = None


class OptimizedPerformanceTracker:
    """ìµœì í™”ëœ ì„±ëŠ¥ ì¶”ì ê¸°"""

    def __init__(self):
        self.metrics = {}
        self.cache_hits = 0
        self.cache_misses = 0
        self.memory_usage = []
        self.processing_times = {}
        self.optimization_stats = {}

    def track_cache_hit(self):
        self.cache_hits += 1

    def track_cache_miss(self):
        self.cache_misses += 1

    def get_cache_hit_rate(self) -> float:
        total = self.cache_hits + self.cache_misses
        return self.cache_hits / total if total > 0 else 0.0

    def track_memory_usage(self):
        memory_info = psutil.virtual_memory()
        self.memory_usage.append(
            {
                "timestamp": time.time(),
                "used_percent": memory_info.percent,
                "available_mb": memory_info.available / (1024 * 1024),
            }
        )

    def track_processing_time(self, operation: str, duration: float):
        if operation not in self.processing_times:
            self.processing_times[operation] = []
        self.processing_times[operation].append(duration)

    def track_optimization_result(self, operation: str, strategy: str, speedup: float):
        """ìµœì í™” ê²°ê³¼ ì¶”ì """
        if operation not in self.optimization_stats:
            self.optimization_stats[operation] = {}

        if strategy not in self.optimization_stats[operation]:
            self.optimization_stats[operation][strategy] = {
                "count": 0,
                "total_speedup": 0.0,
                "avg_speedup": 0.0,
            }

        stats = self.optimization_stats[operation][strategy]
        stats["count"] += 1
        stats["total_speedup"] += speedup
        stats["avg_speedup"] = stats["total_speedup"] / stats["count"]

    def get_performance_summary(self) -> Dict[str, Any]:
        return {
            "cache_hit_rate": self.get_cache_hit_rate(),
            "total_cache_operations": self.cache_hits + self.cache_misses,
            "avg_memory_usage": (
                np.mean([m["used_percent"] for m in self.memory_usage])
                if self.memory_usage
                else 0
            ),
            "processing_times": {
                op: {
                    "avg": np.mean(times),
                    "min": np.min(times),
                    "max": np.max(times),
                    "count": len(times),
                }
                for op, times in self.processing_times.items()
            },
            "optimization_stats": self.optimization_stats,
        }


@optimize(
    task_info={
        "function_type": "analysis",
        "parallelizable": True,
        "gpu_compatible": False,
    }
)
def optimized_pattern_analysis(
    data_chunk: List, analyzer: PatternAnalyzer
) -> Dict[str, Any]:
    """ìµœì í™”ëœ íŒ¨í„´ ë¶„ì„"""
    try:
        # ë©”ëª¨ë¦¬ ê´€ë¦¬ ìŠ¤ì½”í”„ ë‚´ì—ì„œ ì‹¤í–‰
        if memory_manager:
            with memory_manager.allocation_scope():
                return analyzer.analyze(data_chunk)
        else:
            return analyzer.analyze(data_chunk)
    except Exception as e:
        logger.error(f"íŒ¨í„´ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return {}


@optimize(
    task_info={
        "function_type": "vectorize",
        "parallelizable": True,
        "gpu_compatible": False,
    }
)
def optimized_vectorization(
    patterns: List, vectorizer: PatternVectorizer
) -> Tuple[np.ndarray, List[str]]:
    """ìµœì í™”ëœ ë²¡í„°í™”"""
    try:
        if memory_manager:
            with memory_manager.allocation_scope():
                return vectorizer.vectorize(patterns)
        else:
            return vectorizer.vectorize(patterns)
    except Exception as e:
        logger.error(f"ë²¡í„°í™” ì‹¤íŒ¨: {e}")
        return np.array([]), []


def generate_cache_key(data_info: str, operation: str, **kwargs) -> str:
    """ìµœì í™”ëœ ìºì‹œ í‚¤ ìƒì„±"""
    try:
        # ë°ì´í„° ì •ë³´ì™€ ë§¤ê°œë³€ìˆ˜ë¥¼ ê²°í•©í•˜ì—¬ í•´ì‹œ ìƒì„±
        combined_info = f"{data_info}_{operation}_{str(sorted(kwargs.items()))}"
        return hashlib.md5(combined_info.encode()).hexdigest()[:16]
    except Exception:
        # í•´ì‹œ ìƒì„± ì‹¤íŒ¨ì‹œ ê¸°ë³¸ í‚¤
        return f"{operation}_{hash(str(kwargs)) % 100000}"


def check_memory_usage() -> bool:
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸"""
    memory_info = psutil.virtual_memory()
    return memory_info.percent < (CHUNK_PROCESSING_CONFIG["memory_threshold"] * 100)


def process_data_chunks_optimized(data: List, chunk_size: int, process_func, **kwargs):
    """ìµœì í™”ëœ ì²­í¬ ë‹¨ìœ„ ë°ì´í„° ì²˜ë¦¬"""
    global process_pool_manager

    # ProcessPoolì´ ì‚¬ìš© ê°€ëŠ¥í•˜ê³  ë°ì´í„°ê°€ ì¶©ë¶„íˆ í° ê²½ìš° ë³‘ë ¬ ì²˜ë¦¬
    if process_pool_manager and len(data) > 200:
        try:
            chunks = process_pool_manager.chunk_and_split(data, chunk_size)
            results = process_pool_manager.parallel_analyze(
                chunks, process_func, **kwargs
            )
            return process_pool_manager.merge_results(results)
        except Exception as e:
            logger.warning(f"ë³‘ë ¬ ì²˜ë¦¬ ì‹¤íŒ¨, ìˆœì°¨ ì²˜ë¦¬ë¡œ ì „í™˜: {e}")

    # ìˆœì°¨ ì²˜ë¦¬ (ê¸°ì¡´ ë°©ì‹)
    results = []
    total_chunks = len(data) // chunk_size + (1 if len(data) % chunk_size else 0)

    for i in range(0, len(data), chunk_size):
        if not check_memory_usage():
            logger.warning(
                "ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì„ê³„ì ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."
            )
            import gc

            gc.collect()

        chunk = data[i : min(i + chunk_size, len(data))]
        chunk_result = process_func(chunk, **kwargs)
        results.append(chunk_result)

        if (i // chunk_size + 1) % 10 == 0:
            logger.info(f"ì²­í¬ ì²˜ë¦¬ ì§„í–‰ë¥ : {i // chunk_size + 1}/{total_chunks}")

    return results


def safe_analysis_step(step_name: str, func, *args, **kwargs):
    """íŠ¸ëœì­ì…˜ ë˜í¼ - ì•ˆì „í•œ ë¶„ì„ ë‹¨ê³„ ì‹¤í–‰"""
    try:
        logger.info(f"{step_name} ì‹œì‘")
        start_time = time.time()

        result = func(*args, **kwargs)

        duration = time.time() - start_time
        logger.info(f"{step_name} ì™„ë£Œ ({duration:.2f}ì´ˆ)")
        return result

    except Exception as e:
        logger.error(f"{step_name} ì‹¤íŒ¨: {str(e)}")
        # ë¡¤ë°± ë¡œì§ (í•„ìš”ì‹œ ì¶”ê°€)
        raise RuntimeError(f"{step_name} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


@strict_error_handler("ìµœì í™”ëœ ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸", exit_on_error=True)
def run_optimized_data_analysis() -> bool:
    """
    ìµœì í™”ëœ ë°ì´í„° ë¶„ì„ ë° ì „ì²˜ë¦¬ ì‹¤í–‰

    Returns:
        bool: ì‘ì—… ì„±ê³µ ì—¬ë¶€ (ì‹¤íŒ¨ ì‹œ ì‹œìŠ¤í…œ ì¢…ë£Œ)
    """
    start_time = time.time()
    performance_tracker = OptimizedPerformanceTracker()

    # ë©”ëª¨ë¦¬ ê´€ë¦¬ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì‚¬ìš©
    from src.utils.memory_manager import memory_managed_analysis
    from src.utils.profiler import get_profiler

    # í”„ë¡œíŒŒì¼ëŸ¬ ì´ˆê¸°í™”
    profiler = get_profiler()

    with memory_managed_analysis():
        with profiler.profile("ì „ì²´_íŒŒì´í”„ë¼ì¸"):
            logger.info("ğŸš€ ìµœì í™”ëœ ë°ì´í„° ë¶„ì„ ë° ì „ì²˜ë¦¬ ì‹œì‘")

            # 1. ì„¤ì • ë¡œë“œ - ì‹¤íŒ¨ ì‹œ ì¦‰ì‹œ ì¢…ë£Œ
            with profiler.profile("ì„¤ì •_ë¡œë“œ"):
                logger.info("ì„¤ì •_ë¡œë“œ ì‹œì‘")

                config = load_config()
                validate_and_fail_fast(
                    config is not None, "ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ - ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤"
                )

                logger.info(f"ì„¤ì •_ë¡œë“œ ì™„ë£Œ ({time.time() - start_time:.2f}ì´ˆ)")
                logger.info("âœ… ì„¤ì • ë¡œë“œ ì™„ë£Œ")

                # 1.5. ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™”
                initialize_optimization_systems(config)
                logger.info("âœ… ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")

            # 2. ë°ì´í„° ë¡œë“œ ë° ê²€ì¦ - ì‹¤íŒ¨ ì‹œ ì¦‰ì‹œ ì¢…ë£Œ
            with profiler.profile("ë°ì´í„°_ë¡œë“œ"):
                logger.info("ğŸš€ 1ë‹¨ê³„: ìµœì í™”ëœ ê³¼ê±° ë‹¹ì²¨ ë²ˆí˜¸ ë°ì´í„° ë¡œë“œ ì¤‘...")
                logger.info("ë°ì´í„°_ë¡œë“œ ì‹œì‘")

                historical_data = load_draw_history(
                    validate_data=True
                )  # ì—„ê²©í•œ ê²€ì¦ í™œì„±í™”
                validate_and_fail_fast(
                    historical_data and len(historical_data) > 0,
                    f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ ë˜ëŠ” ë¹ˆ ë°ì´í„°: {len(historical_data) if historical_data else 0}ê°œ",
                    historical_data,
                )

                logger.info(f"ë°ì´í„°_ë¡œë“œ ì™„ë£Œ ({time.time() - start_time:.2f}ì´ˆ)")
                logger.info(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(historical_data)}ê°œ íšŒì°¨")

            # ë¶„ì„ê¸° ì´ˆê¸°í™” - ì‹¤íŒ¨ ì‹œ ì¦‰ì‹œ ì¢…ë£Œ
            performance_tracker.track_memory_usage()

            def init_analyzer(analyzer_type: str):
                """ë¶„ì„ê¸° ì´ˆê¸°í™” ë˜í¼"""
                if analyzer_type == "pair":
                    from src.analysis.pair_analyzer import PairAnalyzer

                    analyzer = PairAnalyzer()
                    validate_and_fail_fast(
                        analyzer is not None, f"{analyzer_type} ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨"
                    )
                    return analyzer
                elif analyzer_type == "vectorizer":
                    from src.analysis.pattern_vectorizer import PatternVectorizer

                    analyzer = PatternVectorizer()
                    validate_and_fail_fast(
                        analyzer is not None, f"{analyzer_type} ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨"
                    )
                    return analyzer
                elif analyzer_type == "pattern":
                    from src.analysis.pattern_analyzer import PatternAnalyzer

                    analyzer = PatternAnalyzer()
                    validate_and_fail_fast(
                        analyzer is not None, f"{analyzer_type} ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨"
                    )
                    return analyzer
                else:
                    strict_handler.handle_critical_error(
                        ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ë¶„ì„ê¸° íƒ€ì…: {analyzer_type}"),
                        "ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨",
                    )
                    return None

            # ë¶„ì„ê¸°ë“¤ ì´ˆê¸°í™”
            from src.analysis.pair_analyzer import PairAnalyzer
            from src.analysis.pattern_vectorizer import PatternVectorizer
            from src.analysis.pattern_analyzer import PatternAnalyzer

            pair_analyzer: PairAnalyzer = init_analyzer("pair")
            logger.info("âœ… pair ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")

            pattern_vectorizer: PatternVectorizer = init_analyzer("vectorizer")
            logger.info("âœ… vectorizer ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")

            pattern_analyzer: PatternAnalyzer = init_analyzer("pattern")
            logger.info("âœ… pattern ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")

            logger.info("âœ… ëª¨ë“  ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")

            # ë””ë ‰í† ë¦¬ ì„¤ì • ë° ìƒì„± - ì‹¤íŒ¨ ì‹œ ì¦‰ì‹œ ì¢…ë£Œ
            result_dir = Path("data/result")
            cache_dir = Path("data/cache")
            analysis_dir = result_dir / "analysis"

            # ë””ë ‰í† ë¦¬ ìƒì„±
            for directory in [result_dir, analysis_dir, cache_dir]:
                directory.mkdir(parents=True, exist_ok=True)
                validate_and_fail_fast(
                    directory.exists(), f"ë””ë ‰í† ë¦¬ ìƒì„± ì‹¤íŒ¨: {directory}"
                )

            logger.info("âœ… ë””ë ‰í† ë¦¬ ì„¤ì • ì™„ë£Œ")

            # 3. ìµœì í™”ëœ íŒ¨í„´ ë¶„ì„ ì‹¤í–‰ - ì‹¤íŒ¨ ì‹œ ì¦‰ì‹œ ì¢…ë£Œ
            with profiler.profile("ìµœì í™”ëœ_íŒ¨í„´_ë¶„ì„"):
                logger.info("ğŸš€ 2ë‹¨ê³„: ìµœì í™”ëœ íŒ¨í„´ ë¶„ì„ ìˆ˜í–‰ ì¤‘...")

                analysis_start = time.time()

                # ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„ì„ ìˆ˜í–‰
                chunk_size = CHUNK_PROCESSING_CONFIG["historical_data"]

                def analyze_chunk(chunk_data):
                    result = pattern_analyzer.analyze(chunk_data)
                    validate_and_fail_fast(
                        result is not None,
                        f"íŒ¨í„´ ë¶„ì„ ì‹¤íŒ¨: ì²­í¬ í¬ê¸° {len(chunk_data)}",
                    )
                    return result

                # ì „ì²´ ë°ì´í„° ë¶„ì„ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
                if len(historical_data) > chunk_size:
                    logger.info(
                        f"ëŒ€ìš©ëŸ‰ ë°ì´í„° ê°ì§€: {len(historical_data)}ê°œ -> {chunk_size} ë‹¨ìœ„ë¡œ ì²˜ë¦¬"
                    )
                    chunk_results = process_data_chunks_optimized(
                        historical_data, chunk_size, analyze_chunk
                    )
                    validate_and_fail_fast(
                        chunk_results is not None and len(chunk_results) > 0,
                        "ì²­í¬ ë¶„ì„ ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŒ",
                    )

                    # ì²­í¬ ê²°ê³¼ ë³‘í•©
                    analysis_result = pattern_analyzer.merge_analysis_results(
                        chunk_results
                    )
                    validate_and_fail_fast(
                        analysis_result is not None, "ì²­í¬ ê²°ê³¼ ë³‘í•© ì‹¤íŒ¨"
                    )
                else:
                    analysis_result = analyze_chunk(historical_data)

                analysis_duration = time.time() - analysis_start
                performance_tracker.track_processing_time(
                    "pattern_analysis", analysis_duration
                )

                logger.info(f"âœ… íŒ¨í„´ ë¶„ì„ ì™„ë£Œ ({analysis_duration:.2f}ì´ˆ)")
                performance_tracker.track_memory_usage()

            # 4. ìµœì í™”ëœ ì¶”ê°€ ë¶„ì„ (ë³‘ë ¬ ì²˜ë¦¬) - ì•ˆì „í•œ ThreadPoolExecutor ì‚¬ìš©
            with profiler.profile("ìµœì í™”ëœ_ì¶”ê°€_ë¶„ì„"):
                logger.info("ğŸš€ 3ë‹¨ê³„: ìµœì í™”ëœ ì¶”ê°€ ë¶„ì„ ìˆ˜í–‰ ì¤‘...")

                additional_analysis_start = time.time()

                def run_additional_analysis():
                    """ì¶”ê°€ ë¶„ì„ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰"""
                    analysis_tasks = {}

                    with ThreadPoolExecutor(
                        max_workers=CHUNK_PROCESSING_CONFIG["parallel_workers"]
                    ) as executor:
                        # ê° ë¶„ì„ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
                        futures = {
                            executor.submit(
                                calculate_pair_frequency,
                                historical_data,
                                logger=logger,
                                chunk_size=chunk_size,
                            ): "pair_frequency",
                            executor.submit(
                                calculate_segment_entropy,
                                historical_data,
                                segments=5,
                                logger=logger,
                            ): "segment_entropy",
                            executor.submit(
                                calculate_number_gaps,
                                historical_data,
                                logger=logger,
                            ): "number_gaps",
                            executor.submit(
                                calculate_cluster_distribution,
                                historical_data,
                                n_clusters=5,
                                logger=logger,
                            ): "cluster_distribution",
                        }

                        # ê²°ê³¼ ìˆ˜ì§‘
                        for future in as_completed(futures):
                            analysis_type = futures[future]
                            result = future.result()
                            validate_and_fail_fast(
                                result is not None, f"{analysis_type} ë¶„ì„ ì‹¤íŒ¨"
                            )
                            analysis_tasks[analysis_type] = result
                            logger.info(f"âœ… {analysis_type} ë¶„ì„ ì™„ë£Œ")

                        # ThreadPoolExecutor ì •ë¦¬ í™•ì¸
                        logger.debug("ì¶”ê°€ ë¶„ì„ ThreadPoolExecutor ì •ë¦¬ ì™„ë£Œ")

                    return analysis_tasks

                additional_results = run_additional_analysis()
                validate_and_fail_fast(
                    additional_results is not None and len(additional_results) > 0,
                    "ì¶”ê°€ ë¶„ì„ ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŒ",
                )

                additional_duration = time.time() - additional_analysis_start
                performance_tracker.track_processing_time(
                    "additional_analysis", additional_duration
                )

                logger.info(f"âœ… ì¶”ê°€ ë¶„ì„ ì™„ë£Œ ({additional_duration:.2f}ì´ˆ)")
                performance_tracker.track_memory_usage()

            # 5. ìµœì í™”ëœ ë²¡í„°í™” - ì‹¤íŒ¨ ì‹œ ì¦‰ì‹œ ì¢…ë£Œ
            with profiler.profile("ìµœì í™”ëœ_ë²¡í„°í™”"):
                logger.info("ğŸš€ 4ë‹¨ê³„: ìµœì í™”ëœ ë²¡í„°í™” ìˆ˜í–‰ ì¤‘...")

                vectorization_start = time.time()

                # ë¶„ì„ ê²°ê³¼ë¥¼ ë³‘í•©
                combined_analysis = dict(analysis_result)
                combined_analysis.update(
                    {
                        "pair_frequency": additional_results.get("pair_frequency", {}),
                        "segment_entropy": additional_results.get(
                            "segment_entropy", np.array([])
                        ),
                        "number_gaps": additional_results.get("number_gaps", {}),
                        "cluster_distribution": additional_results.get(
                            "cluster_distribution", ({}, {})
                        ),
                    }
                )

                validate_and_fail_fast(
                    combined_analysis is not None and len(combined_analysis) > 0,
                    "ë³‘í•©ëœ ë¶„ì„ ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŒ",
                )

                # ë²¡í„°í™” ìˆ˜í–‰
                feature_vectors, feature_names = (
                    pattern_vectorizer.vectorize_extended_features(combined_analysis)
                )

                validate_and_fail_fast(
                    feature_vectors is not None and feature_names is not None,
                    "ë²¡í„°í™” ì‹¤íŒ¨",
                )
                validate_and_fail_fast(
                    len(feature_vectors) > 0 and len(feature_names) > 0,
                    f"ë²¡í„°í™” ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŒ: vectors={len(feature_vectors)}, names={len(feature_names)}",
                )

                vectorization_duration = time.time() - vectorization_start
                performance_tracker.track_processing_time(
                    "vectorization", vectorization_duration
                )

                logger.info(
                    f"âœ… ë²¡í„°í™” ì™„ë£Œ: {feature_vectors.shape} ({vectorization_duration:.2f}ì´ˆ)"
                )

            # 6. ê²°ê³¼ ì €ì¥ ë° ê²€ì¦ - ì‹¤íŒ¨ ì‹œ ì¦‰ì‹œ ì¢…ë£Œ
            with profiler.profile("ê²°ê³¼_ì €ì¥_ê²€ì¦"):
                logger.info("ğŸš€ 5ë‹¨ê³„: ê²°ê³¼ ì €ì¥ ë° ê²€ì¦ ì¤‘...")

                # ë²¡í„° ë° íŠ¹ì„± ì´ë¦„ ì €ì¥
                vector_file = cache_dir / "feature_vectors_full.npy"
                names_file = cache_dir / "feature_vector_full.names.json"

                np.save(vector_file, feature_vectors)
                validate_and_fail_fast(
                    vector_file.exists(), f"ë²¡í„° íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {vector_file}"
                )

                with open(names_file, "w", encoding="utf-8") as f:
                    json.dump(feature_names, f, indent=2, ensure_ascii=False)
                validate_and_fail_fast(
                    names_file.exists(), f"íŠ¹ì„± ì´ë¦„ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {names_file}"
                )

                # ë¶„ì„ ê²°ê³¼ ì €ì¥
                analysis_file = analysis_dir / "optimized_analysis_result.json"
                with open(analysis_file, "w", encoding="utf-8") as f:
                    json.dump(
                        combined_analysis,
                        f,
                        indent=2,
                        ensure_ascii=False,
                        cls=LotteryJSONEncoder,
                    )
                validate_and_fail_fast(
                    analysis_file.exists(), f"ë¶„ì„ ê²°ê³¼ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {analysis_file}"
                )

                logger.info("âœ… ëª¨ë“  ê²°ê³¼ ì €ì¥ ì™„ë£Œ")

            # ìµœì¢… ì„±ëŠ¥ ë¦¬í¬íŠ¸
            total_duration = time.time() - start_time
            performance_summary = performance_tracker.get_performance_summary()

            logger.info("âœ… ìµœì í™”ëœ ë°ì´í„° ë¶„ì„ ì™„ë£Œ!")
            logger.info(f"ğŸ“Š ì „ì²´ ì‹¤í–‰ ì‹œê°„: {total_duration:.2f}ì´ˆ")
            logger.info(f"ğŸ“ˆ ì„±ëŠ¥ ìš”ì•½: {performance_summary}")

            # ì„±ëŠ¥ ë¦¬í¬íŠ¸ ì €ì¥
            performance_file = analysis_dir / "performance_report.json"
            with open(performance_file, "w", encoding="utf-8") as f:
                json.dump(
                    {
                        "total_duration": total_duration,
                        "performance_summary": performance_summary,
                        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                    },
                    f,
                    indent=2,
                    ensure_ascii=False,
                )

            return True


def run_data_analysis() -> bool:
    """
    ê¸°ë³¸ ë°ì´í„° ë¶„ì„ í•¨ìˆ˜ (í•˜ìœ„ í˜¸í™˜ì„±)
    """
    return run_optimized_data_analysis()


if __name__ == "__main__":
    success = run_optimized_data_analysis()
    sys.exit(0 if success else 1)
