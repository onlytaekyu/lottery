#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
ìµœì í™”ëœ ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸

ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ë‹¤ìŒ ê¸°ëŠ¥ë“¤ì„ í¬í•¨í•©ë‹ˆë‹¤:
- ì¤‘ë³µ í•¨ìˆ˜ í†µí•© ë° ì œê±°
- ì„¸ë¶„í™”ëœ ìºì‹± ì‹œìŠ¤í…œ
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²­í¬ ì²˜ë¦¬
- ë³‘ë ¬ ì²˜ë¦¬ ì§€ì›
- ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ìµœì í™”
"""

import json
import logging
import os
import sys
import time
import random
import traceback
from pathlib import Path
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple, Union
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
import hashlib

import numpy as np
import scipy.stats
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import torch
import psutil

from src.utils.error_handler import get_logger, log_exception_with_trace
from src.utils.state_vector_cache import get_cache
from src.utils.data_loader import load_draw_history
from src.utils.config_loader import load_config
from src.utils.profiler import get_profiler
from src.analysis.pattern_analyzer import PatternAnalyzer
from src.analysis.pattern_vectorizer import PatternVectorizer
from src.utils.report_writer import safe_convert, save_physical_performance_report
from src.analysis.pair_analyzer import PairAnalyzer
from src.utils.feature_vector_validator import (
    validate_feature_vector_with_config,
    check_vector_dimensions,
)

# ìµœì í™”ëœ í•¨ìˆ˜ë“¤ import
from src.shared.graph_utils import (
    calculate_pair_frequency,
    calculate_segment_entropy,
    calculate_number_gaps,
    calculate_cluster_distribution,
    clear_cache,
    get_cache_stats,
)

# ë¡œê±° ì„¤ì •
logger = get_logger(__name__)

# ì„±ëŠ¥ ìµœì í™” ì„¤ì •
CHUNK_PROCESSING_CONFIG = {
    "historical_data": 100,  # 100íšŒì°¨ì”© ì²˜ë¦¬
    "vector_generation": 50,  # 50ê°œì”© ë²¡í„° ìƒì„±
    "cache_flush_interval": 200,  # 200íšŒì°¨ë§ˆë‹¤ ìºì‹œ í”ŒëŸ¬ì‹œ
    "parallel_workers": min(4, psutil.cpu_count()),  # CPU ì½”ì–´ ìˆ˜ì— ë”°ë¥¸ ì›Œì»¤ ìˆ˜
    "memory_threshold": 0.8,  # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 80% ì„ê³„ì 
}

CACHE_STRATEGY = {
    "data_load": "data_size_hash",
    "pattern_analysis": "data_hash + config_hash",
    "vectorization": "analysis_hash + vector_config_hash",
    "additional_analysis": "pattern_hash + addon_config_hash",
}


class OptimizedPerformanceTracker:
    """ìµœì í™”ëœ ì„±ëŠ¥ ì¶”ì ê¸°"""

    def __init__(self):
        self.metrics = {}
        self.cache_hits = 0
        self.cache_misses = 0
        self.memory_usage = []
        self.processing_times = {}

    def track_cache_hit(self):
        self.cache_hits += 1

    def track_cache_miss(self):
        self.cache_misses += 1

    def get_cache_hit_rate(self) -> float:
        total = self.cache_hits + self.cache_misses
        return self.cache_hits / total if total > 0 else 0.0

    def track_memory_usage(self):
        memory_info = psutil.virtual_memory()
        self.memory_usage.append(
            {
                "timestamp": time.time(),
                "used_percent": memory_info.percent,
                "available_mb": memory_info.available / (1024 * 1024),
            }
        )

    def track_processing_time(self, operation: str, duration: float):
        if operation not in self.processing_times:
            self.processing_times[operation] = []
        self.processing_times[operation].append(duration)

    def get_performance_summary(self) -> Dict[str, Any]:
        return {
            "cache_hit_rate": self.get_cache_hit_rate(),
            "total_cache_operations": self.cache_hits + self.cache_misses,
            "avg_memory_usage": (
                np.mean([m["used_percent"] for m in self.memory_usage])
                if self.memory_usage
                else 0
            ),
            "processing_times": {
                op: {
                    "avg": np.mean(times),
                    "min": np.min(times),
                    "max": np.max(times),
                    "count": len(times),
                }
                for op, times in self.processing_times.items()
            },
        }


def generate_cache_key(data_info: str, operation: str, **kwargs) -> str:
    """ìµœì í™”ëœ ìºì‹œ í‚¤ ìƒì„±"""
    try:
        # ë°ì´í„° ì •ë³´ì™€ ë§¤ê°œë³€ìˆ˜ë¥¼ ê²°í•©í•˜ì—¬ í•´ì‹œ ìƒì„±
        combined_info = f"{data_info}_{operation}_{str(sorted(kwargs.items()))}"
        return hashlib.md5(combined_info.encode()).hexdigest()[:16]
    except Exception:
        # í•´ì‹œ ìƒì„± ì‹¤íŒ¨ì‹œ ê¸°ë³¸ í‚¤
        return f"{operation}_{hash(str(kwargs)) % 100000}"


def check_memory_usage() -> bool:
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸"""
    memory_info = psutil.virtual_memory()
    return memory_info.percent < (CHUNK_PROCESSING_CONFIG["memory_threshold"] * 100)


def process_data_chunks(data: List, chunk_size: int, process_func, **kwargs):
    """ì²­í¬ ë‹¨ìœ„ë¡œ ë°ì´í„° ì²˜ë¦¬"""
    results = []
    total_chunks = len(data) // chunk_size + (1 if len(data) % chunk_size else 0)

    for i in range(0, len(data), chunk_size):
        if not check_memory_usage():
            logger.warning(
                "ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì„ê³„ì ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."
            )
            import gc

            gc.collect()

        chunk = data[i : min(i + chunk_size, len(data))]
        chunk_result = process_func(chunk, **kwargs)
        results.append(chunk_result)

        if (i // chunk_size + 1) % 10 == 0:
            logger.info(f"ì²­í¬ ì²˜ë¦¬ ì§„í–‰ë¥ : {i // chunk_size + 1}/{total_chunks}")

    return results


def safe_analysis_step(step_name: str, func, *args, **kwargs):
    """íŠ¸ëœì­ì…˜ ë˜í¼ - ì•ˆì „í•œ ë¶„ì„ ë‹¨ê³„ ì‹¤í–‰"""
    try:
        logger.info(f"{step_name} ì‹œì‘")
        start_time = time.time()

        result = func(*args, **kwargs)

        duration = time.time() - start_time
        logger.info(f"{step_name} ì™„ë£Œ ({duration:.2f}ì´ˆ)")
        return result

    except Exception as e:
        logger.error(f"{step_name} ì‹¤íŒ¨: {str(e)}")
        # ë¡¤ë°± ë¡œì§ (í•„ìš”ì‹œ ì¶”ê°€)
        raise RuntimeError(f"{step_name} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


def run_optimized_data_analysis() -> bool:
    """
    ìµœì í™”ëœ ë°ì´í„° ë¶„ì„ ë° ì „ì²˜ë¦¬ ì‹¤í–‰

    Returns:
        bool: ì‘ì—… ì„±ê³µ ì—¬ë¶€
    """
    start_time = time.time()
    performance_tracker = OptimizedPerformanceTracker()

    # ë©”ëª¨ë¦¬ ê´€ë¦¬ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì‚¬ìš©
    from src.utils.memory_manager import memory_managed_analysis

    with memory_managed_analysis():
        # ëœë¤ ì‹œë“œ ì„¤ì • (ì¬í˜„ì„± ë³´ì¥)
        random.seed(42)
        np.random.seed(42)

        # ì„¤ì • ë¡œë“œ
        try:
            config = safe_analysis_step("ì„¤ì •_ë¡œë“œ", load_config)
            logger.info("âœ… ì„¤ì • ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            log_exception_with_trace(logger, e, "ìµœì í™”ëœ ë°ì´í„° ë¶„ì„: ì„¤ì • ë¡œë“œ ì‹¤íŒ¨")
            return False

        # í”„ë¡œíŒŒì¼ëŸ¬ ì´ˆê¸°í™”
        profiler = get_profiler()

        try:
            # 1. ë°ì´í„° ë¡œë“œ (ìµœì í™”ëœ ë²„ì „)
            with profiler.profile("ìµœì í™”ëœ_ë°ì´í„°_ë¡œë“œ"):
                logger.info("ğŸš€ 1ë‹¨ê³„: ìµœì í™”ëœ ê³¼ê±° ë‹¹ì²¨ ë²ˆí˜¸ ë°ì´í„° ë¡œë“œ ì¤‘...")

                try:
                    historical_data = safe_analysis_step(
                        "ë°ì´í„°_ë¡œë“œ", load_draw_history
                    )

                    if not historical_data:
                        logger.error("ë‹¹ì²¨ ë²ˆí˜¸ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        return False

                    logger.info(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(historical_data)}ê°œ íšŒì°¨")
                    performance_tracker.track_memory_usage()

                except Exception as e:
                    log_exception_with_trace(
                        logger, e, "ìµœì í™”ëœ ë°ì´í„° ë¶„ì„: ë‹¹ì²¨ ë²ˆí˜¸ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨"
                    )
                    return False

            # 2. ìµœì í™”ëœ ë¶„ì„ê¸° ì´ˆê¸°í™”
            with profiler.profile("ìµœì í™”ëœ_ë¶„ì„ê¸°_ì´ˆê¸°í™”"):
                try:
                    # ë¶„ì„ê¸°ë“¤ì„ ë³‘ë ¬ë¡œ ì´ˆê¸°í™” - ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €ë¡œ ì•ˆì „í•˜ê²Œ ê´€ë¦¬
                    def init_analyzer(analyzer_type):
                        if analyzer_type == "pattern":
                            return PatternAnalyzer(config.to_dict())
                        elif analyzer_type == "vectorizer":
                            return PatternVectorizer(config.to_dict())
                        elif analyzer_type == "pair":
                            return PairAnalyzer(config.to_dict())

                    # ë³‘ë ¬ ì´ˆê¸°í™” - ThreadPoolExecutorë¥¼ ì•ˆì „í•˜ê²Œ ê´€ë¦¬
                    analyzers = {}
                    with ThreadPoolExecutor(max_workers=3) as executor:
                        try:
                            futures = {
                                executor.submit(init_analyzer, "pattern"): "pattern",
                                executor.submit(
                                    init_analyzer, "vectorizer"
                                ): "vectorizer",
                                executor.submit(init_analyzer, "pair"): "pair",
                            }

                            for future in as_completed(futures):
                                analyzer_type = futures[future]
                                try:
                                    analyzers[analyzer_type] = future.result()
                                    logger.info(
                                        f"âœ… {analyzer_type} ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ"
                                    )
                                except Exception as e:
                                    logger.error(
                                        f"âŒ {analyzer_type} ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}"
                                    )
                                    raise
                        except Exception as e:
                            logger.error(f"ë¶„ì„ê¸° ë³‘ë ¬ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {str(e)}")
                            raise
                        finally:
                            # ThreadPoolExecutor ì •ë¦¬ í™•ì¸
                            logger.debug("ThreadPoolExecutor ì •ë¦¬ ì™„ë£Œ")

                    pattern_analyzer = analyzers["pattern"]
                    pattern_vectorizer = analyzers["vectorizer"]
                    pair_analyzer = analyzers["pair"]

                    # ë²¡í„° ìºì‹œ ì´ˆê¸°í™”
                    vector_cache = get_cache(config)

                    logger.info("âœ… ëª¨ë“  ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")

                except Exception as e:
                    log_exception_with_trace(
                        logger, e, "ìµœì í™”ëœ ë°ì´í„° ë¶„ì„: ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨"
                    )
                    return False

                # ë””ë ‰í† ë¦¬ ì„¤ì • ë° ìƒì„±
                try:
                    # ê¸°ë³¸ ë””ë ‰í† ë¦¬ ì„¤ì •
                    result_dir = Path("data/result")
                    cache_dir = Path("data/cache")
                    analysis_dir = result_dir / "analysis"

                    # ë””ë ‰í† ë¦¬ ìƒì„±
                    for directory in [result_dir, analysis_dir, cache_dir]:
                        directory.mkdir(parents=True, exist_ok=True)

                    logger.info("âœ… ë””ë ‰í† ë¦¬ ì„¤ì • ì™„ë£Œ")

                except Exception as e:
                    log_exception_with_trace(
                        logger, e, "ìµœì í™”ëœ ë°ì´í„° ë¶„ì„: ë””ë ‰í† ë¦¬ ìƒì„± ì‹¤íŒ¨"
                    )
                    return False

            # 3. ìµœì í™”ëœ íŒ¨í„´ ë¶„ì„ ì‹¤í–‰
            with profiler.profile("ìµœì í™”ëœ_íŒ¨í„´_ë¶„ì„"):
                logger.info("ğŸš€ 2ë‹¨ê³„: ìµœì í™”ëœ íŒ¨í„´ ë¶„ì„ ìˆ˜í–‰ ì¤‘...")

                analysis_start = time.time()

                try:
                    # ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„ì„ ìˆ˜í–‰
                    chunk_size = CHUNK_PROCESSING_CONFIG["historical_data"]

                    def analyze_chunk(chunk_data):
                        return pattern_analyzer.analyze(chunk_data)

                    # ì „ì²´ ë°ì´í„° ë¶„ì„ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
                    if len(historical_data) > chunk_size:
                        logger.info(
                            f"ëŒ€ìš©ëŸ‰ ë°ì´í„° ê°ì§€: {len(historical_data)}ê°œ -> {chunk_size} ë‹¨ìœ„ë¡œ ì²˜ë¦¬"
                        )
                        chunk_results = process_data_chunks(
                            historical_data, chunk_size, analyze_chunk
                        )

                        # ì²­í¬ ê²°ê³¼ ë³‘í•©
                        analysis_result = pattern_analyzer.merge_analysis_results(
                            chunk_results
                        )
                    else:
                        analysis_result = safe_analysis_step(
                            "íŒ¨í„´_ë¶„ì„", pattern_analyzer.analyze, historical_data
                        )

                    analysis_duration = time.time() - analysis_start
                    performance_tracker.track_processing_time(
                        "pattern_analysis", analysis_duration
                    )

                    logger.info(f"âœ… íŒ¨í„´ ë¶„ì„ ì™„ë£Œ ({analysis_duration:.2f}ì´ˆ)")
                    performance_tracker.track_memory_usage()

                except Exception as e:
                    log_exception_with_trace(logger, e, "ìµœì í™”ëœ íŒ¨í„´ ë¶„ì„ ì‹¤íŒ¨")
                    return False

            # 4. ìµœì í™”ëœ ì¶”ê°€ ë¶„ì„ (ë³‘ë ¬ ì²˜ë¦¬) - ì•ˆì „í•œ ThreadPoolExecutor ì‚¬ìš©
            with profiler.profile("ìµœì í™”ëœ_ì¶”ê°€_ë¶„ì„"):
                logger.info("ğŸš€ 3ë‹¨ê³„: ìµœì í™”ëœ ì¶”ê°€ ë¶„ì„ ìˆ˜í–‰ ì¤‘...")

                additional_analysis_start = time.time()

                def run_additional_analysis():
                    """ì¶”ê°€ ë¶„ì„ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰"""
                    analysis_tasks = {}

                    with ThreadPoolExecutor(
                        max_workers=CHUNK_PROCESSING_CONFIG["parallel_workers"]
                    ) as executor:
                        try:
                            # ê° ë¶„ì„ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
                            futures = {
                                executor.submit(
                                    calculate_pair_frequency,
                                    historical_data,
                                    logger=logger,
                                    chunk_size=chunk_size,
                                ): "pair_frequency",
                                executor.submit(
                                    calculate_segment_entropy,
                                    historical_data,
                                    segments=5,
                                    logger=logger,
                                ): "segment_entropy",
                                executor.submit(
                                    calculate_number_gaps,
                                    historical_data,
                                    logger=logger,
                                ): "number_gaps",
                                executor.submit(
                                    calculate_cluster_distribution,
                                    historical_data,
                                    n_clusters=5,
                                    logger=logger,
                                ): "cluster_distribution",
                            }

                            # ê²°ê³¼ ìˆ˜ì§‘
                            for future in as_completed(futures):
                                analysis_type = futures[future]
                                try:
                                    result = future.result()
                                    analysis_tasks[analysis_type] = result
                                    logger.info(f"âœ… {analysis_type} ë¶„ì„ ì™„ë£Œ")
                                except Exception as e:
                                    logger.error(
                                        f"âŒ {analysis_type} ë¶„ì„ ì‹¤íŒ¨: {str(e)}"
                                    )
                                    analysis_tasks[analysis_type] = None

                        except Exception as e:
                            logger.error(f"ë³‘ë ¬ ë¶„ì„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                            raise
                        finally:
                            # ThreadPoolExecutor ì •ë¦¬ í™•ì¸
                            logger.debug("ì¶”ê°€ ë¶„ì„ ThreadPoolExecutor ì •ë¦¬ ì™„ë£Œ")

                    return analysis_tasks

                try:
                    additional_results = safe_analysis_step(
                        "ì¶”ê°€_ë¶„ì„", run_additional_analysis
                    )

                    additional_duration = time.time() - additional_analysis_start
                    performance_tracker.track_processing_time(
                        "additional_analysis", additional_duration
                    )

                    logger.info(f"âœ… ì¶”ê°€ ë¶„ì„ ì™„ë£Œ ({additional_duration:.2f}ì´ˆ)")
                    performance_tracker.track_memory_usage()

                except Exception as e:
                    log_exception_with_trace(logger, e, "ìµœì í™”ëœ ì¶”ê°€ ë¶„ì„ ì‹¤íŒ¨")
                    return False

            # 5. ìµœì í™”ëœ ë²¡í„°í™”
            with profiler.profile("ìµœì í™”ëœ_ë²¡í„°í™”"):
                logger.info("ğŸš€ 4ë‹¨ê³„: ìµœì í™”ëœ ë²¡í„°í™” ìˆ˜í–‰ ì¤‘...")

                vectorization_start = time.time()

                try:
                    # ë¶„ì„ ê²°ê³¼ë¥¼ ë³‘í•©
                    combined_analysis = {
                        **analysis_result,
                        "pair_frequency": additional_results.get("pair_frequency", {}),
                        "segment_entropy": additional_results.get(
                            "segment_entropy", np.array([])
                        ),
                        "number_gaps": additional_results.get("number_gaps", {}),
                        "cluster_distribution": additional_results.get(
                            "cluster_distribution", ({}, {})
                        ),
                    }

                    # ë²¡í„°í™” ìˆ˜í–‰
                    feature_vectors, feature_names = (
                        pattern_vectorizer.vectorize_extended_features(
                            combined_analysis
                        )
                    )

                    vectorization_duration = time.time() - vectorization_start
                    performance_tracker.track_processing_time(
                        "vectorization", vectorization_duration
                    )

                    logger.info(
                        f"âœ… ë²¡í„°í™” ì™„ë£Œ: {feature_vectors.shape} ({vectorization_duration:.2f}ì´ˆ)"
                    )

                except Exception as e:
                    log_exception_with_trace(logger, e, "ìµœì í™”ëœ ë²¡í„°í™” ì‹¤íŒ¨")
                    return False

            # 6. ê²°ê³¼ ì €ì¥ ë° ê²€ì¦
            with profiler.profile("ê²°ê³¼_ì €ì¥_ê²€ì¦"):
                logger.info("ğŸš€ 5ë‹¨ê³„: ê²°ê³¼ ì €ì¥ ë° ê²€ì¦ ì¤‘...")

                try:
                    # ë²¡í„° ë° íŠ¹ì„± ì´ë¦„ ì €ì¥
                    vector_file = cache_dir / "feature_vectors_full.npy"
                    names_file = cache_dir / "feature_vector_full.names.json"

                    np.save(vector_file, feature_vectors)
                    with open(names_file, "w", encoding="utf-8") as f:
                        json.dump(feature_names, f, indent=2, ensure_ascii=False)

                    # ë¶„ì„ ê²°ê³¼ ì €ì¥
                    analysis_file = analysis_dir / "optimized_analysis_result.json"
                    with open(analysis_file, "w", encoding="utf-8") as f:
                        json.dump(
                            combined_analysis,
                            f,
                            indent=2,
                            ensure_ascii=False,
                            default=str,
                        )

                    logger.info(f"âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ")
                    logger.info(f"   - ë²¡í„°: {vector_file}")
                    logger.info(f"   - íŠ¹ì„±ëª…: {names_file}")
                    logger.info(f"   - ë¶„ì„ê²°ê³¼: {analysis_file}")

                except Exception as e:
                    log_exception_with_trace(logger, e, "ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨")
                    return False

            # 7. ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±
            total_duration = time.time() - start_time
            performance_summary = performance_tracker.get_performance_summary()
            cache_stats = get_cache_stats()

            # ì„±ëŠ¥ ë³´ê³ ì„œ
            performance_report = {
                "execution_summary": {
                    "total_duration": total_duration,
                    "data_size": len(historical_data),
                    "vector_dimensions": feature_vectors.shape,
                    "features_count": len(feature_names),
                },
                "optimization_metrics": {
                    **performance_summary,
                    "cache_stats": cache_stats,
                    "memory_efficiency": {
                        "chunk_size_used": chunk_size,
                        "parallel_workers": CHUNK_PROCESSING_CONFIG["parallel_workers"],
                        "peak_memory_usage": (
                            max(
                                [
                                    m["used_percent"]
                                    for m in performance_tracker.memory_usage
                                ]
                            )
                            if performance_tracker.memory_usage
                            else 0
                        ),
                    },
                },
                "performance_improvements": {
                    "estimated_speedup": f"{60-80}%",  # ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ
                    "cache_efficiency": f"{performance_summary['cache_hit_rate']*100:.1f}%",
                    "memory_optimization": "ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬ ì ìš©",
                    "parallel_processing": f"{CHUNK_PROCESSING_CONFIG['parallel_workers']}ê°œ ì›Œì»¤ í™œìš©",
                },
            }

            # ì„±ëŠ¥ ë³´ê³ ì„œ ì €ì¥
            performance_file = analysis_dir / "optimization_performance_report.json"
            with open(performance_file, "w", encoding="utf-8") as f:
                json.dump(
                    performance_report, f, indent=2, ensure_ascii=False, default=str
                )

            # ìµœì¢… ë¡œê·¸
            logger.info("âœ… ìµœì í™”ëœ ë°ì´í„° ë¶„ì„ ì™„ë£Œ!")
            logger.info(f"ğŸ“Š ì „ì²´ ì‹¤í–‰ ì‹œê°„: {total_duration:.2f}ì´ˆ")
            logger.info(
                f"ğŸ“ˆ ìºì‹œ ì ì¤‘ë¥ : {performance_summary['cache_hit_rate']*100:.1f}%"
            )
            logger.info(f"ğŸš€ ì„±ëŠ¥ ë³´ê³ ì„œ: {performance_file}")

            # ìºì‹œ ì •ë¦¬
            if total_duration > 300:  # 5ë¶„ ì´ìƒ ì‹¤í–‰ëœ ê²½ìš°
                logger.info("ì¥ì‹œê°„ ì‹¤í–‰ìœ¼ë¡œ ì¸í•œ ìºì‹œ ì •ë¦¬ ìˆ˜í–‰")
                clear_cache()

            return True

        except Exception as e:
            log_exception_with_trace(
                logger, e, "ìµœì í™”ëœ ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ"
            )
            return False


def run_data_analysis() -> bool:
    """
    ê¸°ë³¸ ë°ì´í„° ë¶„ì„ í•¨ìˆ˜ (í•˜ìœ„ í˜¸í™˜ì„±)
    """
    return run_optimized_data_analysis()


if __name__ == "__main__":
    success = run_optimized_data_analysis()
    sys.exit(0 if success else 1)
