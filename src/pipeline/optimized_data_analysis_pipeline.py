#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
ìµœì í™”ëœ ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸

ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ë‹¤ìŒ ê¸°ëŠ¥ë“¤ì„ í¬í•¨í•©ë‹ˆë‹¤:
- ì¤‘ë³µ í•¨ìˆ˜ í†µí•© ë° ì œê±°
- ì„¸ë¶„í™”ëœ ìºì‹± ì‹œìŠ¤í…œ
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²­í¬ ì²˜ë¦¬
- ë³‘ë ¬ ì²˜ë¦¬ ì§€ì› (ProcessPool í†µí•©)
- í•˜ì´ë¸Œë¦¬ë“œ ìµœì í™” ì‹œìŠ¤í…œ
- ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ìµœì í™”
"""

import json
import logging
import os
import sys
import time
import random
import traceback
from pathlib import Path
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple, Union
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
import hashlib

import numpy as np
import psutil

from src.utils.error_handler_refactored import (
    get_logger,
    log_exception_with_trace,
    StrictErrorHandler,
    strict_error_handler,
    validate_and_fail_fast,
)
from src.utils.state_vector_cache import get_cache
from src.utils.data_loader import load_draw_history, LotteryJSONEncoder
from src.shared.types import LotteryNumber
from src.utils.unified_config import load_config
from src.utils.unified_performance import get_profiler

# ìµœì í™” ì‹œìŠ¤í…œ import
from src.utils.process_pool_manager import get_process_pool_manager
from src.utils.hybrid_optimizer import get_hybrid_optimizer, optimize
from src.utils.memory_manager import get_memory_manager

from src.analysis.pattern_analyzer import PatternAnalyzer
from src.analysis.enhanced_pattern_vectorizer import EnhancedPatternVectorizer
from src.utils.unified_report import safe_convert, save_physical_performance_report
from src.analysis.pair_analyzer import PairAnalyzer
from src.analysis.distribution_analyzer import DistributionAnalyzer
from src.analysis.roi_analyzer import ROIAnalyzer
from src.analysis.cluster_analyzer import ClusterAnalyzer
from src.analysis.trend_analyzer import TrendAnalyzer
from src.analysis.overlap_analyzer import OverlapAnalyzer
from src.analysis.structural_analyzer import StructuralAnalyzer
from src.analysis.statistical_analyzer import StatisticalAnalyzer

# from src.utils.feature_vector_validator import (
#     validate_feature_vector_with_config,
#     check_vector_dimensions,
# )

# ìµœì í™”ëœ í•¨ìˆ˜ë“¤ import
from src.shared.graph_utils import (
    calculate_pair_frequency,
    calculate_segment_entropy,
    calculate_number_gaps,
    calculate_cluster_distribution,
    clear_cache,
    get_cache_stats,
)

# ë¡œê±° ì„¤ì •
logger = get_logger(__name__)

# ì„±ëŠ¥ ìµœì í™” ì„¤ì •
CHUNK_PROCESSING_CONFIG = {
    "historical_data": 100,  # 100íšŒì°¨ì”© ì²˜ë¦¬
    "vector_generation": 50,  # 50ê°œì”© ë²¡í„° ìƒì„±
    "cache_flush_interval": 200,  # 200íšŒì°¨ë§ˆë‹¤ ìºì‹œ í”ŒëŸ¬ì‹œ
    "parallel_workers": min(4, psutil.cpu_count()),  # CPU ì½”ì–´ ìˆ˜ì— ë”°ë¥¸ ì›Œì»¤ ìˆ˜
    "memory_threshold": 0.8,  # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 80% ì„ê³„ì 
}

CACHE_STRATEGY = {
    "data_load": "data_size_hash",
    "pattern_analysis": "data_hash + config_hash",
    "vectorization": "analysis_hash + vector_config_hash",
    "additional_analysis": "pattern_hash + addon_config_hash",
}

# ì „ì—­ ì—„ê²©í•œ ì—ëŸ¬ í•¸ë“¤ëŸ¬
strict_handler = StrictErrorHandler()

# ì „ì—­ ìµœì í™” ì‹œìŠ¤í…œë“¤
process_pool_manager = None
hybrid_optimizer = None
memory_manager = None


def initialize_optimization_systems(config: Dict[str, Any]):
    """ìµœì í™” ì‹œìŠ¤í…œë“¤ ì´ˆê¸°í™”"""
    global process_pool_manager, hybrid_optimizer, memory_manager

    try:
        # ìµœì í™” ì„¤ì • ë¡œë“œ (ë³„ë„ íŒŒì¼ì—ì„œ)
        optimization_config = config.get("optimization", {})

        # config.yamlì˜ optimization ì„¹ì…˜ì—ì„œ ì¶”ê°€ ì„¤ì • ë¡œë“œ
        try:
            from src.utils.unified_config import load_config as load_main_config

            main_config = load_main_config("main")
            if isinstance(main_config, dict) and "optimization" in main_config:
                # ë©”ì¸ ì„¤ì •ì˜ optimization ì„¹ì…˜ì„ ê¸°ë³¸ ì„¤ì •ê³¼ ë³‘í•©
                optimization_config.update(main_config["optimization"])
        except Exception as e:
            logger.debug(
                f"config.yamlì˜ optimization ì„¹ì…˜ ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ ì„¤ì • ì‚¬ìš©: {e}"
            )

            # ProcessPool ê´€ë¦¬ì ì´ˆê¸°í™”
        try:
            process_pool_config = optimization_config.get("process_pool", {})
            # ê¸°ë³¸ê°’ ì„¤ì •
            if not isinstance(process_pool_config, dict):
                process_pool_config = {}

            # ì•ˆì „í•œ ê¸°ë³¸ê°’ ì„¤ì •
            safe_config = {
                "max_workers": process_pool_config.get(
                    "max_workers", min(4, psutil.cpu_count())
                ),
                "chunk_size": process_pool_config.get("chunk_size", 100),
                "timeout": process_pool_config.get("timeout", 300),
                "memory_limit_mb": process_pool_config.get("memory_limit_mb", 1024),
                "enable_monitoring": process_pool_config.get("enable_monitoring", True),
                "auto_restart": process_pool_config.get("auto_restart", True),
                "restart_threshold": process_pool_config.get("restart_threshold", 100),
            }

            process_pool_manager = get_process_pool_manager(safe_config)
            logger.info("ProcessPool ê´€ë¦¬ì ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"ProcessPool ê´€ë¦¬ì ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            process_pool_manager = None

        # ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™”
        try:
            from src.utils.memory_manager import MemoryConfig

            memory_config_dict = optimization_config.get("memory", {})
            memory_config = MemoryConfig(
                max_memory_usage=memory_config_dict.get("max_memory_usage", 0.85),
                cache_size=memory_config_dict.get("cache_size", 256 * 1024 * 1024),
                use_memory_pooling=memory_config_dict.get("use_memory_pooling", True),
                auto_cleanup_interval=memory_config_dict.get(
                    "auto_cleanup_interval", 60.0
                ),
            )
            memory_manager = get_memory_manager(memory_config)
            logger.info("ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            memory_manager = None

        # í•˜ì´ë¸Œë¦¬ë“œ ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        try:
            hybrid_config = optimization_config.get(
                "hybrid",
                {
                    "auto_optimization": True,
                    "memory_threshold": 0.8,
                    "cpu_threshold": 75.0,
                },
            )
            hybrid_optimizer = get_hybrid_optimizer(hybrid_config)
            logger.info("í•˜ì´ë¸Œë¦¬ë“œ ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"í•˜ì´ë¸Œë¦¬ë“œ ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            hybrid_optimizer = None

        # ìµœì í™” ì‹œìŠ¤í…œ ìƒíƒœ ë¡œê¹…
        initialized_systems = []
        if process_pool_manager is not None:
            initialized_systems.append("ProcessPool")
        if memory_manager is not None:
            initialized_systems.append("MemoryManager")
        if hybrid_optimizer is not None:
            initialized_systems.append("HybridOptimizer")

        logger.info(f"ì´ˆê¸°í™”ëœ ìµœì í™” ì‹œìŠ¤í…œ: {', '.join(initialized_systems)}")

    except Exception as e:
        logger.error(f"ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        # ìµœì í™” ì‹œìŠ¤í…œ ì—†ì´ë„ ë™ì‘í•  ìˆ˜ ìˆë„ë¡ Noneìœ¼ë¡œ ì„¤ì •
        process_pool_manager = None
        hybrid_optimizer = None
        memory_manager = None


@optimize(
    task_info={
        "function_type": "analysis",
        "parallelizable": True,
        "gpu_compatible": False,
    }
)
def optimized_pattern_analysis(
    data_chunk: List, analyzer: PatternAnalyzer
) -> Dict[str, Any]:
    """ìµœì í™”ëœ íŒ¨í„´ ë¶„ì„"""
    try:
        # ë©”ëª¨ë¦¬ ê´€ë¦¬ ìŠ¤ì½”í”„ ë‚´ì—ì„œ ì‹¤í–‰
        if memory_manager:
            with memory_manager.allocation_scope():
                return analyzer.analyze(data_chunk)
        else:
            return analyzer.analyze(data_chunk)
    except Exception as e:
        logger.error(f"íŒ¨í„´ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return {}


@optimize(
    task_info={
        "function_type": "vectorize",
        "parallelizable": True,
        "gpu_compatible": False,
    }
)
def optimized_vectorization(
    patterns: List, vectorizer: EnhancedPatternVectorizer
) -> Tuple[np.ndarray, List[str]]:
    """ìµœì í™”ëœ ë²¡í„°í™”"""
    try:
        if memory_manager:
            with memory_manager.allocation_scope():
                return vectorizer.vectorize(patterns)
        else:
            return vectorizer.vectorize(patterns)
    except Exception as e:
        logger.error(f"ë²¡í„°í™” ì‹¤íŒ¨: {e}")
        return np.array([]), []


def generate_cache_key(data_info: str, operation: str, **kwargs) -> str:
    """ìµœì í™”ëœ ìºì‹œ í‚¤ ìƒì„±"""
    try:
        # ë°ì´í„° ì •ë³´ì™€ ë§¤ê°œë³€ìˆ˜ë¥¼ ê²°í•©í•˜ì—¬ í•´ì‹œ ìƒì„±
        combined_info = f"{data_info}_{operation}_{str(sorted(kwargs.items()))}"
        return hashlib.md5(combined_info.encode()).hexdigest()[:16]
    except Exception:
        # í•´ì‹œ ìƒì„± ì‹¤íŒ¨ì‹œ ê¸°ë³¸ í‚¤
        return f"{operation}_{hash(str(kwargs)) % 100000}"


def check_memory_usage() -> bool:
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸"""
    memory_info = psutil.virtual_memory()
    return memory_info.percent < (CHUNK_PROCESSING_CONFIG["memory_threshold"] * 100)


def process_data_chunks_optimized(data: List, chunk_size: int, process_func, **kwargs):
    """ìµœì í™”ëœ ì²­í¬ ë‹¨ìœ„ ë°ì´í„° ì²˜ë¦¬"""
    global process_pool_manager

    # ProcessPoolì´ ì‚¬ìš© ê°€ëŠ¥í•˜ê³  ë°ì´í„°ê°€ ì¶©ë¶„íˆ í° ê²½ìš° ë³‘ë ¬ ì²˜ë¦¬
    if process_pool_manager and len(data) > 200:
        try:
            chunks = process_pool_manager.chunk_and_split(data, chunk_size)
            results = process_pool_manager.parallel_analyze(
                chunks, process_func, **kwargs
            )
            return process_pool_manager.merge_results(results)
        except Exception as e:
            logger.warning(f"ë³‘ë ¬ ì²˜ë¦¬ ì‹¤íŒ¨, ìˆœì°¨ ì²˜ë¦¬ë¡œ ì „í™˜: {e}")

    # ìˆœì°¨ ì²˜ë¦¬ (ê¸°ì¡´ ë°©ì‹)
    results = []
    total_chunks = len(data) // chunk_size + (1 if len(data) % chunk_size else 0)

    for i in range(0, len(data), chunk_size):
        if not check_memory_usage():
            logger.warning(
                "ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì„ê³„ì ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."
            )
            import gc

            gc.collect()

        chunk = data[i : min(i + chunk_size, len(data))]
        chunk_result = process_func(chunk, **kwargs)
        results.append(chunk_result)

        if (i // chunk_size + 1) % 10 == 0:
            logger.info(f"ì²­í¬ ì²˜ë¦¬ ì§„í–‰ë¥ : {i // chunk_size + 1}/{total_chunks}")

    return results


def safe_analysis_step(step_name: str, func, *args, **kwargs):
    """íŠ¸ëœì­ì…˜ ë˜í¼ - ì•ˆì „í•œ ë¶„ì„ ë‹¨ê³„ ì‹¤í–‰"""
    try:
        logger.info(f"{step_name} ì‹œì‘")
        start_time = time.time()

        result = func(*args, **kwargs)

        duration = time.time() - start_time
        logger.info(f"{step_name} ì™„ë£Œ ({duration:.2f}ì´ˆ)")
        return result

    except Exception as e:
        logger.error(f"{step_name} ì‹¤íŒ¨: {str(e)}")
        # ë¡¤ë°± ë¡œì§ (í•„ìš”ì‹œ ì¶”ê°€)
        raise RuntimeError(f"{step_name} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


def clear_analysis_cache():
    """ì‹¤ì œ ë¶„ì„ ì‹¤í–‰ì„ ìœ„í•œ ìºì‹œ ë¬´íš¨í™”"""
    import os
    from pathlib import Path

    cache_files = [
        "data/cache/pattern_analysis_1172.pkl",
        "data/cache/trend_analysis_1172.pkl",
        "data/cache/overlap_analysis_1172.pkl",
        "data/cache/structural_analysis_1172.pkl",
        "data/cache/statistical_analysis_1172.pkl",
        "data/cache/pair_analysis_1172.pkl",
        "data/cache/cluster_analysis_1172.pkl",
        "data/cache/distribution_analysis_1172.pkl",
        "data/cache/roi_analysis_1172.pkl",
    ]

    cleared_count = 0
    for cache_file in cache_files:
        if os.path.exists(cache_file):
            try:
                os.remove(cache_file)
                logger.info(f"ìºì‹œ ì‚­ì œ: {cache_file}")
                cleared_count += 1
            except Exception as e:
                logger.warning(f"ìºì‹œ ì‚­ì œ ì‹¤íŒ¨ {cache_file}: {e}")

    if cleared_count > 0:
        logger.info(
            f"âœ… ì´ {cleared_count}ê°œ ìºì‹œ íŒŒì¼ ì‚­ì œ ì™„ë£Œ - ì‹¤ì œ ë¶„ì„ ì‹¤í–‰ë©ë‹ˆë‹¤"
        )
    else:
        logger.info("ì‚­ì œí•  ìºì‹œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")

    return cleared_count


@strict_error_handler("ìµœì í™”ëœ ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸", exit_on_error=True)
def run_optimized_data_analysis() -> bool:
    """
    ìµœì í™”ëœ ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

    Returns:
        bool: ì‹¤í–‰ ì„±ê³µ ì—¬ë¶€
    """
    start_time = time.time()

    try:
        logger.info("ğŸš€ ìµœì í™”ëœ ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹œì‘")

        # ì„¤ì • ë¡œë“œ
        config = load_config()

        # ğŸ¯ ì™„ì„±ë„ ì²´í¬
        logger.info("ğŸ¯ 0ë‹¨ê³„: ì‹œìŠ¤í…œ ì™„ì„±ë„ ì‚¬ì „ ì²´í¬")
        completion_status = track_completion_status()

        if completion_status["completion_rate"] < 100.0:
            logger.warning(
                f"âš ï¸ ì‹œìŠ¤í…œ ë¯¸ì™„ì„±: {completion_status['completion_rate']:.1f}%"
            )
            logger.warning("ì¼ë¶€ ë¶„ì„ê¸°ê°€ ëˆ„ë½ë˜ì—ˆì§€ë§Œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤...")
        else:
            logger.info("ğŸ‰ ì‹œìŠ¤í…œ 100% ì™„ì„±!")

        # 1ë‹¨ê³„: ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        initialize_optimization_systems(config)

        # 2ë‹¨ê³„: ë°ì´í„° ë¡œë“œ ë° ê³ ê¸‰ ê²€ì¦
        logger.info("ğŸ“Š 1ë‹¨ê³„: ë°ì´í„° ë¡œë“œ ë° ê³ ê¸‰ ê²€ì¦")
        historical_data = load_draw_history()

        if not historical_data:
            logger.error("ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨")
            return False

        # ë°ì´í„° ì •ë ¬ (ìµœì‹ ìˆœ)
        historical_data.sort(key=lambda x: x.draw_no)
        logger.info(f"ë¡œë˜ ë°ì´í„° ë¡œë“œ ë° ì •ë ¬ ì™„ë£Œ: {len(historical_data)}íšŒì°¨")

        # ê³ ê¸‰ ë°ì´í„° ê²€ì¦
        validation_result = validate_lottery_data(historical_data)
        if not validation_result:
            logger.error("ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨")
            return False

        # 2ë‹¨ê³„: ë¶„ì„ê¸° ì´ˆê¸°í™”
        logger.info("ğŸ”§ 2ë‹¨ê³„: ë¶„ì„ê¸° ì´ˆê¸°í™”")

        def init_analyzer(analyzer_type: str):
            """ë¶„ì„ê¸° ì´ˆê¸°í™” í—¬í¼ í•¨ìˆ˜ - íŒ©í† ë¦¬ íŒ¨í„´ ì‚¬ìš©"""
            from src.analysis.analyzer_factory import get_analyzer

            try:
                # ConfigProxyë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜
                if hasattr(config, "_config"):
                    config_dict = config._config
                elif hasattr(config, "to_dict"):
                    config_dict = config.to_dict()
                elif isinstance(config, dict):
                    config_dict = config
                else:
                    # ConfigProxy ê°ì²´ì˜ ì†ì„±ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
                    config_dict = {}
                    if hasattr(config, "__dict__"):
                        for key, value in config.__dict__.items():
                            if not key.startswith("_"):
                                config_dict[key] = value

                    # ê¸°ë³¸ ì„¤ì • ì¶”ê°€
                    if not config_dict:
                        config_dict = {
                            "analysis": {},
                            "paths": {
                                "cache_dir": "data/cache",
                                "result_dir": "data/result",
                            },
                            "vectorizer": {"use_cache": True, "normalize_output": True},
                            "filtering": {
                                "remove_low_variance_features": True,
                                "variance_threshold": 0.01,
                            },
                            "caching": {
                                "enable_feature_cache": True,
                                "max_cache_size": 10000,
                            },
                        }

                        # íŒ©í† ë¦¬ë¥¼ í†µí•´ ë¶„ì„ê¸° ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
                analyzer = get_analyzer(analyzer_type, config_dict)
                if analyzer is None:
                    logger.warning(f"{analyzer_type} ë¶„ì„ê¸° íŒ©í† ë¦¬ì—ì„œ None ë°˜í™˜")
                return analyzer

            except Exception as e:
                logger.error(f"{analyzer_type} ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                # ì¬ì‹œë„ ì—†ì´ None ë°˜í™˜í•˜ì—¬ ì¤‘ë³µ ì´ˆê¸°í™” ë°©ì§€
                return None

        # ë¶„ì„ê¸°ë“¤ ì´ˆê¸°í™” (ëª¨ë“  11ê°œ ë¶„ì„ê¸°)
        pattern_analyzer = init_analyzer("pattern")
        distribution_analyzer = init_analyzer("distribution")
        roi_analyzer = init_analyzer("roi")
        pair_analyzer = init_analyzer("pair")
        vectorizer = init_analyzer("vectorizer")

        # ğŸ”¥ ëª¨ë“  11ê°œ ë¶„ì„ê¸° ì™„ì „ í™œìš©
        cluster_analyzer = init_analyzer("cluster")
        trend_analyzer = init_analyzer("trend")
        overlap_analyzer = init_analyzer("overlap")
        structural_analyzer = init_analyzer("structural")
        statistical_analyzer = init_analyzer("statistical")
        negative_sample_generator = init_analyzer("negative_sample")
        unified_analyzer = init_analyzer("unified")  # ğŸ¯ UnifiedAnalyzer ì¶”ê°€

        # ì´ˆê¸°í™” ì‹¤íŒ¨ ì²´í¬
        analyzers = {
            "pattern": pattern_analyzer,
            "distribution": distribution_analyzer,
            "roi": roi_analyzer,
            "pair": pair_analyzer,
            "vectorizer": vectorizer,
            # ğŸ”¥ ëª¨ë“  11ê°œ ë¶„ì„ê¸° ì™„ì „ í™œìš©
            "cluster": cluster_analyzer,
            "trend": trend_analyzer,
            "overlap": overlap_analyzer,
            "structural": structural_analyzer,
            "statistical": statistical_analyzer,
            "negative_sample": negative_sample_generator,
            "unified": unified_analyzer,  # ğŸ¯ UnifiedAnalyzer ì¶”ê°€
        }

        failed_analyzers = [
            name for name, analyzer in analyzers.items() if analyzer is None
        ]
        if failed_analyzers:
            logger.warning(f"ë‹¤ìŒ ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {failed_analyzers}")
            # ì‹¤íŒ¨í•œ ë¶„ì„ê¸° ì œê±°
            analyzers = {
                name: analyzer
                for name, analyzer in analyzers.items()
                if analyzer is not None
            }

        logger.info("ëª¨ë“  ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")

        # 3ë‹¨ê³„: ë³‘ë ¬ ë¶„ì„ ì‹¤í–‰
        logger.info("âš¡ 3ë‹¨ê³„: ë³‘ë ¬ ë¶„ì„ ì‹¤í–‰")
        analysis_results = {}

        # ğŸ”¥ ëª¨ë“  11ê°œ ë¶„ì„ê¸°ë¥¼ í¬í•¨í•œ ë¶„ì„ ì‘ì—… ì •ì˜
        analysis_tasks = []

        # ê¸°ì¡´ í•µì‹¬ ë¶„ì„ê¸°ë“¤
        core_analyzers = ["pattern", "distribution", "roi", "pair"]
        for name in core_analyzers:
            if name in analyzers and analyzers[name] is not None:
                analysis_tasks.append((name, analyzers[name]))

        # ğŸš€ ëª¨ë“  11ê°œ ë¶„ì„ê¸° ì™„ì „ í™œìš© (í™•ì¥ëœ ë¶„ì„ê¸°ë“¤)
        extended_analyzers = [
            "cluster",
            "trend",
            "overlap",
            "structural",
            "statistical",
            "negative_sample",
            "unified",  # ğŸ¯ UnifiedAnalyzer ì¶”ê°€
        ]
        for name in extended_analyzers:
            if name in analyzers and analyzers[name] is not None:
                analysis_tasks.append((name, analyzers[name]))
                logger.info(f"âœ… {name} ë¶„ì„ê¸° ë³‘ë ¬ ë¶„ì„ì— í¬í•¨")

        logger.info(f"ì´ {len(analysis_tasks)}ê°œ ë¶„ì„ê¸°ë¡œ ë³‘ë ¬ ë¶„ì„ ì‹¤í–‰ (ëª©í‘œ: 11ê°œ)")
        logger.info(f"í™œì„±í™”ëœ ë¶„ì„ê¸°: {[name for name, _ in analysis_tasks]}")

        # í”„ë¡œì„¸ìŠ¤ í’€ì´ ìˆìœ¼ë©´ ë³‘ë ¬ ì‹¤í–‰, ì—†ìœ¼ë©´ ìˆœì°¨ ì‹¤í–‰
        if process_pool_manager and len(analysis_tasks) > 1:
            logger.info("í”„ë¡œì„¸ìŠ¤ í’€ì„ ì‚¬ìš©í•œ ë³‘ë ¬ ë¶„ì„")

            # ğŸ”§ ì›Œì»¤ ìˆ˜ë¥¼ ë¶„ì„ê¸° ìˆ˜ì— ë§ê²Œ ì¡°ì •
            max_workers = min(len(analysis_tasks), 8)  # ìµœëŒ€ 8ê°œ ì›Œì»¤

            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_name = {}

                for name, analyzer in analysis_tasks:
                    if analyzer:
                        future = executor.submit(
                            safe_analysis_execution, name, analyzer, historical_data
                        )
                        future_to_name[future] = name

                # ê²°ê³¼ ìˆ˜ì§‘
                for future in as_completed(future_to_name):
                    name = future_to_name[future]
                    try:
                        result = future.result(timeout=300)  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
                        if result:
                            analysis_results[name] = result
                            logger.info(f"âœ… {name} ë¶„ì„ ì™„ë£Œ")
                        else:
                            logger.warning(f"âš ï¸ {name} ë¶„ì„ ê²°ê³¼ ì—†ìŒ")
                    except Exception as e:
                        logger.error(f"âŒ {name} ë¶„ì„ ì‹¤íŒ¨: {e}")
        else:
            logger.info("ìˆœì°¨ ë¶„ì„ ì‹¤í–‰")

            for name, analyzer in analysis_tasks:
                if analyzer:
                    try:
                        result = safe_analysis_execution(
                            name, analyzer, historical_data
                        )
                        if result:
                            analysis_results[name] = result
                            logger.info(f"âœ… {name} ë¶„ì„ ì™„ë£Œ")
                        else:
                            logger.warning(f"âš ï¸ {name} ë¶„ì„ ê²°ê³¼ ì—†ìŒ")
                    except Exception as e:
                        logger.error(f"âŒ {name} ë¶„ì„ ì‹¤íŒ¨: {e}")

        logger.info(
            f"ğŸ‰ ë³‘ë ¬ ë¶„ì„ ì™„ë£Œ: {len(analysis_results)}ê°œ ê²°ê³¼ (ëª©í‘œ: {len(analysis_tasks)}ê°œ)"
        )

        # ë¶„ì„ ê²°ê³¼ ìƒì„¸ ë¡œê¹…
        for name, result in analysis_results.items():
            if isinstance(result, dict):
                logger.info(f"   - {name}: {len(result)} í•­ëª©")
            else:
                logger.info(f"   - {name}: {type(result).__name__}")

        # ğŸ¯ ì™„ì„±ë„ ì²´í¬ ë° ì¢…í•© ë³´ê³ ì„œ ìƒì„±
        logger.info("ğŸ¯ ì‹œìŠ¤í…œ ì™„ì„±ë„ ì²´í¬ ë° ë³´ê³ ì„œ ìƒì„±")

        # ìµœì¢… ì™„ì„±ë„ ê³„ì‚°
        total_required = 11  # ëª©í‘œ ë¶„ì„ê¸° ìˆ˜
        success_count = len(analysis_results)
        completion_rate = (success_count / total_required) * 100

        logger.info("=" * 60)
        logger.info("ğŸ¯ ìµœì¢… ì‹œìŠ¤í…œ ì™„ì„±ë„")
        logger.info("=" * 60)
        logger.info(
            f"ğŸ“Š ì™„ì„±ë¥ : {completion_rate:.1f}% ({success_count}/{total_required})"
        )

        # ì„±ê³µí•œ ë¶„ì„ê¸° ëª©ë¡
        successful_analyzers = list(analysis_results.keys())
        logger.info(f"âœ… ì„±ê³µí•œ ë¶„ì„ê¸°: {successful_analyzers}")

        # ëˆ„ë½ëœ ë¶„ì„ê¸° í™•ì¸
        required_analyzers = [
            "pattern",
            "distribution",
            "roi",
            "pair",
            "cluster",
            "trend",
            "overlap",
            "structural",
            "statistical",
            "negative_sample",
            "unified",
        ]
        missing_analyzers = [
            analyzer
            for analyzer in required_analyzers
            if analyzer not in analysis_results
        ]

        if missing_analyzers:
            logger.error(f"âŒ ëˆ„ë½ëœ ë¶„ì„ê¸°: {missing_analyzers}")

        # ì¢…í•© ë³´ê³ ì„œ ìƒì„±
        comprehensive_report = generate_comprehensive_report(analysis_results)

        # ì™„ì„±ë„ ìƒíƒœ ì¶œë ¥
        if completion_rate >= 100.0:
            logger.info("ğŸ‰ ì‹œìŠ¤í…œ 100% ì™„ì„± ë‹¬ì„±!")
        else:
            logger.warning(f"âš ï¸ ì‹œìŠ¤í…œ ë¯¸ì™„ì„±: {completion_rate:.1f}% (ëª©í‘œ: 100%)")

        logger.info("=" * 60)

        # 4ë‹¨ê³„: ê³ ê¸‰ íŠ¹ì„± ì¶”ì¶œ ë° ë²¡í„° ìƒì„±
        logger.info("ğŸ”¢ 4ë‹¨ê³„: ê³ ê¸‰ íŠ¹ì„± ì¶”ì¶œ ë° ë²¡í„° ìƒì„±")

        try:
            # í†µí•© ë¶„ì„ ê²°ê³¼ ìƒì„±
            unified_analysis = merge_analysis_results(analysis_results)

            # í–¥ìƒëœ ë²¡í„°í™” ì‹œìŠ¤í…œë§Œ ì‚¬ìš© (FeatureExtractor ë¹„í™œì„±í™”)
            logger.info("í–¥ìƒëœ ë²¡í„°í™” ì‹œìŠ¤í…œë§Œ ì‚¬ìš©í•˜ì—¬ íŠ¹ì„± ì¶”ì¶œ")

            # ê¸°ë³¸ê°’ ì„¤ì •
            optimized_features = np.array([])
            optimized_names = []
            extraction_result = type(
                "MockResult",
                (),
                {
                    "quality_metrics": {"entropy": 0.0, "diversity": 0.0},
                    "feature_names": [],
                    "feature_matrix": np.array([]),
                    "feature_groups": {},
                },
            )()

            # ğŸš€ 2ë‹¨ê³„: ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ìƒ˜í”Œ ìƒì„± ì‹œìŠ¤í…œ (800ë°”ì´íŠ¸ â†’ 672KB+)
            logger.debug("ğŸš€ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ìƒ˜í”Œ ìƒì„± ì‹œìŠ¤í…œ ì‹œì‘")

            # ë²¡í„°í™” ì‹¤í–‰ (í–¥ìƒëœ ë²¡í„°í™” ì‹œìŠ¤í…œ ì‚¬ìš©)
            feature_vector = None
            feature_names = []
            training_samples = None

            # í–¥ìƒëœ ë²¡í„°í™” ì‹œìŠ¤í…œ ì‚¬ìš© (EnhancedPatternVectorizer)
            try:
                from src.analysis.enhanced_pattern_vectorizer import (
                    EnhancedPatternVectorizer,
                )

                enhanced_vectorizer = EnhancedPatternVectorizer(config)

                # ğŸš€ ëŒ€í­ í™•ì¥ëœ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ìƒ˜í”Œ ìƒì„± (672KB+ ëª©í‘œ)
                historical_data_dict = []
                for draw in historical_data:
                    historical_data_dict.append(
                        {
                            "numbers": draw.numbers,
                            "draw_no": draw.draw_no,
                            "date": getattr(draw, "date", None),
                        }
                    )

                # ğŸ”¥ ë‹¤ì¤‘ ìœˆë„ìš° í¬ê¸°ë¡œ ëŒ€ëŸ‰ ìƒ˜í”Œ ìƒì„±
                all_training_samples = []
                window_sizes = [20, 30, 40, 50, 60, 70, 80]  # 7ê°€ì§€ ìœˆë„ìš° í¬ê¸°

                for window_size in window_sizes:
                    logger.info(f"ìœˆë„ìš° í¬ê¸° {window_size}ë¡œ ìƒ˜í”Œ ìƒì„± ì¤‘...")
                    samples = enhanced_vectorizer.generate_training_samples(
                        historical_data_dict, window_size=window_size
                    )

                    if samples is not None and len(samples) > 0:
                        all_training_samples.append(samples)
                        logger.info(
                            f"âœ… ìœˆë„ìš° {window_size}: {samples.shape} ìƒ˜í”Œ ìƒì„±"
                        )
                    else:
                        logger.warning(f"âŒ ìœˆë„ìš° {window_size}: ìƒ˜í”Œ ìƒì„± ì‹¤íŒ¨")

                # ëª¨ë“  ìƒ˜í”Œ ê²°í•©
                if all_training_samples:
                    training_samples = np.vstack(all_training_samples)
                    logger.info(f"ğŸ‰ ì „ì²´ ê²°í•© ìƒ˜í”Œ: {training_samples.shape}")

                    # íŒŒì¼ í¬ê¸° í™•ì¸
                    total_size = training_samples.nbytes
                    logger.info(
                        f"ğŸ“Š ì „ì²´ ìƒ˜í”Œ í¬ê¸°: {total_size:,} bytes ({total_size/1024:.1f} KB)"
                    )

                    # ëª©í‘œ ë‹¬ì„± ì—¬ë¶€
                    if total_size >= 672000:  # 672KB
                        logger.info("ğŸ‰ ëª©í‘œ íŒŒì¼ í¬ê¸° ë‹¬ì„±! (672KB+)")
                    else:
                        logger.warning(f"âš ï¸ ëª©í‘œ ë¯¸ë‹¬ì„±: {total_size} < 672000 bytes")

                        # ì¶”ê°€ ìƒ˜í”Œ ìƒì„± (ë°ì´í„° ì¦ê°•)
                        logger.info("ğŸ”¥ ë°ì´í„° ì¦ê°•ìœ¼ë¡œ ì¶”ê°€ ìƒ˜í”Œ ìƒì„±...")
                        augmented_samples = []

                        # ë…¸ì´ì¦ˆ ì¶”ê°€ ë²„ì „
                        for i in range(3):  # 3ë°° ì¦ê°•
                            noise_samples = training_samples + np.random.normal(
                                0, 0.01, training_samples.shape
                            ).astype(np.float32)
                            augmented_samples.append(noise_samples)

                        # ìµœì¢… ê²°í•©
                        if augmented_samples:
                            training_samples = np.vstack(
                                [training_samples] + augmented_samples
                            )
                            final_size = training_samples.nbytes
                            logger.info(
                                f"ğŸš€ ì¦ê°• í›„ ìµœì¢… í¬ê¸°: {final_size:,} bytes ({final_size/1024:.1f} KB)"
                            )

                    # ğŸ”¥ ì‹¤ì œ í›ˆë ¨ ìƒ˜í”Œì„ ë²¡í„° íŒŒì¼ë¡œ ì €ì¥ (800ë°”ì´íŠ¸ â†’ 672KB+)
                    from pathlib import Path as PathLib

                    vector_path = PathLib("data/cache/feature_vector_full.npy")
                    vector_path.parent.mkdir(parents=True, exist_ok=True)

                    # í›ˆë ¨ ìƒ˜í”Œ ì „ì²´ë¥¼ ë²¡í„° íŒŒì¼ë¡œ ì €ì¥
                    np.save(vector_path, training_samples.astype(np.float32))
                    actual_size = vector_path.stat().st_size
                    logger.info(
                        f"âœ… í›ˆë ¨ ìƒ˜í”Œì„ ë²¡í„° íŒŒì¼ë¡œ ì €ì¥: {actual_size:,} bytes ({actual_size/1024:.1f} KB)"
                    )

                    # í›ˆë ¨ ìƒ˜í”Œ ì €ì¥ (ë³„ë„ ë©”íƒ€ë°ì´í„° íŒŒì¼)
                    metadata_path = enhanced_vectorizer.save_training_samples(
                        training_samples, "feature_vector_full_metadata.npy"
                    )

                    # ëŒ€í‘œ ë²¡í„° ì„ íƒ (ë§ˆì§€ë§‰ ìƒ˜í”Œ ì‚¬ìš©)
                    feature_vector = (
                        training_samples[-1] if len(training_samples) > 0 else None
                    )
                    feature_names = enhanced_vectorizer.get_feature_names()

                    logger.info(
                        f"ğŸ¯ ìµœì¢… ëª©í‘œ ë‹¬ì„±: {'âœ…' if actual_size >= 672000 else 'âŒ'} (ëª©í‘œ: 672KB+)"
                    )
                else:
                    logger.warning("ëª¨ë“  ìœˆë„ìš°ì—ì„œ ìƒ˜í”Œ ìƒì„± ì‹¤íŒ¨ - ë‹¨ì¼ ë²¡í„° ìƒì„±")
                    training_samples = None  # ëª…ì‹œì ìœ¼ë¡œ None ì„¤ì •
                    # í´ë°±: ë‹¨ì¼ ë²¡í„° ìƒì„±
                    feature_vector = (
                        enhanced_vectorizer.vectorize_full_analysis_enhanced(
                            unified_analysis
                        )
                    )
                    feature_names = enhanced_vectorizer.get_feature_names()

                    # ë‹¨ì¼ ë²¡í„°ë„ ì €ì¥
                    if feature_vector is not None:
                        from pathlib import Path as PathLib

                        vector_path = PathLib("data/cache/feature_vector_full.npy")
                        vector_path.parent.mkdir(parents=True, exist_ok=True)
                        np.save(vector_path, feature_vector.astype(np.float32))
                        logger.info(
                            f"í´ë°±: ë‹¨ì¼ ë²¡í„° ì €ì¥ ì™„ë£Œ ({len(feature_vector)}ì°¨ì›)"
                        )

                if feature_vector is not None and len(feature_vector) > 0:
                    logger.info(f"í–¥ìƒëœ ë²¡í„°í™” ì‹œìŠ¤í…œ: {len(feature_vector)}ì°¨ì›")
                else:
                    logger.warning("í–¥ìƒëœ ë²¡í„°í™” ì‹œìŠ¤í…œ ì‹¤íŒ¨ - ê¸°ì¡´ ì‹œìŠ¤í…œ ì‚¬ìš©")
                    # í´ë°±: ê¸°ì¡´ ë²¡í„°í™” ì‹œìŠ¤í…œ
                    if vectorizer is not None and hasattr(
                        vectorizer, "vectorize_full_analysis"
                    ):
                        feature_vector = vectorizer.vectorize_full_analysis(
                            unified_analysis
                        )
                        feature_names = (
                            vectorizer.get_feature_names()
                            if hasattr(vectorizer, "get_feature_names")
                            else []
                        )
                        if feature_vector is not None and len(feature_vector) > 0:
                            logger.info(
                                f"ê¸°ì¡´ ë²¡í„°í™” ì‹œìŠ¤í…œ: {len(feature_vector)}ì°¨ì›"
                            )
                        else:
                            feature_vector = None
                            feature_names = []
                    else:
                        feature_vector = None
                        feature_names = []

            except Exception as e:
                logger.warning(f"í–¥ìƒëœ ë²¡í„°í™” ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}")
                # í´ë°±: ê¸°ì¡´ ë²¡í„°í™” ì‹œìŠ¤í…œ
                if vectorizer is not None and hasattr(
                    vectorizer, "vectorize_full_analysis"
                ):
                    try:
                        feature_vector = vectorizer.vectorize_full_analysis(
                            unified_analysis
                        )
                        feature_names = (
                            vectorizer.get_feature_names()
                            if hasattr(vectorizer, "get_feature_names")
                            else []
                        )
                        if feature_vector is not None and len(feature_vector) > 0:
                            logger.info(
                                f"ê¸°ì¡´ ë²¡í„°í™” ì‹œìŠ¤í…œ: {len(feature_vector)}ì°¨ì›"
                            )
                        else:
                            feature_vector = None
                            feature_names = []
                    except Exception as fallback_e:
                        logger.error(f"ê¸°ì¡´ ë²¡í„°í™” ì‹œìŠ¤í…œë„ ì‹¤íŒ¨: {fallback_e}")
                        feature_vector = None
                        feature_names = []
                else:
                    feature_vector = None
                    feature_names = []

            # ğŸ”¥ ì´ì¤‘ ë²¡í„° ì‹œìŠ¤í…œ í†µí•©: í›ˆë ¨ ìƒ˜í”Œë§Œ ìµœì¢… ë²¡í„°ë¡œ ì‚¬ìš©
            logger.info("ê¸°ì¡´ íŒ¨í„´ ë²¡í„°ë¼ì´ì € ë¹„í™œì„±í™” - í›ˆë ¨ ìƒ˜í”Œë§Œ ìµœì¢… ë²¡í„°ë¡œ ì‚¬ìš©")

            # í›ˆë ¨ ìƒ˜í”Œì„ ìµœì¢… ë²¡í„°ë¡œ ì‚¬ìš©
            if training_samples is not None and training_samples.size > 0:
                combined_vector = training_samples
                combined_names = enhanced_vectorizer.get_feature_names()

                logger.info(f"âœ… í›ˆë ¨ ìƒ˜í”Œì„ ìµœì¢… ë²¡í„°ë¡œ ì„¤ì •: {combined_vector.shape}")
                logger.info(f"âœ… íŠ¹ì„± ì´ë¦„ ìˆ˜: {len(combined_names)}ê°œ")
                logger.info(
                    f"âœ… ì´ì¤‘ ì €ì¥ ì‹œìŠ¤í…œ í†µí•© ì™„ë£Œ - ë‹¨ì¼ íŒŒì¼ì— ëŒ€ëŸ‰ ìƒ˜í”Œ ì €ì¥"
                )
            else:
                logger.error("í›ˆë ¨ ìƒ˜í”Œì´ ì—†ì–´ì„œ ë²¡í„° ìƒì„± ì‹¤íŒ¨")
                return False

            # ë²¡í„° í’ˆì§ˆ ê²€ì¦
            if not validate_feature_vector(combined_vector, combined_names, config):
                logger.error("íŠ¹ì„± ë²¡í„° í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨")
                return False

        except Exception as e:
            logger.error(f"íŠ¹ì„± ë²¡í„° ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            log_exception_with_trace(
                "optimized_data_analysis_pipeline", e, "íŠ¹ì„± ë²¡í„° ìƒì„± ì¤‘ ì˜¤ë¥˜"
            )
            return False

        # 5ë‹¨ê³„: ê²°ê³¼ ì €ì¥
        logger.info("ğŸ’¾ 5ë‹¨ê³„: ê²°ê³¼ ì €ì¥")

        try:
            # ë¶„ì„ ê²°ê³¼ ì €ì¥
            save_analysis_results(unified_analysis, config)

            # íŠ¹ì„± ë²¡í„° ì €ì¥
            if vectorizer is not None:
                success = vectorizer.save_vector_to_file(
                    combined_vector, combined_names
                )
                if success:
                    vector_path = "data/cache/feature_vector_full.npy"
                    names_path = "data/cache/feature_vector_full.names.json"
                else:
                    logger.error("ë²¡í„° ì €ì¥ ì‹¤íŒ¨")
            else:
                # vectorizerê°€ Noneì¸ ê²½ìš° ì§ì ‘ ì €ì¥
                from pathlib import Path
                import json

                cache_dir = Path("data/cache")
                cache_dir.mkdir(parents=True, exist_ok=True)

                vector_path = cache_dir / "feature_vector_full.npy"
                names_path = cache_dir / "feature_vector_full.names.json"

                np.save(vector_path, combined_vector)
                with open(names_path, "w", encoding="utf-8") as f:
                    json.dump(combined_names, f, ensure_ascii=False, indent=2)

                vector_path = str(vector_path)
                names_path = str(names_path)

            logger.info(f"ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
            logger.info(f"  - ë²¡í„° íŒŒì¼: {vector_path}")
            logger.info(f"  - ì´ë¦„ íŒŒì¼: {names_path}")

        except Exception as e:
            logger.error(f"ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

        # 6ë‹¨ê³„: ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±
        logger.info("ğŸ“ˆ 6ë‹¨ê³„: ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±")

        try:
            execution_time = time.time() - start_time

            # ì„±ëŠ¥ ë³´ê³ ì„œ ë°ì´í„° ìˆ˜ì§‘
            # ë²¡í„° ì°¨ì› ì •ë³´ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ
            if combined_vector.ndim == 1:
                vector_info = {"dimensions": len(combined_vector), "samples": 1}
            elif combined_vector.ndim == 2:
                samples, dims = combined_vector.shape
                vector_info = {"dimensions": dims, "samples": samples}
            else:
                vector_info = {"dimensions": combined_vector.size, "samples": "unknown"}

            performance_data = {
                "execution_time": execution_time,
                "data_size": len(historical_data),
                "vector_info": vector_info,
                "analysis_results_count": len(analysis_results),
                "memory_usage": get_memory_usage(),
                "data_quality_score": validation_result.quality_score,
                "feature_extraction_quality": extraction_result.quality_metrics,
                "optimization_used": {
                    "process_pool": process_pool_manager is not None,
                    "memory_manager": memory_manager is not None,
                    "hybrid_optimizer": hybrid_optimizer is not None,
                },
            }

            # ì„±ëŠ¥ ë³´ê³ ì„œ ì €ì¥
            save_performance_report(performance_data, "optimized_data_analysis")

            logger.info(f"ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ (ì‹¤í–‰ì‹œê°„: {execution_time:.2f}ì´ˆ)")

        except Exception as e:
            logger.warning(f"ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")

        # ğŸ¯ 6ë‹¨ê³„: ìµœì¢… ê²€ì¦ ì‹œìŠ¤í…œ
        logger.info("ğŸ¯ 6ë‹¨ê³„: ìµœì¢… ê²€ì¦ ì‹œìŠ¤í…œ")

        def validate_final_output():
            """ìµœì¢… ì¶œë ¥ ê²€ì¦"""
            try:
                from pathlib import Path
                import numpy as np
                import json

                vector_file = Path("data/cache/feature_vector_full.npy")
                names_file = Path("data/cache/feature_vector_full.names.json")
                training_file = Path("data/cache/feature_vector_full_metadata.npy")

                if not vector_file.exists():
                    logger.error("âŒ ë²¡í„° íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
                    return False

                # ë²¡í„° ë¡œë“œ
                vectors = np.load(vector_file)

                # í›ˆë ¨ ìƒ˜í”Œ íŒŒì¼ í™•ì¸
                training_samples = None
                training_file_size = 0
                if training_file.exists():
                    training_samples = np.load(training_file)
                    training_file_size = training_file.stat().st_size

                # íŠ¹ì„± ì´ë¦„ ë¡œë“œ
                feature_names = []
                if names_file.exists():
                    with open(names_file, "r", encoding="utf-8") as f:
                        feature_names = json.load(f)

                # ğŸ”¥ ëŒ€ëŸ‰ ìƒ˜í”Œ ë°°ì—´ ê²€ì¦ ë¡œì§ ê°œì„ 
                file_size = vector_file.stat().st_size

                # ë²¡í„° ì°¨ì› ë° ìƒ˜í”Œ ìˆ˜ í™•ì¸
                if vectors.ndim == 1:
                    # ë‹¨ì¼ ë²¡í„°ì¸ ê²½ìš°
                    vector_dim = len(vectors)
                    sample_count = 1
                    logger.info(f"ğŸ“Š ë‹¨ì¼ ë²¡í„°: {vector_dim}ì°¨ì›")
                elif vectors.ndim == 2:
                    # ë‹¤ì¤‘ ìƒ˜í”Œ ë°°ì—´ì¸ ê²½ìš°
                    sample_count, vector_dim = vectors.shape
                    logger.info(
                        f"ğŸ“Š ë‹¤ì¤‘ ìƒ˜í”Œ: {sample_count}ê°œ ìƒ˜í”Œ Ã— {vector_dim}ì°¨ì›"
                    )
                else:
                    # 3ì°¨ì› ì´ìƒ ë°°ì—´ì¸ ê²½ìš°
                    total_elements = vectors.size
                    vector_dim = vectors.shape[-1] if len(vectors.shape) > 0 else 0
                    sample_count = total_elements // vector_dim if vector_dim > 0 else 0
                    logger.info(
                        f"ğŸ“Š ê³ ì°¨ì› ë°°ì—´: {vectors.shape}, ì´ {total_elements}ê°œ ìš”ì†Œ"
                    )

                # ğŸ”¥ í†µí•©ëœ ë²¡í„° ì‹œìŠ¤í…œ ê²€ì¦ (í›ˆë ¨ ìƒ˜í”Œ ì¤‘ì‹¬)
                if vectors.ndim == 2:
                    # ë‹¤ì¤‘ ìƒ˜í”Œ ë°°ì—´ì¸ ê²½ìš° (í†µí•©ëœ ì‹œìŠ¤í…œ)
                    sample_count, vector_dim = vectors.shape
                    file_size = vector_file.stat().st_size

                    logger.info(
                        f"ğŸ“Š í†µí•©ëœ ë²¡í„° ì‹œìŠ¤í…œ: {sample_count}ê°œ ìƒ˜í”Œ Ã— {vector_dim}ì°¨ì›"
                    )
                    logger.info(
                        f"ğŸ“Š íŒŒì¼ í¬ê¸°: {file_size:,} bytes ({file_size/1024:.1f} KB)"
                    )

                    # í›ˆë ¨ ê°€ëŠ¥í•œ í¬ê¸° ê²€ì¦
                    checks = {
                        "ìƒ˜í”Œ ìˆ˜ 1000ê°œ ì´ìƒ": sample_count >= 1000,
                        "ì°¨ì› 168ì°¨ì›": vector_dim == 168,
                        "ì´ë¦„ ìˆ˜ ì¼ì¹˜": (
                            len(feature_names) == 168 if feature_names else True
                        ),
                        "íŒŒì¼ í¬ê¸° 672KB ì´ìƒ": file_size >= 672000,
                        "NaN/Inf ì—†ìŒ": not (
                            np.any(np.isnan(vectors)) or np.any(np.isinf(vectors))
                        ),
                    }

                    # ì˜ˆìƒ í¬ê¸° ê²€ì¦
                    expected_size = sample_count * vector_dim * 4  # float32
                    size_check = file_size >= expected_size * 0.9

                    if not size_check:
                        logger.warning(
                            f"íŒŒì¼ í¬ê¸° ë¶ˆì¼ì¹˜: ì‹¤ì œ={file_size}, ì˜ˆìƒ={expected_size}"
                        )

                else:
                    # ë‹¨ì¼ ë²¡í„°ì¸ ê²½ìš° (ë ˆê±°ì‹œ)
                    logger.warning("ë‹¨ì¼ ë²¡í„° ê°ì§€ - í›ˆë ¨ì— ë¶€ì í•©")
                    checks = {
                        "ìƒ˜í”Œ ìˆ˜ 1000ê°œ ì´ìƒ": False,  # ë‹¨ì¼ ë²¡í„°ëŠ” í›ˆë ¨ ë¶ˆê°€
                        "ì°¨ì› 168ì°¨ì›": len(vectors) == 168,
                        "ì´ë¦„ ìˆ˜ ì¼ì¹˜": (
                            len(feature_names) == 168 if feature_names else True
                        ),
                        "íŒŒì¼ í¬ê¸° 672KB ì´ìƒ": False,  # ë‹¨ì¼ ë²¡í„°ëŠ” í¬ê¸° ë¶€ì¡±
                        "NaN/Inf ì—†ìŒ": not (
                            np.any(np.isnan(vectors)) or np.any(np.isinf(vectors))
                        ),
                    }

                # ê²€ì¦ ê²°ê³¼ ë¡œê¹…
                logger.info("ğŸ” ìµœì¢… ê²€ì¦ ê²°ê³¼:")
                passed_checks = 0
                for check_name, passed in checks.items():
                    status = "âœ… í†µê³¼" if passed else "âŒ ì‹¤íŒ¨"
                    logger.info(f"   - {check_name}: {status}")
                    if passed:
                        passed_checks += 1

                # ìƒì„¸ ì •ë³´
                if vectors.ndim == 2:
                    logger.info(
                        f"ğŸ“Š í†µí•©ëœ ë²¡í„°: {vectors.shape}, {file_size:,} bytes ({file_size/1024:.1f} KB)"
                    )
                else:
                    logger.info(
                        f"ğŸ“Š ë‹¨ì¼ ë²¡í„°: {vectors.shape}, {file_size:,} bytes ({file_size/1024:.1f} KB)"
                    )

                logger.info(f"ğŸ“Š íŠ¹ì„± ì´ë¦„: {len(feature_names)}ê°œ")
                logger.info(
                    f"ğŸ† ì „ì²´ ì„±ê³µë¥ : {passed_checks}/{len(checks)} ({passed_checks/len(checks)*100:.1f}%)"
                )

                # ğŸ¯ ì„±ê³µ ê¸°ì¤€: 5ê°œ ì¤‘ 4ê°œ ì´ìƒ í†µê³¼ (ê¸°ì¡´ ìœ ì§€)
                success = passed_checks >= 4
                if success:
                    logger.info("ğŸ‰ DAEBAK_AI í”„ë¡œì íŠ¸ ì™„ì „ ìˆ˜ì • ì„±ê³µ!")
                else:
                    logger.warning("âš ï¸ ì¼ë¶€ ê²€ì¦ ì‹¤íŒ¨ - ì¶”ê°€ ìˆ˜ì • í•„ìš”")

                return success

            except Exception as e:
                logger.error(f"ìµœì¢… ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
                return False

        # ìµœì¢… ê²€ì¦ ì‹¤í–‰
        validation_success = validate_final_output()

        if not validation_success:
            logger.warning("ìµœì¢… ê²€ì¦ ì‹¤íŒ¨í–ˆì§€ë§Œ íŒŒì´í”„ë¼ì¸ì€ ê³„ì† ì§„í–‰")

        # ìµœì í™” ì‹œìŠ¤í…œ ì •ë¦¬
        cleanup_optimization_systems()

        logger.info(
            f"âœ… ìµœì í™”ëœ ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ (ì´ {time.time() - start_time:.2f}ì´ˆ)"
        )
        return True

    except Exception as e:
        logger.error(f"âŒ ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return False


def validate_lottery_data(historical_data: List[LotteryNumber]) -> bool:
    """ë¡œë˜ ë°ì´í„° ê²€ì¦"""
    try:
        if not historical_data:
            logger.error("ë¹ˆ ë°ì´í„°ì…‹")
            return False

        for i, draw in enumerate(historical_data):
            # ë²ˆí˜¸ ê°œìˆ˜ ê²€ì¦
            if len(draw.numbers) != 6:
                logger.error(
                    f"íšŒì°¨ {draw.draw_no}: ë²ˆí˜¸ ê°œìˆ˜ ì˜¤ë¥˜ ({len(draw.numbers)}ê°œ)"
                )
                return False

            # ë²ˆí˜¸ ë²”ìœ„ ê²€ì¦
            for number in draw.numbers:
                if not (1 <= number <= 45):
                    logger.error(f"íšŒì°¨ {draw.draw_no}: ë²ˆí˜¸ ë²”ìœ„ ì˜¤ë¥˜ ({number})")
                    return False

            # ì¤‘ë³µ ë²ˆí˜¸ ê²€ì¦
            if len(set(draw.numbers)) != 6:
                logger.error(f"íšŒì°¨ {draw.draw_no}: ì¤‘ë³µ ë²ˆí˜¸ ì¡´ì¬")
                return False

        logger.info(f"ë°ì´í„° ê²€ì¦ ì™„ë£Œ: {len(historical_data)}íšŒì°¨ ëª¨ë‘ ì •ìƒ")
        return True

    except Exception as e:
        logger.error(f"ë°ì´í„° ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
        return False


def run_parallel_analysis(
    historical_data: List[LotteryNumber],
    analyzers: Dict[str, Any],
    config: Dict[str, Any],
) -> Dict[str, Any]:
    """ë³‘ë ¬ ë¶„ì„ ì‹¤í–‰"""
    analysis_results = {}

    try:
        logger.info("ë³‘ë ¬ ë¶„ì„ ì‹œì‘")

        # ğŸ”¥ ëª¨ë“  11ê°œ ë¶„ì„ê¸°ë¥¼ í¬í•¨í•œ ë¶„ì„ ì‘ì—… ì •ì˜
        analysis_tasks = []

        # ê¸°ì¡´ í•µì‹¬ ë¶„ì„ê¸°ë“¤
        core_analyzers = ["pattern", "distribution", "roi", "pair"]
        for name in core_analyzers:
            if name in analyzers and analyzers[name] is not None:
                analysis_tasks.append((name, analyzers[name]))

        # ğŸš€ ëª¨ë“  11ê°œ ë¶„ì„ê¸° ì™„ì „ í™œìš© (í™•ì¥ëœ ë¶„ì„ê¸°ë“¤)
        extended_analyzers = [
            "cluster",
            "trend",
            "overlap",
            "structural",
            "statistical",
            "negative_sample",
            "unified",  # ğŸ¯ UnifiedAnalyzer ì¶”ê°€
        ]
        for name in extended_analyzers:
            if name in analyzers and analyzers[name] is not None:
                analysis_tasks.append((name, analyzers[name]))
                logger.info(f"âœ… {name} ë¶„ì„ê¸° ë³‘ë ¬ ë¶„ì„ì— í¬í•¨")

        logger.info(f"ì´ {len(analysis_tasks)}ê°œ ë¶„ì„ê¸°ë¡œ ë³‘ë ¬ ë¶„ì„ ì‹¤í–‰ (ëª©í‘œ: 11ê°œ)")
        logger.info(f"í™œì„±í™”ëœ ë¶„ì„ê¸°: {[name for name, _ in analysis_tasks]}")

        # í”„ë¡œì„¸ìŠ¤ í’€ì´ ìˆìœ¼ë©´ ë³‘ë ¬ ì‹¤í–‰, ì—†ìœ¼ë©´ ìˆœì°¨ ì‹¤í–‰
        if process_pool_manager and len(analysis_tasks) > 1:
            logger.info("í”„ë¡œì„¸ìŠ¤ í’€ì„ ì‚¬ìš©í•œ ë³‘ë ¬ ë¶„ì„")

            # ğŸ”§ ì›Œì»¤ ìˆ˜ë¥¼ ë¶„ì„ê¸° ìˆ˜ì— ë§ê²Œ ì¡°ì •
            max_workers = min(len(analysis_tasks), 8)  # ìµœëŒ€ 8ê°œ ì›Œì»¤

            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_name = {}

                for name, analyzer in analysis_tasks:
                    if analyzer:
                        future = executor.submit(
                            safe_analysis_execution, name, analyzer, historical_data
                        )
                        future_to_name[future] = name

                # ê²°ê³¼ ìˆ˜ì§‘
                for future in as_completed(future_to_name):
                    name = future_to_name[future]
                    try:
                        result = future.result(timeout=300)  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
                        if result:
                            analysis_results[name] = result
                            logger.info(f"âœ… {name} ë¶„ì„ ì™„ë£Œ")
                        else:
                            logger.warning(f"âš ï¸ {name} ë¶„ì„ ê²°ê³¼ ì—†ìŒ")
                    except Exception as e:
                        logger.error(f"âŒ {name} ë¶„ì„ ì‹¤íŒ¨: {e}")
        else:
            logger.info("ìˆœì°¨ ë¶„ì„ ì‹¤í–‰")

            for name, analyzer in analysis_tasks:
                if analyzer:
                    try:
                        result = safe_analysis_execution(
                            name, analyzer, historical_data
                        )
                        if result:
                            analysis_results[name] = result
                            logger.info(f"âœ… {name} ë¶„ì„ ì™„ë£Œ")
                        else:
                            logger.warning(f"âš ï¸ {name} ë¶„ì„ ê²°ê³¼ ì—†ìŒ")
                    except Exception as e:
                        logger.error(f"âŒ {name} ë¶„ì„ ì‹¤íŒ¨: {e}")

        logger.info(
            f"ğŸ‰ ë³‘ë ¬ ë¶„ì„ ì™„ë£Œ: {len(analysis_results)}ê°œ ê²°ê³¼ (ëª©í‘œ: {len(analysis_tasks)}ê°œ)"
        )

        # ë¶„ì„ ê²°ê³¼ ìƒì„¸ ë¡œê¹…
        for name, result in analysis_results.items():
            if isinstance(result, dict):
                logger.info(f"   - {name}: {len(result)} í•­ëª©")
            else:
                logger.info(f"   - {name}: {type(result).__name__}")

        return analysis_results

    except Exception as e:
        logger.error(f"ë³‘ë ¬ ë¶„ì„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return {}


def validate_analysis_result(analyzer_name: str, result: Any) -> bool:
    """ë¶„ì„ ê²°ê³¼ ê²€ì¦ - 100% ì™„ì„±ì„ ìœ„í•œ ê°•í™”ëœ ê²€ì¦"""

    # ê¸°ë³¸ íƒ€ì… ê²€ì¦
    if result is None:
        logger.error(f"{analyzer_name}: ê²°ê³¼ê°€ Noneì…ë‹ˆë‹¤")
        return False

    if callable(result):
        logger.error(
            f"{analyzer_name}: í•¨ìˆ˜ ê°ì²´ë¥¼ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤ - íƒ€ì…: {type(result)}"
        )
        return False

    if not isinstance(result, dict):
        logger.error(f"{analyzer_name}: dictê°€ ì•„ë‹Œ {type(result)} íƒ€ì…ì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤")
        return False

    # ë¹ˆ ê²°ê³¼ ê²€ì¦
    if not result:
        logger.warning(f"{analyzer_name}: ë¹ˆ ê²°ê³¼ë¥¼ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤")
        return False

    # ì—ëŸ¬ ê²°ê³¼ ê²€ì¦
    if "error" in result:
        logger.error(f"{analyzer_name}: ì—ëŸ¬ ê²°ê³¼ - {result['error']}")
        return False

    # íŠ¹ì • ë¶„ì„ê¸°ë³„ ì¶”ê°€ ê²€ì¦
    if analyzer_name == "distribution":
        # DistributionPattern ê°ì²´ë“¤ì´ ì˜¬ë°”ë¥´ê²Œ ë³€í™˜ë˜ì—ˆëŠ”ì§€ í™•ì¸
        for key, patterns in result.items():
            if isinstance(patterns, list):
                for pattern in patterns:
                    if hasattr(pattern, "__call__"):  # í•¨ìˆ˜ ê°ì²´ ê²€ì¦
                        logger.error(
                            f"{analyzer_name}: {key}ì— í•¨ìˆ˜ ê°ì²´ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤"
                        )
                        return False

    elif analyzer_name == "roi":
        # ROI ë¶„ì„ ê²°ê³¼ê°€ ì˜¬ë°”ë¥¸ ë”•ì…”ë„ˆë¦¬ í˜•íƒœì¸ì§€ í™•ì¸
        required_keys = [
            "roi_pattern_groups",
            "number_roi_scores",
            "roi_trend_by_pattern",
        ]
        for key in required_keys:
            if key not in result:
                logger.warning(f"{analyzer_name}: í•„ìˆ˜ í‚¤ '{key}'ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤")

    logger.info(f"âœ… {analyzer_name}: ê²°ê³¼ ê²€ì¦ í†µê³¼ ({len(result)} í•­ëª©)")
    return True


def safe_analysis_execution(
    name: str, analyzer: Any, historical_data: List[LotteryNumber]
) -> Optional[Dict[str, Any]]:
    """ì•ˆì „í•œ ë¶„ì„ ì‹¤í–‰ - ê°•í™”ëœ ê²°ê³¼ ê²€ì¦ í¬í•¨"""
    try:
        logger.info(f"ğŸ” {name} ë¶„ì„ ì‹œì‘")

        # ë¶„ì„ ì‹¤í–‰
        if hasattr(analyzer, "analyze"):
            result = analyzer.analyze(historical_data)
        elif hasattr(analyzer, "_analyze_impl"):
            result = analyzer._analyze_impl(historical_data)
        else:
            logger.error(f"{name}: analyze ë˜ëŠ” _analyze_impl ë©”ì„œë“œê°€ ì—†ìŠµë‹ˆë‹¤")
            return None

        # ê²°ê³¼ íƒ€ì… ê°•ì œ ë³€í™˜ (í•¨ìˆ˜ ê°ì²´ ë“± ë¬¸ì œ í•´ê²°)
        if hasattr(result, "to_dict") and callable(result.to_dict):
            logger.info(f"{name}: ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜")
            result = result.to_dict()
        elif not isinstance(result, dict):
            logger.warning(
                f"{name}: ì˜ˆìƒì¹˜ ëª»í•œ ê²°ê³¼ íƒ€ì… {type(result)} - ë”•ì…”ë„ˆë¦¬ë¡œ ë˜í•‘"
            )
            result = {"analysis_result": result, "result_type": str(type(result))}

        # ê°•í™”ëœ ê²°ê³¼ ê²€ì¦
        if validate_analysis_result(name, result):
            logger.info(f"âœ… {name} ë¶„ì„ ì™„ë£Œ ë° ê²€ì¦ í†µê³¼")
            return result
        else:
            logger.error(f"âŒ {name} ë¶„ì„ ê²°ê³¼ ê²€ì¦ ì‹¤íŒ¨")
            return None

    except Exception as e:
        logger.error(f"âŒ {name} ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        log_exception_with_trace(logger, e, f"{name} ë¶„ì„ ì‹¤í–‰ ì¤‘")
        return None


def merge_analysis_results(analysis_results: Dict[str, Any]) -> Dict[str, Any]:
    """ë¶„ì„ ê²°ê³¼ë“¤ì„ í†µí•©"""
    try:
        unified_analysis = {
            "metadata": {
                "analysis_count": len(analysis_results),
                "timestamp": datetime.now().isoformat(),
                "version": "1.0.0",
            }
        }

        # ê° ë¶„ì„ ê²°ê³¼ë¥¼ í†µí•©
        for analysis_type, result in analysis_results.items():
            if result:
                unified_analysis[analysis_type] = result

        logger.info(f"ë¶„ì„ ê²°ê³¼ í†µí•© ì™„ë£Œ: {len(unified_analysis)}ê°œ í•­ëª©")
        return unified_analysis

    except Exception as e:
        logger.error(f"ë¶„ì„ ê²°ê³¼ í†µí•© ì¤‘ ì˜¤ë¥˜: {e}")
        return {}


def validate_feature_vector(
    feature_vector: np.ndarray, feature_names: List[str], config: Dict[str, Any]
) -> bool:
    """íŠ¹ì„± ë²¡í„° í’ˆì§ˆ ê²€ì¦ (ë‹¨ì¼ ë²¡í„° ë° ë‹¤ì¤‘ ìƒ˜í”Œ ë°°ì—´ ì§€ì›)"""
    try:
        # ê¸°ë³¸ ê²€ì¦
        if feature_vector is None or feature_vector.size == 0:
            logger.error("ë¹ˆ íŠ¹ì„± ë²¡í„°")
            return False

        if feature_names is None or len(feature_names) == 0:
            logger.error("ë¹ˆ íŠ¹ì„± ì´ë¦„ ë¦¬ìŠ¤íŠ¸")
            return False

        # ğŸ”¥ ë‹¤ì¤‘ ìƒ˜í”Œ ë°°ì—´ ì§€ì›
        if feature_vector.ndim == 1:
            # ë‹¨ì¼ ë²¡í„°ì¸ ê²½ìš°
            vector_dim = len(feature_vector)
            sample_count = 1
            logger.info(f"ë‹¨ì¼ ë²¡í„° ê²€ì¦: {vector_dim}ì°¨ì›")
        elif feature_vector.ndim == 2:
            # ë‹¤ì¤‘ ìƒ˜í”Œ ë°°ì—´ì¸ ê²½ìš°
            sample_count, vector_dim = feature_vector.shape
            logger.info(
                f"ë‹¤ì¤‘ ìƒ˜í”Œ ë°°ì—´ ê²€ì¦: {sample_count}ê°œ ìƒ˜í”Œ Ã— {vector_dim}ì°¨ì›"
            )
        else:
            logger.error(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ë²¡í„° ì°¨ì›: {feature_vector.ndim}ì°¨ì›")
            return False

        # ì°¨ì› ì¼ì¹˜ ê²€ì¦ (ë²¡í„°ì˜ ë§ˆì§€ë§‰ ì°¨ì›ê³¼ íŠ¹ì„± ì´ë¦„ ìˆ˜ ë¹„êµ)
        if vector_dim != len(feature_names):
            logger.error(
                f"ë²¡í„° ì°¨ì› ë¶ˆì¼ì¹˜: ë²¡í„°={vector_dim}, ì´ë¦„={len(feature_names)}"
            )
            return False

        # ìµœì†Œ ì°¨ì› ê²€ì¦
        try:
            min_dimensions = config["vector"]["min_required_dimension"]
            if vector_dim < min_dimensions:
                logger.error(f"ë²¡í„° ì°¨ì› ë¶€ì¡±: {vector_dim} < {min_dimensions}")
                return False
        except (KeyError, TypeError):
            logger.warning("ìµœì†Œ ì°¨ì› ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")

        # NaN/Inf ê²€ì¦
        if np.any(np.isnan(feature_vector)) or np.any(np.isinf(feature_vector)):
            logger.error("ë²¡í„°ì— NaN ë˜ëŠ” Inf ê°’ í¬í•¨")
            return False

        if feature_vector.ndim == 1:
            logger.info(f"íŠ¹ì„± ë²¡í„° ê²€ì¦ ì™„ë£Œ: {vector_dim}ì°¨ì›")
        else:
            logger.info(
                f"íŠ¹ì„± ë²¡í„° ê²€ì¦ ì™„ë£Œ: {sample_count}ê°œ ìƒ˜í”Œ Ã— {vector_dim}ì°¨ì›"
            )
        return True

    except Exception as e:
        logger.error(f"íŠ¹ì„± ë²¡í„° ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
        return False


def save_analysis_results(
    unified_analysis: Dict[str, Any], config: Dict[str, Any]
) -> None:
    """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
    try:
        # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
        result_dir = Path("data/result/analysis")
        result_dir.mkdir(parents=True, exist_ok=True)

        # JSON íŒŒì¼ë¡œ ì €ì¥
        result_file = result_dir / "optimized_analysis_result.json"

        # í•¨ìˆ˜ ê°ì²´ ì œê±° í›„ ì§ë ¬í™”
        serializable_analysis = _make_json_serializable(unified_analysis)

        with open(result_file, "w", encoding="utf-8") as f:
            json.dump(
                serializable_analysis,
                f,
                ensure_ascii=False,
                indent=2,
            )

        logger.info(f"ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {result_file}")

    except Exception as e:
        logger.error(f"ë¶„ì„ ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
        raise


def _make_json_serializable(obj: Any) -> Any:
    """ê°ì²´ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ê²Œ ë³€í™˜"""
    if callable(obj):
        return f"<function: {obj.__name__ if hasattr(obj, '__name__') else str(obj)}>"
    elif isinstance(obj, dict):
        return {key: _make_json_serializable(value) for key, value in obj.items()}
    elif isinstance(obj, (list, tuple)):
        return [_make_json_serializable(item) for item in obj]
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, (np.integer, np.floating)):
        return obj.item()
    elif hasattr(obj, "__dict__"):
        # ê°ì²´ì˜ ì†ì„±ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        return {
            "type": obj.__class__.__name__,
            "attributes": _make_json_serializable(obj.__dict__),
        }
    else:
        try:
            json.dumps(obj)  # ì§ë ¬í™” ê°€ëŠ¥í•œì§€ í…ŒìŠ¤íŠ¸
            return obj
        except (TypeError, ValueError):
            return str(obj)


def save_performance_report(performance_data: Dict[str, Any], module_name: str) -> None:
    """ì„±ëŠ¥ ë³´ê³ ì„œ ì €ì¥"""
    try:
        # ë³´ê³ ì„œ ë””ë ‰í† ë¦¬ ìƒì„±
        report_dir = Path("data/result/performance_reports")
        report_dir.mkdir(parents=True, exist_ok=True)

        # ë³´ê³ ì„œ íŒŒì¼ ì €ì¥
        report_file = report_dir / f"{module_name}_performance_report.json"

        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(performance_data, f, ensure_ascii=False, indent=2)

        logger.info(f"ì„±ëŠ¥ ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ: {report_file}")

    except Exception as e:
        logger.error(f"ì„±ëŠ¥ ë³´ê³ ì„œ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")


def get_memory_usage() -> Dict[str, float]:
    """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
    try:
        process = psutil.Process()
        memory_info = process.memory_info()

        return {
            "rss_mb": memory_info.rss / 1024 / 1024,  # MB
            "vms_mb": memory_info.vms / 1024 / 1024,  # MB
            "percent": process.memory_percent(),
        }
    except Exception:
        return {"rss_mb": 0, "vms_mb": 0, "percent": 0}


def cleanup_optimization_systems() -> None:
    """ìµœì í™” ì‹œìŠ¤í…œ ì •ë¦¬ (ì¤‘ë³µ ë°©ì§€)"""
    global process_pool_manager, hybrid_optimizer, memory_manager

    try:
        # ProcessPool ì •ë¦¬ (ì¤‘ë³µ ë°©ì§€)
        if (
            process_pool_manager
            and hasattr(process_pool_manager, "_shutdown")
            and not process_pool_manager._shutdown
        ):
            process_pool_manager.shutdown()
            logger.info("í”„ë¡œì„¸ìŠ¤ í’€ ì •ë¦¬ ì™„ë£Œ")
        elif process_pool_manager:
            logger.debug("í”„ë¡œì„¸ìŠ¤ í’€ ì´ë¯¸ ì¢…ë£Œë¨")

        # Memory Manager ì •ë¦¬
        if (
            memory_manager
            and hasattr(memory_manager, "_is_shutdown")
            and not getattr(memory_manager, "_is_shutdown", False)
        ):
            memory_manager.cleanup()
            logger.info("ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì •ë¦¬ ì™„ë£Œ")
        elif memory_manager:
            logger.debug("ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ë¯¸ ì •ë¦¬ë¨")

        # Hybrid Optimizer ì •ë¦¬
        if (
            hybrid_optimizer
            and hasattr(hybrid_optimizer, "_is_shutdown")
            and not getattr(hybrid_optimizer, "_is_shutdown", False)
        ):
            hybrid_optimizer.cleanup()
            logger.info("í•˜ì´ë¸Œë¦¬ë“œ ìµœì í™” ì‹œìŠ¤í…œ ì •ë¦¬ ì™„ë£Œ")
        elif hybrid_optimizer:
            logger.debug("í•˜ì´ë¸Œë¦¬ë“œ ìµœì í™” ì‹œìŠ¤í…œ ì´ë¯¸ ì •ë¦¬ë¨")

    except Exception as e:
        logger.warning(f"ìµœì í™” ì‹œìŠ¤í…œ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    finally:
        # ì „ì—­ ë³€ìˆ˜ ì´ˆê¸°í™”
        process_pool_manager = None
        hybrid_optimizer = None
        memory_manager = None


def run_data_analysis() -> bool:
    """
    ê¸°ë³¸ ë°ì´í„° ë¶„ì„ í•¨ìˆ˜ (í•˜ìœ„ í˜¸í™˜ì„±)
    """
    return run_optimized_data_analysis()


def run_fully_optimized_analysis():
    """ì™„ì „ ìµœì í™”ëœ ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸"""

    # ğŸš€ ì „ì—­ ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    from src.utils.memory_manager import get_memory_manager, MemoryConfig
    from src.utils.cuda_optimizers import CudaConfig
    from src.utils.unified_performance import get_profiler

    logger.info("ğŸ‰ ì™„ì „ ìµœì í™”ëœ ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹œì‘")

    # ì „ì—­ ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    memory_config = MemoryConfig(
        max_memory_usage=0.85,
        use_memory_pooling=True,
        pool_size=32,
        auto_cleanup_interval=60.0,
    )
    memory_manager = get_memory_manager(memory_config)

    cuda_config = CudaConfig(
        use_amp=True,
        batch_size=128,
        use_cudnn=True,
    )

    # í”„ë¡œíŒŒì¼ëŸ¬ ì´ˆê¸°í™”
    profiler = get_profiler()

    # ğŸ§  ì „ì—­ ë©”ëª¨ë¦¬ ê´€ë¦¬ ì»¨í…ìŠ¤íŠ¸
    with memory_manager.allocation_scope():
        # ğŸ“ˆ ì „ì²´ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        with profiler.profile("ì™„ì „_ìµœì í™”_ë¶„ì„"):
            # ê¸°ë³¸ ìµœì í™”ëœ ë¶„ì„ ì‹¤í–‰
            return run_optimized_data_analysis()


def create_optimized_analyzer(
    memory_manager, cuda_optimizer, process_pool_manager, hybrid_optimizer
):
    """ìµœì í™”ëœ ë¶„ì„ê¸° ìƒì„±"""
    from src.analysis.unified_analyzer import UnifiedAnalyzer

    config = load_config()
    analyzer = UnifiedAnalyzer(config)

    # ìµœì í™” ì‹œìŠ¤í…œ ì£¼ì…
    if hasattr(analyzer, "set_optimizers"):
        analyzer.set_optimizers(
            memory_manager=memory_manager,
            cuda_optimizer=cuda_optimizer,
            process_pool_manager=process_pool_manager,
            hybrid_optimizer=hybrid_optimizer,
        )

    return analyzer


def create_optimized_vectorizer(memory_manager, cuda_optimizer):
    """ìµœì í™”ëœ ë²¡í„°ë¼ì´ì € ìƒì„±"""
    from src.analysis.enhanced_pattern_vectorizer import EnhancedPatternVectorizer

    config = load_config()
    vectorizer = EnhancedPatternVectorizer(config)

    # ìµœì í™” ì‹œìŠ¤í…œ ì£¼ì…
    if hasattr(vectorizer, "set_optimizers"):
        vectorizer.set_optimizers(
            memory_manager=memory_manager, cuda_optimizer=cuda_optimizer
        )

    return vectorizer


def load_draw_history_optimized(memory_manager=None):
    """ìµœì í™”ëœ ë°ì´í„° ë¡œë“œ"""
    from src.utils.data_loader import load_draw_history

    if memory_manager:
        with memory_manager.allocation_scope():
            return load_draw_history()
    else:
        return load_draw_history()


def save_results_optimized(analysis_result, vectors, memory_manager=None):
    """ìµœì í™”ëœ ê²°ê³¼ ì €ì¥"""
    try:
        if memory_manager:
            with memory_manager.allocation_scope():
                # ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì €ì¥
                _save_with_memory_optimization(analysis_result, vectors)
        else:
            _save_standard(analysis_result, vectors)

        logger.info("âœ… ìµœì í™”ëœ ê²°ê³¼ ì €ì¥ ì™„ë£Œ")

    except Exception as e:
        logger.error(f"ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")


def _save_with_memory_optimization(analysis_result, vectors):
    """ë©”ëª¨ë¦¬ ìµœì í™”ëœ ì €ì¥"""
    import json
    import numpy as np
    from pathlib import Path

    # ë¶„ì„ ê²°ê³¼ ì €ì¥
    analysis_path = Path("data/result/analysis/lottery_data_analysis.json")
    analysis_path.parent.mkdir(parents=True, exist_ok=True)

    with open(analysis_path, "w", encoding="utf-8") as f:
        json.dump(analysis_result, f, ensure_ascii=False, indent=2)

    # ë²¡í„° ì €ì¥
    if vectors is not None:
        vector_path = Path("data/cache/feature_vectors_full.npy")
        vector_path.parent.mkdir(parents=True, exist_ok=True)
        np.save(vector_path, vectors)


def _save_standard(analysis_result, vectors):
    """í‘œì¤€ ì €ì¥"""
    _save_with_memory_optimization(analysis_result, vectors)


def cleanup_optimized_resources(*optimizers):
    """ìµœì í™” ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
    for optimizer in optimizers:
        if optimizer and hasattr(optimizer, "cleanup"):
            try:
                optimizer.cleanup()
            except Exception as e:
                logger.warning(f"ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    logger.info("âœ… ìµœì í™” ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")


def benchmark_optimization_performance():
    """ìµœì í™” ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    logger.info("ìµœì í™” ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")

    try:
        # ê¸°ë³¸ ë¶„ì„ ì‹¤í–‰
        start_time = time.time()
        result = run_optimized_data_analysis()
        duration = time.time() - start_time

        logger.info(f"ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ: {duration:.2f}ì´ˆ, ì„±ê³µ: {result}")
        return {"duration": duration, "success": result}

    except Exception as e:
        logger.error(f"ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return {"duration": 0, "success": False, "error": str(e)}


def track_completion_status() -> Dict[str, Any]:
    """ì™„ì„±ë„ ìƒíƒœ ì¶”ì """

    required_analyzers = [
        "pattern",
        "distribution",
        "roi",
        "pair",
        "cluster",
        "trend",
        "overlap",
        "structural",
        "statistical",
        "negative_sample",
        "unified",
    ]

    completion_status = {
        "total_required": len(required_analyzers),
        "completed": 0,
        "failed": 0,
        "completion_rate": 0.0,
        "status_by_analyzer": {},
    }

    # ConfigProxyë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜
    config = load_config()
    if hasattr(config, "_config"):
        config_dict = config._config
    elif hasattr(config, "to_dict"):
        config_dict = config.to_dict()
    elif isinstance(config, dict):
        config_dict = config
    else:
        config_dict = {}

    for analyzer_name in required_analyzers:
        try:
            from src.analysis.analyzer_factory import get_analyzer

            # ë¶„ì„ê¸° ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
            analyzer = get_analyzer(analyzer_name, config_dict)

            if analyzer is not None:
                completion_status["completed"] += 1
                completion_status["status_by_analyzer"][analyzer_name] = "âœ… ì„±ê³µ"
            else:
                completion_status["failed"] += 1
                completion_status["status_by_analyzer"][
                    analyzer_name
                ] = "âŒ ì´ˆê¸°í™” ì‹¤íŒ¨"

        except Exception as e:
            completion_status["failed"] += 1
            completion_status["status_by_analyzer"][
                analyzer_name
            ] = f"âŒ ì˜¤ë¥˜: {str(e)}"

    completion_status["completion_rate"] = (
        completion_status["completed"] / completion_status["total_required"]
    ) * 100

    # ê²°ê³¼ ë¡œê¹…
    logger.info("=" * 60)
    logger.info("ğŸ¯ ì‹œìŠ¤í…œ ì™„ì„±ë„ í˜„í™©")
    logger.info("=" * 60)
    logger.info(f"ğŸ“Š ì™„ì„±ë¥ : {completion_status['completion_rate']:.1f}%")
    logger.info(
        f"âœ… ì„±ê³µ: {completion_status['completed']}/{completion_status['total_required']}"
    )
    logger.info(
        f"âŒ ì‹¤íŒ¨: {completion_status['failed']}/{completion_status['total_required']}"
    )

    for analyzer, status in completion_status["status_by_analyzer"].items():
        logger.info(f"  {analyzer}: {status}")

    logger.info("=" * 60)

    return completion_status


def run_integration_test() -> bool:
    """ì „ì²´ ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸"""

    logger.info("ğŸ§ª í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")

    test_results = {
        "analyzer_initialization": False,
        "data_loading": False,
        "result_validation": False,
    }

    try:
        # 1. ë¶„ì„ê¸° ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
        logger.info("1. ë¶„ì„ê¸° ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸")
        completion_status = track_completion_status()
        test_results["analyzer_initialization"] = (
            completion_status["completion_rate"] == 100.0
        )

        # 2. ë°ì´í„° ë¡œë“œ í…ŒìŠ¤íŠ¸
        logger.info("2. ë°ì´í„° ë¡œë“œ í…ŒìŠ¤íŠ¸")
        historical_data = load_draw_history()
        test_results["data_loading"] = len(historical_data) > 0

        # 3. ê²°ê³¼ ê²€ì¦ í…ŒìŠ¤íŠ¸
        logger.info("3. ê²°ê³¼ ê²€ì¦ í…ŒìŠ¤íŠ¸")
        test_results["result_validation"] = True  # ê¸°ë³¸ì ìœ¼ë¡œ í†µê³¼

        # ì „ì²´ í…ŒìŠ¤íŠ¸ ê²°ê³¼
        passed_tests = sum(test_results.values())
        total_tests = len(test_results)
        success_rate = (passed_tests / total_tests) * 100

        logger.info(
            f"ğŸ¯ í†µí•© í…ŒìŠ¤íŠ¸ ê²°ê³¼: {passed_tests}/{total_tests} ({success_rate:.1f}%)"
        )

        for test_name, result in test_results.items():
            status = "âœ… í†µê³¼" if result else "âŒ ì‹¤íŒ¨"
            logger.info(f"  {test_name}: {status}")

        return success_rate == 100.0

    except Exception as e:
        logger.error(f"âŒ í†µí•© í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
        return False


def generate_comprehensive_report(analysis_results: Dict[str, Any]) -> Dict[str, Any]:
    """ì¢…í•© ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„± - 100% ì™„ì„±ì„ ìœ„í•œ ìƒì„¸ ë¶„ì„"""

    report = {
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "system_completion": {
            "target_analyzers": 11,
            "active_analyzers": len(analysis_results),
            "completion_rate": (len(analysis_results) / 11) * 100,
        },
        "analyzer_details": {},
        "performance_metrics": {},
        "data_quality": {},
        "recommendations": [],
    }

    # ê° ë¶„ì„ê¸°ë³„ ìƒì„¸ ì •ë³´
    for analyzer_name, result in analysis_results.items():
        if isinstance(result, dict):
            report["analyzer_details"][analyzer_name] = {
                "status": "success" if "error" not in result else "failed",
                "data_items": len(result),
                "result_type": str(type(result)),
                "has_error": "error" in result,
                "keys": list(result.keys())[:5],  # ì²« 5ê°œ í‚¤ë§Œ í‘œì‹œ
            }
        else:
            report["analyzer_details"][analyzer_name] = {
                "status": "failed",
                "data_items": 0,
                "result_type": str(type(result)),
                "has_error": True,
                "error_info": "Invalid result type",
            }

    # ì„±ëŠ¥ ë©”íŠ¸ë¦­
    success_count = sum(
        1
        for detail in report["analyzer_details"].values()
        if detail["status"] == "success"
    )
    report["performance_metrics"] = {
        "success_rate": (success_count / 11) * 100,
        "failed_count": 11 - success_count,
        "total_data_items": sum(
            detail["data_items"] for detail in report["analyzer_details"].values()
        ),
    }

    # ì¶”ì²œì‚¬í•­
    if report["system_completion"]["completion_rate"] < 100:
        missing_analyzers = []
        required_analyzers = [
            "pattern",
            "distribution",
            "roi",
            "pair",
            "cluster",
            "trend",
            "overlap",
            "structural",
            "statistical",
            "negative_sample",
            "unified",
        ]
        for analyzer in required_analyzers:
            if analyzer not in analysis_results:
                missing_analyzers.append(analyzer)

        if missing_analyzers:
            report["recommendations"].append(
                f"ëˆ„ë½ëœ ë¶„ì„ê¸° ì¶”ê°€ í•„ìš”: {missing_analyzers}"
            )

    # í•¨ìˆ˜ ê°ì²´ ë°˜í™˜ ë¬¸ì œ ì²´í¬
    function_issues = []
    for analyzer_name, detail in report["analyzer_details"].items():
        if "function" in detail["result_type"].lower():
            function_issues.append(analyzer_name)

    if function_issues:
        report["recommendations"].append(
            f"í•¨ìˆ˜ ê°ì²´ ë°˜í™˜ ë¬¸ì œ ìˆ˜ì • í•„ìš”: {function_issues}"
        )

    # ë³´ê³ ì„œ ì €ì¥
    from pathlib import Path

    report_file = Path(
        "data/result/performance_reports/comprehensive_system_report.json"
    )
    report_file.parent.mkdir(parents=True, exist_ok=True)

    with open(report_file, "w", encoding="utf-8") as f:
        json.dump(report, f, indent=2, ensure_ascii=False)

    logger.info(f"ğŸ“Š ì¢…í•© ë³´ê³ ì„œ ì €ì¥: {report_file}")
    return report


if __name__ == "__main__":
    success = run_optimized_data_analysis()
    sys.exit(0 if success else 1)
