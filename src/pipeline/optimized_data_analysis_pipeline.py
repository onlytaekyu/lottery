#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
ìµœì í™”ëœ ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸

ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ë‹¤ìŒ ê¸°ëŠ¥ë“¤ì„ í¬í•¨í•©ë‹ˆë‹¤:
- ì¤‘ë³µ í•¨ìˆ˜ í†µí•© ë° ì œê±°
- ì„¸ë¶„í™”ëœ ìºì‹± ì‹œìŠ¤í…œ
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²­í¬ ì²˜ë¦¬
- ë³‘ë ¬ ì²˜ë¦¬ ì§€ì› (ProcessPool í†µí•©)
- í•˜ì´ë¸Œë¦¬ë“œ ìµœì í™” ì‹œìŠ¤í…œ
- ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ìµœì í™”
"""

import json
import logging
import os
import sys
import time
import random
import traceback
from pathlib import Path
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple, Union
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
import hashlib

import numpy as np
import psutil

from src.utils.error_handler_refactored import (
    get_logger,
    log_exception_with_trace,
    StrictErrorHandler,
    strict_error_handler,
    validate_and_fail_fast,
)
from src.utils.state_vector_cache import get_cache
from src.utils.data_loader import load_draw_history, LotteryJSONEncoder
from src.shared.types import LotteryNumber
from src.utils.unified_config import load_config
from src.utils.unified_performance import get_profiler

# ìµœì í™” ì‹œìŠ¤í…œ import
from src.utils.process_pool_manager import get_process_pool_manager
from src.utils.hybrid_optimizer import get_hybrid_optimizer, optimize
from src.utils.memory_manager import get_memory_manager

from src.analysis.pattern_analyzer import PatternAnalyzer
from src.analysis.pattern_vectorizer import PatternVectorizer
from src.utils.unified_report import safe_convert, save_physical_performance_report
from src.analysis.pair_analyzer import PairAnalyzer
from src.analysis.distribution_analyzer import DistributionAnalyzer
from src.analysis.roi_analyzer import ROIAnalyzer
from src.analysis.cluster_analyzer import ClusterAnalyzer
from src.analysis.trend_analyzer import TrendAnalyzer
from src.analysis.overlap_analyzer import OverlapAnalyzer
from src.analysis.structural_analyzer import StructuralAnalyzer
from src.analysis.statistical_analyzer import StatisticalAnalyzer

# from src.utils.feature_vector_validator import (
#     validate_feature_vector_with_config,
#     check_vector_dimensions,
# )

# ìµœì í™”ëœ í•¨ìˆ˜ë“¤ import
from src.shared.graph_utils import (
    calculate_pair_frequency,
    calculate_segment_entropy,
    calculate_number_gaps,
    calculate_cluster_distribution,
    clear_cache,
    get_cache_stats,
)

# ë¡œê±° ì„¤ì •
logger = get_logger(__name__)

# ì„±ëŠ¥ ìµœì í™” ì„¤ì •
CHUNK_PROCESSING_CONFIG = {
    "historical_data": 100,  # 100íšŒì°¨ì”© ì²˜ë¦¬
    "vector_generation": 50,  # 50ê°œì”© ë²¡í„° ìƒì„±
    "cache_flush_interval": 200,  # 200íšŒì°¨ë§ˆë‹¤ ìºì‹œ í”ŒëŸ¬ì‹œ
    "parallel_workers": min(4, psutil.cpu_count()),  # CPU ì½”ì–´ ìˆ˜ì— ë”°ë¥¸ ì›Œì»¤ ìˆ˜
    "memory_threshold": 0.8,  # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 80% ì„ê³„ì 
}

CACHE_STRATEGY = {
    "data_load": "data_size_hash",
    "pattern_analysis": "data_hash + config_hash",
    "vectorization": "analysis_hash + vector_config_hash",
    "additional_analysis": "pattern_hash + addon_config_hash",
}

# ì „ì—­ ì—„ê²©í•œ ì—ëŸ¬ í•¸ë“¤ëŸ¬
strict_handler = StrictErrorHandler()

# ì „ì—­ ìµœì í™” ì‹œìŠ¤í…œë“¤
process_pool_manager = None
hybrid_optimizer = None
memory_manager = None


def initialize_optimization_systems(config: Dict[str, Any]):
    """ìµœì í™” ì‹œìŠ¤í…œë“¤ ì´ˆê¸°í™”"""
    global process_pool_manager, hybrid_optimizer, memory_manager

    try:
        # ìµœì í™” ì„¤ì • ë¡œë“œ (ë³„ë„ íŒŒì¼ì—ì„œ)
        optimization_config = config.get("optimization", {})

        # optimization.yaml íŒŒì¼ì—ì„œ ì¶”ê°€ ì„¤ì • ë¡œë“œ
        try:
            from src.utils.unified_config import load_config as load_optimization_config

            optimization_file_config = load_optimization_config("optimization")
            if isinstance(optimization_file_config, dict):
                # íŒŒì¼ ì„¤ì •ì„ ê¸°ë³¸ ì„¤ì •ê³¼ ë³‘í•©
                optimization_config.update(optimization_file_config)
        except Exception as e:
            logger.debug(f"optimization.yaml ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ ì„¤ì • ì‚¬ìš©: {e}")

            # ProcessPool ê´€ë¦¬ì ì´ˆê¸°í™”
        try:
            process_pool_config = optimization_config.get("process_pool", {})
            # ê¸°ë³¸ê°’ ì„¤ì •
            if not isinstance(process_pool_config, dict):
                process_pool_config = {}

            # ì•ˆì „í•œ ê¸°ë³¸ê°’ ì„¤ì •
            safe_config = {
                "max_workers": process_pool_config.get(
                    "max_workers", min(4, psutil.cpu_count())
                ),
                "chunk_size": process_pool_config.get("chunk_size", 100),
                "timeout": process_pool_config.get("timeout", 300),
                "memory_limit_mb": process_pool_config.get("memory_limit_mb", 1024),
                "enable_monitoring": process_pool_config.get("enable_monitoring", True),
                "auto_restart": process_pool_config.get("auto_restart", True),
                "restart_threshold": process_pool_config.get("restart_threshold", 100),
            }

            process_pool_manager = get_process_pool_manager(safe_config)
            logger.info("ProcessPool ê´€ë¦¬ì ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"ProcessPool ê´€ë¦¬ì ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            process_pool_manager = None

        # ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™”
        try:
            from src.utils.memory_manager import MemoryConfig

            memory_config_dict = optimization_config.get("memory", {})
            memory_config = MemoryConfig(
                max_memory_usage=memory_config_dict.get("max_memory_usage", 0.85),
                cache_size=memory_config_dict.get("cache_size", 256 * 1024 * 1024),
                use_memory_pooling=memory_config_dict.get("use_memory_pooling", True),
                auto_cleanup_interval=memory_config_dict.get(
                    "auto_cleanup_interval", 60.0
                ),
            )
            memory_manager = get_memory_manager(memory_config)
            logger.info("ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            memory_manager = None

        # í•˜ì´ë¸Œë¦¬ë“œ ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        try:
            hybrid_config = optimization_config.get(
                "hybrid",
                {
                    "auto_optimization": True,
                    "memory_threshold": 0.8,
                    "cpu_threshold": 75.0,
                },
            )
            hybrid_optimizer = get_hybrid_optimizer(hybrid_config)
            logger.info("í•˜ì´ë¸Œë¦¬ë“œ ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"í•˜ì´ë¸Œë¦¬ë“œ ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            hybrid_optimizer = None

        # ìµœì í™” ì‹œìŠ¤í…œ ìƒíƒœ ë¡œê¹…
        initialized_systems = []
        if process_pool_manager is not None:
            initialized_systems.append("ProcessPool")
        if memory_manager is not None:
            initialized_systems.append("MemoryManager")
        if hybrid_optimizer is not None:
            initialized_systems.append("HybridOptimizer")

        logger.info(f"ì´ˆê¸°í™”ëœ ìµœì í™” ì‹œìŠ¤í…œ: {', '.join(initialized_systems)}")

    except Exception as e:
        logger.error(f"ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        # ìµœì í™” ì‹œìŠ¤í…œ ì—†ì´ë„ ë™ì‘í•  ìˆ˜ ìˆë„ë¡ Noneìœ¼ë¡œ ì„¤ì •
        process_pool_manager = None
        hybrid_optimizer = None
        memory_manager = None


@optimize(
    task_info={
        "function_type": "analysis",
        "parallelizable": True,
        "gpu_compatible": False,
    }
)
def optimized_pattern_analysis(
    data_chunk: List, analyzer: PatternAnalyzer
) -> Dict[str, Any]:
    """ìµœì í™”ëœ íŒ¨í„´ ë¶„ì„"""
    try:
        # ë©”ëª¨ë¦¬ ê´€ë¦¬ ìŠ¤ì½”í”„ ë‚´ì—ì„œ ì‹¤í–‰
        if memory_manager:
            with memory_manager.allocation_scope():
                return analyzer.analyze(data_chunk)
        else:
            return analyzer.analyze(data_chunk)
    except Exception as e:
        logger.error(f"íŒ¨í„´ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return {}


@optimize(
    task_info={
        "function_type": "vectorize",
        "parallelizable": True,
        "gpu_compatible": False,
    }
)
def optimized_vectorization(
    patterns: List, vectorizer: PatternVectorizer
) -> Tuple[np.ndarray, List[str]]:
    """ìµœì í™”ëœ ë²¡í„°í™”"""
    try:
        if memory_manager:
            with memory_manager.allocation_scope():
                return vectorizer.vectorize(patterns)
        else:
            return vectorizer.vectorize(patterns)
    except Exception as e:
        logger.error(f"ë²¡í„°í™” ì‹¤íŒ¨: {e}")
        return np.array([]), []


def generate_cache_key(data_info: str, operation: str, **kwargs) -> str:
    """ìµœì í™”ëœ ìºì‹œ í‚¤ ìƒì„±"""
    try:
        # ë°ì´í„° ì •ë³´ì™€ ë§¤ê°œë³€ìˆ˜ë¥¼ ê²°í•©í•˜ì—¬ í•´ì‹œ ìƒì„±
        combined_info = f"{data_info}_{operation}_{str(sorted(kwargs.items()))}"
        return hashlib.md5(combined_info.encode()).hexdigest()[:16]
    except Exception:
        # í•´ì‹œ ìƒì„± ì‹¤íŒ¨ì‹œ ê¸°ë³¸ í‚¤
        return f"{operation}_{hash(str(kwargs)) % 100000}"


def check_memory_usage() -> bool:
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸"""
    memory_info = psutil.virtual_memory()
    return memory_info.percent < (CHUNK_PROCESSING_CONFIG["memory_threshold"] * 100)


def process_data_chunks_optimized(data: List, chunk_size: int, process_func, **kwargs):
    """ìµœì í™”ëœ ì²­í¬ ë‹¨ìœ„ ë°ì´í„° ì²˜ë¦¬"""
    global process_pool_manager

    # ProcessPoolì´ ì‚¬ìš© ê°€ëŠ¥í•˜ê³  ë°ì´í„°ê°€ ì¶©ë¶„íˆ í° ê²½ìš° ë³‘ë ¬ ì²˜ë¦¬
    if process_pool_manager and len(data) > 200:
        try:
            chunks = process_pool_manager.chunk_and_split(data, chunk_size)
            results = process_pool_manager.parallel_analyze(
                chunks, process_func, **kwargs
            )
            return process_pool_manager.merge_results(results)
        except Exception as e:
            logger.warning(f"ë³‘ë ¬ ì²˜ë¦¬ ì‹¤íŒ¨, ìˆœì°¨ ì²˜ë¦¬ë¡œ ì „í™˜: {e}")

    # ìˆœì°¨ ì²˜ë¦¬ (ê¸°ì¡´ ë°©ì‹)
    results = []
    total_chunks = len(data) // chunk_size + (1 if len(data) % chunk_size else 0)

    for i in range(0, len(data), chunk_size):
        if not check_memory_usage():
            logger.warning(
                "ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì„ê³„ì ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."
            )
            import gc

            gc.collect()

        chunk = data[i : min(i + chunk_size, len(data))]
        chunk_result = process_func(chunk, **kwargs)
        results.append(chunk_result)

        if (i // chunk_size + 1) % 10 == 0:
            logger.info(f"ì²­í¬ ì²˜ë¦¬ ì§„í–‰ë¥ : {i // chunk_size + 1}/{total_chunks}")

    return results


def safe_analysis_step(step_name: str, func, *args, **kwargs):
    """íŠ¸ëœì­ì…˜ ë˜í¼ - ì•ˆì „í•œ ë¶„ì„ ë‹¨ê³„ ì‹¤í–‰"""
    try:
        logger.info(f"{step_name} ì‹œì‘")
        start_time = time.time()

        result = func(*args, **kwargs)

        duration = time.time() - start_time
        logger.info(f"{step_name} ì™„ë£Œ ({duration:.2f}ì´ˆ)")
        return result

    except Exception as e:
        logger.error(f"{step_name} ì‹¤íŒ¨: {str(e)}")
        # ë¡¤ë°± ë¡œì§ (í•„ìš”ì‹œ ì¶”ê°€)
        raise RuntimeError(f"{step_name} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


@strict_error_handler("ìµœì í™”ëœ ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸", exit_on_error=True)
def run_optimized_data_analysis() -> bool:
    """
    ìµœì í™”ëœ ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

    Returns:
        bool: ì‹¤í–‰ ì„±ê³µ ì—¬ë¶€
    """
    start_time = time.time()

    try:
        logger.info("ğŸš€ ìµœì í™”ëœ ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹œì‘")

        # ì„¤ì • ë¡œë“œ
        config = load_config()

        # ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        initialize_optimization_systems(config)

        # 1ë‹¨ê³„: ë°ì´í„° ë¡œë“œ ë° ê³ ê¸‰ ê²€ì¦
        logger.info("ğŸ“Š 1ë‹¨ê³„: ë°ì´í„° ë¡œë“œ ë° ê³ ê¸‰ ê²€ì¦")
        historical_data = load_draw_history()

        if not historical_data:
            logger.error("ë¡œë˜ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False

        logger.info(f"ë¡œë˜ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(historical_data)}íšŒì°¨")

        # ğŸ” ê³ ê¸‰ ë°ì´í„° ê²€ì¦ ì‹œìŠ¤í…œ ì ìš©
        from src.pipeline.data_validation import DataValidator

        validator = DataValidator(config)
        validation_result = validator.validate_lottery_data(historical_data)

        if not validation_result.is_valid:
            logger.error(f"ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {len(validation_result.errors)}ê°œ ì˜¤ë¥˜")
            for error in validation_result.errors[:5]:  # ì²˜ìŒ 5ê°œ ì˜¤ë¥˜ë§Œ í‘œì‹œ
                logger.error(f"  - {error}")
            return False

        logger.info(
            f"âœ… ë°ì´í„° ê²€ì¦ ì™„ë£Œ (í’ˆì§ˆ ì ìˆ˜: {validation_result.quality_score})"
        )

        # ê²½ê³  ì‚¬í•­ ë¡œê¹…
        if validation_result.warnings:
            logger.warning(f"ë°ì´í„° í’ˆì§ˆ ê²½ê³  {len(validation_result.warnings)}ê°œ:")
            for warning in validation_result.warnings[:3]:
                logger.warning(f"  - {warning}")

        # ì´ìƒì¹˜ ì •ë³´ ë¡œê¹…
        if validation_result.anomalies:
            logger.info(f"ê°ì§€ëœ ì´ìƒì¹˜: {len(validation_result.anomalies)}ê°œ")

        # í’ˆì§ˆ ë³´ê³ ì„œ ìƒì„± ë° ì €ì¥
        quality_report = validator.generate_quality_report(historical_data)
        validator.save_quality_report(quality_report)

        # 2ë‹¨ê³„: ë¶„ì„ê¸° ì´ˆê¸°í™”
        logger.info("ğŸ”§ 2ë‹¨ê³„: ë¶„ì„ê¸° ì´ˆê¸°í™”")

        def init_analyzer(analyzer_type: str):
            """ë¶„ì„ê¸° ì´ˆê¸°í™” í—¬í¼ í•¨ìˆ˜ - íŒ©í† ë¦¬ íŒ¨í„´ ì‚¬ìš©"""
            from src.analysis.analyzer_factory import get_analyzer

            try:
                # ConfigProxyë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜
                if hasattr(config, "_config"):
                    config_dict = config._config
                elif hasattr(config, "to_dict"):
                    config_dict = config.to_dict()
                elif isinstance(config, dict):
                    config_dict = config
                else:
                    # ConfigProxy ê°ì²´ì˜ ì†ì„±ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
                    config_dict = {}
                    if hasattr(config, "__dict__"):
                        for key, value in config.__dict__.items():
                            if not key.startswith("_"):
                                config_dict[key] = value

                    # ê¸°ë³¸ ì„¤ì • ì¶”ê°€
                    if not config_dict:
                        config_dict = {
                            "analysis": {},
                            "paths": {
                                "cache_dir": "data/cache",
                                "result_dir": "data/result",
                            },
                            "vectorizer": {"use_cache": True, "normalize_output": True},
                            "filtering": {
                                "remove_low_variance_features": True,
                                "variance_threshold": 0.01,
                            },
                            "caching": {
                                "enable_feature_cache": True,
                                "max_cache_size": 10000,
                            },
                        }

                        # íŒ©í† ë¦¬ë¥¼ í†µí•´ ë¶„ì„ê¸° ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
                analyzer = get_analyzer(analyzer_type, config_dict)
                if analyzer is None:
                    logger.warning(f"{analyzer_type} ë¶„ì„ê¸° íŒ©í† ë¦¬ì—ì„œ None ë°˜í™˜")
                return analyzer

            except Exception as e:
                logger.error(f"{analyzer_type} ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                # ì¬ì‹œë„ ì—†ì´ None ë°˜í™˜í•˜ì—¬ ì¤‘ë³µ ì´ˆê¸°í™” ë°©ì§€
                return None

        # ë¶„ì„ê¸°ë“¤ ì´ˆê¸°í™” (ê¸°ì¡´ 4ê°œ + ìƒˆë¡œìš´ 5ê°œ)
        pattern_analyzer = init_analyzer("pattern")
        distribution_analyzer = init_analyzer("distribution")
        roi_analyzer = init_analyzer("roi")
        pair_analyzer = init_analyzer("pair")
        vectorizer = init_analyzer("vectorizer")

        # ğŸ”¥ ìƒˆë¡œ ì¶”ê°€ëœ ë¯¸ì‚¬ìš© ë¶„ì„ê¸°ë“¤ í™œì„±í™”
        cluster_analyzer = init_analyzer("cluster")
        trend_analyzer = init_analyzer("trend")
        overlap_analyzer = init_analyzer("overlap")
        structural_analyzer = init_analyzer("structural")
        statistical_analyzer = init_analyzer("statistical")

        # ì´ˆê¸°í™” ì‹¤íŒ¨ ì²´í¬
        analyzers = {
            "pattern": pattern_analyzer,
            "distribution": distribution_analyzer,
            "roi": roi_analyzer,
            "pair": pair_analyzer,
            "vectorizer": vectorizer,
            # ğŸ”¥ ìƒˆë¡œ ì¶”ê°€ëœ ë¶„ì„ê¸°ë“¤
            "cluster": cluster_analyzer,
            "trend": trend_analyzer,
            "overlap": overlap_analyzer,
            "structural": structural_analyzer,
            "statistical": statistical_analyzer,
        }

        failed_analyzers = [
            name for name, analyzer in analyzers.items() if analyzer is None
        ]
        if failed_analyzers:
            logger.warning(f"ë‹¤ìŒ ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {failed_analyzers}")
            # ì‹¤íŒ¨í•œ ë¶„ì„ê¸° ì œê±°
            analyzers = {
                name: analyzer
                for name, analyzer in analyzers.items()
                if analyzer is not None
            }

        logger.info("ëª¨ë“  ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")

        # 3ë‹¨ê³„: ë³‘ë ¬ ë¶„ì„ ì‹¤í–‰
        logger.info("âš¡ 3ë‹¨ê³„: ë³‘ë ¬ ë¶„ì„ ì‹¤í–‰")
        analysis_results = {}

        # ë©”ëª¨ë¦¬ ê´€ë¦¬ ìŠ¤ì½”í”„ ë‚´ì—ì„œ ì‹¤í–‰
        if memory_manager:
            with memory_manager.allocation_scope():
                analysis_results = run_parallel_analysis(
                    historical_data, analyzers, config
                )
        else:
            analysis_results = run_parallel_analysis(historical_data, analyzers, config)

        if not analysis_results:
            logger.error("ë¶„ì„ ì‹¤í–‰ ì‹¤íŒ¨")
            return False

        logger.info(f"ë¶„ì„ ì™„ë£Œ: {len(analysis_results)}ê°œ ê²°ê³¼")

        # 4ë‹¨ê³„: ê³ ê¸‰ íŠ¹ì„± ì¶”ì¶œ ë° ë²¡í„° ìƒì„±
        logger.info("ğŸ”¢ 4ë‹¨ê³„: ê³ ê¸‰ íŠ¹ì„± ì¶”ì¶œ ë° ë²¡í„° ìƒì„±")

        try:
            # í†µí•© ë¶„ì„ ê²°ê³¼ ìƒì„±
            unified_analysis = merge_analysis_results(analysis_results)

            # í–¥ìƒëœ ë²¡í„°í™” ì‹œìŠ¤í…œë§Œ ì‚¬ìš© (FeatureExtractor ë¹„í™œì„±í™”)
            logger.info("í–¥ìƒëœ ë²¡í„°í™” ì‹œìŠ¤í…œë§Œ ì‚¬ìš©í•˜ì—¬ íŠ¹ì„± ì¶”ì¶œ")

            # ê¸°ë³¸ê°’ ì„¤ì •
            optimized_features = np.array([])
            optimized_names = []
            extraction_result = type(
                "MockResult",
                (),
                {
                    "quality_metrics": {"entropy": 0.0, "diversity": 0.0},
                    "feature_names": [],
                    "feature_matrix": np.array([]),
                    "feature_groups": {},
                },
            )()

            # ğŸš€ 2ë‹¨ê³„: ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ìƒ˜í”Œ ìƒì„± ì‹œìŠ¤í…œ (800ë°”ì´íŠ¸ â†’ 672KB+)
            logger.info("ğŸš€ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ìƒ˜í”Œ ìƒì„± ì‹œìŠ¤í…œ ì‹œì‘")

            # ë²¡í„°í™” ì‹¤í–‰ (í–¥ìƒëœ ë²¡í„°í™” ì‹œìŠ¤í…œ ì‚¬ìš©)
            feature_vector = None
            feature_names = []
            training_samples = None

            # í–¥ìƒëœ ë²¡í„°í™” ì‹œìŠ¤í…œ ì‚¬ìš© (EnhancedPatternVectorizer)
            try:
                from src.analysis.enhanced_pattern_vectorizer import (
                    EnhancedPatternVectorizer,
                )

                enhanced_vectorizer = EnhancedPatternVectorizer(config)

                # ğŸš€ ëŒ€í­ í™•ì¥ëœ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ìƒ˜í”Œ ìƒì„± (672KB+ ëª©í‘œ)
                historical_data_dict = []
                for draw in historical_data:
                    historical_data_dict.append(
                        {
                            "numbers": draw.numbers,
                            "draw_no": draw.draw_no,
                            "date": getattr(draw, "date", None),
                        }
                    )

                # ğŸ”¥ ë‹¤ì¤‘ ìœˆë„ìš° í¬ê¸°ë¡œ ëŒ€ëŸ‰ ìƒ˜í”Œ ìƒì„±
                all_training_samples = []
                window_sizes = [20, 30, 40, 50, 60, 70, 80]  # 7ê°€ì§€ ìœˆë„ìš° í¬ê¸°

                for window_size in window_sizes:
                    logger.info(f"ìœˆë„ìš° í¬ê¸° {window_size}ë¡œ ìƒ˜í”Œ ìƒì„± ì¤‘...")
                    samples = enhanced_vectorizer.generate_training_samples(
                        historical_data_dict, window_size=window_size
                    )

                    if samples is not None and len(samples) > 0:
                        all_training_samples.append(samples)
                        logger.info(
                            f"âœ… ìœˆë„ìš° {window_size}: {samples.shape} ìƒ˜í”Œ ìƒì„±"
                        )
                    else:
                        logger.warning(f"âŒ ìœˆë„ìš° {window_size}: ìƒ˜í”Œ ìƒì„± ì‹¤íŒ¨")

                # ëª¨ë“  ìƒ˜í”Œ ê²°í•©
                if all_training_samples:
                    training_samples = np.vstack(all_training_samples)
                    logger.info(f"ğŸ‰ ì „ì²´ ê²°í•© ìƒ˜í”Œ: {training_samples.shape}")

                    # íŒŒì¼ í¬ê¸° í™•ì¸
                    total_size = training_samples.nbytes
                    logger.info(
                        f"ğŸ“Š ì „ì²´ ìƒ˜í”Œ í¬ê¸°: {total_size:,} bytes ({total_size/1024:.1f} KB)"
                    )

                    # ëª©í‘œ ë‹¬ì„± ì—¬ë¶€
                    if total_size >= 672000:  # 672KB
                        logger.info("ğŸ‰ ëª©í‘œ íŒŒì¼ í¬ê¸° ë‹¬ì„±! (672KB+)")
                    else:
                        logger.warning(f"âš ï¸ ëª©í‘œ ë¯¸ë‹¬ì„±: {total_size} < 672000 bytes")

                        # ì¶”ê°€ ìƒ˜í”Œ ìƒì„± (ë°ì´í„° ì¦ê°•)
                        logger.info("ğŸ”¥ ë°ì´í„° ì¦ê°•ìœ¼ë¡œ ì¶”ê°€ ìƒ˜í”Œ ìƒì„±...")
                        augmented_samples = []

                        # ë…¸ì´ì¦ˆ ì¶”ê°€ ë²„ì „
                        for i in range(3):  # 3ë°° ì¦ê°•
                            noise_samples = training_samples + np.random.normal(
                                0, 0.01, training_samples.shape
                            ).astype(np.float32)
                            augmented_samples.append(noise_samples)

                        # ìµœì¢… ê²°í•©
                        if augmented_samples:
                            training_samples = np.vstack(
                                [training_samples] + augmented_samples
                            )
                            final_size = training_samples.nbytes
                            logger.info(
                                f"ğŸš€ ì¦ê°• í›„ ìµœì¢… í¬ê¸°: {final_size:,} bytes ({final_size/1024:.1f} KB)"
                            )

                    # í›ˆë ¨ ìƒ˜í”Œ ì €ì¥
                    samples_path = enhanced_vectorizer.save_training_samples(
                        training_samples, "feature_vector_full.npy"
                    )

                    # ëŒ€í‘œ ë²¡í„° ì„ íƒ (ë§ˆì§€ë§‰ ìƒ˜í”Œ ì‚¬ìš©)
                    feature_vector = (
                        training_samples[-1] if len(training_samples) > 0 else None
                    )
                    feature_names = enhanced_vectorizer.get_feature_names()

                    logger.info(
                        f"ğŸ¯ ìµœì¢… ëª©í‘œ ë‹¬ì„±: {'âœ…' if training_samples.nbytes >= 672000 else 'âŒ'} (ëª©í‘œ: 672KB+)"
                    )
                else:
                    logger.warning("ëª¨ë“  ìœˆë„ìš°ì—ì„œ ìƒ˜í”Œ ìƒì„± ì‹¤íŒ¨ - ë‹¨ì¼ ë²¡í„° ìƒì„±")
                    training_samples = None  # ëª…ì‹œì ìœ¼ë¡œ None ì„¤ì •
                    # í´ë°±: ë‹¨ì¼ ë²¡í„° ìƒì„±
                    feature_vector = (
                        enhanced_vectorizer.vectorize_full_analysis_enhanced(
                            unified_analysis
                        )
                    )
                    feature_names = enhanced_vectorizer.get_feature_names()

                if feature_vector is not None and len(feature_vector) > 0:
                    logger.info(f"í–¥ìƒëœ ë²¡í„°í™” ì‹œìŠ¤í…œ: {len(feature_vector)}ì°¨ì›")
                else:
                    logger.warning("í–¥ìƒëœ ë²¡í„°í™” ì‹œìŠ¤í…œ ì‹¤íŒ¨ - ê¸°ì¡´ ì‹œìŠ¤í…œ ì‚¬ìš©")
                    # í´ë°±: ê¸°ì¡´ ë²¡í„°í™” ì‹œìŠ¤í…œ
                    if vectorizer is not None and hasattr(
                        vectorizer, "vectorize_full_analysis"
                    ):
                        feature_vector = vectorizer.vectorize_full_analysis(
                            unified_analysis
                        )
                        feature_names = (
                            vectorizer.get_feature_names()
                            if hasattr(vectorizer, "get_feature_names")
                            else []
                        )
                        if feature_vector is not None and len(feature_vector) > 0:
                            logger.info(
                                f"ê¸°ì¡´ ë²¡í„°í™” ì‹œìŠ¤í…œ: {len(feature_vector)}ì°¨ì›"
                            )
                        else:
                            feature_vector = None
                            feature_names = []
                    else:
                        feature_vector = None
                        feature_names = []

            except Exception as e:
                logger.warning(f"í–¥ìƒëœ ë²¡í„°í™” ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}")
                # í´ë°±: ê¸°ì¡´ ë²¡í„°í™” ì‹œìŠ¤í…œ
                if vectorizer is not None and hasattr(
                    vectorizer, "vectorize_full_analysis"
                ):
                    try:
                        feature_vector = vectorizer.vectorize_full_analysis(
                            unified_analysis
                        )
                        feature_names = (
                            vectorizer.get_feature_names()
                            if hasattr(vectorizer, "get_feature_names")
                            else []
                        )
                        if feature_vector is not None and len(feature_vector) > 0:
                            logger.info(
                                f"ê¸°ì¡´ ë²¡í„°í™” ì‹œìŠ¤í…œ: {len(feature_vector)}ì°¨ì›"
                            )
                        else:
                            feature_vector = None
                            feature_names = []
                    except Exception as fallback_e:
                        logger.error(f"ê¸°ì¡´ ë²¡í„°í™” ì‹œìŠ¤í…œë„ ì‹¤íŒ¨: {fallback_e}")
                        feature_vector = None
                        feature_names = []
                else:
                    feature_vector = None
                    feature_names = []

            # ë‘ ë²¡í„° ì‹œìŠ¤í…œ ê²°í•©
            if feature_vector is not None and len(feature_vector) > 0:
                if optimized_features.size > 0:
                    # ì°¨ì›ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ê²°í•©
                    combined_vector = np.concatenate(
                        [feature_vector.flatten(), optimized_features.flatten()]
                    )
                    combined_names = feature_names + optimized_names
                    logger.info(f"ë²¡í„° ì‹œìŠ¤í…œ ê²°í•© ì™„ë£Œ: {len(combined_vector)}ì°¨ì›")
                else:
                    combined_vector = feature_vector
                    combined_names = feature_names
            else:
                # ê¸°ì¡´ ë²¡í„°í™” ì‹¤íŒ¨ì‹œ ìƒˆë¡œìš´ íŠ¹ì„± ì¶”ì¶œ ê²°ê³¼ ì‚¬ìš©
                combined_vector = (
                    optimized_features.flatten()
                    if optimized_features.size > 0
                    else np.array([])
                )
                combined_names = optimized_names

            if len(combined_vector) == 0:
                logger.error("íŠ¹ì„± ë²¡í„° ìƒì„± ì‹¤íŒ¨")
                return False

            logger.info(f"ìµœì¢… íŠ¹ì„± ë²¡í„° ìƒì„± ì™„ë£Œ: {len(combined_vector)}ì°¨ì›")

            # ë²¡í„° í’ˆì§ˆ ê²€ì¦
            if not validate_feature_vector(combined_vector, combined_names, config):
                logger.error("íŠ¹ì„± ë²¡í„° í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨")
                return False

        except Exception as e:
            logger.error(f"íŠ¹ì„± ë²¡í„° ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            log_exception_with_trace(
                "optimized_data_analysis_pipeline", e, "íŠ¹ì„± ë²¡í„° ìƒì„± ì¤‘ ì˜¤ë¥˜"
            )
            return False

        # 5ë‹¨ê³„: ê²°ê³¼ ì €ì¥
        logger.info("ğŸ’¾ 5ë‹¨ê³„: ê²°ê³¼ ì €ì¥")

        try:
            # ë¶„ì„ ê²°ê³¼ ì €ì¥
            save_analysis_results(unified_analysis, config)

            # íŠ¹ì„± ë²¡í„° ì €ì¥
            if vectorizer is not None:
                success = vectorizer.save_vector_to_file(
                    combined_vector, combined_names
                )
                if success:
                    vector_path = "data/cache/feature_vector_full.npy"
                    names_path = "data/cache/feature_vector_full.names.json"
                else:
                    logger.error("ë²¡í„° ì €ì¥ ì‹¤íŒ¨")
            else:
                # vectorizerê°€ Noneì¸ ê²½ìš° ì§ì ‘ ì €ì¥
                from pathlib import Path
                import json

                cache_dir = Path("data/cache")
                cache_dir.mkdir(parents=True, exist_ok=True)

                vector_path = cache_dir / "feature_vector_full.npy"
                names_path = cache_dir / "feature_vector_full.names.json"

                np.save(vector_path, combined_vector)
                with open(names_path, "w", encoding="utf-8") as f:
                    json.dump(combined_names, f, ensure_ascii=False, indent=2)

                vector_path = str(vector_path)
                names_path = str(names_path)

            logger.info(f"ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
            logger.info(f"  - ë²¡í„° íŒŒì¼: {vector_path}")
            logger.info(f"  - ì´ë¦„ íŒŒì¼: {names_path}")

        except Exception as e:
            logger.error(f"ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

        # 6ë‹¨ê³„: ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±
        logger.info("ğŸ“ˆ 6ë‹¨ê³„: ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±")

        try:
            execution_time = time.time() - start_time

            # ì„±ëŠ¥ ë³´ê³ ì„œ ë°ì´í„° ìˆ˜ì§‘
            performance_data = {
                "execution_time": execution_time,
                "data_size": len(historical_data),
                "vector_dimensions": len(combined_vector),
                "analysis_results_count": len(analysis_results),
                "memory_usage": get_memory_usage(),
                "data_quality_score": validation_result.quality_score,
                "feature_extraction_quality": extraction_result.quality_metrics,
                "optimization_used": {
                    "process_pool": process_pool_manager is not None,
                    "memory_manager": memory_manager is not None,
                    "hybrid_optimizer": hybrid_optimizer is not None,
                },
            }

            # ì„±ëŠ¥ ë³´ê³ ì„œ ì €ì¥
            save_performance_report(performance_data, "optimized_data_analysis")

            logger.info(f"ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ (ì‹¤í–‰ì‹œê°„: {execution_time:.2f}ì´ˆ)")

        except Exception as e:
            logger.warning(f"ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")

        # ğŸ¯ 6ë‹¨ê³„: ìµœì¢… ê²€ì¦ ì‹œìŠ¤í…œ
        logger.info("ğŸ¯ 6ë‹¨ê³„: ìµœì¢… ê²€ì¦ ì‹œìŠ¤í…œ")

        def validate_final_output():
            """ìµœì¢… ì¶œë ¥ ê²€ì¦"""
            try:
                from pathlib import Path
                import numpy as np
                import json

                vector_file = Path("data/cache/feature_vector_full.npy")
                names_file = Path("data/cache/feature_vector_full.names.json")

                if not vector_file.exists():
                    logger.error("âŒ ë²¡í„° íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
                    return False

                # ë²¡í„° ë¡œë“œ
                vectors = np.load(vector_file)

                # íŠ¹ì„± ì´ë¦„ ë¡œë“œ
                feature_names = []
                if names_file.exists():
                    with open(names_file, "r", encoding="utf-8") as f:
                        feature_names = json.load(f)

                # í•„ìˆ˜ ê²€ì¦
                checks = {
                    "ìƒ˜í”Œ ìˆ˜ 1000ê°œ ì´ìƒ": (
                        len(vectors) >= 1000 if vectors.ndim > 1 else False
                    ),
                    "ì°¨ì› 168ì°¨ì›": (
                        vectors.shape[-1] == 168
                        if vectors.ndim > 0
                        else len(vectors) == 168
                    ),
                    "ì´ë¦„ ìˆ˜ ì¼ì¹˜": (
                        len(feature_names) == 168 if feature_names else False
                    ),
                    "íŒŒì¼ í¬ê¸° 672KB ì´ìƒ": vector_file.stat().st_size >= 672000,
                    "NaN/Inf ì—†ìŒ": not (
                        np.any(np.isnan(vectors)) or np.any(np.isinf(vectors))
                    ),
                }

                # ê²€ì¦ ê²°ê³¼ ë¡œê¹…
                logger.info("ğŸ” ìµœì¢… ê²€ì¦ ê²°ê³¼:")
                passed_checks = 0
                for check_name, passed in checks.items():
                    status = "âœ… í†µê³¼" if passed else "âŒ ì‹¤íŒ¨"
                    logger.info(f"   - {check_name}: {status}")
                    if passed:
                        passed_checks += 1

                # ìƒì„¸ ì •ë³´
                if vectors.ndim > 1:
                    logger.info(
                        f"ğŸ“Š ë²¡í„° ì •ë³´: {vectors.shape}, {vector_file.stat().st_size:,} bytes"
                    )
                else:
                    logger.info(
                        f"ğŸ“Š ë²¡í„° ì •ë³´: {len(vectors)}ì°¨ì›, {vector_file.stat().st_size:,} bytes"
                    )

                logger.info(f"ğŸ“Š íŠ¹ì„± ì´ë¦„: {len(feature_names)}ê°œ")
                logger.info(
                    f"ğŸ† ì „ì²´ ì„±ê³µë¥ : {passed_checks}/{len(checks)} ({passed_checks/len(checks)*100:.1f}%)"
                )

                success = passed_checks >= 4  # 5ê°œ ì¤‘ 4ê°œ ì´ìƒ í†µê³¼
                if success:
                    logger.info("ğŸ‰ DAEBAK_AI í”„ë¡œì íŠ¸ ì™„ì „ ìˆ˜ì • ì„±ê³µ!")
                else:
                    logger.warning("âš ï¸ ì¼ë¶€ ê²€ì¦ ì‹¤íŒ¨ - ì¶”ê°€ ìˆ˜ì • í•„ìš”")

                return success

            except Exception as e:
                logger.error(f"ìµœì¢… ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
                return False

        # ìµœì¢… ê²€ì¦ ì‹¤í–‰
        validation_success = validate_final_output()

        if not validation_success:
            logger.warning("ìµœì¢… ê²€ì¦ ì‹¤íŒ¨í–ˆì§€ë§Œ íŒŒì´í”„ë¼ì¸ì€ ê³„ì† ì§„í–‰")

        # ìµœì í™” ì‹œìŠ¤í…œ ì •ë¦¬
        cleanup_optimization_systems()

        logger.info(
            f"âœ… ìµœì í™”ëœ ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ (ì´ {time.time() - start_time:.2f}ì´ˆ)"
        )
        return True

    except Exception as e:
        logger.error(f"âŒ ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return False


def validate_lottery_data(historical_data: List[LotteryNumber]) -> bool:
    """ë¡œë˜ ë°ì´í„° ê²€ì¦"""
    try:
        if not historical_data:
            logger.error("ë¹ˆ ë°ì´í„°ì…‹")
            return False

        for i, draw in enumerate(historical_data):
            # ë²ˆí˜¸ ê°œìˆ˜ ê²€ì¦
            if len(draw.numbers) != 6:
                logger.error(
                    f"íšŒì°¨ {draw.draw_no}: ë²ˆí˜¸ ê°œìˆ˜ ì˜¤ë¥˜ ({len(draw.numbers)}ê°œ)"
                )
                return False

            # ë²ˆí˜¸ ë²”ìœ„ ê²€ì¦
            for number in draw.numbers:
                if not (1 <= number <= 45):
                    logger.error(f"íšŒì°¨ {draw.draw_no}: ë²ˆí˜¸ ë²”ìœ„ ì˜¤ë¥˜ ({number})")
                    return False

            # ì¤‘ë³µ ë²ˆí˜¸ ê²€ì¦
            if len(set(draw.numbers)) != 6:
                logger.error(f"íšŒì°¨ {draw.draw_no}: ì¤‘ë³µ ë²ˆí˜¸ ì¡´ì¬")
                return False

        logger.info(f"ë°ì´í„° ê²€ì¦ ì™„ë£Œ: {len(historical_data)}íšŒì°¨ ëª¨ë‘ ì •ìƒ")
        return True

    except Exception as e:
        logger.error(f"ë°ì´í„° ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
        return False


def run_parallel_analysis(
    historical_data: List[LotteryNumber],
    analyzers: Dict[str, Any],
    config: Dict[str, Any],
) -> Dict[str, Any]:
    """ë³‘ë ¬ ë¶„ì„ ì‹¤í–‰"""
    analysis_results = {}

    try:
        logger.info("ë³‘ë ¬ ë¶„ì„ ì‹œì‘")

        # ë¶„ì„ ì‘ì—… ì •ì˜
        analysis_tasks = [
            ("pattern", analyzers["pattern"]),
            ("distribution", analyzers["distribution"]),
            ("roi", analyzers["roi"]),
            ("pair", analyzers["pair"]),
        ]

        # í”„ë¡œì„¸ìŠ¤ í’€ì´ ìˆìœ¼ë©´ ë³‘ë ¬ ì‹¤í–‰, ì—†ìœ¼ë©´ ìˆœì°¨ ì‹¤í–‰
        if process_pool_manager and len(analysis_tasks) > 1:
            logger.info("í”„ë¡œì„¸ìŠ¤ í’€ì„ ì‚¬ìš©í•œ ë³‘ë ¬ ë¶„ì„")

            with ThreadPoolExecutor(max_workers=len(analysis_tasks)) as executor:
                future_to_name = {}

                for name, analyzer in analysis_tasks:
                    if analyzer:
                        future = executor.submit(
                            safe_analysis_execution, name, analyzer, historical_data
                        )
                        future_to_name[future] = name

                # ê²°ê³¼ ìˆ˜ì§‘
                for future in as_completed(future_to_name):
                    name = future_to_name[future]
                    try:
                        result = future.result(timeout=300)  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
                        if result:
                            analysis_results[name] = result
                            logger.info(f"{name} ë¶„ì„ ì™„ë£Œ")
                        else:
                            logger.warning(f"{name} ë¶„ì„ ê²°ê³¼ ì—†ìŒ")
                    except Exception as e:
                        logger.error(f"{name} ë¶„ì„ ì‹¤íŒ¨: {e}")
        else:
            logger.info("ìˆœì°¨ ë¶„ì„ ì‹¤í–‰")

            for name, analyzer in analysis_tasks:
                if analyzer:
                    try:
                        result = safe_analysis_execution(
                            name, analyzer, historical_data
                        )
                        if result:
                            analysis_results[name] = result
                            logger.info(f"{name} ë¶„ì„ ì™„ë£Œ")
                    except Exception as e:
                        logger.error(f"{name} ë¶„ì„ ì‹¤íŒ¨: {e}")

        logger.info(f"ë³‘ë ¬ ë¶„ì„ ì™„ë£Œ: {len(analysis_results)}ê°œ ê²°ê³¼")
        return analysis_results

    except Exception as e:
        logger.error(f"ë³‘ë ¬ ë¶„ì„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return {}


def safe_analysis_execution(
    name: str, analyzer: Any, historical_data: List[LotteryNumber]
) -> Optional[Dict[str, Any]]:
    """ì•ˆì „í•œ ë¶„ì„ ì‹¤í–‰"""
    try:
        logger.info(f"{name} ë¶„ì„ ì‹œì‘")
        result = analyzer.analyze(historical_data)
        logger.info(f"{name} ë¶„ì„ ì„±ê³µ")
        return result
    except Exception as e:
        logger.error(f"{name} ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        return None


def merge_analysis_results(analysis_results: Dict[str, Any]) -> Dict[str, Any]:
    """ë¶„ì„ ê²°ê³¼ë“¤ì„ í†µí•©"""
    try:
        unified_analysis = {
            "metadata": {
                "analysis_count": len(analysis_results),
                "timestamp": datetime.now().isoformat(),
                "version": "1.0.0",
            }
        }

        # ê° ë¶„ì„ ê²°ê³¼ë¥¼ í†µí•©
        for analysis_type, result in analysis_results.items():
            if result:
                unified_analysis[analysis_type] = result

        logger.info(f"ë¶„ì„ ê²°ê³¼ í†µí•© ì™„ë£Œ: {len(unified_analysis)}ê°œ í•­ëª©")
        return unified_analysis

    except Exception as e:
        logger.error(f"ë¶„ì„ ê²°ê³¼ í†µí•© ì¤‘ ì˜¤ë¥˜: {e}")
        return {}


def validate_feature_vector(
    feature_vector: np.ndarray, feature_names: List[str], config: Dict[str, Any]
) -> bool:
    """íŠ¹ì„± ë²¡í„° í’ˆì§ˆ ê²€ì¦"""
    try:
        # ê¸°ë³¸ ê²€ì¦
        if feature_vector is None or len(feature_vector) == 0:
            logger.error("ë¹ˆ íŠ¹ì„± ë²¡í„°")
            return False

        if feature_names is None or len(feature_names) == 0:
            logger.error("ë¹ˆ íŠ¹ì„± ì´ë¦„ ë¦¬ìŠ¤íŠ¸")
            return False

        # ì°¨ì› ì¼ì¹˜ ê²€ì¦
        if len(feature_vector) != len(feature_names):
            logger.error(
                f"ë²¡í„° ì°¨ì› ë¶ˆì¼ì¹˜: ë²¡í„°={len(feature_vector)}, ì´ë¦„={len(feature_names)}"
            )
            return False

        # ìµœì†Œ ì°¨ì› ê²€ì¦
        try:
            min_dimensions = config["vector"]["min_required_dimension"]
            if len(feature_vector) < min_dimensions:
                logger.error(
                    f"ë²¡í„° ì°¨ì› ë¶€ì¡±: {len(feature_vector)} < {min_dimensions}"
                )
                return False
        except (KeyError, TypeError):
            logger.warning("ìµœì†Œ ì°¨ì› ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")

        # NaN/Inf ê²€ì¦
        if np.any(np.isnan(feature_vector)) or np.any(np.isinf(feature_vector)):
            logger.error("ë²¡í„°ì— NaN ë˜ëŠ” Inf ê°’ í¬í•¨")
            return False

        logger.info(f"íŠ¹ì„± ë²¡í„° ê²€ì¦ ì™„ë£Œ: {len(feature_vector)}ì°¨ì›")
        return True

    except Exception as e:
        logger.error(f"íŠ¹ì„± ë²¡í„° ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
        return False


def save_analysis_results(
    unified_analysis: Dict[str, Any], config: Dict[str, Any]
) -> None:
    """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
    try:
        # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
        result_dir = Path("data/result/analysis")
        result_dir.mkdir(parents=True, exist_ok=True)

        # JSON íŒŒì¼ë¡œ ì €ì¥
        result_file = result_dir / "optimized_analysis_result.json"

        # í•¨ìˆ˜ ê°ì²´ ì œê±° í›„ ì§ë ¬í™”
        serializable_analysis = _make_json_serializable(unified_analysis)

        with open(result_file, "w", encoding="utf-8") as f:
            json.dump(
                serializable_analysis,
                f,
                ensure_ascii=False,
                indent=2,
            )

        logger.info(f"ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {result_file}")

    except Exception as e:
        logger.error(f"ë¶„ì„ ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
        raise


def _make_json_serializable(obj: Any) -> Any:
    """ê°ì²´ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ê²Œ ë³€í™˜"""
    if callable(obj):
        return f"<function: {obj.__name__ if hasattr(obj, '__name__') else str(obj)}>"
    elif isinstance(obj, dict):
        return {key: _make_json_serializable(value) for key, value in obj.items()}
    elif isinstance(obj, (list, tuple)):
        return [_make_json_serializable(item) for item in obj]
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, (np.integer, np.floating)):
        return obj.item()
    elif hasattr(obj, "__dict__"):
        # ê°ì²´ì˜ ì†ì„±ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        return {
            "type": obj.__class__.__name__,
            "attributes": _make_json_serializable(obj.__dict__),
        }
    else:
        try:
            json.dumps(obj)  # ì§ë ¬í™” ê°€ëŠ¥í•œì§€ í…ŒìŠ¤íŠ¸
            return obj
        except (TypeError, ValueError):
            return str(obj)


def save_performance_report(performance_data: Dict[str, Any], module_name: str) -> None:
    """ì„±ëŠ¥ ë³´ê³ ì„œ ì €ì¥"""
    try:
        # ë³´ê³ ì„œ ë””ë ‰í† ë¦¬ ìƒì„±
        report_dir = Path("data/result/performance_reports")
        report_dir.mkdir(parents=True, exist_ok=True)

        # ë³´ê³ ì„œ íŒŒì¼ ì €ì¥
        report_file = report_dir / f"{module_name}_performance_report.json"

        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(performance_data, f, ensure_ascii=False, indent=2)

        logger.info(f"ì„±ëŠ¥ ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ: {report_file}")

    except Exception as e:
        logger.error(f"ì„±ëŠ¥ ë³´ê³ ì„œ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")


def get_memory_usage() -> Dict[str, float]:
    """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
    try:
        process = psutil.Process()
        memory_info = process.memory_info()

        return {
            "rss_mb": memory_info.rss / 1024 / 1024,  # MB
            "vms_mb": memory_info.vms / 1024 / 1024,  # MB
            "percent": process.memory_percent(),
        }
    except Exception:
        return {"rss_mb": 0, "vms_mb": 0, "percent": 0}


def cleanup_optimization_systems() -> None:
    """ìµœì í™” ì‹œìŠ¤í…œ ì •ë¦¬"""
    global process_pool_manager, hybrid_optimizer, memory_manager

    try:
        if process_pool_manager:
            process_pool_manager.shutdown()
            logger.info("í”„ë¡œì„¸ìŠ¤ í’€ ì •ë¦¬ ì™„ë£Œ")

        if memory_manager:
            memory_manager.cleanup()
            logger.info("ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì •ë¦¬ ì™„ë£Œ")

        if hybrid_optimizer:
            hybrid_optimizer.cleanup()
            logger.info("í•˜ì´ë¸Œë¦¬ë“œ ìµœì í™” ì‹œìŠ¤í…œ ì •ë¦¬ ì™„ë£Œ")

    except Exception as e:
        logger.warning(f"ìµœì í™” ì‹œìŠ¤í…œ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    finally:
        # ì „ì—­ ë³€ìˆ˜ ì´ˆê¸°í™”
        process_pool_manager = None
        hybrid_optimizer = None
        memory_manager = None


def run_data_analysis() -> bool:
    """
    ê¸°ë³¸ ë°ì´í„° ë¶„ì„ í•¨ìˆ˜ (í•˜ìœ„ í˜¸í™˜ì„±)
    """
    return run_optimized_data_analysis()


def run_fully_optimized_analysis():
    """ì™„ì „ ìµœì í™”ëœ ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸"""

    # ğŸš€ ì „ì—­ ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    from src.utils.memory_manager import get_memory_manager, MemoryConfig
    from src.utils.cuda_optimizers import CudaConfig
    from src.utils.unified_performance import get_profiler

    logger.info("ğŸ‰ ì™„ì „ ìµœì í™”ëœ ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹œì‘")

    # ì „ì—­ ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    memory_config = MemoryConfig(
        max_memory_usage=0.85,
        use_memory_pooling=True,
        pool_size=32,
        auto_cleanup_interval=60.0,
    )
    memory_manager = get_memory_manager(memory_config)

    cuda_config = CudaConfig(
        use_amp=True,
        batch_size=128,
        use_cudnn=True,
    )

    # í”„ë¡œíŒŒì¼ëŸ¬ ì´ˆê¸°í™”
    profiler = get_profiler()

    # ğŸ§  ì „ì—­ ë©”ëª¨ë¦¬ ê´€ë¦¬ ì»¨í…ìŠ¤íŠ¸
    with memory_manager.allocation_scope():
        # ğŸ“ˆ ì „ì²´ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        with profiler.profile("ì™„ì „_ìµœì í™”_ë¶„ì„"):
            # ê¸°ë³¸ ìµœì í™”ëœ ë¶„ì„ ì‹¤í–‰
            return run_optimized_data_analysis()


def create_optimized_analyzer(
    memory_manager, cuda_optimizer, process_pool_manager, hybrid_optimizer
):
    """ìµœì í™”ëœ ë¶„ì„ê¸° ìƒì„±"""
    from src.analysis.unified_analyzer import UnifiedAnalyzer

    config = load_config()
    analyzer = UnifiedAnalyzer(config)

    # ìµœì í™” ì‹œìŠ¤í…œ ì£¼ì…
    if hasattr(analyzer, "set_optimizers"):
        analyzer.set_optimizers(
            memory_manager=memory_manager,
            cuda_optimizer=cuda_optimizer,
            process_pool_manager=process_pool_manager,
            hybrid_optimizer=hybrid_optimizer,
        )

    return analyzer


def create_optimized_vectorizer(memory_manager, cuda_optimizer):
    """ìµœì í™”ëœ ë²¡í„°ë¼ì´ì € ìƒì„±"""
    from src.analysis.pattern_vectorizer import PatternVectorizer

    config = load_config()
    vectorizer = PatternVectorizer(config)

    # ìµœì í™” ì‹œìŠ¤í…œ ì£¼ì…
    if hasattr(vectorizer, "set_optimizers"):
        vectorizer.set_optimizers(
            memory_manager=memory_manager, cuda_optimizer=cuda_optimizer
        )

    return vectorizer


def load_draw_history_optimized(memory_manager=None):
    """ìµœì í™”ëœ ë°ì´í„° ë¡œë“œ"""
    from src.utils.data_loader import load_draw_history

    if memory_manager:
        with memory_manager.allocation_scope():
            return load_draw_history()
    else:
        return load_draw_history()


def save_results_optimized(analysis_result, vectors, memory_manager=None):
    """ìµœì í™”ëœ ê²°ê³¼ ì €ì¥"""
    try:
        if memory_manager:
            with memory_manager.allocation_scope():
                # ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì €ì¥
                _save_with_memory_optimization(analysis_result, vectors)
        else:
            _save_standard(analysis_result, vectors)

        logger.info("âœ… ìµœì í™”ëœ ê²°ê³¼ ì €ì¥ ì™„ë£Œ")

    except Exception as e:
        logger.error(f"ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")


def _save_with_memory_optimization(analysis_result, vectors):
    """ë©”ëª¨ë¦¬ ìµœì í™”ëœ ì €ì¥"""
    import json
    import numpy as np
    from pathlib import Path

    # ë¶„ì„ ê²°ê³¼ ì €ì¥
    analysis_path = Path("data/result/analysis/lottery_data_analysis.json")
    analysis_path.parent.mkdir(parents=True, exist_ok=True)

    with open(analysis_path, "w", encoding="utf-8") as f:
        json.dump(analysis_result, f, ensure_ascii=False, indent=2)

    # ë²¡í„° ì €ì¥
    if vectors is not None:
        vector_path = Path("data/cache/feature_vectors_full.npy")
        vector_path.parent.mkdir(parents=True, exist_ok=True)
        np.save(vector_path, vectors)


def _save_standard(analysis_result, vectors):
    """í‘œì¤€ ì €ì¥"""
    _save_with_memory_optimization(analysis_result, vectors)


def cleanup_optimized_resources(*optimizers):
    """ìµœì í™” ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
    for optimizer in optimizers:
        if optimizer and hasattr(optimizer, "cleanup"):
            try:
                optimizer.cleanup()
            except Exception as e:
                logger.warning(f"ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    logger.info("âœ… ìµœì í™” ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")


def benchmark_optimization_performance():
    """ìµœì í™” ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    logger.info("ìµœì í™” ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")

    try:
        # ê¸°ë³¸ ë¶„ì„ ì‹¤í–‰
        start_time = time.time()
        result = run_optimized_data_analysis()
        duration = time.time() - start_time

        logger.info(f"ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ: {duration:.2f}ì´ˆ, ì„±ê³µ: {result}")
        return {"duration": duration, "success": result}

    except Exception as e:
        logger.error(f"ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return {"duration": 0, "success": False, "error": str(e)}


if __name__ == "__main__":
    success = run_optimized_data_analysis()
    sys.exit(0 if success else 1)
