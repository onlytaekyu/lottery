"""
ë¡œë˜ ë²ˆí˜¸ íŒ¨í„´ ë¶„ì„ê¸° ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ ë¡œë˜ ë²ˆí˜¸ì˜ íŒ¨í„´ì„ ë¶„ì„í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import sys
import json
import datetime
import random
import networkx as nx
import numpy as np
import hashlib
import os
from datetime import datetime
from typing import Dict, Any, List, Set, Tuple, TypedDict, Optional, Union, cast
from collections import Counter, defaultdict
import time
import logging
from pathlib import Path
import math

from ..utils.error_handler_refactored import get_logger
from ..shared.types import LotteryNumber, PatternAnalysis
from ..utils.memory_manager import MemoryManager
from ..analysis.base_analyzer import BaseAnalyzer
from ..utils.unified_config import ConfigProxy
from ..utils.unified_report import safe_convert, save_analysis_performance_report
from ..utils.unified_performance import performance_monitor
from ..shared.graph_utils import calculate_pair_frequency, calculate_pair_centrality


# ë¡œê·¸ ì„¤ì •
logger = get_logger(__name__)


# íŒ¨í„´ íŠ¹ì„±ì„ ìœ„í•œ TypedDict ì •ì˜
class PatternFeatures(TypedDict, total=False):
    max_consecutive_length: int
    total_sum: int
    odd_count: int
    even_count: int
    gap_avg: float
    gap_std: float
    range_counts: list[int]
    cluster_overlap_ratio: float
    frequent_pair_score: float
    roi_weight: float
    consecutive_score: float
    trend_score_avg: float
    trend_score_max: float
    trend_score_min: float
    risk_score: float


class PatternAnalyzer(BaseAnalyzer[PatternAnalysis]):
    """ë¡œë˜ ë²ˆí˜¸ì˜ íŒ¨í„´ì„ ë¶„ì„í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        PatternAnalyzer ì´ˆê¸°í™”

        Args:
            config: íŒ¨í„´ ë¶„ì„ì— ì‚¬ìš©í•  ì„¤ì •
        """
        super().__init__(config or {}, "pattern")

        # ğŸš€ ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ í†µí•© ì´ˆê¸°í™”
        from ..utils.memory_manager import MemoryManager, MemoryConfig
        from ..utils.cuda_optimizers import get_cuda_optimizer, CudaConfig
        from ..utils.process_pool_manager import get_process_pool_manager
        from ..utils.hybrid_optimizer import get_hybrid_optimizer

        # ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™” (ê¸°ì¡´ ìœ ì§€)
        self.memory_manager = MemoryManager()

        # CUDA ìµœì í™” ì´ˆê¸°í™”
        try:
            cuda_config = CudaConfig(
                use_amp=True,  # ìë™ í˜¼í•© ì •ë°€ë„
                batch_size=64,
                use_cudnn=True,
            )
            self.cuda_optimizer = get_cuda_optimizer(cuda_config)
            self.logger.info("âœ… CUDA ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"CUDA ìµœì í™” ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.cuda_optimizer = None

        # í”„ë¡œì„¸ìŠ¤ í’€ ê´€ë¦¬ì ì´ˆê¸°í™”
        try:
            pool_config = {
                "max_workers": min(4, os.cpu_count() or 1),
                "chunk_size": 100,
                "timeout": 300,
            }
            self.process_pool_manager = get_process_pool_manager(pool_config)
            self.logger.info("âœ… í”„ë¡œì„¸ìŠ¤ í’€ ê´€ë¦¬ì ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"í”„ë¡œì„¸ìŠ¤ í’€ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.process_pool_manager = None

        # í•˜ì´ë¸Œë¦¬ë“œ ìµœì í™” ì´ˆê¸°í™”
        try:
            hybrid_config = {
                "auto_optimization": True,
                "memory_threshold": 0.8,
                "cpu_threshold": 80.0,
                "gpu_threshold": 0.9,
            }
            self.hybrid_optimizer = get_hybrid_optimizer(hybrid_config)
            self.logger.info("âœ… í•˜ì´ë¸Œë¦¬ë“œ ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"í•˜ì´ë¸Œë¦¬ë“œ ìµœì í™” ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.hybrid_optimizer = None

        # ê¸°ì¡´ ì´ˆê¸°í™” ì½”ë“œ ìœ ì§€
        self.pattern_stats = {}
        self.scoped_analyses = {}  # ìŠ¤ì½”í”„ë³„ ë¶„ì„ ê²°ê³¼ ì €ì¥
        self.logger = get_logger(__name__)  # ë¡œê±° ëª…ì‹œì  ì´ˆê¸°í™”

        self.logger.info("ğŸ‰ PatternAnalyzer ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")

    def load_data(self, limit: Optional[int] = None) -> List[LotteryNumber]:
        """
        ë¡œë˜ ë‹¹ì²¨ ë²ˆí˜¸ ë°ì´í„° ë¡œë“œ

        Args:
            limit: ë¡œë“œí•  ìµœëŒ€ ë°ì´í„° ìˆ˜ (Noneì´ë©´ ì „ì²´ ë¡œë“œ)

        Returns:
            ë¡œë˜ ë‹¹ì²¨ ë²ˆí˜¸ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        # ìˆœí™˜ ì°¸ì¡° ë°©ì§€ë¥¼ ìœ„í•œ ì§€ì—° ì„í¬íŠ¸
        from ..utils.data_loader import load_draw_history

        data = load_draw_history()
        if limit is not None and limit > 0:
            data = data[-limit:]

        return data

    def analyze_patterns(self, historical_data: List[LotteryNumber]) -> PatternAnalysis:
        """
        ê³¼ê±° ë¡œë˜ ë‹¹ì²¨ ë²ˆí˜¸ì˜ íŒ¨í„´ì„ ë¶„ì„í•©ë‹ˆë‹¤ (analyze ë©”ì„œë“œì˜ ë³„ì¹­).

        Args:
            historical_data: ë¶„ì„í•  ê³¼ê±° ë‹¹ì²¨ ë²ˆí˜¸ ëª©ë¡

        Returns:
            PatternAnalysis: íŒ¨í„´ ë¶„ì„ ê²°ê³¼
        """
        return self.analyze(historical_data)

    def _analyze_impl(
        self,
        historical_data: List[LotteryNumber],
        use_optimization: bool = True,
        optimization_level: str = "auto",
        *args,
        **kwargs,
    ) -> PatternAnalysis:
        """
        ìµœì í™”ëœ íŒ¨í„´ ë¶„ì„ êµ¬í˜„

        Args:
            historical_data: ë¶„ì„í•  ê³¼ê±° ë‹¹ì²¨ ë²ˆí˜¸ ëª©ë¡
            use_optimization: ìµœì í™” ì‚¬ìš© ì—¬ë¶€
            optimization_level: ìµœì í™” ë ˆë²¨ ("auto", "basic", "balanced", "maximum")
            *args, **kwargs: ì¶”ê°€ ë§¤ê°œë³€ìˆ˜

        Returns:
            PatternAnalysis: íŒ¨í„´ ë¶„ì„ ê²°ê³¼
        """
        self.logger.info(
            f"ìµœì í™”ëœ íŒ¨í„´ ë¶„ì„ ì‹œì‘: {len(historical_data)}ê°œ ë°ì´í„° (ìµœì í™”: {use_optimization}, ë ˆë²¨: {optimization_level})"
        )

        if not use_optimization:
            return self._standard_analysis(historical_data)

        # ğŸ§  ë©”ëª¨ë¦¬ ê´€ë¦¬ ìŠ¤ì½”í”„ ì ìš©
        with self.memory_manager.allocation_scope():

            # ğŸ“Š ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
            with performance_monitor("pattern_analysis_optimized"):

                # ğŸ”§ ìµœì í™” ë ˆë²¨ ìë™ ê²°ì •
                if optimization_level == "auto" and self.hybrid_optimizer:
                    optimization_level = self.hybrid_optimizer.determine_optimal_level(
                        data_size=len(historical_data),
                        available_memory=self.memory_manager.get_available_memory(),
                        gpu_available=(
                            self.cuda_optimizer.is_available()
                            if self.cuda_optimizer
                            else False
                        ),
                    )

                # ğŸ” ë°ì´í„° í¬ê¸°ì— ë”°ë¥¸ ì²˜ë¦¬ ë°©ì‹ ê²°ì •
                if optimization_level == "maximum" or len(historical_data) > 5000:
                    return self._maximum_optimization_analysis(historical_data)
                elif optimization_level == "balanced" or len(historical_data) > 1000:
                    return self._balanced_optimization_analysis(historical_data)
                elif optimization_level == "basic" or len(historical_data) > 500:
                    return self._basic_optimization_analysis(historical_data)
                else:
                    return self._standard_analysis(historical_data)

    def _maximum_optimization_analysis(
        self, data: List[LotteryNumber]
    ) -> PatternAnalysis:
        """ìµœëŒ€ ìµœì í™”: GPU + ë³‘ë ¬ + ë©”ëª¨ë¦¬í’€ë§"""
        self.logger.info("ğŸš€ ìµœëŒ€ ìµœì í™” ë¶„ì„ ìˆ˜í–‰")

        try:
            # GPU ê°€ì† ë¶„ì„ ì‹œë„
            if self.cuda_optimizer and self.cuda_optimizer.is_available():
                # GPU ë©”ëª¨ë¦¬ ì²´í¬
                required_memory = len(data) * 8  # ëŒ€ëµì ì¸ ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰ (bytes)
                if self.memory_manager.check_gpu_memory(required_memory):
                    self.logger.info("ğŸ¯ GPU ê°€ì† ë¶„ì„ ìˆ˜í–‰")
                    return self._gpu_accelerated_analysis(data)
                else:
                    self.logger.warning("GPU ë©”ëª¨ë¦¬ ë¶€ì¡±, ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì „í™˜")

            # ë³‘ë ¬ ì²˜ë¦¬ ë¶„ì„
            if self.process_pool_manager:
                self.logger.info("ğŸ”„ ë³‘ë ¬ ì²˜ë¦¬ ë¶„ì„ ìˆ˜í–‰")
                return self._parallel_analysis(data)
            else:
                self.logger.info("âš–ï¸ ê· í˜• ìµœì í™”ë¡œ í´ë°±")
                return self._balanced_optimization_analysis(data)

        except Exception as e:
            self.logger.error(f"ìµœëŒ€ ìµœì í™” ë¶„ì„ ì‹¤íŒ¨: {e}, í‘œì¤€ ë¶„ì„ìœ¼ë¡œ í´ë°±")
            return self._standard_analysis(data)

    def _balanced_optimization_analysis(
        self, data: List[LotteryNumber]
    ) -> PatternAnalysis:
        """ê· í˜• ìµœì í™”: GPU ë˜ëŠ” ë³‘ë ¬ ì¤‘ í•˜ë‚˜"""
        self.logger.info("âš–ï¸ ê· í˜• ìµœì í™” ë¶„ì„ ìˆ˜í–‰")

        try:
            # GPU ìš°ì„  ì‹œë„
            if (
                self.cuda_optimizer
                and self.cuda_optimizer.is_available()
                and self.memory_manager.check_gpu_memory(len(data) * 4)
            ):
                self.logger.info("ğŸ¯ GPU ê°€ì† ë¶„ì„ ìˆ˜í–‰")
                return self._gpu_accelerated_analysis(data)

            # ë³‘ë ¬ ì²˜ë¦¬ ì‹œë„
            elif self.process_pool_manager and len(data) > 1000:
                self.logger.info("ğŸ”„ ë³‘ë ¬ ì²˜ë¦¬ ë¶„ì„ ìˆ˜í–‰")
                return self._parallel_analysis(data)

            # ê¸°ë³¸ ìµœì í™”ë¡œ í´ë°±
            else:
                self.logger.info("ğŸ”§ ê¸°ë³¸ ìµœì í™”ë¡œ í´ë°±")
                return self._basic_optimization_analysis(data)

        except Exception as e:
            self.logger.error(f"ê· í˜• ìµœì í™” ë¶„ì„ ì‹¤íŒ¨: {e}, ê¸°ë³¸ ìµœì í™”ë¡œ í´ë°±")
            return self._basic_optimization_analysis(data)

    def _basic_optimization_analysis(
        self, data: List[LotteryNumber]
    ) -> PatternAnalysis:
        """ê¸°ë³¸ ìµœì í™”: ë©”ëª¨ë¦¬ ê´€ë¦¬ë§Œ"""
        self.logger.info("ğŸ”§ ê¸°ë³¸ ìµœì í™” ë¶„ì„ ìˆ˜í–‰")

        # ğŸ§  ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬
        with self.memory_manager.batch_processing():
            return self._standard_analysis_with_memory_optimization(data)

    def _gpu_accelerated_analysis(self, data: List[LotteryNumber]) -> PatternAnalysis:
        """GPU ê°€ì† ë¶„ì„"""
        if not self.cuda_optimizer or not self.cuda_optimizer.is_available():
            return self._standard_analysis(data)

        self.logger.info("ğŸš€ GPU ê°€ì† íŒ¨í„´ ë¶„ì„ ì‹œì‘")

        try:
            # ğŸš€ CUDA ê°€ì† ì ìš©
            with self.cuda_optimizer.device_context():
                # GPUì—ì„œ íŒ¨í„´ ë¶„ì„ ìˆ˜í–‰
                gpu_result = self._perform_gpu_pattern_analysis(data)
                return gpu_result

        except Exception as e:
            self.logger.error(f"GPU ê°€ì† ë¶„ì„ ì‹¤íŒ¨: {e}, CPU ì²˜ë¦¬ë¡œ ì „í™˜")
            return self._standard_analysis(data)

    def _parallel_analysis(self, data: List[LotteryNumber]) -> PatternAnalysis:
        """ë³‘ë ¬ ì²˜ë¦¬ ë¶„ì„"""
        if not self.process_pool_manager:
            return self._standard_analysis(data)

        self.logger.info("ğŸ”„ ë³‘ë ¬ íŒ¨í„´ ë¶„ì„ ì‹œì‘")

        try:
            # ğŸ“¦ ë°ì´í„° ì²­í‚¹
            chunk_size = max(
                100, len(data) // (self.process_pool_manager.max_workers * 2)
            )
            chunks = self.process_pool_manager.create_chunks(
                data, chunk_size=chunk_size
            )

            self.logger.info(
                f"ë°ì´í„°ë¥¼ {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í•  (ì²­í¬ í¬ê¸°: {chunk_size})"
            )

            # ğŸ”„ ë³‘ë ¬ ë¶„ì„ ì‹¤í–‰
            results = self.process_pool_manager.parallel_analyze(
                chunks, self._analyze_chunk, use_memory_pooling=True
            )

            # ğŸ“Š ê²°ê³¼ ë³‘í•©
            return self._merge_analysis_results(results, data)

        except Exception as e:
            self.logger.error(f"ë³‘ë ¬ ë¶„ì„ ì‹¤íŒ¨: {e}, í‘œì¤€ ë¶„ì„ìœ¼ë¡œ í´ë°±")
            return self._standard_analysis(data)

    def _standard_analysis_with_memory_optimization(
        self, data: List[LotteryNumber]
    ) -> PatternAnalysis:
        """ë©”ëª¨ë¦¬ ìµœì í™”ê°€ ì ìš©ëœ í‘œì¤€ ë¶„ì„"""
        # ê¸°ì¡´ _analyze_impl ë¡œì§ì„ ë©”ëª¨ë¦¬ ìµœì í™”ì™€ í•¨ê»˜ ìˆ˜í–‰
        return self._standard_analysis(data)

    def _standard_analysis(self, data: List[LotteryNumber]) -> PatternAnalysis:
        """í‘œì¤€ ë¶„ì„ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)"""
        # ê¸°ì¡´ ë¶„ì„ ë¡œì§ì„ ì—¬ê¸°ì— êµ¬í˜„
        # ê°€ì¤‘ì¹˜ ë¹ˆë„ ê³„ì‚°
        weighted_frequencies = self._calculate_weighted_frequencies(data)

        # ìµœê·¼ì„± ë§µ ê³„ì‚°
        recency_map = self._calculate_recency_map(data)

        result = PatternAnalysis(
            frequency_map=weighted_frequencies, recency_map=recency_map
        )
        result.metadata = {}

        # ê¸°ì¡´ ë¶„ì„ë“¤ ìˆ˜í–‰
        consecutive_length_distribution = self.analyze_consecutive_length_distribution(
            data
        )
        odd_even_distribution = self.analyze_odd_even_distribution(data)
        sum_distribution = self.analyze_number_sum_distribution(data)
        network_analysis = self.analyze_network(data)
        gap_patterns = self.analyze_gap_patterns(data)
        segment_entropy = self.calculate_segment_entropy(data)

        # ê²°ê³¼ ì„¤ì •
        result.metadata.update(
            {
                "consecutive_length_distribution": consecutive_length_distribution,
                "odd_even_distribution": odd_even_distribution,
                "sum_distribution": sum_distribution,
                "network_analysis": network_analysis,
                "gap_patterns": gap_patterns,
                "segment_entropy": segment_entropy,
            }
        )

        return result

    def _perform_gpu_pattern_analysis(
        self, data: List[LotteryNumber]
    ) -> PatternAnalysis:
        """GPUì—ì„œ íŒ¨í„´ ë¶„ì„ ìˆ˜í–‰"""
        try:
            # GPUì—ì„œ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ë¶„ì„ë“¤
            self.logger.info("GPUì—ì„œ íŒ¨í„´ ë¶„ì„ ìˆ˜í–‰ ì¤‘...")

            # í˜„ì¬ëŠ” ê¸°ë³¸ ë¶„ì„ìœ¼ë¡œ í´ë°± (í–¥í›„ CUDA êµ¬í˜„ í™•ì¥ ê°€ëŠ¥)
            return self._standard_analysis(data)

        except Exception as e:
            self.logger.error(f"GPU íŒ¨í„´ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return self._standard_analysis(data)

    def _analyze_chunk(self, chunk: List[LotteryNumber]) -> Dict[str, Any]:
        """ì²­í¬ ë‹¨ìœ„ ë¶„ì„"""
        try:
            # ì²­í¬ë³„ íŒ¨í„´ ë¶„ì„
            chunk_result = self._standard_analysis(chunk)

            return {
                "frequency_map": chunk_result.frequency_map,
                "recency_map": chunk_result.recency_map,
                "metadata": chunk_result.metadata,
                "chunk_size": len(chunk),
            }

        except Exception as e:
            self.logger.error(f"ì²­í¬ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"error": str(e), "chunk_size": len(chunk)}

    def _merge_analysis_results(
        self, results: List[Dict[str, Any]], original_data: List[LotteryNumber]
    ) -> PatternAnalysis:
        """ë³‘ë ¬ ë¶„ì„ ê²°ê³¼ ë³‘í•©"""
        try:
            self.logger.info(f"ë³‘ë ¬ ë¶„ì„ ê²°ê³¼ ë³‘í•© ì‹œì‘: {len(results)}ê°œ ê²°ê³¼")

            # ë¹ˆë„ ë§µ ë³‘í•©
            merged_frequency = {}
            merged_recency = {}
            total_chunks = 0

            for result in results:
                if "error" in result:
                    self.logger.warning(f"ì˜¤ë¥˜ê°€ ìˆëŠ” ì²­í¬ ê±´ë„ˆëœ€: {result['error']}")
                    continue

                # ë¹ˆë„ ë§µ ë³‘í•©
                for num, freq in result.get("frequency_map", {}).items():
                    merged_frequency[num] = merged_frequency.get(num, 0) + freq

                # ìµœê·¼ì„± ë§µ ë³‘í•© (ìµœëŒ€ê°’ ì‚¬ìš©)
                for num, recency in result.get("recency_map", {}).items():
                    merged_recency[num] = max(merged_recency.get(num, 0), recency)

                total_chunks += 1

            # í‰ê· í™”
            if total_chunks > 0:
                for num in merged_frequency:
                    merged_frequency[num] /= total_chunks

            # ìµœì¢… ê²°ê³¼ ìƒì„±
            final_result = PatternAnalysis(
                frequency_map=merged_frequency, recency_map=merged_recency
            )

            # ì „ì²´ ë°ì´í„°ë¡œ ì¶”ê°€ ë¶„ì„ ìˆ˜í–‰
            final_result.metadata = self._calculate_additional_metadata(original_data)

            self.logger.info("ë³‘ë ¬ ë¶„ì„ ê²°ê³¼ ë³‘í•© ì™„ë£Œ")
            return final_result

        except Exception as e:
            self.logger.error(f"ê²°ê³¼ ë³‘í•© ì‹¤íŒ¨: {e}, í‘œì¤€ ë¶„ì„ìœ¼ë¡œ í´ë°±")
            return self._standard_analysis(original_data)

    def _calculate_additional_metadata(
        self, data: List[LotteryNumber]
    ) -> Dict[str, Any]:
        """ì¶”ê°€ ë©”íƒ€ë°ì´í„° ê³„ì‚°"""
        try:
            return {
                "data_count": len(data),
                "analysis_timestamp": time.time(),
                "optimization_used": True,
            }
        except Exception as e:
            self.logger.error(f"ë©”íƒ€ë°ì´í„° ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {}

    def set_optimizers(self, **optimizers):
        """ì™¸ë¶€ì—ì„œ ìµœì í™” ì‹œìŠ¤í…œ ì£¼ì…"""
        if "memory_manager" in optimizers:
            self.memory_manager = optimizers["memory_manager"]
        if "cuda_optimizer" in optimizers:
            self.cuda_optimizer = optimizers["cuda_optimizer"]
        if "process_pool_manager" in optimizers:
            self.process_pool_manager = optimizers["process_pool_manager"]
        if "hybrid_optimizer" in optimizers:
            self.hybrid_optimizer = optimizers["hybrid_optimizer"]

        self.logger.info("ì™¸ë¶€ ìµœì í™” ì‹œìŠ¤í…œ ì£¼ì… ì™„ë£Œ")

    def _analyze_physical_structure(
        self, historical_data: List[LotteryNumber]
    ) -> Dict[str, Any]:
        """
        ë¡œë˜ ì¶”ì²¨ê¸°ì˜ ë¬¼ë¦¬ì  êµ¬ì¡°ì™€ ê´€ë ¨ëœ íŠ¹ì„±ì„ ë¶„ì„í•©ë‹ˆë‹¤.

        ë¬¼ë¦¬ì  êµ¬ì¡° íŠ¹ì„±ì—ëŠ” ë²ˆí˜¸ ê°„ ê±°ë¦¬ ë¶„ì‚°, ì—°ì† ìŒ ë¹„ìœ¨, ê° ìœ„ì¹˜ë³„ Z-ì ìˆ˜,
        ì´í•­ë¶„í¬ ë§¤ì¹­ ì ìˆ˜, ë²ˆí˜¸ í‘œì¤€í¸ì°¨ ì ìˆ˜ ë“±ì´ í¬í•¨ë©ë‹ˆë‹¤.

        Args:
            historical_data: ë¶„ì„í•  ê³¼ê±° ë‹¹ì²¨ ë²ˆí˜¸ ëª©ë¡

        Returns:
            Dict[str, Any]: ë¬¼ë¦¬ì  êµ¬ì¡° íŠ¹ì„± ë¶„ì„ ê²°ê³¼
        """
        self.logger.info("ë¬¼ë¦¬ì  êµ¬ì¡° íŠ¹ì„± ë¶„ì„ ì‹œì‘...")

        # ê²°ê³¼ ì €ì¥ìš© ë”•ì…”ë„ˆë¦¬
        physical_structure_features = {}

        try:
            # 1. ê±°ë¦¬ ë¶„ì‚° (distance_variance) ê³„ì‚°
            distance_variance = self._calculate_distance_variance(historical_data)
            physical_structure_features["distance_variance"] = distance_variance

            # 2. ì—°ì† ìŒ ë¹„ìœ¨ (sequential_pair_rate) ê³„ì‚°
            sequential_pair_rate = self._calculate_sequential_pair_rate(historical_data)
            physical_structure_features["sequential_pair_rate"] = sequential_pair_rate

            # 3. ìœ„ì¹˜ë³„ Z-ì ìˆ˜ ê³„ì‚°
            z_scores = self._calculate_position_z_scores(historical_data)
            physical_structure_features.update(z_scores)

            # 4. ì´í•­ë¶„í¬ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
            binomial_match_score = self._calculate_binomial_match_score(historical_data)
            physical_structure_features["binomial_match_score"] = binomial_match_score

            # 5. ë²ˆí˜¸ í‘œì¤€í¸ì°¨ ì ìˆ˜ ê³„ì‚°
            number_std_score = self._calculate_number_std_score(historical_data)
            physical_structure_features["number_std_score"] = number_std_score

            self.logger.info("ë¬¼ë¦¬ì  êµ¬ì¡° íŠ¹ì„± ë¶„ì„ ì™„ë£Œ")

        except Exception as e:
            self.logger.error(f"ë¬¼ë¦¬ì  êµ¬ì¡° íŠ¹ì„± ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            # ê¸°ë³¸ê°’ ì„¤ì •
            physical_structure_features = {
                "distance_variance": {"average": 0.0, "std": 0.0},
                "sequential_pair_rate": {"avg_rate": 0.0},
                "zscore_num1": 0.0,
                "zscore_num2": 0.0,
                "zscore_num3": 0.0,
                "zscore_num4": 0.0,
                "zscore_num5": 0.0,
                "zscore_num6": 0.0,
                "binomial_match_score": 0.0,
                "number_std_score": 0.0,
            }

        return physical_structure_features

    def _calculate_distance_variance(
        self, historical_data: List[LotteryNumber]
    ) -> Dict[str, Any]:
        """
        ê° ë‹¹ì²¨ ì¡°í•©ì˜ ë²ˆí˜¸ ê°„ ê±°ë¦¬ ë¶„ì‚°ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

        Args:
            historical_data: ë¶„ì„í•  ê³¼ê±° ë‹¹ì²¨ ë²ˆí˜¸ ëª©ë¡

        Returns:
            Dict[str, Any]: ê±°ë¦¬ ë¶„ì‚° ê²°ê³¼
                - ê° íšŒì°¨ë³„ ê±°ë¦¬ ë¶„ì‚°
                - í‰ê· , í‘œì¤€í¸ì°¨ ë“± í†µê³„
        """
        try:
            # ê²°ê³¼ ì €ì¥ìš© ë”•ì…”ë„ˆë¦¬
            result = {}
            all_variances = []

            # ê° íšŒì°¨ë³„ë¡œ ê±°ë¦¬ ë¶„ì‚° ê³„ì‚°
            for draw in historical_data:
                if not hasattr(draw, "draw_no") or not draw.numbers:
                    continue

                draw_key = f"draw_{draw.draw_no}"
                sorted_numbers = sorted(draw.numbers)

                # ì¸ì ‘í•œ ë²ˆí˜¸ ê°„ ê±°ë¦¬ ê³„ì‚°
                distances = [
                    sorted_numbers[i + 1] - sorted_numbers[i]
                    for i in range(len(sorted_numbers) - 1)
                ]

                # ê±°ë¦¬ì˜ ë¶„ì‚° ê³„ì‚°
                if len(distances) > 0:
                    variance = float(np.var(distances))
                    result[draw_key] = variance
                    all_variances.append(variance)

            # ì „ì²´ í†µê³„ ê³„ì‚°
            if all_variances:
                result["average"] = float(np.mean(all_variances))
                result["std"] = float(np.std(all_variances))
                result["min"] = float(np.min(all_variances))
                result["max"] = float(np.max(all_variances))
            else:
                result["average"] = 0.0
                result["std"] = 0.0
                result["min"] = 0.0
                result["max"] = 0.0

            return result

        except Exception as e:
            self.logger.error(f"ê±°ë¦¬ ë¶„ì‚° ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return {"average": 0.0, "std": 0.0, "min": 0.0, "max": 0.0}

    def _calculate_sequential_pair_rate(
        self, historical_data: List[LotteryNumber]
    ) -> Dict[str, Any]:
        """
        ê° ë‹¹ì²¨ ì¡°í•©ì—ì„œ ì—°ì†ëœ ë²ˆí˜¸ ìŒì˜ ë¹„ìœ¨ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

        Args:
            historical_data: ë¶„ì„í•  ê³¼ê±° ë‹¹ì²¨ ë²ˆí˜¸ ëª©ë¡

        Returns:
            Dict[str, Any]: ì—°ì† ìŒ ë¹„ìœ¨ ê²°ê³¼
                - ê° íšŒì°¨ë³„ ì—°ì† ìŒ ìˆ˜
                - í‰ê·  ë¹„ìœ¨ ë“± í†µê³„
        """
        try:
            # ê²°ê³¼ ì €ì¥ìš© ë”•ì…”ë„ˆë¦¬
            result = {}
            all_sequential_pairs = []

            # ê° íšŒì°¨ë³„ë¡œ ì—°ì† ìŒ ê³„ì‚°
            for draw in historical_data:
                if not hasattr(draw, "draw_no") or not draw.numbers:
                    continue

                draw_key = f"draw_{draw.draw_no}"
                sorted_numbers = sorted(draw.numbers)

                # ì—°ì†ëœ ë²ˆí˜¸ ìŒ ì¹´ìš´íŠ¸
                sequential_pairs = sum(
                    1
                    for i in range(len(sorted_numbers) - 1)
                    if sorted_numbers[i + 1] - sorted_numbers[i] == 1
                )

                result[draw_key] = sequential_pairs
                all_sequential_pairs.append(sequential_pairs)

            # í‰ê·  ì—°ì† ìŒ ë¹„ìœ¨ ê³„ì‚°
            if all_sequential_pairs:
                result["avg_rate"] = float(np.mean(all_sequential_pairs))
                result["max_rate"] = float(np.max(all_sequential_pairs))
                result["min_rate"] = float(np.min(all_sequential_pairs))
                result["std_rate"] = float(np.std(all_sequential_pairs))
            else:
                result["avg_rate"] = 0.0
                result["max_rate"] = 0.0
                result["min_rate"] = 0.0
                result["std_rate"] = 0.0

            return result

        except Exception as e:
            self.logger.error(f"ì—°ì† ìŒ ë¹„ìœ¨ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return {"avg_rate": 0.0, "max_rate": 0.0, "min_rate": 0.0, "std_rate": 0.0}

    def _calculate_position_z_scores(
        self, historical_data: List[LotteryNumber]
    ) -> Dict[str, float]:
        """
        ê° ìœ„ì¹˜(1~6)ë³„ ë²ˆí˜¸ì˜ Z-ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

        Args:
            historical_data: ë¶„ì„í•  ê³¼ê±° ë‹¹ì²¨ ë²ˆí˜¸ ëª©ë¡

        Returns:
            Dict[str, float]: ê° ìœ„ì¹˜ë³„ Z-ì ìˆ˜
        """
        try:
            # ìœ„ì¹˜ë³„ ë²ˆí˜¸ ì €ì¥
            position_numbers = [[] for _ in range(6)]

            # ìµœê·¼ íšŒì°¨ì˜ ë²ˆí˜¸ (Z-ì ìˆ˜ ê³„ì‚° ëŒ€ìƒ)
            latest_draw = historical_data[-1] if historical_data else None

            if not latest_draw or not latest_draw.numbers:
                return {f"zscore_num{i+1}": 0.0 for i in range(6)}

            latest_numbers = sorted(latest_draw.numbers)

            # ìœ„ì¹˜ë³„ ë²ˆí˜¸ ìˆ˜ì§‘ (ìµœê·¼ íšŒì°¨ ì œì™¸)
            for draw in historical_data[:-1]:
                sorted_numbers = sorted(draw.numbers)
                for i, num in enumerate(sorted_numbers):
                    if i < 6:  # ì•ˆì „ ê²€ì‚¬
                        position_numbers[i].append(num)

            # ìœ„ì¹˜ë³„ Z-ì ìˆ˜ ê³„ì‚°
            z_scores = {}
            for i in range(6):
                if position_numbers[i]:
                    mean = np.mean(position_numbers[i])
                    std = np.std(position_numbers[i])
                    # í‘œì¤€í¸ì°¨ê°€ 0ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ Z-ì ìˆ˜ ê³„ì‚°
                    if std > 0 and i < len(latest_numbers):
                        z_score = (latest_numbers[i] - mean) / std
                    else:
                        z_score = 0.0
                else:
                    z_score = 0.0

                z_scores[f"zscore_num{i+1}"] = float(z_score)

            return z_scores

        except Exception as e:
            self.logger.error(f"ìœ„ì¹˜ë³„ Z-ì ìˆ˜ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return {f"zscore_num{i+1}": 0.0 for i in range(6)}

    def _calculate_binomial_match_score(
        self, historical_data: List[LotteryNumber]
    ) -> float:
        """
        ê° ë‹¹ì²¨ ì¡°í•©ì´ ì´ë¡ ì  ì´í•­ë¶„í¬ì™€ ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ëŠ”ì§€ ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

        Args:
            historical_data: ë¶„ì„í•  ê³¼ê±° ë‹¹ì²¨ ë²ˆí˜¸ ëª©ë¡

        Returns:
            float: ì´í•­ë¶„í¬ ë§¤ì¹­ ì ìˆ˜ (0~1)
        """
        try:
            # ìµœê·¼ íšŒì°¨ì˜ ë²ˆí˜¸
            latest_draw = historical_data[-1] if historical_data else None

            if not latest_draw or not latest_draw.numbers:
                return 0.0

            # ì´ë¡ ì  ì´í•­ë¶„í¬ ìƒì„± (45ê°œ ë²ˆí˜¸ì— ëŒ€í•œ)
            # ì¤‘ì•™ì— ê°€ì¥ ë†’ì€ í™•ë¥ , ì–‘ ëì— ë‚®ì€ í™•ë¥ ì„ ê°€ì§€ëŠ” ë¶„í¬
            x = np.arange(1, 46)
            theoretical_dist = self._generate_binomial_distribution(45)

            # ì‹¤ì œ ë²ˆí˜¸ ë¶„í¬ (íˆìŠ¤í† ê·¸ë¨)
            actual_hist = np.zeros(45)
            for num in latest_draw.numbers:
                if 1 <= num <= 45:
                    actual_hist[num - 1] = 1

            # ì •ê·œí™”
            theoretical_dist_norm = theoretical_dist / np.sum(theoretical_dist)
            actual_hist_norm = actual_hist / np.sum(actual_hist)

            # KL ë°œì‚° ê³„ì‚° (ì‘ì„ìˆ˜ë¡ ìœ ì‚¬)
            # 0ì— ê°€ê¹Œìš´ ê°’ì„ í”¼í•˜ê¸° ìœ„í•´ ìŠ¤ë¬´ë”© ì ìš©
            epsilon = 1e-10
            actual_hist_smoothed = actual_hist_norm + epsilon
            theoretical_dist_smoothed = theoretical_dist_norm + epsilon

            # ì •ê·œí™”
            actual_hist_smoothed = actual_hist_smoothed / np.sum(actual_hist_smoothed)
            theoretical_dist_smoothed = theoretical_dist_smoothed / np.sum(
                theoretical_dist_smoothed
            )

            # KL ë°œì‚° ê³„ì‚°
            kl_div = np.sum(
                actual_hist_smoothed
                * np.log(actual_hist_smoothed / theoretical_dist_smoothed)
            )

            # ì ìˆ˜ë¡œ ë³€í™˜ (KL ë°œì‚°ì´ ì‘ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)
            score = 1.0 / (1.0 + kl_div)

            return float(score)

        except Exception as e:
            self.logger.error(f"ì´í•­ë¶„í¬ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return 0.0

    def _generate_binomial_distribution(self, size: int) -> np.ndarray:
        """
        ì´í•­ë¶„í¬ì™€ ìœ ì‚¬í•œ í™•ë¥  ë¶„í¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            size: ë¶„í¬ í¬ê¸°

        Returns:
            np.ndarray: ì´í•­ë¶„í¬ í™•ë¥  ë²¡í„°
        """
        # ì´í•­ë¶„í¬ íŒŒë¼ë¯¸í„°
        n = size - 1
        p = 0.5

        # ì´í•­ë¶„í¬ í™•ë¥  ì§ˆëŸ‰ í•¨ìˆ˜
        x = np.arange(size)
        from scipy.stats import binom

        probabilities = binom.pmf(x, n, p)

        return probabilities

    def _calculate_number_std_score(
        self, historical_data: List[LotteryNumber]
    ) -> float:
        """
        ê° ë‹¹ì²¨ ì¡°í•©ì˜ ë²ˆí˜¸ í‘œì¤€í¸ì°¨ ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

        Args:
            historical_data: ë¶„ì„í•  ê³¼ê±° ë‹¹ì²¨ ë²ˆí˜¸ ëª©ë¡

        Returns:
            float: ë²ˆí˜¸ í‘œì¤€í¸ì°¨ ì ìˆ˜ (0~1)
        """
        try:
            # ëª¨ë“  íšŒì°¨ì˜ ë²ˆí˜¸ í‘œì¤€í¸ì°¨ ê³„ì‚°
            std_scores = []

            # ì´ë¡ ì  ìµœëŒ€ í‘œì¤€í¸ì°¨ (1,2,3,4,5,45 ì¡°í•©)
            max_theoretical_std = np.std([1, 2, 3, 4, 5, 45])

            for draw in historical_data:
                if not draw.numbers:
                    continue

                # ë²ˆí˜¸ í‘œì¤€í¸ì°¨ ê³„ì‚°
                std_dev = float(np.std(draw.numbers))

                # ì •ê·œí™” (ìµœëŒ€ ì´ë¡ ê°’ìœ¼ë¡œ ë‚˜ëˆ”)
                normalized_score = std_dev / max_theoretical_std
                std_scores.append(min(1.0, normalized_score))

            # í‰ê·  ì ìˆ˜ ê³„ì‚°
            if std_scores:
                return float(np.mean(std_scores))
            else:
                return 0.0

        except Exception as e:
            self.logger.error(f"ë²ˆí˜¸ í‘œì¤€í¸ì°¨ ì ìˆ˜ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return 0.0

    def _calculate_weighted_frequencies(
        self, data: List[LotteryNumber]
    ) -> Dict[int, float]:
        """
        ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ ë²ˆí˜¸ ì¶œí˜„ ë¹ˆë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        ì¥ê¸°, ì¤‘ê¸°, ë‹¨ê¸°ë¡œ êµ¬ë¶„í•˜ì—¬ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•©ë‹ˆë‹¤.

        Args:
            data: ë¶„ì„í•  ê³¼ê±° ë‹¹ì²¨ ë²ˆí˜¸ ëª©ë¡

        Returns:
            ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ ë²ˆí˜¸ë³„ ì¶œí˜„ ë¹ˆë„ (0~1 ì‚¬ì´ì˜ ê°’)
        """
        if not data:
            return {num: 1.0 / 45 for num in range(1, 46)}

        # ì„¤ì •ì—ì„œ ê°€ì¤‘ì¹˜ ê°€ì ¸ì˜¤ê¸°
        try:
            long_term_weight = self.config["frequency_weights"]["long_term"]
            mid_term_weight = self.config["frequency_weights"]["mid_term"]
            short_term_weight = self.config["frequency_weights"]["short_term"]

            # ê¸°ê°„ ë¹„ìœ¨ ì„¤ì •
            mid_term_ratio = self.config["periods"][
                "mid_term"
            ]  # ì¤‘ê¸° = ì „ì²´ ë°ì´í„°ì˜ 2.5%
            short_term_ratio = self.config["periods"][
                "short_term"
            ]  # ë‹¨ê¸° = ì „ì²´ ë°ì´í„°ì˜ 1.2%
        except KeyError as e:
            raise KeyError(f"í•„ìˆ˜ ì„¤ì •ê°’ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {e}")

        # ê° ê¸°ê°„ë³„ ë°ì´í„° ë¶„í• 
        total_count = len(data)
        mid_idx = max(1, int(total_count * (1 - mid_term_ratio)))
        short_idx = max(1, int(total_count * (1 - short_term_ratio)))

        long_term_data = data[:]  # ì „ì²´ ë°ì´í„°
        mid_term_data = data[mid_idx:]  # ì¤‘ê¸° ë°ì´í„°
        short_term_data = data[short_idx:]  # ë‹¨ê¸° ë°ì´í„°

        # ê° ê¸°ê°„ë³„ ë¹ˆë„ ê³„ì‚°
        long_term_freq = self._calculate_frequencies(long_term_data)
        mid_term_freq = self._calculate_frequencies(mid_term_data)
        short_term_freq = self._calculate_frequencies(short_term_data)

        # ê°€ì¤‘ì¹˜ ì ìš©í•œ ë¹ˆë„ ê³„ì‚°
        weighted_freq = {}
        for num in range(1, 46):
            weighted_freq[num] = (
                long_term_weight * long_term_freq.get(num, 0)
                + mid_term_weight * mid_term_freq.get(num, 0)
                + short_term_weight * short_term_freq.get(num, 0)
            )

        return weighted_freq

    def _calculate_recency_map(self, data: List[LotteryNumber]) -> Dict[int, float]:
        """ìµœê·¼ì„± ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        # ì´ˆê¸° ì„¤ì •
        total_draws = len(data)
        recency_map = {num: 1.0 for num in range(1, 46)}  # ëª¨ë“  ë²ˆí˜¸ì˜ ì´ˆê¸° ì ìˆ˜ëŠ” 1.0

        # ê°€ì¥ ìµœê·¼ ë°ì´í„°ê°€ ê°€ì¥ í° ê°€ì¤‘ì¹˜ë¥¼ ê°–ë„ë¡ í•¨
        for i, draw in enumerate(data):
            normalized_weight = i / total_draws  # 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì˜¤ë˜ëœ ë°ì´í„°

            # ê° ë²ˆí˜¸ë§ˆë‹¤ ë“±ì¥ ìœ„ì¹˜ì— ë”°ë¥¸ ìµœê·¼ì„± ì ìˆ˜ ê³„ì‚°
            for num in draw.numbers:
                current_score = recency_map.get(num, 1.0)
                # ìµœê·¼ì— ë“±ì¥í• ìˆ˜ë¡ ì ìˆ˜ê°€ ë‚®ì•„ì§ (ê°€ì¤‘ì¹˜ëŠ” ë°ì´í„°ê°€ ìµœì‹ ì¼ìˆ˜ë¡ ì¦ê°€)
                recency_map[num] = min(current_score, 1.0 - normalized_weight)

        # ì ìˆ˜ ë²”ìœ„ ì •ê·œí™” (0.1 ~ 1.0)
        min_score = min(recency_map.values()) if recency_map else 0.1
        max_score = max(recency_map.values()) if recency_map else 1.0
        score_range = max_score - min_score

        if score_range > 0:
            for num in recency_map:
                normalized_score = (recency_map[num] - min_score) / score_range
                recency_map[num] = 0.1 + normalized_score * 0.9  # 0.1 ~ 1.0 ë²”ìœ„ë¡œ ì¡°ì •

        return recency_map

    def _calculate_frequencies(self, data: List[LotteryNumber]) -> Dict[int, float]:
        """ê° ë²ˆí˜¸ì˜ ë¹ˆë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        number_counts = {}
        total_draws = len(data)

        for draw in data:
            for num in draw.numbers:
                number_counts[num] = number_counts.get(num, 0) + 1

        return {num: count / total_draws for num, count in number_counts.items()}

    def _detect_trending_numbers(self, data: List[LotteryNumber]) -> List[int]:
        """ì¶œí˜„ ë¹ˆë„ê°€ ê¾¸ì¤€íˆ ì¦ê°€í•˜ëŠ” ë²ˆí˜¸ë¥¼ ê°ì§€í•©ë‹ˆë‹¤."""
        total_draws = len(data)
        window_size = max(
            10, int(total_draws * 0.05)
        )  # ë°ì´í„°ì˜ 5%ë¥¼ íŠ¸ë Œë“œ ë¶„ì„ì— ì‚¬ìš©
        windows = []

        # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ìƒì„±
        for i in range(0, total_draws - window_size + 1, window_size // 2):
            window_data = data[i : i + window_size]
            window_freq = self._calculate_frequencies(window_data)
            windows.append(window_freq)

        # íŠ¸ë Œë“œ ë¶„ì„
        trending_numbers = []
        for num in range(1, 46):
            appearances = [freq.get(num, 0) for freq in windows]
            if len(appearances) < 2:
                continue

            # ì„ í˜• íšŒê·€ë¥¼ ì‚¬ìš©í•œ íŠ¸ë Œë“œ ê³„ì‚°
            x = np.arange(len(appearances))
            slope = np.polyfit(x, appearances, 1)[0]

            # íŠ¸ë Œë“œê°€ ì¼ê´€ë˜ê²Œ ì–‘ìˆ˜ì¸ì§€ í™•ì¸
            if slope > 0 and all(
                appearances[i] <= appearances[i + 1]
                for i in range(len(appearances) - 1)
            ):
                trending_numbers.append(num)

        return trending_numbers

    def _identify_hot_cold_numbers(
        self, frequency: Dict[int, float]
    ) -> Tuple[Set[int], Set[int]]:
        """ë¹ˆë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¸ê¸° ë° ë¹„ì¸ê¸° ë²ˆí˜¸ë¥¼ ì‹ë³„í•©ë‹ˆë‹¤."""
        sorted_numbers = sorted(frequency.items(), key=lambda x: x[1], reverse=True)
        hot_threshold = np.percentile([f for _, f in sorted_numbers], 75)
        cold_threshold = np.percentile([f for _, f in sorted_numbers], 25)

        hot_numbers = {num for num, freq in sorted_numbers if freq >= hot_threshold}
        cold_numbers = {num for num, freq in sorted_numbers if freq <= cold_threshold}

        return hot_numbers, cold_numbers

    def _find_number_clusters(self, data: List[LotteryNumber]) -> Dict[str, Any]:
        """
        ë²ˆí˜¸ ê°„ ë™ì‹œ ì¶œí˜„ íŒ¨í„´ì„ ê¸°ë°˜ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë¥¼ ì°¾ìŠµë‹ˆë‹¤.

        KMeans í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ë²ˆí˜¸ë¥¼ ê·¸ë£¹í™”í•©ë‹ˆë‹¤.

        Returns:
            Dict[str, Any]: í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼
                - clusters: í´ëŸ¬ìŠ¤í„° ëª©ë¡ [[ë²ˆí˜¸1, ë²ˆí˜¸2, ...], ...]
                - cluster_labels: ê° ë²ˆí˜¸ê°€ ì†í•œ í´ëŸ¬ìŠ¤í„° ID (ë²ˆí˜¸ -> í´ëŸ¬ìŠ¤í„° ID ë§¤í•‘)
                - cluster_size_distribution: ê° í´ëŸ¬ìŠ¤í„°ì˜ í¬ê¸°
                - cooccurrence_matrix: ë™ì‹œ ì¶œí˜„ í–‰ë ¬ (ë¶„ì„ìš©)
        """
        try:
            self.logger.info("KMeans ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ë²ˆí˜¸ í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘...")

            # 1. ë™ì‹œ ì¶œí˜„ í–‰ë ¬ ìƒì„±
            cooccurrence_matrix = np.zeros((45, 45), dtype=np.float32)

            # ëª¨ë“  ë‹¹ì²¨ ì„¸íŠ¸ì—ì„œ ë™ì‹œ ì¶œí˜„ íšŸìˆ˜ ê³„ì‚°
            for draw in data:
                numbers = [num - 1 for num in draw.numbers]  # 0-ì¸ë±ìŠ¤ë¡œ ë³€í™˜
                for i in range(len(numbers)):
                    for j in range(i + 1, len(numbers)):
                        cooccurrence_matrix[numbers[i], numbers[j]] += 1
                        cooccurrence_matrix[numbers[j], numbers[i]] += 1

            # ë™ì‹œ ì¶œí˜„ í–‰ë ¬ ì •ê·œí™”
            total_draws = len(data)
            if total_draws > 0:
                cooccurrence_matrix /= total_draws

            # 2. KMeans í´ëŸ¬ìŠ¤í„°ë§ ì ìš©
            from sklearn.cluster import KMeans
            from sklearn.preprocessing import StandardScaler

            # íŠ¹ì„± ì •ê·œí™”
            scaler = StandardScaler()
            scaled_matrix = scaler.fit_transform(cooccurrence_matrix)

            # ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ê²°ì • (5-9 ì‚¬ì´ì—ì„œ ì‹¤ë£¨ì—£ ì ìˆ˜ ê¸°ì¤€)
            from sklearn.metrics import silhouette_score

            best_n_clusters = 5
            best_score = -1

            for n_clusters in range(5, 10):
                try:
                    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
                    cluster_labels = kmeans.fit_predict(scaled_matrix)

                    # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ë§Œ ì‹¤ë£¨ì—£ ì ìˆ˜ ê³„ì‚°
                    if len(set(cluster_labels)) > 1:
                        score = silhouette_score(scaled_matrix, cluster_labels)
                        self.logger.info(
                            f"í´ëŸ¬ìŠ¤í„° ìˆ˜ {n_clusters}ì˜ ì‹¤ë£¨ì—£ ì ìˆ˜: {score:.4f}"
                        )

                        if score > best_score:
                            best_score = score
                            best_n_clusters = n_clusters
                except Exception as e:
                    self.logger.warning(f"í´ëŸ¬ìŠ¤í„° ìˆ˜ {n_clusters} í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")

            # ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ë¡œ ìµœì¢… í´ëŸ¬ìŠ¤í„°ë§
            kmeans = KMeans(n_clusters=best_n_clusters, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(scaled_matrix)

            # 3. ê²°ê³¼ í¬ë§·íŒ…
            # í´ëŸ¬ìŠ¤í„° ëª©ë¡ êµ¬ì„± (1-ì¸ë±ìŠ¤ë¡œ ë³€í™˜)
            clusters = [[] for _ in range(best_n_clusters)]
            for i in range(45):
                clusters[cluster_labels[i]].append(i + 1)  # 1-ì¸ë±ìŠ¤ë¡œ ë³€í™˜

            # í´ëŸ¬ìŠ¤í„° ID ë§µí•‘
            cluster_labels_map = {str(i + 1): int(cluster_labels[i]) for i in range(45)}

            # í´ëŸ¬ìŠ¤í„° í¬ê¸° ë¶„í¬
            cluster_size_distribution = {
                f"cluster_{i}": len(cluster) for i, cluster in enumerate(clusters)
            }

            self.logger.info(
                f"ë²ˆí˜¸ í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ: {best_n_clusters}ê°œ í´ëŸ¬ìŠ¤í„°, ì‹¤ë£¨ì—£ ì ìˆ˜: {best_score:.4f}"
            )

            # 4. í´ëŸ¬ìŠ¤í„° ë°°ì • ê²°ê³¼ ì €ì¥ (ì„ íƒì )
            try:
                import os
                import json

                project_root = os.path.dirname(
                    os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
                )

                # ê²°ê³¼ ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
                data_dir = os.path.join(project_root, "data")
                os.makedirs(data_dir, exist_ok=True)

                # í´ëŸ¬ìŠ¤í„° ë°°ì • ì €ì¥
                cluster_file = os.path.join(data_dir, "cluster_assignments.json")
                with open(cluster_file, "w") as f:
                    json.dump({"number_clusters": cluster_labels_map}, f, indent=2)

                self.logger.info(f"í´ëŸ¬ìŠ¤í„° ë°°ì • ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {cluster_file}")
            except Exception as e:
                self.logger.warning(f"í´ëŸ¬ìŠ¤í„° ë°°ì • ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")

            return {
                "clusters": clusters,
                "cluster_labels": cluster_labels_map,
                "cluster_size_distribution": cluster_size_distribution,
                "cooccurrence_matrix": cooccurrence_matrix.tolist(),
            }

        except Exception as e:
            self.logger.error(f"ë²ˆí˜¸ í´ëŸ¬ìŠ¤í„°ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
            default_clusters = [
                [i for i in range(1, 10)],
                [i for i in range(10, 19)],
                [i for i in range(19, 28)],
                [i for i in range(28, 37)],
                [i for i in range(37, 46)],
            ]

            default_labels = {str(i): (i - 1) // 9 for i in range(1, 46)}
            default_distribution = {
                f"cluster_{i}": len(cluster)
                for i, cluster in enumerate(default_clusters)
            }

            return {
                "clusters": default_clusters,
                "cluster_labels": default_labels,
                "cluster_size_distribution": default_distribution,
                "cooccurrence_matrix": [[0.0 for _ in range(45)] for _ in range(45)],
            }

    def _calculate_roi_metrics(
        self, data: List[LotteryNumber]
    ) -> Dict[Tuple[int, int], float]:
        """
        ë²ˆí˜¸ ìŒì˜ íŒ¨í„´ ë©”íŠ¸ë¦­ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
        ì´ ë©”íŠ¸ë¦­ì€ íŠ¹ì • ë²ˆí˜¸ ìŒì´ í¬í•¨ëœ ì¶”ì²¨ ë²ˆí˜¸ì˜ íŒ¨í„´ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

        ì˜ˆì „ì—ëŠ” ROI ë©”íŠ¸ë¦­ì´ì—ˆìœ¼ë‚˜, ìˆœìˆ˜ íŒ¨í„´ ê¸°ë°˜ ë¶„ì„ìœ¼ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.

        Args:
            data: ë¶„ì„í•  ë¡œë˜ ë‹¹ì²¨ ë²ˆí˜¸ ëª©ë¡

        Returns:
            ë²ˆí˜¸ ìŒë³„ íŒ¨í„´ ë©”íŠ¸ë¦­
        """
        # ê° ë²ˆí˜¸ ìŒì˜ ì¶œí˜„ íšŸìˆ˜ ê³„ì‚°
        pair_occurrences = {}
        total_draws = len(data)

        # ëª¨ë“  ê°€ëŠ¥í•œ ë²ˆí˜¸ ìŒì— ëŒ€í•œ íŒ¨í„´ ê³„ì‚° ì¤€ë¹„
        for i in range(1, 46):
            for j in range(i + 1, 46):
                pair = (i, j)
                pair_occurrences[pair] = 0

        # ê³¼ê±° ì¶”ì²¨ì—ì„œ ë²ˆí˜¸ ìŒì˜ ì¶œí˜„ íšŸìˆ˜ ê³„ì‚°
        for draw in data:
            numbers = draw.numbers
            for i in range(len(numbers)):
                for j in range(i + 1, len(numbers)):
                    num1, num2 = min(numbers[i], numbers[j]), max(
                        numbers[i], numbers[j]
                    )
                    pair = (num1, num2)
                    pair_occurrences[pair] = pair_occurrences.get(pair, 0) + 1

        # íŒ¨í„´ ì ìˆ˜ ê³„ì‚° (ì´ì „ì˜ ROI ëŒ€ì‹  íŒ¨í„´ ì¤‘ìš”ë„ ì ìˆ˜)
        pair_pattern_score = {}
        for pair, occurrences in pair_occurrences.items():
            if total_draws > 0:
                frequency = occurrences / total_draws
                # ìˆ˜ì •: ë‹¨ìˆœ ë¹ˆë„ ê¸°ë°˜ ì ìˆ˜ë¡œ ëŒ€ì²´
                significance = 1.0 + frequency * 5.0  # ë¹ˆë„ê°€ ë†’ì„ìˆ˜ë¡ ì¤‘ìš”ë„ ì¦ê°€
                pair_pattern_score[pair] = significance
            else:
                pair_pattern_score[pair] = 1.0

        return pair_pattern_score

    def analyze_consecutive_length_distribution(
        self, data: List[LotteryNumber]
    ) -> PatternAnalysis:
        """
        ì—°ì† ë²ˆí˜¸ ê¸¸ì´ ë¶„í¬ ë¶„ì„

        Args:
            data: ë¡œë˜ ë‹¹ì²¨ ë²ˆí˜¸ ë°ì´í„° ëª©ë¡

        Returns:
            íŒ¨í„´ ë¶„ì„ ê²°ê³¼ (consecutive_length_distribution í¬í•¨)
        """
        # ê²°ê³¼ ê°ì²´ ì´ˆê¸°í™”
        result = PatternAnalysis(
            frequency_map={},
            recency_map={},
        )

        # ê° ë‹¹ì²¨ ë²ˆí˜¸ ì„¸íŠ¸ì˜ ìµœëŒ€ ì—°ì† ë²ˆí˜¸ ê¸¸ì´ ê³„ì‚°
        consecutive_counts = {}  # ê¸¸ì´ë³„ ê°œìˆ˜ ì¹´ìš´íŠ¸

        for draw in data:
            # ì •ë ¬ëœ ë²ˆí˜¸ ëª©ë¡
            sorted_numbers = sorted(draw.numbers)

            # ìµœëŒ€ ì—°ì† ë²ˆí˜¸ ê¸¸ì´ ê³„ì‚°
            max_length = self.get_max_consecutive_length(sorted_numbers)

            # ì¹´ìš´íŠ¸ ì¦ê°€
            if max_length in consecutive_counts:
                consecutive_counts[max_length] += 1
            else:
                consecutive_counts[max_length] = 1

        # ì „ì²´ ë°ì´í„° ìˆ˜
        total_count = len(data)

        # ê° ê¸¸ì´ë³„ ë¹„ìœ¨ ê³„ì‚° - í‚¤ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
        consecutive_distribution = {}
        for length, count in consecutive_counts.items():
            # ë¬¸ìì—´ í‚¤ë¡œ ì €ì¥
            consecutive_distribution[str(length)] = count / total_count

        # ê²°ê³¼ ì €ì¥
        result.metadata = {}
        result.metadata["consecutive_length_distribution"] = consecutive_distribution

        # ë‚´ë¶€ ìƒíƒœì—ë„ ì €ì¥
        self.pattern_stats = {}
        self.pattern_stats["consecutive_length_distribution"] = consecutive_distribution

        return result

    def get_max_consecutive_length(self, numbers: List[int]) -> int:
        """
        ë²ˆí˜¸ ëª©ë¡ì—ì„œ ìµœëŒ€ ì—°ì† ë²ˆí˜¸ ê¸¸ì´ ì°¾ê¸°

        Args:
            numbers: ì •ë ¬ëœ ë²ˆí˜¸ ëª©ë¡

        Returns:
            ìµœëŒ€ ì—°ì† ë²ˆí˜¸ ê¸¸ì´
        """
        if not numbers:
            return 0

        sorted_numbers = sorted(numbers)
        max_length = 1
        current_length = 1

        for i in range(1, len(sorted_numbers)):
            if sorted_numbers[i] == sorted_numbers[i - 1] + 1:
                current_length += 1
                max_length = max(max_length, current_length)
            else:
                current_length = 1

        return max_length

    def score_by_consecutive_pattern(self, numbers: List[int]) -> float:
        """
        ì—°ì†ëœ ë²ˆí˜¸ íŒ¨í„´ì— ë”°ë¼ ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        ì—°ì† ë²ˆí˜¸ê°€ ë§ì„ìˆ˜ë¡ ë‚®ì€ ì ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

        Args:
            numbers: ì ìˆ˜ë¥¼ ê³„ì‚°í•  ë²ˆí˜¸ ëª©ë¡

        Returns:
            0~1 ì‚¬ì´ì˜ ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì„ í˜¸ë˜ëŠ” íŒ¨í„´)
        """
        # ì„¤ì •ì—ì„œ ìµœëŒ€ ì—°ì† ë²ˆí˜¸ ê°œìˆ˜ ê°€ì ¸ì˜¤ê¸°
        try:
            max_consecutive = self.config["filters"]["max_consecutive"]
        except KeyError:
            raise KeyError("ì„¤ì •ì—ì„œ 'filters.max_consecutive' í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # ì—°ì† ë²ˆí˜¸ ê¸¸ì´ ê³„ì‚°
        consecutive_length = self.get_max_consecutive_length(sorted(numbers))

        # ì—°ì† ë²ˆí˜¸ê°€ ì—†ëŠ” ê²½ìš°
        if consecutive_length <= 1:
            return 1.0

        # ìµœëŒ€ í—ˆìš© ì—°ì† ë²ˆí˜¸ë¥¼ ì´ˆê³¼í•˜ëŠ” ê²½ìš°
        if consecutive_length > max_consecutive:
            return 0.0

        # ì—°ì† ë²ˆí˜¸ì— ë”°ë¥¸ ê°ì  (2ê°œ ì—°ì†: -0.1, 3ê°œ ì—°ì†: -0.3, 4ê°œ ì—°ì†: -0.6)
        penalty = ((consecutive_length - 1) ** 2) / (max_consecutive**2)

        # ìµœì¢… ì ìˆ˜ ê³„ì‚° (1 - íŒ¨ë„í‹°)
        return max(0.0, 1.0 - penalty)

    def extract_pattern_features(
        self, numbers: list[int], historical_data: Optional[list[LotteryNumber]] = None
    ) -> dict[str, Any]:
        """
        ë¡œë˜ ë²ˆí˜¸ ì¡°í•©ì—ì„œ íŒ¨í„´ íŠ¹ì„± ë²¡í„° ì¶”ì¶œ

        Args:
            numbers: ë¶„ì„í•  ë²ˆí˜¸ ëª©ë¡
            historical_data: ì°¸ì¡°í•  ê³¼ê±° ë°ì´í„° (ì„ íƒ ì‚¬í•­)

        Returns:
            ì¶”ì¶œëœ íŒ¨í„´ íŠ¹ì„± ë²¡í„°
        """
        # ì •ë ¬ëœ ë²ˆí˜¸
        sorted_numbers = sorted(numbers)

        # 1. ìµœëŒ€ ì—°ì† ê¸¸ì´
        max_consecutive_length = self.get_max_consecutive_length(sorted_numbers)

        # 2. ë²ˆí˜¸ í•©ê³„
        total_sum = sum(sorted_numbers)

        # 3. í™€ìˆ˜/ì§ìˆ˜ ê°œìˆ˜
        odd_count = sum(1 for num in sorted_numbers if num % 2 == 1)
        even_count = len(sorted_numbers) - odd_count

        # 4. ê°„ê²© í‰ê·  ë° í‘œì¤€í¸ì°¨
        gaps = [
            sorted_numbers[i + 1] - sorted_numbers[i]
            for i in range(len(sorted_numbers) - 1)
        ]
        gap_avg = sum(gaps) / len(gaps) if gaps else 0
        gap_std = np.std(gaps) if gaps else 0

        # 5. ë²”ìœ„ë³„ ë¶„í¬ (1-9, 10-19, 20-29, 30-39, 40-45)
        range_counts = [0, 0, 0, 0, 0]
        for num in sorted_numbers:
            if 1 <= num <= 9:
                range_counts[0] += 1
            elif 10 <= num <= 19:
                range_counts[1] += 1
            elif 20 <= num <= 29:
                range_counts[2] += 1
            elif 30 <= num <= 39:
                range_counts[3] += 1
            else:  # 40-45
                range_counts[4] += 1

        # ê³ ê¸‰ íŠ¹ì„±ë“¤ (ê³¼ê±° ë°ì´í„°ê°€ ìˆì„ ê²½ìš°)
        cluster_overlap_ratio = 0.0
        frequent_pair_score = 0.0
        roi_weight = 0.0

        if historical_data:
            # íŒ¨í„´ ë¶„ì„ ìˆ˜í–‰
            pattern_analysis = self.analyze(historical_data)

            # 6. í´ëŸ¬ìŠ¤í„° ì˜¤ë²„ë© ë¹„ìœ¨
            if pattern_analysis.clusters:
                overlap_count = 0
                for cluster in pattern_analysis.clusters:
                    cluster_set = set(cluster)
                    numbers_set = set(sorted_numbers)
                    overlap = cluster_set.intersection(numbers_set)
                    if len(overlap) >= 2:  # í´ëŸ¬ìŠ¤í„°ì™€ 2ê°œ ì´ìƒ ì¼ì¹˜
                        overlap_count += len(overlap)

                cluster_overlap_ratio = (
                    overlap_count / len(sorted_numbers) if overlap_count > 0 else 0
                )

            # 7. ë¹ˆë„ í˜ì–´ ì ìˆ˜
            if pattern_analysis.pair_frequency:
                pair_scores = []
                for i in range(len(sorted_numbers)):
                    for j in range(i + 1, len(sorted_numbers)):
                        num1, num2 = min(sorted_numbers[i], sorted_numbers[j]), max(
                            sorted_numbers[i], sorted_numbers[j]
                        )
                        pair = (num1, num2)
                        if pair in pattern_analysis.pair_frequency:
                            pair_scores.append(pattern_analysis.pair_frequency[pair])

                frequent_pair_score = (
                    sum(pair_scores) / len(pair_scores) if pair_scores else 0
                )

            # 8. ROI ê°€ì¤‘ì¹˜ (ìˆ˜ìµì„± ì ìˆ˜)
            if pattern_analysis.roi_matrix:
                roi_scores = []
                for i in range(len(sorted_numbers)):
                    for j in range(i + 1, len(sorted_numbers)):
                        num1, num2 = min(sorted_numbers[i], sorted_numbers[j]), max(
                            sorted_numbers[i], sorted_numbers[j]
                        )
                        pair = (num1, num2)
                        if pair in pattern_analysis.roi_matrix:
                            roi_scores.append(pattern_analysis.roi_matrix[pair])

                roi_weight = sum(roi_scores) / len(roi_scores) if roi_scores else 0

        # 9. ì—°ì† ë²ˆí˜¸ ì ìˆ˜
        consecutive_score = self.score_by_consecutive_pattern(sorted_numbers)

        # 10. ìƒˆë¡œìš´ íŠ¹ì„±: íŠ¸ë Œë“œ ì ìˆ˜ (ê³¼ê±° ë°ì´í„°ê°€ ìˆì„ ê²½ìš°)
        trend_score_avg = 0.0
        trend_score_max = 0.0
        trend_score_min = 0.0

        if historical_data and len(historical_data) > 0:
            # íŠ¸ë Œë“œ ì ìˆ˜ ê³„ì‚°
            trend_scores = self.get_number_trend_scores(historical_data)
            number_trend_scores = [trend_scores.get(num, 0.0) for num in sorted_numbers]

            if number_trend_scores:
                trend_score_avg = sum(number_trend_scores) / len(number_trend_scores)
                trend_score_max = max(number_trend_scores)
                trend_score_min = min(number_trend_scores)

        # 11. ìƒˆë¡œìš´ íŠ¹ì„±: ìœ„í—˜ë„ ì ìˆ˜
        risk_score = self.calculate_risk_score(sorted_numbers, historical_data)

        # ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ìƒì„±
        result = {
            "max_consecutive_length": max_consecutive_length,
            "total_sum": total_sum,
            "odd_count": odd_count,
            "even_count": even_count,
            "gap_avg": gap_avg,
            "gap_std": gap_std,
            "range_counts": range_counts,
            "cluster_overlap_ratio": cluster_overlap_ratio,
            "frequent_pair_score": frequent_pair_score,
            "roi_weight": roi_weight,
            "consecutive_score": consecutive_score,
            "trend_score_avg": trend_score_avg,
            "trend_score_max": trend_score_max,
            "trend_score_min": trend_score_min,
            "risk_score": risk_score,
        }

        return result

    def vectorize_pattern_features(
        self, features: Union[PatternFeatures, Dict[str, Any]]
    ) -> np.ndarray:
        """
        íŒ¨í„´ íŠ¹ì„± ë”•ì…”ë„ˆë¦¬ë¥¼ ëª¨ë¸ ì…ë ¥ìš© íŠ¹ì„± ë²¡í„°ë¡œ ë³€í™˜

        Args:
            features: extract_pattern_features()ë¡œ ì¶”ì¶œí•œ íŠ¹ì„± ë”•ì…”ë„ˆë¦¬

        Returns:
            ê³ ì • ê¸¸ì´ ë²¡í„° (numpy ë°°ì—´)
        """
        # ì•ˆì „í•˜ê²Œ ê°’ ì¶”ì¶œ (ê¸°ë³¸ê°’ ì œê³µ)
        max_consecutive_length = features.get("max_consecutive_length", 0)
        total_sum = features.get("total_sum", 0)
        odd_count = features.get("odd_count", 0)
        even_count = features.get("even_count", 0)
        gap_avg = features.get("gap_avg", 0.0)
        gap_std = features.get("gap_std", 0.0)
        range_counts = features.get("range_counts", [0, 0, 0, 0, 0])
        cluster_overlap_ratio = features.get("cluster_overlap_ratio", 0.0)
        frequent_pair_score = features.get("frequent_pair_score", 0.0)
        roi_weight = features.get("roi_weight", 0.0)
        consecutive_score = features.get("consecutive_score", 0.0)
        trend_score_avg = features.get("trend_score_avg", 0.0)
        trend_score_max = features.get("trend_score_max", 0.0)
        trend_score_min = features.get("trend_score_min", 0.0)
        risk_score = features.get("risk_score", 0.0)

        # ê³ ì •ëœ ìˆœì„œë¡œ íŠ¹ì„± ì¶”ì¶œ (ì¼ê´€ì„±ì„ ìœ„í•´)
        feature_vector = np.array(
            [
                max_consecutive_length / 6.0,  # ì •ê·œí™”: ìµœëŒ€ 6
                total_sum
                / 270.0,  # ì •ê·œí™”: ìµœëŒ€ í•©ê³„ (45+44+43+42+41+40=255, ì•½ê°„ ì—¬ìœ )
                odd_count / 6.0,
                even_count / 6.0,
                gap_avg / 20.0,  # ì •ê·œí™”: ì¼ë°˜ì ì¸ ìµœëŒ€ ê°„ê²©
                gap_std / 15.0,  # ì •ê·œí™”: ì¼ë°˜ì ì¸ ìµœëŒ€ í‘œì¤€í¸ì°¨
                *[count / 6.0 for count in range_counts],  # 5ê°œ ìš”ì†Œ
                cluster_overlap_ratio,  # ì´ë¯¸ 0-1 ë²”ìœ„
                frequent_pair_score * 10.0,  # ê°•í™”: ì¼ë°˜ì ìœ¼ë¡œ ì‘ì€ ê°’
                roi_weight / 2.0,  # ì •ê·œí™”: ì¼ë°˜ì ì¸ ìµœëŒ€ê°’
                consecutive_score + 0.3,  # ì‹œí”„íŠ¸: -0.3~0.25 â†’ 0~0.55
                trend_score_avg * 10.0,  # ê°•í™”: ì¼ë°˜ì ìœ¼ë¡œ ì‘ì€ ê°’
                trend_score_max * 10.0,  # ê°•í™”: ì¼ë°˜ì ìœ¼ë¡œ ì‘ì€ ê°’
                trend_score_min * 10.0,  # ê°•í™”: ì¼ë°˜ì ìœ¼ë¡œ ì‘ì€ ê°’
                risk_score,  # ì´ë¯¸ 0-1 ë²”ìœ„
            ]
        )

        return feature_vector

    def pattern_penalty(self, features: PatternFeatures) -> float:
        """
        íŒ¨í„´ íŠ¹ì„±ì— ë”°ë¥¸ íŒ¨ë„í‹° ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        íŠ¹ì • íŒ¨í„´ì´ í†µê³„ì ìœ¼ë¡œ ë¶ˆë¦¬í•  ê²½ìš° ë†’ì€ íŒ¨ë„í‹°ë¥¼ ì ìš©í•©ë‹ˆë‹¤.

        Args:
            features: íŒ¨í„´ íŠ¹ì„± ê°ì²´

        Returns:
            0~1 ì‚¬ì´ì˜ íŒ¨ë„í‹° ì ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ì„ í˜¸ë˜ëŠ” íŒ¨í„´)
        """
        penalty = 0.0

        # ì—°ì†ëœ ë²ˆí˜¸ì— ëŒ€í•œ íŒ¨ë„í‹°
        if "max_consecutive_length" in features:
            try:
                max_consecutive = self.config["filters"]["max_consecutive"]
            except KeyError:
                max_consecutive = 4  # ê¸°ë³¸ê°’
                self.logger.warning(
                    "'filters.max_consecutive' ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’(4)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
                )

            if features["max_consecutive_length"] > max_consecutive:
                penalty += 0.3
            elif features["max_consecutive_length"] == max_consecutive:
                penalty += 0.1

        # í•©ê³„ì— ëŒ€í•œ íŒ¨ë„í‹°
        if "total_sum" in features:
            try:
                min_sum = self.config["filters"]["min_sum"]
            except KeyError:
                min_sum = 90  # ê¸°ë³¸ê°’
                self.logger.warning(
                    "'filters.min_sum' ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’(90)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
                )

            try:
                max_sum = self.config["filters"]["max_sum"]
            except KeyError:
                max_sum = 210  # ê¸°ë³¸ê°’
                self.logger.warning(
                    "'filters.max_sum' ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’(210)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
                )

            if features["total_sum"] < min_sum or features["total_sum"] > max_sum:
                penalty += 0.3

        # í™€ì§ ë¹„ìœ¨ì— ëŒ€í•œ íŒ¨ë„í‹°
        if "odd_count" in features and "even_count" in features:
            try:
                min_even = self.config["filters"]["min_even_numbers"]
            except KeyError:
                min_even = 2  # ê¸°ë³¸ê°’
                self.logger.warning(
                    "'filters.min_even_numbers' ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’(2)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
                )

            try:
                max_even = self.config["filters"]["max_even_numbers"]
            except KeyError:
                max_even = 4  # ê¸°ë³¸ê°’
                self.logger.warning(
                    "'filters.max_even_numbers' ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’(4)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
                )

            even_count = features["even_count"]
            if even_count < min_even or even_count > max_even:
                penalty += 0.2

        # ë‹¤ë¥¸ íŒ¨í„´ íŠ¹ì„±ì— ë”°ë¥¸ íŒ¨ë„í‹°...
        if "roi_weight" in features and features["roi_weight"] < 0.3:
            penalty += 0.15

        # íŠ¸ë Œë“œ ì ìˆ˜ì— ë”°ë¥¸ íŒ¨ë„í‹°
        if "trend_score_avg" in features:
            try:
                trend_threshold = self.config["pattern_features"]["thresholds"][
                    "trend_score_threshold"
                ]
            except KeyError:
                trend_threshold = 0.75  # ê¸°ë³¸ê°’
                self.logger.warning(
                    "'pattern_features.thresholds.trend_score_threshold' ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’(0.75)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
                )

            if features["trend_score_avg"] < trend_threshold:
                penalty += 0.1

        # ë¦¬ìŠ¤í¬ ì ìˆ˜ì— ë”°ë¥¸ íŒ¨ë„í‹°
        if "risk_score" in features:
            try:
                risk_threshold = self.config["pattern_features"]["thresholds"][
                    "risk_score_threshold"
                ]
            except KeyError:
                risk_threshold = 0.65  # ê¸°ë³¸ê°’
                self.logger.warning(
                    "'pattern_features.thresholds.risk_score_threshold' ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’(0.65)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
                )

            if features["risk_score"] > risk_threshold:
                penalty += 0.2

        # ìµœì¢… íŒ¨ë„í‹°ëŠ” 0~1 ì‚¬ì´ë¡œ ì œí•œ
        return min(1.0, penalty)

    # íŠ¸ë Œë“œ ë¶„ì„ ë©”ì„œë“œ ì¶”ê°€
    def get_number_trend_scores(
        self,
        historical_data: List[LotteryNumber],
        window_size: int = 30,
        alpha: float = 0.1,
    ) -> Dict[int, float]:
        """
        ê° ë²ˆí˜¸ì˜ ì¶œí˜„ íŠ¸ë Œë“œë¥¼ EWMA(Exponentially Weighted Moving Average)ë¥¼ ì‚¬ìš©í•˜ì—¬ ê³„ì‚°í•©ë‹ˆë‹¤.

        Args:
            historical_data: ê³¼ê±° ë‹¹ì²¨ ë²ˆí˜¸ ë°ì´í„°
            window_size: ë¶„ì„í•  ìµœê·¼ íšŒì°¨ ìˆ˜
            alpha: EWMA ê°€ì¤‘ì¹˜ íŒŒë¼ë¯¸í„° (0-1 ì‚¬ì´, ì‘ì„ìˆ˜ë¡ ê³¼ê±° ê°’ì˜ ì˜í–¥ì´ í¼)

        Returns:
            ê° ë²ˆí˜¸ì˜ íŠ¸ë Œë“œ ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ìµœê·¼ì— ìì£¼ ë“±ì¥)
        """
        # ìµœê·¼ window_size íšŒì°¨ ë°ì´í„°ë§Œ ì‚¬ìš©
        recent_data = (
            historical_data[-window_size:]
            if len(historical_data) > window_size
            else historical_data
        )

        # ëª¨ë“  ë²ˆí˜¸ì— ëŒ€í•œ EWMA ê³„ì‚°ì„ ìœ„í•œ ì¤€ë¹„
        number_appearances = {num: [] for num in range(1, 46)}

        # ê° íšŒì°¨ë³„ë¡œ ë²ˆí˜¸ ì¶œí˜„ ì—¬ë¶€ ê¸°ë¡ (1: ì¶œí˜„, 0: ë¯¸ì¶œí˜„)
        for draw in recent_data:
            draw_numbers = set(draw.numbers)
            for num in range(1, 46):
                number_appearances[num].append(1 if num in draw_numbers else 0)

        # EWMA ê³„ì‚°
        trend_scores = {}
        for num in range(1, 46):
            if not number_appearances[num]:
                trend_scores[num] = 0.0
                continue

            # EWMA ê³„ì‚°
            ewma = number_appearances[num][0]
            for i in range(1, len(number_appearances[num])):
                ewma = alpha * number_appearances[num][i] + (1 - alpha) * ewma

            trend_scores[num] = ewma

        # ì •ê·œí™” (í•©ì´ 1ì´ ë˜ë„ë¡)
        total_score = sum(trend_scores.values())
        if total_score > 0:
            trend_scores = {
                num: score / total_score for num, score in trend_scores.items()
            }

        return trend_scores

    # ìœ„í—˜ë„ ì ìˆ˜ ê³„ì‚° ë©”ì„œë“œ ì¶”ê°€
    def calculate_risk_score(
        self, numbers: List[int], historical_data: Optional[List[LotteryNumber]] = None
    ) -> float:
        """
        ë²ˆí˜¸ ì¡°í•©ì˜ ìœ„í—˜ë„ ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

        Args:
            numbers: í‰ê°€í•  ë²ˆí˜¸ ëª©ë¡
            historical_data: ì°¸ì¡°í•  ê³¼ê±° ë°ì´í„° (ì„ íƒ ì‚¬í•­)

        Returns:
            ìœ„í—˜ë„ ì ìˆ˜ (0-1 ì‚¬ì´, 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ„í—˜)
        """
        if not numbers or len(numbers) != 6:
            return 1.0  # ì˜ëª»ëœ í˜•ì‹ì˜ ë²ˆí˜¸ëŠ” ìµœëŒ€ ìœ„í—˜ë„

        sorted_numbers = sorted(numbers)
        risk_score = 0.0

        # 1. í•©ê³„ ìœ„í—˜ë„ (70 ë¯¸ë§Œ ë˜ëŠ” 210 ì´ˆê³¼)
        total_sum = sum(sorted_numbers)
        if total_sum < 70:
            risk_score += 0.25
        elif total_sum > 210:
            risk_score += 0.25
        elif total_sum < 90 or total_sum > 170:
            risk_score += 0.1

        # 2. í™€ì§ ë¶ˆê· í˜• ìœ„í—˜ë„
        odd_count = sum(1 for num in sorted_numbers if num % 2 == 1)
        even_count = 6 - odd_count

        if odd_count == 6 or even_count == 6:
            risk_score += 0.25  # ëª¨ë‘ í™€ìˆ˜ ë˜ëŠ” ëª¨ë‘ ì§ìˆ˜
        elif odd_count == 5 or even_count == 5:
            risk_score += 0.1  # 5:1 ë¶ˆê· í˜•

        # 3. ì—°ì† ë²ˆí˜¸ ìœ„í—˜ë„
        max_consecutive = self.get_max_consecutive_length(sorted_numbers)
        if max_consecutive >= 5:
            risk_score += 0.25  # 5ê°œ ì´ìƒ ì—°ì†
        elif max_consecutive == 4:
            risk_score += 0.15  # 4ê°œ ì—°ì†
        elif max_consecutive == 3:
            risk_score += 0.05  # 3ê°œ ì—°ì†

        # 4. ë²ˆí˜¸ ë¶„í¬ ìœ„í—˜ë„ (1-9, 10-19, 20-29, 30-39, 40-45)
        range_counts = [0] * 5
        for num in sorted_numbers:
            if 1 <= num <= 9:
                range_counts[0] += 1
            elif 10 <= num <= 19:
                range_counts[1] += 1
            elif 20 <= num <= 29:
                range_counts[2] += 1
            elif 30 <= num <= 39:
                range_counts[3] += 1
            else:
                range_counts[4] += 1

        # í•œ êµ¬ê°„ì— 3ê°œ ì´ìƒ ëª°ë¦° ê²½ìš°
        if max(range_counts) >= 4:
            risk_score += 0.2
        elif max(range_counts) == 3:
            risk_score += 0.1

        # 5. ê³¼ê±° ë¹ˆë„ ê¸°ë°˜ ìœ„í—˜ë„ (ì„ íƒì )
        if historical_data:
            repeat_count = 0
            # ê³¼ê±° ë™ì¼í•œ ë²ˆí˜¸ ì¡°í•© ì¶œí˜„ íšŸìˆ˜ í™•ì¸
            for draw in historical_data:
                common_numbers = set(draw.numbers).intersection(set(numbers))
                if len(common_numbers) >= 5:  # 5ê°œ ì´ìƒ ì¼ì¹˜
                    repeat_count += 1

            if repeat_count > 0:
                risk_score += min(0.2, repeat_count * 0.05)  # ìµœëŒ€ 0.2ê¹Œì§€ ì¶”ê°€

        # ìµœì¢… ìœ„í—˜ë„ ì ìˆ˜ (0-1 ë²”ìœ„ë¡œ ì œí•œ)
        return min(1.0, risk_score)

    def get_number_frequencies(self, data: List[LotteryNumber]) -> Dict[int, int]:
        """
        ê° ë²ˆí˜¸ë³„ ì¶œí˜„ íšŸìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

        Args:
            data: ë¶„ì„í•  ê³¼ê±° ë‹¹ì²¨ ë²ˆí˜¸ ëª©ë¡

        Returns:
            ê° ë²ˆí˜¸ì˜ ì¶œí˜„ íšŸìˆ˜ (ë²ˆí˜¸ -> ì¶œí˜„ íšŸìˆ˜)
        """
        frequencies = {}
        for draw in data:
            for num in draw.numbers:
                frequencies[num] = frequencies.get(num, 0) + 1

        return frequencies

    def analyze_scope(
        self, historical_data: List[LotteryNumber], scope: str = "full"
    ) -> PatternAnalysis:
        """
        ì§€ì •ëœ ìŠ¤ì½”í”„ì— ëŒ€í•œ íŒ¨í„´ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

        Args:
            historical_data: ë¶„ì„í•  ê³¼ê±° ë‹¹ì²¨ ë²ˆí˜¸ ëª©ë¡
            scope: ë¶„ì„ ìŠ¤ì½”í”„ ("full", "mid", "short" ì¤‘ í•˜ë‚˜)

        Returns:
            PatternAnalysis: íŒ¨í„´ ë¶„ì„ ê²°ê³¼
        """
        with performance_monitor(f"pattern_analysis_{scope}"):
            try:
                # ìºì‹œ í‚¤ ìƒì„±
                cache_key = self._create_cache_key(
                    f"pattern_analysis_{scope}", len(historical_data)
                )

                # ìºì‹œ í™•ì¸
                cached_result = self._check_cache(cache_key)
                if cached_result:
                    # ìŠ¤ì½”í”„ë³„ ê²°ê³¼ ì €ì¥
                    self.scoped_analyses[scope] = cached_result
                    return cached_result

                # íŒ¨í„´ ë¶„ì„ ìˆ˜í–‰
                result = self.analyze(historical_data)

                # ê²°ê³¼ ê°ì²´ì— ìŠ¤ì½”í”„ ì •ë³´ ì¶”ê°€
                result.metadata["scope"] = scope
                result.metadata["data_count"] = len(historical_data)

                # ìŠ¤ì½”í”„ë³„ ê²°ê³¼ ì €ì¥
                self.scoped_analyses[scope] = result

                # ê²°ê³¼ ìºì‹±
                self.cache_manager.set(cache_key, result)

                return result
            except Exception as e:
                self.logger.error(f"{scope} ìŠ¤ì½”í”„ íŒ¨í„´ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                return PatternAnalysis(
                    frequency_map={num: 1.0 / 45 for num in range(1, 46)},
                    recency_map={num: 0.5 for num in range(1, 46)},
                )

    def run_all_analyses(self, draw_data: List[LotteryNumber]) -> Dict[str, Any]:
        """
        ëª¨ë“  ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ í†µí•©í•©ë‹ˆë‹¤.

        Args:
            draw_data: ë¶„ì„í•  ë¡œë˜ ë²ˆí˜¸ ë°ì´í„°

        Returns:
            Dict[str, Any]: í†µí•©ëœ ë¶„ì„ ê²°ê³¼
        """
        try:
            # ë¶„ì„ ì‹œì‘ ì‹œê°„ ê¸°ë¡
            start_time = datetime.now()
            self.logger.info(f"ì „ì²´ ë¶„ì„ ì‹œì‘ (ë°ì´í„°: {len(draw_data)}ê°œ íšŒì°¨)")

            # ìŠ¤ì½”í”„ë³„ ë¶„ì„ ìˆ˜í–‰
            self.logger.info("ê¸°ë³¸ íŒ¨í„´ ë¶„ì„ ìˆ˜í–‰ ì¤‘...")
            full_analysis = self.analyze_scope(draw_data, "full")
            recent_100 = self.analyze_scope(
                draw_data[-100:] if len(draw_data) >= 100 else draw_data, "recent_100"
            )
            last_year_count = min(52, len(draw_data))
            last_year = self.analyze_scope(draw_data[-last_year_count:], "last_year")
            last_month_count = min(4, len(draw_data))
            last_month = self.analyze_scope(draw_data[-last_month_count:], "last_month")

            # ì„¸ê·¸ë¨¼íŠ¸ ë¹ˆë„ ë¶„ì„
            self.logger.info("ì„¸ê·¸ë¨¼íŠ¸ ë¹ˆë„ ë¶„ì„ ìˆ˜í–‰ ì¤‘...")
            segment_10_frequency = self.analyze_segment_frequency_10(draw_data)
            segment_5_frequency = self.analyze_segment_frequency_5(draw_data)

            # ê°­ í†µê³„ ë¶„ì„
            self.logger.info("ê°­ í†µê³„ ë¶„ì„ ìˆ˜í–‰ ì¤‘...")
            gap_statistics = self.analyze_gap_statistics(draw_data)

            # ëª…ì‹œì ìœ¼ë¡œ gap_stddev í•„ë“œ ì¶”ê°€
            if "std" in gap_statistics:
                gap_stddev = gap_statistics["std"]
            else:
                # ì§ì ‘ ê³„ì‚°
                gaps = []
                for i in range(1, len(draw_data)):
                    prev_numbers = set(draw_data[i - 1].numbers)
                    curr_numbers = set(draw_data[i].numbers)
                    gap = 6 - len(prev_numbers.intersection(curr_numbers))
                    gaps.append(gap)
                gap_stddev = np.std(gaps) if gaps else 0.0

            # í•„ìˆ˜ í•„ë“œ ì¶”ê°€
            self.logger.info("í•„ìˆ˜ í•„ë“œ gap_stddev ê³„ì‚° ì¤‘...")

            # íŒ¨í„´ ì¬ì¶œí˜„ ë¶„ì„
            self.logger.info("íŒ¨í„´ ì¬ì¶œí˜„ ë¶„ì„ ìˆ˜í–‰ ì¤‘...")
            pattern_reappearance = self.analyze_pattern_reappearance(draw_data)

            # ìµœê·¼ ì¬ì¶œí˜„ ê°„ê²© ë¶„ì„
            self.logger.info("ìµœê·¼ ì¬ì¶œí˜„ ê°„ê²© ë¶„ì„ ìˆ˜í–‰ ì¤‘...")
            recent_reappearance_gap = self.analyze_recent_reappearance_gap(draw_data)

            # íšŒì°¨ë³„ êµ¬ê°„ ë¹ˆë„ íˆìŠ¤í† ë¦¬ ìƒì„±
            self.logger.info("êµ¬ê°„ ë¹ˆë„ íˆìŠ¤í† ë¦¬ ìƒì„± ì¤‘...")
            segment_10_history = self.generate_segment_10_history(draw_data)
            segment_5_history = self.generate_segment_5_history(draw_data)

            # ì„¸ê·¸ë¨¼íŠ¸ ì¤‘ì‹¬ì„± ë¶„ì„
            self.logger.info("ì„¸ê·¸ë¨¼íŠ¸ ì¤‘ì‹¬ì„± ë¶„ì„ ìˆ˜í–‰ ì¤‘...")
            segment_centrality = self.analyze_segment_centrality(draw_data)

            # ì„¸ê·¸ë¨¼íŠ¸ ì—°ì† íŒ¨í„´ ë¶„ì„
            self.logger.info("ì„¸ê·¸ë¨¼íŠ¸ ì—°ì† íŒ¨í„´ ë¶„ì„ ìˆ˜í–‰ ì¤‘...")
            segment_consecutive_patterns = self.analyze_segment_consecutive_patterns(
                draw_data
            )

            # ì¤‘ë³µ ë‹¹ì²¨ ë²ˆí˜¸ ë¶„ì„
            self.logger.info("ì¤‘ë³µ ë‹¹ì²¨ ë²ˆí˜¸ ë¶„ì„ ìˆ˜í–‰ ì¤‘...")
            identical_draw_check = self.analyze_identical_draws(draw_data)

            # í•„ìˆ˜ í•„ë“œ: ì¤‘ë³µ í”Œë˜ê·¸
            duplicate_flag = (
                1 if identical_draw_check.get("duplicate_count", 0) > 0 else 0
            )

            # ===== ì¶”ê°€ ë¶„ì„ í•­ëª© ì‹¤í–‰ =====
            # ìœ„ì¹˜ë³„ ë²ˆí˜¸ ë¹ˆë„ ê³„ì‚°
            self.logger.info("ìœ„ì¹˜ë³„ ë²ˆí˜¸ ë¹ˆë„ ê³„ì‚° ì¤‘...")
            position_frequency = self.calculate_position_frequency(draw_data)

            # ìœ„ì¹˜ë³„ ë²ˆí˜¸ í†µê³„ ê³„ì‚°
            self.logger.info("ìœ„ì¹˜ë³„ ë²ˆí˜¸ í†µê³„ ê³„ì‚° ì¤‘...")
            position_number_stats = self.calculate_position_number_stats(draw_data)

            # ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì„¸ íˆìŠ¤í† ë¦¬ ê³„ì‚°
            self.logger.info("ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì„¸ íˆìŠ¤í† ë¦¬ ê³„ì‚° ì¤‘...")
            segment_trend_history = self.calculate_segment_trend_history(draw_data)

            # í•„ìˆ˜ í•„ë“œ: ì„¸ê·¸ë¨¼íŠ¸ ì—”íŠ¸ë¡œí”¼ ê³„ì‚°
            self.logger.info("ì„¸ê·¸ë¨¼íŠ¸ ì—”íŠ¸ë¡œí”¼ ê³„ì‚° ì¤‘...")
            segment_entropy = self.calculate_segment_entropy(draw_data)

            # í•„ìˆ˜ í•„ë“œ: ROI ê·¸ë£¹ ì ìˆ˜ ê³„ì‚°
            self.logger.info("ROI íŒ¨í„´ ê·¸ë£¹ ì‹ë³„ ì¤‘...")
            roi_pattern_groups = self.identify_roi_pattern_groups(draw_data)
            roi_group_score = 0.0
            if (
                isinstance(roi_pattern_groups, dict)
                and "group_scores" in roi_pattern_groups
            ):
                if isinstance(roi_pattern_groups["group_scores"], dict):
                    roi_group_score = sum(
                        roi_pattern_groups["group_scores"].values()
                    ) / max(1, len(roi_pattern_groups["group_scores"]))

            # í•„ìˆ˜ í•„ë“œ: Hot-Cold ë¯¹ìŠ¤ ì ìˆ˜ ê³„ì‚°
            self.logger.info("Hot-Cold ë¯¹ìŠ¤ ì ìˆ˜ ê³„ì‚° ì¤‘...")
            hot_cold_mix_score = 0.0

            # ë¹ˆë„ì— ë”°ë¼ í•«/ì½œë“œ ë²ˆí˜¸ ì‹ë³„
            frequencies = self._calculate_frequencies(draw_data)
            hot_numbers, cold_numbers = self._identify_hot_cold_numbers(frequencies)

            # ìµœê·¼ 100íšŒ ë°ì´í„°ì—ì„œ í•«/ì½œë“œ ë¯¹ìŠ¤ íŒ¨í„´ ë¶„ì„
            recent_draws = draw_data[-100:] if len(draw_data) >= 100 else draw_data
            hot_cold_mix_counts = []

            for draw in recent_draws:
                hot_count = sum(1 for num in draw.numbers if num in hot_numbers)
                cold_count = sum(1 for num in draw.numbers if num in cold_numbers)
                # í•«/ì½œë“œ ë¯¹ìŠ¤ ë¹„ìœ¨ (0: ëª¨ë‘ í•« ë˜ëŠ” ëª¨ë‘ ì½œë“œ, 1: ê· ë“± ë¯¹ìŠ¤)
                mix_ratio = min(hot_count, cold_count) / 3  # ìµœëŒ€ 3:3 ê· í˜•ì´ë©´ 1.0
                hot_cold_mix_counts.append(mix_ratio)

            hot_cold_mix_score = sum(hot_cold_mix_counts) / max(
                1, len(hot_cold_mix_counts)
            )

            # ê°­ í¸ì°¨ ì ìˆ˜ ê³„ì‚°
            self.logger.info("ê°­ í¸ì°¨ ì ìˆ˜ ê³„ì‚° ì¤‘...")
            gap_deviation_score = self.calculate_gap_deviation_score(draw_data)

            # ì¡°í•© ë‹¤ì–‘ì„± ì ìˆ˜ ê³„ì‚°
            self.logger.info("ì¡°í•© ë‹¤ì–‘ì„± ì ìˆ˜ ê³„ì‚° ì¤‘...")
            combination_diversity = self.calculate_combination_diversity_score(
                draw_data
            )

            # ROI íŠ¸ë Œë“œ ê³„ì‚°
            self.logger.info("ROI íŠ¸ë Œë“œ ê³„ì‚° ì¤‘...")
            roi_trend_by_pattern = self.calculate_roi_trend_by_pattern(draw_data)

            # ëª¨ë“  ë¶„ì„ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ë”•ì…”ë„ˆë¦¬ë¡œ í†µí•©
            combined_result = {
                # ê¸°ì¡´ ë¶„ì„ ê²°ê³¼
                "frequency_analysis": {
                    "full": full_analysis.frequency_map,
                    "recent_100": recent_100.frequency_map,
                    "last_year": last_year.frequency_map,
                    "last_month": last_month.frequency_map,
                },
                "recency_analysis": {
                    "full": full_analysis.recency_map,
                    "recent_100": recent_100.recency_map,
                    "last_year": last_year.recency_map,
                    "last_month": last_month.recency_map,
                },
                "segment_10_frequency": segment_10_frequency,
                "segment_5_frequency": segment_5_frequency,
                "gap_statistics": gap_statistics,
                "pattern_reappearance": pattern_reappearance,
                "recent_reappearance_gap": recent_reappearance_gap,
                "segment_centrality": segment_centrality,
                "segment_consecutive_patterns": segment_consecutive_patterns,
                "identical_draws": identical_draw_check,
                # ì¶”ê°€ ë¶„ì„ ê²°ê³¼
                "position_frequency": (
                    position_frequency.tolist()
                    if isinstance(position_frequency, np.ndarray)
                    else position_frequency
                ),
                "position_number_stats": position_number_stats,
                "segment_trend_history": (
                    segment_trend_history.tolist()
                    if isinstance(segment_trend_history, np.ndarray)
                    else segment_trend_history
                ),
                "gap_deviation_score": gap_deviation_score,
                "combination_diversity": combination_diversity,
                "roi_trend_by_pattern": roi_trend_by_pattern,
                "roi_pattern_groups": roi_pattern_groups,
                # í•„ìˆ˜ í•„ë“œ ëª…ì‹œì  ì¶”ê°€
                "gap_stddev": gap_stddev,
                "hot_cold_mix_score": hot_cold_mix_score,
                "segment_entropy": segment_entropy,
                "roi_group_score": roi_group_score,
                "duplicate_flag": duplicate_flag,
                "pair_centrality": segment_centrality,  # ê¸°ì¡´ segment_centralityë¥¼ pair_centralityë¡œë„ ì‚¬ìš©
            }

            # ì„±ëŠ¥ ë³´ê³ ì„œ ì €ì¥
            try:
                self._save_analysis_performance_report(
                    combined_result, draw_data, start_time
                )
            except Exception as e:
                self.logger.warning(f"ì„±ëŠ¥ ë³´ê³ ì„œ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")

            return combined_result

        except Exception as e:
            self.logger.error(f"ì „ì²´ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            # ì„±ëŠ¥ ë³´ê³ ì„œì— ì˜¤ë¥˜ ê¸°ë¡
            try:
                self._save_analysis_performance_report(
                    {}, draw_data, start_time, str(e)
                )
            except Exception as log_error:
                self.logger.error(f"ì˜¤ë¥˜ ë¡œê¹… ì¤‘ ì¶”ê°€ ì˜¤ë¥˜ ë°œìƒ: {str(log_error)}")

            # ìµœì†Œí•œì˜ ê²°ê³¼ ë°˜í™˜
            return {
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
            }

    def _save_analysis_performance_report(
        self,
        analysis_result: Dict[str, Any],
        draw_data: List[LotteryNumber],
        start_time: datetime,
        error: Optional[str] = None,
    ) -> None:
        """
        íŒ¨í„´ ë¶„ì„ ì„±ëŠ¥ ë³´ê³ ì„œ ì €ì¥
        unified_report.pyì˜ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„±ëŠ¥ ë¦¬í¬íŠ¸ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.

        Args:
            analysis_result: ë¶„ì„ ê²°ê³¼
            draw_data: ë¶„ì„í•œ ë‹¹ì²¨ ë²ˆí˜¸ ë°ì´í„°
            start_time: ë¶„ì„ ì‹œì‘ ì‹œê°„
            error: ì—ëŸ¬ ë©”ì‹œì§€ (ìˆëŠ” ê²½ìš°)
        """
        try:
            # ì„±ëŠ¥ ë°ì´í„° ìˆ˜ì§‘
            end_time = datetime.now()
            execution_time = (end_time - start_time).total_seconds()

            # ë¶„ì„ ë©”íŠ¸ë¦­ìŠ¤ ì •ë³´ (í•„ìˆ˜ í•„ë“œë§Œ í¬í•¨)
            data_metrics = {
                "record_count": len(draw_data) if draw_data else 0,
                "execution_time_sec": execution_time,
                "status": "error" if error else "success",
            }

            # ì˜¤ë¥˜ ì •ë³´ ì¶”ê°€
            if error:
                data_metrics["error"] = error

            # í†µí•© ë³´ê³ ì„œ ì‹œìŠ¤í…œ ì‚¬ìš©
            save_analysis_performance_report(
                None,  # profiler
                None,  # performance_tracker (í†µí•© ì‹œìŠ¤í…œ ì‚¬ìš©)
                self.config,
                "pattern_analyzer",
                data_metrics,
            )

            self.logger.info("íŒ¨í„´ ë¶„ì„ ì„±ëŠ¥ ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ")

        except Exception as e:
            self.logger.error(f"ì„±ëŠ¥ ë³´ê³ ì„œ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    def get_analysis_by_scope(self, scope: str = "full") -> Optional[PatternAnalysis]:
        """
        íŠ¹ì • ìŠ¤ì½”í”„ì˜ íŒ¨í„´ ë¶„ì„ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

        Args:
            scope: ë¶„ì„ ìŠ¤ì½”í”„ ("full", "mid", "short" ì¤‘ í•˜ë‚˜)

        Returns:
            PatternAnalysis: íŒ¨í„´ ë¶„ì„ ê²°ê³¼ ë˜ëŠ” None
        """
        return self.scoped_analyses.get(scope)

    def analyze_odd_even_distribution(
        self, data: List[LotteryNumber]
    ) -> Dict[str, float]:
        """
        í™€ì§ ë²ˆí˜¸ ë¶„í¬ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.

        Args:
            data: ë¶„ì„í•  ê³¼ê±° ë‹¹ì²¨ ë²ˆí˜¸ ëª©ë¡

        Returns:
            Dict[str, float]: í™€ì§ ë¶„í¬ ë¶„ì„ ê²°ê³¼
        """
        try:
            from collections import Counter
            import numpy as np

            # ê° ë‹¹ì²¨ ì¡°í•©ì˜ í™€ì§ ê°œìˆ˜ ê³„ì‚°
            odd_counts = []
            even_counts = []

            for draw in data:
                odd = sum(1 for num in draw.numbers if num % 2 == 1)
                even = len(draw.numbers) - odd

                odd_counts.append(odd)
                even_counts.append(even)

            # í™€ì§ ì¡°í•© ë¹ˆë„ ê³„ì‚°
            odd_even_counter = Counter(zip(odd_counts, even_counts))
            total_draws = len(data)

            result = {}

            # ê° ì¡°í•©ì˜ ë¹ˆë„ë¥¼ ë°±ë¶„ìœ¨ë¡œ ë³€í™˜
            for (odd, even), count in odd_even_counter.items():
                result[f"odd_{odd}_even_{even}"] = float(count / total_draws)

            # í™€ìˆ˜/ì§ìˆ˜ í‰ê· 
            result["avg_odd"] = float(np.mean(odd_counts))
            result["avg_even"] = float(np.mean(even_counts))

            # ê°€ì¥ ë§ì´ ë‚˜ì˜¨ í™€ì§ ì¡°í•©
            most_common = odd_even_counter.most_common(1)
            if most_common:
                (odd, even), count = most_common[0]
                result["most_common_odd"] = odd
                result["most_common_even"] = even
                result["most_common_frequency"] = float(count / total_draws)

            return result

        except Exception as e:
            self.logger.error(f"í™€ì§ ë¶„í¬ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                "avg_odd": 3.0,
                "avg_even": 3.0,
                "most_common_odd": 3,
                "most_common_even": 3,
                "most_common_frequency": 0.2,
            }

    def analyze_number_sum_distribution(
        self, data: List[LotteryNumber]
    ) -> Dict[str, float]:
        """
        ë‹¹ì²¨ ë²ˆí˜¸ í•©ê³„ ë¶„í¬ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.

        Args:
            data: ë¶„ì„í•  ê³¼ê±° ë‹¹ì²¨ ë²ˆí˜¸ ëª©ë¡

        Returns:
            Dict[str, float]: í•©ê³„ ë¶„í¬ ë¶„ì„ ê²°ê³¼
        """
        try:
            import numpy as np

            # ê° ë‹¹ì²¨ ì¡°í•©ì˜ í•©ê³„ ê³„ì‚°
            sums = [sum(draw.numbers) for draw in data]

            # í•©ê³„ í†µê³„ ê³„ì‚°
            result = {
                "mean_sum": float(np.mean(sums)),
                "median_sum": float(np.median(sums)),
                "std_sum": float(np.std(sums)),
                "min_sum": float(min(sums)),
                "max_sum": float(max(sums)),
                "range_sum": float(max(sums) - min(sums)),
            }

            # í•©ê³„ êµ¬ê°„ë³„ ë¹ˆë„ ê³„ì‚°
            sum_bins = {
                "sum_70_100": 0,
                "sum_101_125": 0,
                "sum_126_150": 0,
                "sum_151_175": 0,
                "sum_176_200": 0,
                "sum_201_225": 0,
                "sum_226_255": 0,
            }

            for total in sums:
                if 70 <= total <= 100:
                    sum_bins["sum_70_100"] += 1
                elif 101 <= total <= 125:
                    sum_bins["sum_101_125"] += 1
                elif 126 <= total <= 150:
                    sum_bins["sum_126_150"] += 1
                elif 151 <= total <= 175:
                    sum_bins["sum_151_175"] += 1
                elif 176 <= total <= 200:
                    sum_bins["sum_176_200"] += 1
                elif 201 <= total <= 225:
                    sum_bins["sum_201_225"] += 1
                elif 226 <= total <= 255:
                    sum_bins["sum_226_255"] += 1

            # êµ¬ê°„ë³„ ë¹ˆë„ë¥¼ ë°±ë¶„ìœ¨ë¡œ ë³€í™˜
            for key, count in sum_bins.items():
                result[key] = float(count / len(sums))

            return result

        except Exception as e:
            self.logger.error(f"ë²ˆí˜¸ í•©ê³„ ë¶„í¬ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                "mean_sum": 135.0,
                "median_sum": 135.0,
                "std_sum": 25.0,
                "min_sum": 70.0,
                "max_sum": 255.0,
                "range_sum": 185.0,
                "sum_70_100": 0.05,
                "sum_101_125": 0.15,
                "sum_126_150": 0.30,
                "sum_151_175": 0.30,
                "sum_176_200": 0.15,
                "sum_201_225": 0.04,
                "sum_226_255": 0.01,
            }

    def analyze_network(self, data: List[LotteryNumber]) -> Dict[str, Any]:
        """
        ë„¤íŠ¸ì›Œí¬ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

        Args:
            data: ë¶„ì„í•  ê³¼ê±° ë‹¹ì²¨ ë²ˆí˜¸ ëª©ë¡

        Returns:
            Dict[str, Any]: ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ê²°ê³¼
        """
        try:
            import numpy as np
            from collections import defaultdict

            # ë²ˆí˜¸ ê°„ ì—°ê²°ì„± ë¶„ì„
            co_occurrence = defaultdict(int)
            total_pairs = 0

            for draw in data:
                numbers = sorted(draw.numbers)
                # ëª¨ë“  ë²ˆí˜¸ ìŒì— ëŒ€í•´ ë™ì‹œ ì¶œí˜„ íšŸìˆ˜ ê³„ì‚°
                for i in range(len(numbers)):
                    for j in range(i + 1, len(numbers)):
                        pair = (numbers[i], numbers[j])
                        co_occurrence[pair] += 1
                        total_pairs += 1

            # ë„¤íŠ¸ì›Œí¬ í†µê³„ ê³„ì‚°
            if total_pairs > 0:
                # ê°€ì¥ ê°•í•œ ì—°ê²°
                strongest_pair = max(co_occurrence.items(), key=lambda x: x[1])

                # í‰ê·  ì—°ê²° ê°•ë„
                avg_strength = sum(co_occurrence.values()) / len(co_occurrence)

                # ì—°ê²° ë°€ë„ (ì‹¤ì œ ì—°ê²° / ê°€ëŠ¥í•œ ëª¨ë“  ì—°ê²°)
                possible_pairs = 45 * 44 // 2  # 45ê°œ ë²ˆí˜¸ì—ì„œ ê°€ëŠ¥í•œ ëª¨ë“  ìŒ
                density = len(co_occurrence) / possible_pairs

                result = {
                    "total_connections": len(co_occurrence),
                    "total_occurrences": total_pairs,
                    "avg_connection_strength": float(avg_strength),
                    "network_density": float(density),
                    "strongest_pair": strongest_pair[0],
                    "strongest_pair_count": strongest_pair[1],
                    "strongest_pair_strength": float(strongest_pair[1] / len(data)),
                }

                # ê° ë²ˆí˜¸ì˜ ì—°ê²° ìˆ˜ ê³„ì‚°
                node_connections = defaultdict(int)
                for (num1, num2), count in co_occurrence.items():
                    node_connections[num1] += 1
                    node_connections[num2] += 1

                if node_connections:
                    # ê°€ì¥ ì—°ê²°ì´ ë§ì€ ë²ˆí˜¸
                    most_connected = max(node_connections.items(), key=lambda x: x[1])
                    result["most_connected_number"] = most_connected[0]
                    result["most_connected_count"] = most_connected[1]
                    result["avg_node_connections"] = float(
                        sum(node_connections.values()) / len(node_connections)
                    )

            else:
                result = {
                    "total_connections": 0,
                    "total_occurrences": 0,
                    "avg_connection_strength": 0.0,
                    "network_density": 0.0,
                    "strongest_pair": (1, 2),
                    "strongest_pair_count": 0,
                    "strongest_pair_strength": 0.0,
                    "most_connected_number": 1,
                    "most_connected_count": 0,
                    "avg_node_connections": 0.0,
                }

            return result

        except Exception as e:
            self.logger.error(f"ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                "total_connections": 100,
                "total_occurrences": 1000,
                "avg_connection_strength": 10.0,
                "network_density": 0.5,
                "strongest_pair": (7, 14),
                "strongest_pair_count": 50,
                "strongest_pair_strength": 0.05,
                "most_connected_number": 7,
                "most_connected_count": 30,
                "avg_node_connections": 25.0,
            }

    def analyze_gap_patterns(self, data: List[LotteryNumber]) -> Dict[str, Any]:
        """
        ë²ˆí˜¸ ê°„ ê°„ê²© íŒ¨í„´ì„ ë¶„ì„í•©ë‹ˆë‹¤.

        Args:
            data: ë¶„ì„í•  ê³¼ê±° ë‹¹ì²¨ ë²ˆí˜¸ ëª©ë¡

        Returns:
            Dict[str, Any]: ê°„ê²© íŒ¨í„´ ë¶„ì„ ê²°ê³¼
        """
        try:
            import numpy as np
            from collections import defaultdict

            gap_data = []
            gap_frequencies = defaultdict(int)

            for draw in data:
                numbers = sorted(draw.numbers)
                # ì—°ì†ëœ ë²ˆí˜¸ ê°„ì˜ ê°„ê²© ê³„ì‚°
                gaps = []
                for i in range(len(numbers) - 1):
                    gap = numbers[i + 1] - numbers[i]
                    gaps.append(gap)
                    gap_frequencies[gap] += 1

                gap_data.append(gaps)

            # ê°„ê²© í†µê³„ ê³„ì‚°
            all_gaps = [gap for gaps in gap_data for gap in gaps]

            if all_gaps:
                result = {
                    "avg_gap": float(np.mean(all_gaps)),
                    "median_gap": float(np.median(all_gaps)),
                    "std_gap": float(np.std(all_gaps)),
                    "min_gap": int(min(all_gaps)),
                    "max_gap": int(max(all_gaps)),
                    "total_gaps": len(all_gaps),
                }

                # ê°€ì¥ ë¹ˆë²ˆí•œ ê°„ê²©
                most_common_gap = max(gap_frequencies.items(), key=lambda x: x[1])
                result["most_common_gap"] = most_common_gap[0]
                result["most_common_gap_count"] = most_common_gap[1]
                result["most_common_gap_frequency"] = float(
                    most_common_gap[1] / len(all_gaps)
                )

                # ê°„ê²© ë¶„í¬
                gap_1_count = gap_frequencies.get(1, 0)
                gap_2_count = gap_frequencies.get(2, 0)
                gap_3_count = gap_frequencies.get(3, 0)

                result["gap_1_frequency"] = float(gap_1_count / len(all_gaps))
                result["gap_2_frequency"] = float(gap_2_count / len(all_gaps))
                result["gap_3_frequency"] = float(gap_3_count / len(all_gaps))

                # í° ê°„ê²© (10 ì´ìƒ) ë¹„ìœ¨
                large_gaps = sum(1 for gap in all_gaps if gap >= 10)
                result["large_gap_ratio"] = float(large_gaps / len(all_gaps))

            else:
                result = {
                    "avg_gap": 5.0,
                    "median_gap": 4.0,
                    "std_gap": 3.0,
                    "min_gap": 1,
                    "max_gap": 20,
                    "total_gaps": 0,
                    "most_common_gap": 3,
                    "most_common_gap_count": 0,
                    "most_common_gap_frequency": 0.0,
                    "gap_1_frequency": 0.1,
                    "gap_2_frequency": 0.15,
                    "gap_3_frequency": 0.2,
                    "large_gap_ratio": 0.3,
                }

            return result

        except Exception as e:
            self.logger.error(f"ê°„ê²© íŒ¨í„´ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                "avg_gap": 5.0,
                "median_gap": 4.0,
                "std_gap": 3.0,
                "min_gap": 1,
                "max_gap": 20,
                "total_gaps": 100,
                "most_common_gap": 3,
                "most_common_gap_count": 25,
                "most_common_gap_frequency": 0.25,
                "gap_1_frequency": 0.1,
                "gap_2_frequency": 0.15,
                "gap_3_frequency": 0.2,
                "large_gap_ratio": 0.3,
            }

    def calculate_segment_entropy(self, data: List[LotteryNumber]) -> float:
        """
        ì„¸ê·¸ë¨¼íŠ¸ë³„ ì—”íŠ¸ë¡œí”¼ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

        Args:
            data: ë¶„ì„í•  ê³¼ê±° ë‹¹ì²¨ ë²ˆí˜¸ ëª©ë¡

        Returns:
            float: ì„¸ê·¸ë¨¼íŠ¸ ì—”íŠ¸ë¡œí”¼ ê°’
        """
        try:
            import numpy as np
            from collections import defaultdict

            # ë²ˆí˜¸ë¥¼ 5ê°œ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ë‚˜ëˆ„ê¸° (1-9, 10-18, 19-27, 28-36, 37-45)
            segment_counts = defaultdict(int)
            total_numbers = 0

            for draw in data:
                for number in draw.numbers:
                    if 1 <= number <= 9:
                        segment_counts[1] += 1
                    elif 10 <= number <= 18:
                        segment_counts[2] += 1
                    elif 19 <= number <= 27:
                        segment_counts[3] += 1
                    elif 28 <= number <= 36:
                        segment_counts[4] += 1
                    elif 37 <= number <= 45:
                        segment_counts[5] += 1
                    total_numbers += 1

            if total_numbers == 0:
                return 0.0

            # ê° ì„¸ê·¸ë¨¼íŠ¸ì˜ í™•ë¥  ê³„ì‚°
            probabilities = []
            for segment in range(1, 6):
                prob = segment_counts[segment] / total_numbers
                if prob > 0:
                    probabilities.append(prob)

            # ì—”íŠ¸ë¡œí”¼ ê³„ì‚°: -sum(p * log2(p))
            if not probabilities:
                return 0.0

            entropy = -sum(p * np.log2(p) for p in probabilities if p > 0)
            return float(entropy)

        except Exception as e:
            self.logger.error(f"ì„¸ê·¸ë¨¼íŠ¸ ì—”íŠ¸ë¡œí”¼ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
            return 2.0  # ê¸°ë³¸ê°’ (ê· ë“± ë¶„í¬ ì‹œ ì•½ 2.32)
