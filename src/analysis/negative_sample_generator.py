"""
ë¹„ë‹¹ì²¨ ìƒ˜í”Œ ìƒì„± ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ í•™ìŠµ ë°ì´í„°ì˜ ë‹¤ì–‘ì„±ì„ ë†’ì´ê¸° ìœ„í•´ ë¹„ë‹¹ì²¨ ë²ˆí˜¸ ì¡°í•©ì„ ìƒì„±í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import numpy as np
import random
import time
import os
import gc
import threading
from pathlib import Path
from typing import List, Dict, Any, Set, Tuple, Optional, Union

# logging ì œê±° - unified_logging ì‚¬ìš©
import json
import psutil

from ..utils.unified_logging import get_logger
from ..utils.memory_manager import get_memory_manager
from ..utils.batch_controller import DynamicBatchSizeController, CPUBatchProcessor
from ..utils.cuda_singleton_manager import get_singleton_cuda_optimizer
from ..shared.types import LotteryNumber
from .pattern_analyzer import PatternAnalyzer
from .base_analyzer import BaseAnalyzer

import torch

# ë¡œê±° ì„¤ì •
logger = get_logger(__name__)


class NegativeSampleGenerator(BaseAnalyzer[Dict[str, Any]]):
    """ë¹„ë‹¹ì²¨ ìƒ˜í”Œ ìƒì„± í´ë˜ìŠ¤"""

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """ì´ˆê¸°í™”"""
        super().__init__(config, name="negative_sampler")

        # GPU ê°€ì† ì„¤ì •
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.use_gpu = torch.cuda.is_available()

        if self.use_gpu:
            logger.info(f"ğŸš€ GPU ê°€ì† í™œì„±í™”: {torch.cuda.get_device_name()}")
            torch.cuda.empty_cache()
            torch.backends.cudnn.benchmark = True
        else:
            logger.warning("âš ï¸ GPU ì‚¬ìš© ë¶ˆê°€, CPU ëª¨ë“œë¡œ ì‹¤í–‰")

        # ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì •
        try:
            self.cache_dir = self.config.get("paths", {}).get("cache_dir", "data/cache")
        except KeyError:
            raise KeyError("ì„¤ì •ì—ì„œ 'paths.cache_dir' í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        Path(self.cache_dir).mkdir(exist_ok=True, parents=True)

        negative_sampler_config = self.config.get("negative_sampler", {})

        # ì»¨íŠ¸ë¡¤ëŸ¬ ì´ˆê¸°í™”
        self.batch_controller = DynamicBatchSizeController(
            config=self.config,
            initial_batch_size=negative_sampler_config.get("batch_size", 2000),
            min_batch_size=500,
            max_batch_size=5000,
        )

        # íŒ¨í„´ ë¶„ì„ê¸° ì´ˆê¸°í™”
        self.pattern_analyzer = PatternAnalyzer(config)

        # CPU/GPUë³„ ìµœì í™” ë„êµ¬ ì„¤ì •
        if self.use_gpu:
            self.cuda_optimizer = get_singleton_cuda_optimizer()
            self.amp_scaler = torch.cuda.amp.GradScaler()
            self.cpu_batch_processor = None
        else:
            self.cuda_optimizer = None
            self.cpu_batch_processor = CPUBatchProcessor(
                n_jobs=negative_sampler_config.get("vectorize_workers", -1),
                batch_size=negative_sampler_config.get("vectorize_batch", 1000),
                backend="multiprocessing",  # GIL íšŒí”¼
            )

        # ì§„í–‰ ìƒí™© ì¶”ì  ë³€ìˆ˜
        self.progress = 0
        self.total = 0

        # ë©”ëª¨ë¦¬ ê´€ë¦¬ì (ì‹±ê¸€í†¤)
        self.memory_tracker = get_memory_manager()

        # ì§„í–‰ ìƒí™© ì¶”ì  ì ê¸ˆ
        self._progress_lock = threading.Lock()

        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        self.performance_stats = {
            "total_samples": 0,
            "gpu_utilization": 0.0,
            "memory_usage_mb": 0.0,
            "processing_time": 0.0,
            "samples_per_second": 0.0,
        }

    def _analyze_impl(
        self, historical_data: List[LotteryNumber], *args, **kwargs
    ) -> Dict[str, Any]:
        """BaseAnalyzer ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„"""
        sample_size = kwargs.get("sample_size", 100000)
        return self.generate_samples(historical_data, sample_size)

    def generate_samples(
        self, draw_data: List[LotteryNumber], sample_size: int = 100000
    ) -> Dict[str, Any]:
        """
        ë¹„ë‹¹ì²¨ ì¡°í•© ìƒ˜í”Œ ìƒì„±

        Args:
            draw_data: ë‹¹ì²¨ ë²ˆí˜¸ ë°ì´í„°
            sample_size: ìƒì„±í•  ìƒ˜í”Œ í¬ê¸°

        Returns:
            ìƒì„± ê²°ê³¼ ì •ë³´
        """
        self.logger.info(f"ë¹„ë‹¹ì²¨ ì¡°í•© ìƒì„± ì‹œì‘: ëª©í‘œ {sample_size:,}ê°œ")

        # ì „ì—­ ì‹œì‘ ì‹œê°„
        global_start_time = time.time()

        # ê²½ê³  ë©”ì‹œì§€ ì €ì¥ ë¦¬ìŠ¤íŠ¸
        warnings_list: List[str] = []

        # ì´ë¯¸ ë‹¹ì²¨ëœ ë²ˆí˜¸ ì¡°í•© setìœ¼ë¡œ ì €ì¥ (ë¹ ë¥¸ ê²€ìƒ‰)
        existing_combinations = self._extract_winning_combinations(draw_data)

        # ì§„í–‰ ìƒí™© ì´ˆê¸°í™”
        self.progress = 0
        self.total = sample_size

        # ë¹„ë‹¹ì²¨ ì¡°í•© ìƒì„±
        negative_samples = self._generate_samples(existing_combinations, sample_size)

        # ë²¡í„°í™” ìˆ˜í–‰
        vector_path = self._vectorize_samples(negative_samples, draw_data, sample_size)

        # ì „ì²´ ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
        global_end_time = time.time()
        elapsed_time = global_end_time - global_start_time

        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •
        memory_used_mb = len(negative_samples) * 6 * 4 / (1024 * 1024)  # ëŒ€ëµì  ê³„ì‚°

        # ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±
        report_path = self._generate_performance_report(
            start_time=global_start_time,
            end_time=global_end_time,
            sample_count=len(negative_samples),
            memory_used_mb=memory_used_mb,
            vector_path=vector_path,
            warnings=warnings_list,
        )

        self.logger.info(
            f"ë¹„ë‹¹ì²¨ ì¡°í•© ìƒì„± ì™„ë£Œ: {len(negative_samples):,}ê°œ ({elapsed_time:.2f}ì´ˆ)"
        )
        self.logger.info(f"ë©”ëª¨ë¦¬ ì‚¬ìš©: {memory_used_mb:.2f}MB")
        self.logger.info(f"ì„±ëŠ¥ ë³´ê³ ì„œ: {report_path}")

        return {
            "success": True,
            "elapsed_time": elapsed_time,
            "sample_count": len(negative_samples),
            "memory_used_mb": memory_used_mb,
            "raw_path": self._save_raw_samples(negative_samples, sample_size),
            "vector_path": vector_path,
            "report_path": report_path,
        }

    def _extract_winning_combinations(
        self, draw_data: List[LotteryNumber]
    ) -> Set[Tuple[int, ...]]:
        """
        ë‹¹ì²¨ ë²ˆí˜¸ ì¡°í•© ì¶”ì¶œ

        Args:
            draw_data: ë‹¹ì²¨ ë²ˆí˜¸ ë°ì´í„°

        Returns:
            ë‹¹ì²¨ ë²ˆí˜¸ ì¡°í•© ì§‘í•©
        """
        self.logger.info(f"ë‹¹ì²¨ ë²ˆí˜¸ ì¡°í•© ì¶”ì¶œ ì‹œì‘: {len(draw_data)}ê°œ")
        combinations = set()

        for draw in draw_data:
            # ì •ë ¬ëœ ë²ˆí˜¸ë¥¼ íŠœí”Œë¡œ ë³€í™˜í•˜ì—¬ ì§‘í•©ì— ì¶”ê°€
            combinations.add(tuple(sorted(draw.numbers)))

        self.logger.info(f"ë‹¹ì²¨ ë²ˆí˜¸ ì¡°í•© ì¶”ì¶œ ì™„ë£Œ: {len(combinations)}ê°œ")
        return combinations

    def _generate_samples(
        self, existing_combinations: Set[Tuple[int, ...]], sample_size: int
    ) -> List[List[int]]:
        """
        ë¹„ë‹¹ì²¨ ì¡°í•© ìƒì„± (ê· í˜•ì¡íŒ ìƒ˜í”Œë§)
        - GPU ìš°ì„ , CPU í´ë°±
        """
        if self.use_gpu:
            logger.info("ğŸš€ GPUë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ë‹¹ì²¨ ì¡°í•© ìƒì„±")
            return self._generate_samples_gpu(existing_combinations, sample_size)
        else:
            logger.info("ğŸ’» CPUë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ë‹¹ì²¨ ì¡°í•© ìƒì„±")
            n_random = int(sample_size * 0.7)
            n_pattern = sample_size - n_random

            samples = self._random_negative_samples_cpu(existing_combinations, n_random)
            samples.extend(
                self._pattern_based_negative_samples(existing_combinations, n_pattern)
            )

            # ì¤‘ë³µ ì œê±° ë° ìµœì¢… ë°˜í™˜
            final_samples = []
            seen = set(map(tuple, samples))
            for s in samples:
                if tuple(s) in seen:
                    final_samples.append(s)
                    seen.remove(tuple(s))
            return final_samples[:sample_size]

    def _generate_samples_gpu(
        self, existing_combinations: Set[Tuple[int, ...]], sample_size: int
    ) -> List[List[int]]:
        """
        GPUë¥¼ ì‚¬ìš©í•œ ê³ ì„±ëŠ¥ ë¹„ë‹¹ì²¨ ì¡°í•© ìƒì„±
        - ì¤‘ë³µ ì œê±° í¬í•¨, ì™„ì „íˆ ë²¡í„°í™”ëœ ë°©ì‹
        """
        all_numbers = torch.arange(1, 46, device=self.device, dtype=torch.int16)
        existing_tensor = torch.tensor(
            list(existing_combinations), device=self.device, dtype=torch.int16
        )

        final_samples = torch.empty((0, 6), device=self.device, dtype=torch.int16)

        # ëª©í‘œ ìˆ˜ëŸ‰ë³´ë‹¤ ë” ë§ì´ ìƒì„±í•˜ì—¬ í•„í„°ë§ ì†ì‹¤ ë³´ìƒ
        OVERSAMPLING_FACTOR = 1.5

        with torch.no_grad():
            while len(final_samples) < sample_size:
                needed = sample_size - len(final_samples)
                batch_size = int(needed * OVERSAMPLING_FACTOR)
                batch_size = min(batch_size, 200000)  # ë©”ëª¨ë¦¬ ì œí•œ

                # 1. ëœë¤ ì¸ë±ìŠ¤ ìƒì„±
                # torch.randì—ì„œ ì§ì ‘ topkë¥¼ ì‚¬ìš©í•˜ì—¬ ê³ ìœ  ì¸ë±ìŠ¤ ì¶”ì¶œ
                _, random_indices = torch.topk(
                    torch.rand(batch_size, 45, device=self.device), k=6, dim=1
                )

                # 2. ì¸ë±ìŠ¤ë¥¼ ë²ˆí˜¸ë¡œ ë³€í™˜
                new_samples = all_numbers[random_indices]
                new_samples, _ = torch.sort(new_samples, dim=1)

                # 3. ê¸°ì¡´ ë‹¹ì²¨ ë²ˆí˜¸ì™€ ì¤‘ë³µ ì œê±°
                # (batch, 1, 6) vs (1, M, 6) -> (batch, M)
                is_in_existing = (
                    (new_samples.unsqueeze(1) == existing_tensor.unsqueeze(0))
                    .all(dim=2)
                    .any(dim=1)
                )
                new_samples = new_samples[~is_in_existing]

                # 4. ìƒì„±ëœ ë°°ì¹˜ ë‚´ ì¤‘ë³µ ì œê±°
                # ì •ë ¬ëœ í…ì„œë¥¼ ì‚¬ìš©í•˜ì—¬ ê³ ìœ ê°’ ì°¾ê¸° (ë” íš¨ìœ¨ì )
                unique_mask = torch.cat(
                    [
                        torch.tensor([True], device=self.device),
                        (new_samples[1:] != new_samples[:-1]).any(dim=1),
                    ]
                )
                new_samples = new_samples[unique_mask]

                # 5. ìµœì¢… ìƒ˜í”Œì…‹ê³¼ ì¤‘ë³µ ì œê±°
                if len(final_samples) > 0:
                    is_in_final = (
                        (new_samples.unsqueeze(1) == final_samples.unsqueeze(0))
                        .all(dim=2)
                        .any(dim=1)
                    )
                    new_samples = new_samples[~is_in_final]

                final_samples = torch.cat([final_samples, new_samples], dim=0)

                # ë©”ëª¨ë¦¬ ì •ë¦¬
                del new_samples, random_indices, unique_mask
                if "is_in_existing" in locals():
                    del is_in_existing
                if "is_in_final" in locals():
                    del is_in_final
                torch.cuda.empty_cache()

        return final_samples[:sample_size].cpu().tolist()

    def _random_negative_samples_cpu(
        self, existing_combinations: Set[Tuple[int, ...]], n: int
    ) -> List[List[int]]:
        """ë‹¨ìˆœ ëœë¤ ë¹„ë‹¹ì²¨ ì¡°í•© ìƒì„± (CPU)"""
        samples = []
        all_numbers = list(range(1, 46))

        seen_combinations = existing_combinations.copy()

        while len(samples) < n:
            combo = tuple(sorted(random.sample(all_numbers, 6)))
            if combo not in seen_combinations:
                samples.append(list(combo))
                seen_combinations.add(combo)
        return samples

    def _pattern_based_negative_samples(
        self, existing_combinations: Set[Tuple[int, ...]], n: int
    ) -> List[List[int]]:
        """íŒ¨í„´ ê¸°ë°˜(í™€ì§, ì—°ì†, ë¶„í¬ ë“±) ê·¹ë‹¨/ì •ìƒ ì¼€ì´ìŠ¤ í¬í•¨"""
        samples = []
        all_numbers = np.arange(1, 46)
        # ê·¹ë‹¨: ëª¨ë‘ í™€ìˆ˜
        if n > 0:
            odd = [i for i in all_numbers if i % 2 == 1]
            if len(odd) >= 6:
                samples.append(sorted(random.sample(odd, 6)))
        # ê·¹ë‹¨: ëª¨ë‘ ì§ìˆ˜
        if n > 1:
            even = [i for i in all_numbers if i % 2 == 0]
            if len(even) >= 6:
                samples.append(sorted(random.sample(even, 6)))
        # ê·¹ë‹¨: ì—°ì†ë²ˆí˜¸
        if n > 2:
            start = random.randint(1, 40)
            samples.append(list(range(start, start + 6)))
        # ì •ìƒ ë¶„í¬: í™€ì§ 3:3, ë¶„ì‚° ë†’ì€ ì¡°í•©
        while len(samples) < n:
            combo = random.sample(list(all_numbers), 6)
            odds = sum(1 for x in combo if x % 2 == 1)
            evens = 6 - odds
            if odds == 3 and evens == 3:
                samples.append(sorted(combo))
        # ì¤‘ë³µ/ë‹¹ì²¨ ì œì™¸
        filtered = []
        seen = set()
        for s in samples:
            t = tuple(sorted(s))
            if t not in existing_combinations and t not in seen:
                filtered.append(list(t))
                seen.add(t)
            if len(filtered) >= n:
                break
        return filtered

    def auto_label(
        self, samples: List[List[int]], positive: bool = False
    ) -> List[Dict[str, Any]]:
        """ML í•™ìŠµìš© ë ˆì´ë¸” ìë™ ë¶€ì—¬ (positive: ë‹¹ì²¨, negative: ë¹„ë‹¹ì²¨)"""
        label = 1 if positive else 0
        return [{"numbers": s, "label": label} for s in samples]

    def _save_raw_samples(
        self, negative_samples: List[List[int]], sample_size: int
    ) -> str:
        """
        ë¹„ë‹¹ì²¨ ì¡°í•© ì €ì¥

        Args:
            negative_samples: ë¹„ë‹¹ì²¨ ë²ˆí˜¸ ì¡°í•© ëª©ë¡
            sample_size: ìƒ˜í”Œ í¬ê¸°

        Returns:
            ì €ì¥ íŒŒì¼ ê²½ë¡œ
        """
        # ê³ ì • íŒŒì¼ ê²½ë¡œ (íƒ€ì„ìŠ¤íƒ¬í”„ ì œê±°)
        file_path = Path(self.cache_dir) / f"negative_samples_{sample_size}.npy"

        # NumPy ë°°ì—´ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
        np_array = np.array(negative_samples, dtype=np.int16)
        np.save(file_path, np_array)

        # ìµœì‹  ë²„ì „ ë§í¬ (ë®ì–´ì“°ê¸°)
        latest_path = Path(self.cache_dir) / "negative_samples_latest.npy"
        np.save(latest_path, np_array)

        self.logger.info(f"ë¹„ë‹¹ì²¨ ì¡°í•© ì €ì¥ ì™„ë£Œ: {file_path}")
        self.logger.info(f"ìµœì‹  ë²„ì „: {latest_path}")

        return str(file_path)

    def _vectorize_samples(
        self,
        negative_samples: List[List[int]],
        draw_data: List[LotteryNumber],
        sample_size: int,
    ) -> str:
        """
        GPU ê°€ì† ë¹„ë‹¹ì²¨ ì¡°í•© ë²¡í„°í™”

        Args:
            negative_samples: ë¹„ë‹¹ì²¨ ë²ˆí˜¸ ì¡°í•© ëª©ë¡
            draw_data: ë‹¹ì²¨ ë²ˆí˜¸ ë°ì´í„°
            sample_size: ìƒ˜í”Œ í¬ê¸°

        Returns:
            ì €ì¥ íŒŒì¼ ê²½ë¡œ
        """
        self.logger.info(f"ğŸš€ GPU ê°€ì† ë²¡í„°í™” ì‹œì‘: {len(negative_samples):,}ê°œ")
        start_time = time.time()

        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ì— ë”°ë¼ ì²˜ë¦¬ ë°©ì‹ ì„ íƒ
        if self.use_gpu and len(negative_samples) > 1000:
            vector_path = self._vectorize_samples_gpu(
                negative_samples, draw_data, sample_size
            )
        else:
            vector_path = self._vectorize_samples_cpu(
                negative_samples, draw_data, sample_size
            )

        # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
        processing_time = time.time() - start_time
        self.performance_stats.update(
            {
                "total_samples": len(negative_samples),
                "processing_time": processing_time,
                "samples_per_second": len(negative_samples) / processing_time,
            }
        )

        if self.use_gpu:
            self.performance_stats["gpu_utilization"] = self._get_gpu_utilization()
            self.performance_stats["memory_usage_mb"] = self._get_gpu_memory_usage()

        self.logger.info(
            f"âœ… ë²¡í„°í™” ì™„ë£Œ: {processing_time:.2f}ì´ˆ ({self.performance_stats['samples_per_second']:.0f} samples/sec)"
        )

        return vector_path

    def _vectorize_samples_gpu(
        self,
        negative_samples: List[List[int]],
        draw_data: List[LotteryNumber],
        sample_size: int,
    ) -> str:
        """GPU ê°€ì† ë²¡í„°í™”"""
        self.logger.info("ğŸ”¥ GPU ê°€ì† ë²¡í„°í™” ì‹¤í–‰")

        try:
            # ë°°ì¹˜ í¬ê¸° ìµœì í™”
            batch_size = self.batch_controller.get_current_batch_size()
            self.logger.info(f"ë°°ì¹˜ í¬ê¸°: {batch_size}")

            # íŠ¹ì„± ë²¡í„° í¬ê¸° í™•ì¸
            expected_num_features = self._estimate_feature_vector_size()
            actual_num_features = self._get_actual_vector_size(
                negative_samples[0], draw_data
            )

            # ê²°ê³¼ ì €ì¥ ë°°ì—´
            feature_vectors = np.zeros(
                (len(negative_samples), actual_num_features), dtype=np.float32
            )

            # GPU ë©”ëª¨ë¦¬ ì‚¬ì „ í• ë‹¹
            with torch.cuda.device(self.device):
                torch.cuda.empty_cache()

                # ë²¡í„°í™” ê²°ê³¼ ì €ì¥
                processed_count = 0

                # ë°°ì¹˜ ë‹¨ìœ„ë¡œ GPU ì²˜ë¦¬
                for i in range(0, len(negative_samples), batch_size):
                    batch = negative_samples[i : i + batch_size]

                    try:
                        # GPU ë°°ì¹˜ ë²¡í„°í™”
                        batch_vectors = self._process_batch_gpu(batch, draw_data)

                        # ê²°ê³¼ ì €ì¥
                        for j, vector in enumerate(batch_vectors):
                            if i + j < len(feature_vectors):
                                feature_vectors[i + j] = vector

                        processed_count += len(batch_vectors)

                        # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                        with self._progress_lock:
                            self.progress = processed_count
                            progress_pct = (self.progress / len(negative_samples)) * 100
                            self.logger.info(f"GPU ë²¡í„°í™” ì§„í–‰: {progress_pct:.1f}%")

                        # ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì •
                        if i % (batch_size * 3) == 0:  # 3ë°°ì¹˜ë§ˆë‹¤ ì²´í¬
                            self._adjust_batch_size_based_on_memory()
                            batch_size = self.batch_controller.get_current_batch_size()

                    except torch.cuda.OutOfMemoryError:
                        self.logger.warning("GPU ë©”ëª¨ë¦¬ ë¶€ì¡±, ë°°ì¹˜ í¬ê¸° ê°ì†Œ")
                        batch_size = self.batch_controller.handle_oom()
                        torch.cuda.empty_cache()

                        # ì‘ì€ ë°°ì¹˜ë¡œ ì¬ì‹œë„
                        small_batch = batch[:batch_size]
                        batch_vectors = self._process_batch_gpu(small_batch, draw_data)

                        # ê²°ê³¼ ì €ì¥
                        for j, vector in enumerate(batch_vectors):
                            if i + j < len(feature_vectors):
                                feature_vectors[i + j] = vector

                        processed_count += len(batch_vectors)

                # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                torch.cuda.empty_cache()

        except Exception as e:
            self.logger.error(f"GPU ë²¡í„°í™” ì‹¤íŒ¨: {e}")
            # CPU í´ë°±
            return self._vectorize_samples_cpu(negative_samples, draw_data, sample_size)

        # ê²°ê³¼ ì €ì¥
        return self._save_vectorized_results(feature_vectors, sample_size)

    def _process_batch_gpu(
        self, batch: List[List[int]], draw_data: List[LotteryNumber]
    ) -> List[np.ndarray]:
        """GPU ë°°ì¹˜ ì²˜ë¦¬"""
        try:
            # PyTorch 2.0+ AMP API ì§ì ‘ ì‚¬ìš©
            with torch.cuda.amp.autocast():
                batch_vectors = []

                # ë°°ì¹˜ ë‚´ ê° ì¡°í•© ì²˜ë¦¬
                for combination in batch:
                    # ë¹ ë¥¸ íŒ¨í„´ íŠ¹ì„± ì¶”ì¶œ
                    features = self._extract_pattern_features_fast(
                        combination, draw_data
                    )

                    # ë²¡í„°í™”
                    vector = self._vectorize_features_gpu(features)
                    batch_vectors.append(vector)

                return batch_vectors

        except Exception as e:
            self.logger.error(f"GPU ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            # CPU í´ë°±
            return [
                self._vectorize_combination_cpu(combo, draw_data) for combo in batch
            ]

    def _extract_pattern_features_fast(
        self, combination: List[int], draw_data: List[LotteryNumber]
    ) -> Dict[str, Any]:
        """ë¹ ë¥¸ íŒ¨í„´ íŠ¹ì„± ì¶”ì¶œ (GPU ìµœì í™”)"""
        try:
            # ê¸°ë³¸ íŠ¹ì„± ë¹ ë¥¸ ê³„ì‚°
            features = {
                "max_consecutive_length": self._calc_consecutive_fast(combination),
                "total_sum": sum(combination),
                "odd_count": sum(1 for x in combination if x % 2 == 1),
                "even_count": sum(1 for x in combination if x % 2 == 0),
                "gap_avg": self._calc_gap_avg_fast(combination),
                "gap_std": self._calc_gap_std_fast(combination),
                "range_counts": self._calc_range_counts_fast(combination),
                "cluster_overlap_ratio": 0.3,  # ê¸°ë³¸ê°’ (ë¹ ë¥¸ ì²˜ë¦¬)
                "frequent_pair_score": 0.05,  # ê¸°ë³¸ê°’
                "roi_weight": 1.0,  # ê¸°ë³¸ê°’
                "consecutive_score": 0.0,  # ê¸°ë³¸ê°’
                "trend_score_avg": 0.5,  # ê¸°ë³¸ê°’
                "trend_score_max": 0.8,  # ê¸°ë³¸ê°’
                "trend_score_min": 0.2,  # ê¸°ë³¸ê°’
                "risk_score": 0.5,  # ê¸°ë³¸ê°’
            }

            return features

        except Exception as e:
            self.logger.debug(f"ë¹ ë¥¸ íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ íŠ¹ì„± ë°˜í™˜
            return {
                "max_consecutive_length": 0,
                "total_sum": sum(combination),
                "odd_count": 3,
                "even_count": 3,
                "gap_avg": 7.5,
                "gap_std": 5.0,
                "range_counts": [1, 1, 1, 1, 2],
                "cluster_overlap_ratio": 0.3,
                "frequent_pair_score": 0.05,
                "roi_weight": 1.0,
                "consecutive_score": 0.0,
                "trend_score_avg": 0.5,
                "trend_score_max": 0.8,
                "trend_score_min": 0.2,
                "risk_score": 0.5,
            }

    def _calc_consecutive_fast(self, combination: List[int]) -> int:
        """ë¹ ë¥¸ ì—°ì† ë²ˆí˜¸ ê³„ì‚°"""
        if len(combination) < 2:
            return 0

        sorted_combo = sorted(combination)
        max_consecutive = 1
        current_consecutive = 1

        for i in range(1, len(sorted_combo)):
            if sorted_combo[i] == sorted_combo[i - 1] + 1:
                current_consecutive += 1
                max_consecutive = max(max_consecutive, current_consecutive)
            else:
                current_consecutive = 1

        return max_consecutive

    def _calc_gap_avg_fast(self, combination: List[int]) -> float:
        """ë¹ ë¥¸ ê°„ê²© í‰ê·  ê³„ì‚°"""
        if len(combination) < 2:
            return 0.0

        sorted_combo = sorted(combination)
        gaps = [
            sorted_combo[i] - sorted_combo[i - 1] for i in range(1, len(sorted_combo))
        ]
        return sum(gaps) / len(gaps)

    def _calc_gap_std_fast(self, combination: List[int]) -> float:
        """ë¹ ë¥¸ ê°„ê²© í‘œì¤€í¸ì°¨ ê³„ì‚°"""
        if len(combination) < 2:
            return 0.0

        sorted_combo = sorted(combination)
        gaps = [
            sorted_combo[i] - sorted_combo[i - 1] for i in range(1, len(sorted_combo))
        ]

        if len(gaps) < 2:
            return 0.0

        mean_gap = sum(gaps) / len(gaps)
        variance = sum((gap - mean_gap) ** 2 for gap in gaps) / len(gaps)
        return variance**0.5

    def _calc_range_counts_fast(self, combination: List[int]) -> List[int]:
        """ë¹ ë¥¸ ë²”ìœ„ë³„ ê°œìˆ˜ ê³„ì‚°"""
        ranges = [0, 0, 0, 0, 0]  # 1-9, 10-18, 19-27, 28-36, 37-45

        for num in combination:
            if 1 <= num <= 9:
                ranges[0] += 1
            elif 10 <= num <= 18:
                ranges[1] += 1
            elif 19 <= num <= 27:
                ranges[2] += 1
            elif 28 <= num <= 36:
                ranges[3] += 1
            elif 37 <= num <= 45:
                ranges[4] += 1

        return ranges

    def _vectorize_features_gpu(self, features: Dict[str, Any]) -> np.ndarray:
        """GPU ê¸°ë°˜ íŠ¹ì„± ë²¡í„°í™”"""
        try:
            # íŒ¨í„´ ë¶„ì„ê¸°ì™€ ë™ì¼í•œ ë²¡í„°í™” (19ì°¨ì›)
            vector = np.array(
                [
                    features["max_consecutive_length"] / 6.0,
                    features["total_sum"] / 270.0,
                    features["odd_count"] / 6.0,
                    features["even_count"] / 6.0,
                    features["gap_avg"] / 20.0,
                    features["gap_std"] / 15.0,
                    *[count / 6.0 for count in features["range_counts"][:5]],
                    features["cluster_overlap_ratio"],
                    features["frequent_pair_score"] * 10.0,
                    features["roi_weight"] / 2.0,
                    features["consecutive_score"] + 0.3,
                    features["trend_score_avg"] * 10.0,
                    features["trend_score_max"] * 10.0,
                    features["trend_score_min"] * 10.0,
                    features["risk_score"],
                ],
                dtype=np.float32,
            )

            return vector

        except Exception as e:
            self.logger.error(f"GPU ë²¡í„°í™” ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ë²¡í„° ë°˜í™˜
            return np.array([0.5] * 19, dtype=np.float32)

    def _vectorize_samples_cpu(
        self,
        negative_samples: List[List[int]],
        draw_data: List[LotteryNumber],
        sample_size: int,
    ) -> str:
        """CPU ê¸°ë°˜ ë²¡í„°í™” (í´ë°±)"""
        self.logger.info(f"ğŸ’» CPU ë²¡í„°í™” ì‹¤í–‰ (CPUBatchProcessor ì‚¬ìš©)")

        if not self.cpu_batch_processor:
            logger.error(
                "CPUBatchProcessorê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. CPU ëª¨ë“œì—ì„œ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
            # ë¹ˆ ê²°ê³¼ë¥¼ ì €ì¥í•˜ê³  ê²½ë¡œë¥¼ ë°˜í™˜í•˜ê±°ë‚˜ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            # ì—¬ê¸°ì„œëŠ” ë¹ˆ ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
            empty_vectors = np.array([])
            return self._save_vectorized_results(empty_vectors, sample_size)

        actual_num_features = self._get_actual_vector_size(
            negative_samples[0], draw_data
        )

        # ë²¡í„°í™” ì‘ì—…ì„ ì²˜ë¦¬í•  í•¨ìˆ˜ ì •ì˜
        def vectorize_worker(combination_batch):
            return [
                self._vectorize_combination_cpu(combo, draw_data)
                for combo in combination_batch
            ]

        # CPUBatchProcessorë¥¼ ì‚¬ìš©í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬
        results = self.cpu_batch_processor.process_batches(
            negative_samples, vectorize_worker
        )

        # ê²°ê³¼ ë°°ì—´ ìƒì„± ë° ì±„ìš°ê¸°
        feature_vectors = np.zeros(
            (len(negative_samples), actual_num_features), dtype=np.float32
        )
        for i, vector in enumerate(results):
            if vector is not None and i < len(feature_vectors):
                feature_vectors[i] = vector

        return self._save_vectorized_results(feature_vectors, sample_size)

    def _get_actual_vector_size(
        self, sample_combination: List[int], draw_data: List[LotteryNumber]
    ) -> int:
        """ì‹¤ì œ ë²¡í„° í¬ê¸° í™•ì¸"""
        try:
            # ë¹ ë¥¸ íŠ¹ì„± ì¶”ì¶œë¡œ ë²¡í„° í¬ê¸° í™•ì¸
            features = self._extract_pattern_features_fast(
                sample_combination, draw_data
            )
            vector = self._vectorize_features_gpu(features)
            return len(vector)
        except Exception as e:
            self.logger.warning(f"ë²¡í„° í¬ê¸° í™•ì¸ ì‹¤íŒ¨: {e}")
            return 19  # ê¸°ë³¸ í¬ê¸°

    def _vectorize_combination_cpu(
        self, combination: List[int], draw_data: List[LotteryNumber]
    ) -> np.ndarray:
        """CPU ê¸°ë°˜ ë‹¨ì¼ ì¡°í•© ë²¡í„°í™”"""
        try:
            # ë¹ ë¥¸ íŠ¹ì„± ì¶”ì¶œ
            features = self._extract_pattern_features_fast(combination, draw_data)

            # ë²¡í„°í™”
            return self._vectorize_features_gpu(features)  # ë™ì¼í•œ ë²¡í„°í™” í•¨ìˆ˜ ì‚¬ìš©

        except Exception as e:
            self.logger.debug(f"CPU ë²¡í„°í™” ì‹¤íŒ¨: {e}")
            return np.array([0.5] * 19, dtype=np.float32)

    def _save_vectorized_results(
        self, feature_vectors: np.ndarray, sample_size: int
    ) -> str:
        """ë²¡í„°í™” ê²°ê³¼ ì €ì¥"""
        try:
            # ê³ ì • íŒŒì¼ ê²½ë¡œ (íƒ€ì„ìŠ¤íƒ¬í”„ ì œê±°)
            file_path = Path(self.cache_dir) / f"negative_vectors_{sample_size}.npy"
            latest_path = Path(self.cache_dir) / "negative_vectors_latest.npy"

            # ì €ì¥
            np.save(file_path, feature_vectors)
            np.save(latest_path, feature_vectors)

            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •
            mem_used = feature_vectors.nbytes / (1024 * 1024)  # MB

            self.logger.info(f"ë²¡í„°í™” ê²°ê³¼ ì €ì¥: {file_path}")
            self.logger.info(
                f"ë²¡í„° í˜•íƒœ: {feature_vectors.shape}, ë©”ëª¨ë¦¬: {mem_used:.1f}MB"
            )

            return str(file_path)

        except Exception as e:
            self.logger.error(f"ë²¡í„° ì €ì¥ ì‹¤íŒ¨: {e}")
            raise

    def _adjust_batch_size_based_on_memory(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸°ë°˜ ë°°ì¹˜ í¬ê¸° ì¡°ì •"""
        if not self.use_gpu:
            return

        try:
            # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  í™•ì¸
            memory_usage = self._get_gpu_memory_usage()
            memory_total = torch.cuda.get_device_properties(
                self.device
            ).total_memory / (
                1024**3
            )  # GB
            usage_ratio = memory_usage / (memory_total * 1024)  # ë¹„ìœ¨

            if usage_ratio > 0.8:  # 80% ì´ìƒ ì‚¬ìš©ì‹œ
                self.batch_controller.reduce_batch_size()
                self.logger.info(
                    f"ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  {usage_ratio*100:.1f}% - ë°°ì¹˜ í¬ê¸° ê°ì†Œ"
                )
            elif usage_ratio < 0.5:  # 50% ë¯¸ë§Œ ì‚¬ìš©ì‹œ
                self.batch_controller.increase_batch_size()
                self.logger.info(
                    f"ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  {usage_ratio*100:.1f}% - ë°°ì¹˜ í¬ê¸° ì¦ê°€"
                )

        except Exception as e:
            self.logger.debug(f"ë°°ì¹˜ í¬ê¸° ì¡°ì • ì‹¤íŒ¨: {e}")

    def _get_gpu_utilization(self) -> float:
        """GPU ì‚¬ìš©ë¥  ì¡°íšŒ"""
        try:
            if self.cuda_optimizer and hasattr(
                self.cuda_optimizer, "get_gpu_utilization"
            ):
                return self.cuda_optimizer.get_gpu_utilization()
            return 0.0
        except Exception:
            return 0.0

    def _get_gpu_memory_usage(self) -> float:
        """GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ (MB)"""
        try:
            if self.use_gpu:
                return torch.cuda.memory_allocated(self.device) / (1024**2)
            return 0.0
        except Exception:
            return 0.0

    def _estimate_feature_vector_size(self) -> int:
        """
        íŠ¹ì„± ë²¡í„° í¬ê¸° ì¶”ì •

        Returns:
            ì˜ˆìƒ ë²¡í„° í¬ê¸°
        """
        # ê¸°ë³¸ê°’ (íŒ¨í„´ ë²¡í„°ë¼ì´ì €ì˜ ì¼ë°˜ì ì¸ ì¶œë ¥ í¬ê¸°)
        default_size = 100

        # ê¸°ì¡´ feature_vector_full.npy íŒŒì¼ì´ ìˆìœ¼ë©´ ê·¸ í¬ê¸°ë¥¼ ì°¸ì¡°
        vector_file = Path(self.cache_dir) / "feature_vector_full.npy"
        if vector_file.exists():
            try:
                sample_vector = np.load(vector_file)
                if len(sample_vector.shape) > 0:
                    return sample_vector.shape[0]
            except Exception as e:
                self.logger.warning(f"ë²¡í„° íŒŒì¼ ë¡œë”© ì‹¤íŒ¨: {e}")

        return default_size

    def _check_memory_usage(self, memory_limit_bytes: Union[int, float]) -> float:
        """
        ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸ ë° ì¡°ì¹˜

        Args:
            memory_limit_bytes: ë©”ëª¨ë¦¬ í•œê³„ (ë°”ì´íŠ¸)

        Returns:
            í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ë°”ì´íŠ¸)
        """
        # í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
        try:
            process = psutil.Process(os.getpid())
            memory_info = process.memory_info()
            memory_usage = memory_info.rss  # í˜„ì¬ ì‹¤ì œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰

            # ë©”ëª¨ë¦¬ í•œê³„ì— ê·¼ì ‘í•˜ë©´ ë¡œê¹…
            if memory_usage > memory_limit_bytes * 0.8:
                self.logger.warning(
                    f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê²½ê³ : {memory_usage / (1024**2):.1f}MB / "
                    f"{memory_limit_bytes / (1024**2):.1f}MB ({memory_usage / memory_limit_bytes * 100:.1f}%)"
                )

                # ë©”ëª¨ë¦¬ í•œê³„ë¥¼ ì´ˆê³¼í•˜ë©´ GC ê°•ì œ ì‹¤í–‰
                if memory_usage > memory_limit_bytes:
                    self.logger.warning("ë©”ëª¨ë¦¬ í•œê³„ ì´ˆê³¼, ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì‹¤í–‰")
                    gc.collect()

            return memory_usage

        except ImportError:
            # psutilì´ ì—†ìœ¼ë©´ ë¡œê¹…ë§Œ
            self.logger.info(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ì„ ìœ„í•´ psutil íŒ¨í‚¤ì§€ ì„¤ì¹˜ ê¶Œì¥")
            return 0.0
        except Exception as e:
            self.logger.warning(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return 0.0

    def _generate_performance_report(
        self,
        start_time: float,
        end_time: float,
        sample_count: int,
        memory_used_mb: float,
        vector_path: str,
        warnings: Optional[List[str]] = None,
    ) -> str:
        """
        ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„± ë° ì €ì¥

        Args:
            start_time: ì‹œì‘ ì‹œê°„
            end_time: ì¢…ë£Œ ì‹œê°„
            sample_count: ìƒ˜í”Œ ìˆ˜
            memory_used_mb: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)
            vector_path: ë²¡í„° íŒŒì¼ ê²½ë¡œ
            warnings: ê²½ê³  ë©”ì‹œì§€ ëª©ë¡

        Returns:
            ì„±ëŠ¥ ë³´ê³ ì„œ íŒŒì¼ ê²½ë¡œ
        """
        # ì„±ëŠ¥ ë³´ê³ ì„œ ë°ì´í„°
        report_data = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(start_time)),
            "execution_time": end_time - start_time,
            "sample_count": sample_count,
            "memory_used_mb": memory_used_mb,
            "vector_path": vector_path,
            "warnings": warnings or [],
            "config": {
                "batch_size": self.batch_controller.get_batch_size(),
                "cache_dir": self.cache_dir,
            },
        }

        # ì„±ëŠ¥ ë³´ê³ ì„œ ì €ì¥
        try:
            # logs_dir ì„¤ì •ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
            logs_dir = self.config.get("paths", {}).get("logs_dir", "logs")
            Path(logs_dir).mkdir(exist_ok=True, parents=True)

            report_filename = (
                f"negative_sample_performance_{sample_count}_{int(start_time)}.json"
            )
            report_path = Path(logs_dir) / report_filename

            with open(report_path, "w", encoding="utf-8") as f:
                json.dump(report_data, f, indent=2, ensure_ascii=False)

            return str(report_path)

        except Exception as e:
            self.logger.error(f"ì„±ëŠ¥ ë³´ê³ ì„œ ì €ì¥ ì‹¤íŒ¨: {e}")
            return ""


def generate_negative_samples(
    draw_data: List[LotteryNumber], sample_size: int = 100000
) -> Dict[str, Any]:
    """
    ë¹„ë‹¹ì²¨ ì¡°í•© ìƒ˜í”Œ ìƒì„± (ëª¨ë“ˆ ë ˆë²¨ í•¨ìˆ˜)

    Args:
        draw_data: ë‹¹ì²¨ ë²ˆí˜¸ ë°ì´í„°
        sample_size: ìƒì„±í•  ìƒ˜í”Œ í¬ê¸°

    Returns:
        ìƒì„± ê²°ê³¼ ì •ë³´
    """
    generator = NegativeSampleGenerator()
    return generator.generate_samples(draw_data, sample_size)
