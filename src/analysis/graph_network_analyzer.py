"""
ê·¸ë˜í”„ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ê¸°
- ë²ˆí˜¸ ê°„ ê´€ê³„ë¥¼ ê·¸ë˜í”„ë¡œ ëª¨ë¸ë§
- ì»¤ë®¤ë‹ˆí‹° íƒì§€, ì¤‘ì‹¬ì„± ë¶„ì„, Node2Vec ì„ë² ë”© ë“±ì„ í¬í•¨í•©ë‹ˆë‹¤.
"""

import networkx as nx
import numpy as np
from typing import Dict, Any, List, Optional
from collections import defaultdict

# ê¸°ì¡´ ì‹œìŠ¤í…œ import (ìˆœí™˜ ì°¸ì¡° ë°©ì§€)
from ..utils.unified_logging import get_logger
from ..shared.types import LotteryNumber
from ..analysis.base_analyzer import BaseAnalyzer

logger = get_logger(__name__)


class GraphNetworkAnalyzer(BaseAnalyzer):
    """
    ê·¸ë˜í”„ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ê¸°
    - ë²ˆí˜¸ ê°„ ë™ì‹œ ì¶œí˜„ ê´€ê³„ë¥¼ ê·¸ë˜í”„ë¡œ ëª¨ë¸ë§
    - ì»¤ë®¤ë‹ˆí‹° íƒì§€ë¡œ ë²ˆí˜¸ ê·¸ë£¹ ë°œê²¬
    - ë‹¤ì–‘í•œ ì¤‘ì‹¬ì„± ì§€í‘œ ê³„ì‚°
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__(config or {}, "graph_network")
        self.graph = None
        self.communities = None
        self.centrality_metrics = {}
        self.node_embeddings = {}

        # ì„¤ì • ì´ˆê¸°í™”
        self.min_co_occurrence = self.config.get("min_co_occurrence", 2)
        self.use_node2vec = self.config.get("use_node2vec", False)
        self.embedding_dim = self.config.get("embedding_dim", 64)

        logger.info("âœ… GraphNetworkAnalyzer ì´ˆê¸°í™” ì™„ë£Œ")

    def _analyze_impl(self, data: List[LotteryNumber]) -> Dict[str, Any]:
        """ê·¸ë˜í”„ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ë©”ì¸ ë¡œì§"""
        try:
            logger.info(f"ğŸ”— ê·¸ë˜í”„ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì‹œì‘: {len(data)}ê°œ íšŒì°¨")

            # 1. ë²ˆí˜¸ ê³µì¶œí˜„ ê·¸ë˜í”„ ìƒì„±
            co_occurrence_graph = self._build_co_occurrence_graph(data)

            # 2. ì»¤ë®¤ë‹ˆí‹° íƒì§€
            communities = self._detect_communities(co_occurrence_graph)

            # 3. ì¤‘ì‹¬ì„± ë¶„ì„
            centrality_analysis = self._analyze_centrality(co_occurrence_graph)

            # 4. ê·¸ë˜í”„ ì„ë² ë”© (ì„ íƒì )
            embeddings = self._generate_graph_embeddings(co_occurrence_graph)

            # 5. ê·¸ë˜í”„ í†µê³„
            graph_statistics = self._calculate_graph_statistics(co_occurrence_graph)

            # 6. ë²ˆí˜¸ë³„ ê·¸ë˜í”„ íŠ¹ì„±
            number_graph_features = self._extract_number_graph_features(
                co_occurrence_graph, communities, centrality_analysis
            )

            result = {
                "graph_statistics": graph_statistics,
                "communities": communities,
                "centrality_analysis": centrality_analysis,
                "embeddings": embeddings,
                "number_graph_features": number_graph_features,
                "co_occurrence_matrix": self._graph_to_matrix(co_occurrence_graph),
            }

            # ë‚´ë¶€ ìƒíƒœ ì €ì¥
            self.graph = co_occurrence_graph
            self.communities = communities
            self.centrality_metrics = centrality_analysis
            self.node_embeddings = embeddings

            logger.info("âœ… ê·¸ë˜í”„ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì™„ë£Œ")
            return result

        except Exception as e:
            logger.error(f"ê·¸ë˜í”„ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return self._get_fallback_result()

    def _build_co_occurrence_graph(self, data: List[LotteryNumber]) -> nx.Graph:
        """ë²ˆí˜¸ ê³µì¶œí˜„ ê·¸ë˜í”„ ìƒì„±"""
        try:
            # ê³µì¶œí˜„ íšŸìˆ˜ ê³„ì‚°
            co_occurrence_counts = defaultdict(int)

            for draw in data:
                numbers = sorted(draw.numbers)
                # ëª¨ë“  ë²ˆí˜¸ ìŒì˜ ê³µì¶œí˜„ íšŸìˆ˜ ê³„ì‚°
                for i in range(len(numbers)):
                    for j in range(i + 1, len(numbers)):
                        pair = (numbers[i], numbers[j])
                        co_occurrence_counts[pair] += 1

            # ê·¸ë˜í”„ ìƒì„±
            G = nx.Graph()

            # ëª¨ë“  ë²ˆí˜¸ë¥¼ ë…¸ë“œë¡œ ì¶”ê°€
            for num in range(1, 46):
                G.add_node(num)

            # ìµœì†Œ ê³µì¶œí˜„ íšŸìˆ˜ ì´ìƒì¸ ìŒë§Œ ì—£ì§€ë¡œ ì¶”ê°€
            for (num1, num2), count in co_occurrence_counts.items():
                if count >= self.min_co_occurrence:
                    # ê°€ì¤‘ì¹˜ëŠ” ê³µì¶œí˜„ íšŸìˆ˜
                    weight = count / len(data)  # ì •ê·œí™”
                    G.add_edge(num1, num2, weight=weight, count=count)

            logger.info(
                f"ê·¸ë˜í”„ ìƒì„± ì™„ë£Œ: {G.number_of_nodes()}ê°œ ë…¸ë“œ, {G.number_of_edges()}ê°œ ì—£ì§€"
            )
            return G

        except Exception as e:
            logger.error(f"ê·¸ë˜í”„ ìƒì„± ì‹¤íŒ¨: {e}")
            return nx.Graph()

    def _detect_communities(self, graph: nx.Graph) -> Dict[str, Any]:
        """ì»¤ë®¤ë‹ˆí‹° íƒì§€ (Louvain ì•Œê³ ë¦¬ì¦˜)"""
        try:
            communities_result = {}

            # 1. NetworkX ë‚´ì¥ ì»¤ë®¤ë‹ˆí‹° íƒì§€
            try:
                from networkx.algorithms import community

                communities_greedy = community.greedy_modularity_communities(graph)
                communities_result["greedy"] = [list(c) for c in communities_greedy]

                # ëª¨ë“ˆëŸ¬ë¦¬í‹° ê³„ì‚°
                modularity = community.modularity(graph, communities_greedy)
                communities_result["modularity"] = modularity

            except ImportError:
                logger.warning("NetworkX ì»¤ë®¤ë‹ˆí‹° ëª¨ë“ˆ ì—†ìŒ")

            # 2. Louvain ì•Œê³ ë¦¬ì¦˜ (ì„ íƒì )
            try:
                import community as community_louvain

                partition = community_louvain.best_partition(graph)

                # ì»¤ë®¤ë‹ˆí‹°ë³„ ë…¸ë“œ ê·¸ë£¹í™”
                communities_louvain = defaultdict(list)
                for node, comm_id in partition.items():
                    communities_louvain[comm_id].append(node)

                communities_result["louvain"] = list(communities_louvain.values())
                communities_result["louvain_modularity"] = community_louvain.modularity(
                    partition, graph
                )

            except ImportError:
                logger.warning("Louvain ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ, ê¸°ë³¸ ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©")

            # 3. ì»¤ë®¤ë‹ˆí‹° í†µê³„
            if "greedy" in communities_result:
                communities_stats = self._analyze_community_statistics(
                    communities_result["greedy"], graph
                )
                communities_result["statistics"] = communities_stats

            logger.info(
                f"ì»¤ë®¤ë‹ˆí‹° íƒì§€ ì™„ë£Œ: {len(communities_result.get('greedy', []))}ê°œ ì»¤ë®¤ë‹ˆí‹°"
            )
            return communities_result

        except Exception as e:
            logger.error(f"ì»¤ë®¤ë‹ˆí‹° íƒì§€ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    def _analyze_centrality(self, graph: nx.Graph) -> Dict[str, Any]:
        """ë‹¤ì–‘í•œ ì¤‘ì‹¬ì„± ì§€í‘œ ë¶„ì„"""
        try:
            centrality_result = {}

            # 1. ì°¨ìˆ˜ ì¤‘ì‹¬ì„± (Degree Centrality)
            degree_centrality = nx.degree_centrality(graph)
            centrality_result["degree"] = degree_centrality

            # 2. ê·¼ì ‘ ì¤‘ì‹¬ì„± (Closeness Centrality)
            try:
                closeness_centrality = nx.closeness_centrality(graph)
                centrality_result["closeness"] = closeness_centrality
            except:
                logger.warning("ê·¼ì ‘ ì¤‘ì‹¬ì„± ê³„ì‚° ì‹¤íŒ¨")

            # 3. ë§¤ê°œ ì¤‘ì‹¬ì„± (Betweenness Centrality)
            try:
                betweenness_centrality = nx.betweenness_centrality(graph)
                centrality_result["betweenness"] = betweenness_centrality
            except:
                logger.warning("ë§¤ê°œ ì¤‘ì‹¬ì„± ê³„ì‚° ì‹¤íŒ¨")

            # 4. ê³ ìœ ë²¡í„° ì¤‘ì‹¬ì„± (Eigenvector Centrality)
            try:
                eigenvector_centrality = nx.eigenvector_centrality(graph, max_iter=500)
                centrality_result["eigenvector"] = eigenvector_centrality
            except:
                logger.warning("ê³ ìœ ë²¡í„° ì¤‘ì‹¬ì„± ê³„ì‚° ì‹¤íŒ¨")

            # 5. PageRank
            try:
                pagerank = nx.pagerank(graph)
                centrality_result["pagerank"] = pagerank
            except:
                logger.warning("PageRank ê³„ì‚° ì‹¤íŒ¨")

            # 6. ì¤‘ì‹¬ì„± í†µê³„
            centrality_stats = self._analyze_centrality_statistics(centrality_result)
            centrality_result["statistics"] = centrality_stats

            logger.info(f"ì¤‘ì‹¬ì„± ë¶„ì„ ì™„ë£Œ: {len(centrality_result)}ê°œ ì§€í‘œ")
            return centrality_result

        except Exception as e:
            logger.error(f"ì¤‘ì‹¬ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    def _generate_graph_embeddings(self, graph: nx.Graph) -> Dict[str, Any]:
        """ê·¸ë˜í”„ ì„ë² ë”© ìƒì„± (Node2Vec)"""
        try:
            embeddings_result = {}

            if not self.use_node2vec:
                logger.info("Node2Vec ì‚¬ìš© ì•ˆí•¨")
                return embeddings_result

            # Node2Vec ì„ë² ë”© (ì„ íƒì )
            try:
                from node2vec import Node2Vec

                # Node2Vec ëª¨ë¸ ìƒì„±
                node2vec = Node2Vec(
                    graph,
                    dimensions=self.embedding_dim,
                    walk_length=30,
                    num_walks=200,
                    workers=4,
                )

                # ëª¨ë¸ í•™ìŠµ
                model = node2vec.fit(window=10, min_count=1, batch_words=4)

                # ì„ë² ë”© ë²¡í„° ì¶”ì¶œ
                embeddings = {}
                for node in graph.nodes():
                    try:
                        embeddings[node] = model.wv[str(node)].tolist()
                    except KeyError:
                        embeddings[node] = [0.0] * self.embedding_dim

                embeddings_result["node2vec"] = embeddings
                embeddings_result["embedding_dim"] = self.embedding_dim

                logger.info(f"Node2Vec ì„ë² ë”© ì™„ë£Œ: {len(embeddings)}ê°œ ë…¸ë“œ")

            except ImportError:
                logger.warning("Node2Vec ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ")
            except Exception as e:
                logger.warning(f"Node2Vec ì‹¤í–‰ ì‹¤íŒ¨: {e}")

            # ëŒ€ì²´ ì„ë² ë”© ë°©ë²• (ì¸ì ‘ í–‰ë ¬ ê¸°ë°˜)
            if "node2vec" not in embeddings_result:
                adjacency_embeddings = self._generate_adjacency_embeddings(graph)
                embeddings_result["adjacency"] = adjacency_embeddings

            return embeddings_result

        except Exception as e:
            logger.error(f"ê·¸ë˜í”„ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    def _generate_adjacency_embeddings(self, graph: nx.Graph) -> Dict[int, List[float]]:
        """ì¸ì ‘ í–‰ë ¬ ê¸°ë°˜ ì„ë² ë”© ìƒì„±"""
        try:
            # ì¸ì ‘ í–‰ë ¬ ìƒì„±
            adjacency_matrix = nx.adjacency_matrix(
                graph, nodelist=range(1, 46)
            ).toarray()

            # SVD ë¶„í•´ë¡œ ì°¨ì› ì¶•ì†Œ
            from sklearn.decomposition import TruncatedSVD

            svd = TruncatedSVD(n_components=min(32, adjacency_matrix.shape[0] - 1))
            embeddings_matrix = svd.fit_transform(adjacency_matrix)

            # ë…¸ë“œë³„ ì„ë² ë”© ë”•ì…”ë„ˆë¦¬ ìƒì„±
            embeddings = {}
            for i, node in enumerate(range(1, 46)):
                embeddings[node] = embeddings_matrix[i].tolist()

            logger.info(f"ì¸ì ‘ í–‰ë ¬ ì„ë² ë”© ì™„ë£Œ: {len(embeddings)}ê°œ ë…¸ë“œ")
            return embeddings

        except Exception as e:
            logger.error(f"ì¸ì ‘ í–‰ë ¬ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return {}

    def _calculate_graph_statistics(self, graph: nx.Graph) -> Dict[str, Any]:
        """ê·¸ë˜í”„ í†µê³„ ê³„ì‚°"""
        try:
            stats = {
                "nodes": graph.number_of_nodes(),
                "edges": graph.number_of_edges(),
                "density": nx.density(graph),
                "is_connected": nx.is_connected(graph),
                "number_of_components": nx.number_connected_components(graph),
            }

            # ì—°ê²°ëœ ê·¸ë˜í”„ì¸ ê²½ìš° ì¶”ê°€ í†µê³„
            if stats["is_connected"]:
                stats["diameter"] = nx.diameter(graph)
                stats["average_path_length"] = nx.average_shortest_path_length(graph)
                stats["radius"] = nx.radius(graph)

            # í´ëŸ¬ìŠ¤í„°ë§ ê³„ìˆ˜
            stats["average_clustering"] = nx.average_clustering(graph)
            stats["transitivity"] = nx.transitivity(graph)

            # ì°¨ìˆ˜ í†µê³„
            degrees = dict(graph.degree())
            degree_values = list(degrees.values())
            stats["degree_statistics"] = {
                "mean": np.mean(degree_values),
                "std": np.std(degree_values),
                "max": np.max(degree_values),
                "min": np.min(degree_values),
                "median": np.median(degree_values),
            }

            return stats

        except Exception as e:
            logger.error(f"ê·¸ë˜í”„ í†µê³„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    def _extract_number_graph_features(
        self, graph: nx.Graph, communities: Dict[str, Any], centrality: Dict[str, Any]
    ) -> Dict[int, Dict[str, float]]:
        """ë²ˆí˜¸ë³„ ê·¸ë˜í”„ íŠ¹ì„± ì¶”ì¶œ"""
        try:
            number_features = {}

            for num in range(1, 46):
                features = {}

                # ê¸°ë³¸ ê·¸ë˜í”„ íŠ¹ì„±
                features["degree"] = graph.degree(num)
                features["weighted_degree"] = graph.degree(num, weight="weight")

                # ì¤‘ì‹¬ì„± ì§€í‘œ
                for centrality_type, centrality_values in centrality.items():
                    if centrality_type != "statistics" and isinstance(
                        centrality_values, dict
                    ):
                        features[f"centrality_{centrality_type}"] = (
                            centrality_values.get(num, 0.0)
                        )

                # í´ëŸ¬ìŠ¤í„°ë§ ê³„ìˆ˜
                features["clustering_coefficient"] = nx.clustering(graph, num)

                # ì»¤ë®¤ë‹ˆí‹° ì •ë³´
                if "greedy" in communities:
                    community_id = self._find_community_id(num, communities["greedy"])
                    features["community_id"] = community_id
                    features["community_size"] = (
                        len(communities["greedy"][community_id])
                        if community_id >= 0
                        else 0
                    )

                # ì´ì›ƒ ë…¸ë“œ í†µê³„
                neighbors = list(graph.neighbors(num))
                features["neighbor_count"] = len(neighbors)
                if neighbors:
                    features["neighbor_avg"] = np.mean(neighbors)
                    features["neighbor_std"] = np.std(neighbors)

                number_features[num] = features

            return number_features

        except Exception as e:
            logger.error(f"ë²ˆí˜¸ë³„ ê·¸ë˜í”„ íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {}

    def _find_community_id(self, node: int, communities: List[List[int]]) -> int:
        """ë…¸ë“œê°€ ì†í•œ ì»¤ë®¤ë‹ˆí‹° ID ì°¾ê¸°"""
        for i, community in enumerate(communities):
            if node in community:
                return i
        return -1

    def _analyze_community_statistics(
        self, communities: List[List[int]], graph: nx.Graph
    ) -> Dict[str, Any]:
        """ì»¤ë®¤ë‹ˆí‹° í†µê³„ ë¶„ì„"""
        try:
            stats = {
                "community_count": len(communities),
                "community_sizes": [len(comm) for comm in communities],
                "largest_community_size": max(len(comm) for comm in communities),
                "smallest_community_size": min(len(comm) for comm in communities),
                "average_community_size": np.mean([len(comm) for comm in communities]),
            }

            # ì»¤ë®¤ë‹ˆí‹° ë‚´ ë°€ë„
            community_densities = []
            for community in communities:
                subgraph = graph.subgraph(community)
                community_densities.append(nx.density(subgraph))

            stats["community_densities"] = community_densities
            stats["average_community_density"] = np.mean(community_densities)

            return stats

        except Exception as e:
            logger.error(f"ì»¤ë®¤ë‹ˆí‹° í†µê³„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    def _analyze_centrality_statistics(
        self, centrality_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ì¤‘ì‹¬ì„± í†µê³„ ë¶„ì„"""
        try:
            stats = {}

            for centrality_type, centrality_values in centrality_result.items():
                if isinstance(centrality_values, dict):
                    values = list(centrality_values.values())
                    stats[centrality_type] = {
                        "mean": np.mean(values),
                        "std": np.std(values),
                        "max": np.max(values),
                        "min": np.min(values),
                        "median": np.median(values),
                        "top_5_nodes": sorted(
                            centrality_values.items(), key=lambda x: x[1], reverse=True
                        )[:5],
                    }

            return stats

        except Exception as e:
            logger.error(f"ì¤‘ì‹¬ì„± í†µê³„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    def _graph_to_matrix(self, graph: nx.Graph) -> List[List[float]]:
        """ê·¸ë˜í”„ë¥¼ ì¸ì ‘ í–‰ë ¬ë¡œ ë³€í™˜"""
        try:
            adjacency_matrix = nx.adjacency_matrix(
                graph, nodelist=range(1, 46), weight="weight"
            )
            return adjacency_matrix.toarray().tolist()
        except Exception as e:
            logger.error(f"ê·¸ë˜í”„ í–‰ë ¬ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return [[0.0] * 45 for _ in range(45)]

    def _get_fallback_result(self) -> Dict[str, Any]:
        """í´ë°± ê²°ê³¼ ë°˜í™˜"""
        return {
            "graph_statistics": {"error": "analysis_failed"},
            "communities": {"error": "analysis_failed"},
            "centrality_analysis": {"error": "analysis_failed"},
            "embeddings": {"error": "analysis_failed"},
            "number_graph_features": {},
            "co_occurrence_matrix": [[0.0] * 45 for _ in range(45)],
        }

    def get_graph_features_vector(
        self, number_graph_features: Dict[int, Dict[str, float]]
    ) -> np.ndarray:
        """ê·¸ë˜í”„ íŠ¹ì„±ì„ ë²¡í„°ë¡œ ë³€í™˜"""
        try:
            # íŠ¹ì„± ì´ë¦„ ì •ì˜
            feature_names = [
                "degree",
                "weighted_degree",
                "clustering_coefficient",
                "centrality_degree",
                "centrality_closeness",
                "centrality_betweenness",
                "centrality_eigenvector",
                "centrality_pagerank",
                "community_id",
                "community_size",
                "neighbor_count",
                "neighbor_avg",
                "neighbor_std",
            ]

            # ëª¨ë“  ë²ˆí˜¸ì˜ íŠ¹ì„±ì„ ë²¡í„°ë¡œ ë³€í™˜
            all_features = []
            for num in range(1, 46):
                number_features = number_graph_features.get(num, {})
                feature_vector = []

                for feature_name in feature_names:
                    value = number_features.get(feature_name, 0.0)
                    feature_vector.append(float(value))

                all_features.extend(feature_vector)

            return np.array(all_features, dtype=np.float32)

        except Exception as e:
            logger.error(f"ê·¸ë˜í”„ íŠ¹ì„± ë²¡í„° ë³€í™˜ ì‹¤íŒ¨: {e}")
            return np.zeros(45 * 13, dtype=np.float32)  # 45ê°œ ë²ˆí˜¸ * 13ê°œ íŠ¹ì„±
