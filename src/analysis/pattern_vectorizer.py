"""
íŒ¨í„´ ë²¡í„°í™” ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ íŒ¨í„´ ë¶„ì„ ê²°ê³¼ë¥¼ ë²¡í„°í™”í•˜ì—¬ ML/DL ëª¨ë¸ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.
"""

import numpy as np
import os
import logging
import hashlib
import json
import time
from pathlib import Path
from typing import Dict, List, Any, Optional, Union, Tuple, Set
from datetime import datetime
from collections import Counter
import gc
import torch

from ..utils.error_handler_refactored import get_logger
from ..utils.unified_performance import performance_monitor
from ..utils.unified_config import ConfigProxy
from ..utils.normalizer import Normalizer
from ..shared.types import LotteryNumber, PatternAnalysis, PatternFeatures
from ..shared.graph_utils import calculate_segment_entropy

# type: ignore ì£¼ì„ ì¶”ê°€
# ë¡œê±° ì„¤ì •
logger = get_logger(__name__)


class PatternVectorizer:
    """
    íŒ¨í„´ íŠ¹ì„±ì„ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” í´ë˜ìŠ¤

    ë‹¤ì–‘í•œ íŒ¨í„´ ë¶„ì„ ê²°ê³¼ë¥¼ ë¨¸ì‹ ëŸ¬ë‹/ë”¥ëŸ¬ë‹ ëª¨ë¸ì´ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ”
    íŠ¹ì„± ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """

    def __init__(self, config: Optional[Union[Dict[str, Any], ConfigProxy]] = None):
        """
        ì´ˆê¸°í™”

        Args:
            config: ì„¤ì • ê°ì²´
        """
        self.config = config if config is not None else {}
        self.logger = get_logger(__name__)

        # ğŸš€ ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        try:
            from ..utils.memory_manager import MemoryManager, MemoryConfig
            from ..utils.cuda_optimizers import get_cuda_optimizer, CudaConfig

            # ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™”
            memory_config = MemoryConfig(
                max_memory_usage=0.8,
                use_memory_pooling=True,
                pool_size=32,
            )
            self.memory_manager = MemoryManager(memory_config)

            # CUDA ìµœì í™” ì´ˆê¸°í™” (ë²¡í„°í™” ì‘ì—…ì— íŠ¹í™”)
            cuda_config = CudaConfig(
                use_amp=False,  # ë²¡í„°í™”ì—ì„œëŠ” ì •í™•ë„ ìš°ì„ 
                batch_size=32,
                use_cudnn=True,
            )
            self.cuda_optimizer = get_cuda_optimizer(cuda_config)

            self.logger.info("âœ… PatternVectorizer ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")

        except Exception as e:
            self.logger.warning(f"ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.memory_manager = None
            self.cuda_optimizer = None

        # ìºì‹œ ì„¤ì •
        try:
            self.use_cache = self.config["vectorizer"]["use_cache"]
        except (KeyError, TypeError):
            self.logger.warning(
                "ì„¤ì •ì—ì„œ 'vectorizer.use_cache'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’(True)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
            )
            self.use_cache = True

        try:
            self.cache_dir = Path(self.config["paths"]["cache_dir"])
        except (KeyError, TypeError):
            self.logger.warning(
                "ì„¤ì •ì—ì„œ 'paths.cache_dir'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’('data/cache')ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
            )
            self.cache_dir = Path("data/cache")

        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # ì €ë¶„ì‚° íŠ¹ì„± ì œê±° ì„¤ì •
        try:
            self.remove_low_variance = self.config["filtering"][
                "remove_low_variance_features"
            ]
        except (KeyError, TypeError):
            self.logger.warning(
                "ì„¤ì •ì—ì„œ 'filtering.remove_low_variance_features'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’(False)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
            )
            self.remove_low_variance = False

        try:
            self.variance_threshold = self.config["filtering"]["variance_threshold"]
        except (KeyError, TypeError):
            self.logger.warning(
                "ì„¤ì •ì—ì„œ 'filtering.variance_threshold'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’(0.01)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
            )
            self.variance_threshold = 0.01

        # ì œê±°ëœ ì €ë¶„ì‚° íŠ¹ì„± ì´ë¦„ ì €ì¥
        self.removed_low_variance_features = []

        # ë²¡í„° ìºì‹œ
        self._pattern_cache = {}
        self._vector_cache = {}

        # íŠ¹ì„± ì´ë¦„ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
        self.feature_names = []

        # ìºì‹œ ì ì¬ ì‹œë„
        if self.use_cache:
            self._load_cache()

        logger.info(
            f"PatternVectorizer ì´ˆê¸°í™” ì™„ë£Œ (ìºì‹œ ì‚¬ìš©: {self.use_cache}, ì €ë¶„ì‚° íŠ¹ì„± ì œê±°: {self.remove_low_variance})"
        )

        # ì •ê·œí™” ìœ í‹¸ë¦¬í‹° ì´ˆê¸°í™”
        self.normalizer = Normalizer(self.config)

        # íŠ¹ì„± ë²¡í„° ìºì‹œ ì´ˆê¸°í™”
        from ..utils.state_vector_cache import get_cache

        self.vector_cache = get_cache(self.config)

        # ë²¡í„° ì°¨ì› ì²­ì‚¬ì§„ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self._initialize_vector_blueprint()

        logger.info(
            f"ë²¡í„° ì²­ì‚¬ì§„ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ: ì´ {self.total_expected_dims}ì°¨ì›"
        )

    def _initialize_vector_blueprint(self):
        """
        ë²¡í„° ì°¨ì› ì²­ì‚¬ì§„ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        ê° ê·¸ë£¹ë³„ ê³ ì • ì°¨ì›ì„ ì‚¬ì „ ì •ì˜í•˜ì—¬ ì°¨ì› ë¶ˆì¼ì¹˜ ë¬¸ì œ í•´ê²°
        """
        # ê·¸ë£¹ë³„ ê³ ì • ì°¨ì› ì •ì˜ (ì´ 95ì°¨ì›ìœ¼ë¡œ í‘œì¤€í™”)
        self.vector_blueprint = {
            # ê¸°ë³¸ íŒ¨í„´ ë¶„ì„ (25ì°¨ì›)
            "pattern_analysis": 25,
            # ë¶„í¬ íŒ¨í„´ (10ì°¨ì›)
            "distribution_pattern": 10,
            # ì„¸ê·¸ë¨¼íŠ¸ ë¹ˆë„ (15ì°¨ì›: 10êµ¬ê°„ + 5êµ¬ê°„)
            "segment_frequency": 15,
            # ì¤‘ì‹¬ì„± ë° ì—°ì†ì„± (12ì°¨ì›)
            "centrality_consecutive": 12,
            # ê°­ í†µê³„ ë° ì¬ì¶œí˜„ (8ì°¨ì›)
            "gap_reappearance": 8,
            # ROI íŠ¹ì„± (15ì°¨ì›)
            "roi_features": 15,
            # í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ (10ì°¨ì›)
            "cluster_features": 10,
            # ì¤‘ë³µ íŒ¨í„´ íŠ¹ì„± (20ì°¨ì›)
            "overlap_patterns": 20,
            # ë¬¼ë¦¬ì  êµ¬ì¡° íŠ¹ì„± (11ì°¨ì›)
            "physical_structure": 11,
            # ìŒ ê·¸ë˜í”„ ì••ì¶• ë²¡í„° (ìµœëŒ€ 20ì°¨ì›)
            "pair_graph_vector": 20,
        }

        # ì´ ì˜ˆìƒ ì°¨ì› ê³„ì‚°
        self.total_expected_dims = sum(self.vector_blueprint.values())

        # ê·¸ë£¹ë³„ íŠ¹ì„± ì´ë¦„ í…œí”Œë¦¿
        self.feature_name_templates = {
            "pattern_analysis": [f"pattern_{i+1}" for i in range(25)],
            "distribution_pattern": [f"dist_{i+1}" for i in range(10)],
            "segment_frequency": [f"seg10_{i+1}" for i in range(10)]
            + [f"seg5_{i+1}" for i in range(5)],
            "centrality_consecutive": [f"centrality_{i+1}" for i in range(6)]
            + [f"consecutive_{i+1}" for i in range(6)],
            "gap_reappearance": [f"gap_{i+1}" for i in range(4)]
            + [f"reappear_{i+1}" for i in range(4)],
            "roi_features": [f"roi_{i+1}" for i in range(15)],
            "cluster_features": [f"cluster_{i+1}" for i in range(10)],
            "overlap_patterns": [f"overlap_{i+1}" for i in range(20)],
            "physical_structure": [f"physical_{i+1}" for i in range(11)],
            "pair_graph_vector": [f"pair_graph_{i+1}" for i in range(20)],
        }

        self.logger.info(
            f"ë²¡í„° ì²­ì‚¬ì§„ ì •ì˜ ì™„ë£Œ: {len(self.vector_blueprint)}ê°œ ê·¸ë£¹, ì´ {self.total_expected_dims}ì°¨ì›"
        )

    def _vectorize_group_safe(
        self, group_name: str, data: Any, vectorize_func
    ) -> Tuple[np.ndarray, List[str]]:
        """
        ê·¸ë£¹ë³„ ì•ˆì „í•œ ë²¡í„°í™” ìˆ˜í–‰
        ì˜ˆìƒ ì°¨ì›ê³¼ ì¼ì¹˜í•˜ì§€ ì•Šìœ¼ë©´ íŒ¨ë”© ë˜ëŠ” ì ˆë‹¨í•˜ì—¬ ì°¨ì› ë³´ì¥

        Args:
            group_name: ê·¸ë£¹ ì´ë¦„
            data: ë²¡í„°í™”í•  ë°ì´í„°
            vectorize_func: ë²¡í„°í™” í•¨ìˆ˜

        Returns:
            Tuple[np.ndarray, List[str]]: ì°¨ì›ì´ ë³´ì¥ëœ ë²¡í„°ì™€ íŠ¹ì„± ì´ë¦„
        """
        try:
            # ë²¡í„°í™” ìˆ˜í–‰
            if vectorize_func is None:
                # ê¸°ë³¸ ë²¡í„°í™” (ëª¨ë“  ê°’ì„ 0ìœ¼ë¡œ)
                vector = np.zeros(self.vector_blueprint[group_name], dtype=np.float32)
                feature_names = self.feature_name_templates[group_name][
                    : self.vector_blueprint[group_name]
                ]
            else:
                result = vectorize_func(data)
                if isinstance(result, tuple) and len(result) == 2:
                    vector, feature_names = result
                elif isinstance(result, np.ndarray):
                    vector = result
                    feature_names = [f"{group_name}_{i+1}" for i in range(len(vector))]
                else:
                    # ì˜ˆìƒì¹˜ ëª»í•œ ë°˜í™˜ í˜•íƒœ
                    self.logger.warning(
                        f"ê·¸ë£¹ '{group_name}': ì˜ˆìƒì¹˜ ëª»í•œ ë°˜í™˜ í˜•íƒœ {type(result)}"
                    )
                    vector = np.zeros(
                        self.vector_blueprint[group_name], dtype=np.float32
                    )
                    feature_names = self.feature_name_templates[group_name][
                        : self.vector_blueprint[group_name]
                    ]

            expected_dims = self.vector_blueprint.get(group_name, len(vector))

            # ì°¨ì› ì¡°ì •
            vector, feature_names = self._pad_or_truncate_vector(
                vector, feature_names, expected_dims, group_name
            )

            # NaN/Inf ê²€ì¦ ë° ì²˜ë¦¬
            vector = self._sanitize_vector(vector, group_name)

            self.logger.debug(f"ê·¸ë£¹ '{group_name}' ë²¡í„°í™” ì™„ë£Œ: {len(vector)}ì°¨ì›")
            return vector, feature_names

        except Exception as e:
            self.logger.warning(f"ê·¸ë£¹ '{group_name}' ë²¡í„°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ ë²¡í„° ë°˜í™˜
            expected_dims = self.vector_blueprint.get(group_name, 10)
            vector = np.zeros(expected_dims, dtype=np.float32)
            feature_names = self.feature_name_templates.get(
                group_name, [f"{group_name}_{i+1}" for i in range(expected_dims)]
            )[:expected_dims]
            return vector, feature_names

    def _pad_or_truncate_vector(
        self,
        vector: np.ndarray,
        feature_names: List[str],
        expected_dims: int,
        group_name: str,
    ) -> Tuple[np.ndarray, List[str]]:
        """
        ë²¡í„°ë¥¼ ì˜ˆìƒ ì°¨ì›ì— ë§ê²Œ íŒ¨ë”© ë˜ëŠ” ì ˆë‹¨

        Args:
            vector: ì›ë³¸ ë²¡í„°
            feature_names: ì›ë³¸ íŠ¹ì„± ì´ë¦„
            expected_dims: ì˜ˆìƒ ì°¨ì›
            group_name: ê·¸ë£¹ ì´ë¦„

        Returns:
            Tuple[np.ndarray, List[str]]: ì¡°ì •ëœ ë²¡í„°ì™€ íŠ¹ì„± ì´ë¦„
        """
        current_dims = len(vector)

        if current_dims == expected_dims:
            return vector, feature_names
        elif current_dims < expected_dims:
            # íŒ¨ë”© (ë¶€ì¡±í•œ ì°¨ì›ì„ 0ìœ¼ë¡œ ì±„ì›€)
            padding_size = expected_dims - current_dims
            padded_vector = np.pad(
                vector, (0, padding_size), mode="constant", constant_values=0.0
            )

            # íŠ¹ì„± ì´ë¦„ë„ íŒ¨ë”©
            template_names = self.feature_name_templates.get(
                group_name, [f"{group_name}_{i+1}" for i in range(expected_dims)]
            )
            padded_names = feature_names + template_names[current_dims:expected_dims]

            self.logger.debug(
                f"ê·¸ë£¹ '{group_name}': {current_dims}ì°¨ì› â†’ {expected_dims}ì°¨ì›ìœ¼ë¡œ íŒ¨ë”©"
            )
            return padded_vector.astype(np.float32), padded_names
        else:
            # ì ˆë‹¨ (ì´ˆê³¼ ì°¨ì› ì œê±°)
            truncated_vector = vector[:expected_dims]
            truncated_names = feature_names[:expected_dims]

            self.logger.debug(
                f"ê·¸ë£¹ '{group_name}': {current_dims}ì°¨ì› â†’ {expected_dims}ì°¨ì›ìœ¼ë¡œ ì ˆë‹¨"
            )
            return truncated_vector.astype(np.float32), truncated_names

    def _sanitize_vector(self, vector: np.ndarray, group_name: str) -> np.ndarray:
        """
        ë²¡í„°ì˜ NaN/Inf ê°’ì„ ì²˜ë¦¬í•˜ì—¬ ì•ˆì „í•œ ë²¡í„°ë¡œ ë³€í™˜

        Args:
            vector: ì›ë³¸ ë²¡í„°
            group_name: ê·¸ë£¹ ì´ë¦„

        Returns:
            np.ndarray: ì •ì œëœ ë²¡í„°
        """
        # NaNì„ 0ìœ¼ë¡œ ëŒ€ì²´
        nan_count = np.isnan(vector).sum()
        if nan_count > 0:
            vector = np.nan_to_num(vector, nan=0.0)
            self.logger.debug(f"ê·¸ë£¹ '{group_name}': {nan_count}ê°œ NaN ê°’ì„ 0ìœ¼ë¡œ ëŒ€ì²´")

        # Infë¥¼ ìœ í•œí•œ ê°’ìœ¼ë¡œ ëŒ€ì²´
        inf_count = np.isinf(vector).sum()
        if inf_count > 0:
            vector = np.nan_to_num(vector, posinf=1.0, neginf=-1.0)
            self.logger.debug(
                f"ê·¸ë£¹ '{group_name}': {inf_count}ê°œ Inf ê°’ì„ ìœ í•œê°’ìœ¼ë¡œ ëŒ€ì²´"
            )

        # ê°’ ë²”ìœ„ ì œí•œ (-10 ~ 10)
        vector = np.clip(vector, -10.0, 10.0)

        return vector.astype(np.float32)

    def validate_vector_integrity(
        self, vector: np.ndarray, feature_names: List[str]
    ) -> bool:
        """
        ë²¡í„° ë¬´ê²°ì„± ê²€ì¦

        Args:
            vector: ê²€ì¦í•  ë²¡í„°
            feature_names: íŠ¹ì„± ì´ë¦„ ë¦¬ìŠ¤íŠ¸

        Returns:
            bool: ê²€ì¦ í†µê³¼ ì—¬ë¶€
        """
        try:
            # ì°¨ì› ì¼ì¹˜ í™•ì¸ (ê²½ê³ ë§Œ ì¶œë ¥, ì‹¤íŒ¨ë¡œ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ)
            if len(vector) != len(feature_names):
                self.logger.warning(
                    f"ë²¡í„° ì°¨ì›({len(vector)})ê³¼ íŠ¹ì„± ì´ë¦„ ìˆ˜({len(feature_names)})ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"
                )
                if len(feature_names) == 0:
                    self.logger.warning(
                        "íŠ¹ì„± ì´ë¦„ì´ ì „í˜€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì´ë¦„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
                    )
                elif len(vector) > len(feature_names):
                    diff = len(vector) - len(feature_names)
                    self.logger.warning(
                        f"ì°¨ì› ë¶ˆì¼ì¹˜ ì •ë„: {diff}ê°œ ({diff/len(vector)*100:.2f}%)"
                    )
                    self.logger.warning(
                        f"ë²¡í„° ì°¨ì›ì´ íŠ¹ì„± ì´ë¦„ ìˆ˜ë³´ë‹¤ {diff}ê°œ ë” ë§ìŠµë‹ˆë‹¤. ì´ë¦„ ëª©ë¡ì„ í™•ì¥í•©ë‹ˆë‹¤."
                    )

            # ì˜ˆìƒ ì´ ì°¨ì›ê³¼ ì¼ì¹˜ í™•ì¸
            if len(vector) != self.total_expected_dims:
                self.logger.error(
                    f"ì´ ì°¨ì› ë¶ˆì¼ì¹˜: ì˜ˆìƒ {self.total_expected_dims}ì°¨ì› vs ì‹¤ì œ {len(vector)}ì°¨ì›"
                )
                return False

            # NaN/Inf ë¹„ìœ¨ í™•ì¸ (1% ì´ˆê³¼ ì‹œ ì‹¤íŒ¨)
            nan_inf_ratio = (np.isnan(vector).sum() + np.isinf(vector).sum()) / len(
                vector
            )
            if nan_inf_ratio > 0.01:
                self.logger.error(f"NaN/Inf ë¹„ìœ¨ ì´ˆê³¼: {nan_inf_ratio:.4f} > 0.01")
                return False

            # 0 ê°’ ë¹„ìœ¨ ê²€ì¦
            zero_ratio = np.sum(vector == 0) / len(vector)
            if zero_ratio > 0.8:
                self.logger.warning(
                    f"ë²¡í„°ì˜ {zero_ratio:.1%}ê°€ 0ê°’ì…ë‹ˆë‹¤. íŠ¹ì„± ì¶”ì¶œì„ ê°œì„ í•´ì•¼ í•©ë‹ˆë‹¤."
                )

            # íŠ¹ì„± ë‹¤ì–‘ì„± ì—”íŠ¸ë¡œí”¼ ê³„ì‚° (ì •ë³´ í’ˆì§ˆ ê²€ì¦)
            entropy_score = self._calculate_feature_entropy(vector)
            if entropy_score < 1.0:
                self.logger.info(
                    f"íŠ¹ì„± ë‹¤ì–‘ì„± ë‚®ìŒ: ì—”íŠ¸ë¡œí”¼ {entropy_score:.4f} (ê°œì„  ê¶Œì¥)"
                )
            else:
                self.logger.info(f"íŠ¹ì„± ë‹¤ì–‘ì„± ì–‘í˜¸: ì—”íŠ¸ë¡œí”¼ {entropy_score:.4f}")

            self.logger.info(
                f"ë²¡í„° ë¬´ê²°ì„± ê²€ì¦ í†µê³¼: {len(vector)}ì°¨ì›, 0ê°’ ë¹„ìœ¨: {zero_ratio:.1%}"
            )
            return True

        except Exception as e:
            self.logger.error(f"ë²¡í„° ë¬´ê²°ì„± ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def _calculate_feature_entropy(self, vector: np.ndarray) -> float:
        """
        íŠ¹ì„± ë²¡í„°ì˜ ì •ë³´ ì—”íŠ¸ë¡œí”¼ ê³„ì‚°

        Args:
            vector: íŠ¹ì„± ë²¡í„°

        Returns:
            float: ì—”íŠ¸ë¡œí”¼ ì ìˆ˜ (0~1)
        """
        try:
            # ë²¡í„°ë¥¼ íˆìŠ¤í† ê·¸ë¨ìœ¼ë¡œ ë³€í™˜
            hist, _ = np.histogram(vector, bins=50, density=True)

            # 0ì´ ì•„ë‹Œ í™•ë¥ ë§Œ ì‚¬ìš©
            hist = hist[hist > 0]

            if len(hist) == 0:
                return 0.0

            # ì—”íŠ¸ë¡œí”¼ ê³„ì‚°
            entropy = -np.sum(hist * np.log2(hist + 1e-10))

            # 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”
            max_entropy = np.log2(len(hist))
            normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0.0

            return float(normalized_entropy)

        except Exception as e:
            self.logger.warning(f"ì—”íŠ¸ë¡œí”¼ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
            return 0.0

    def _get_config_value(self, key_path: str, default_value: Any) -> Any:
        """ì„¤ì • ê°’ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        # ConfigProxy ê°ì²´ì¸ ê²½ìš°
        if hasattr(self.config, "get") or hasattr(self.config, "__getitem__"):
            try:
                # key_pathë¥¼ ë¶„í•´í•˜ì—¬ ì§ì ‘ ì ‘ê·¼
                keys = key_path.split(".")
                value = self.config
                for k in keys:
                    value = value[k]
                return value
            except (KeyError, TypeError):
                self.logger.warning(
                    f"ì„¤ì •ì—ì„œ '{key_path}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
                )
                return default_value
        # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš°
        elif isinstance(self.config, dict):
            # ì¤‘ì²©ëœ í‚¤ ì²˜ë¦¬
            keys = key_path.split(".")
            value = self.config
            for k in keys:
                if isinstance(value, dict) and k in value:
                    value = value[k]
                else:
                    return default_value
            return value
        # ê¸°íƒ€ ê²½ìš°
        return default_value

    def _compute_pattern_hash(self, pattern_data: Dict[str, Any]) -> str:
        """
        íŒ¨í„´ ë°ì´í„°ì˜ í•´ì‹œ ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

        Args:
            pattern_data: íŒ¨í„´ ë°ì´í„° ë”•ì…”ë„ˆë¦¬

        Returns:
            íŒ¨í„´ ë°ì´í„°ì˜ ê³ ìœ  í•´ì‹œ ê°’
        """
        try:
            # ì •ë ¬ëœ í‚¤ë¡œ JSON ë¬¸ìì—´ ìƒì„±
            pattern_json = json.dumps(pattern_data, sort_keys=True)
            # SHA-256 í•´ì‹œ ê³„ì‚°
            return hashlib.sha256(pattern_json.encode()).hexdigest()
        except Exception as e:
            self.logger.warning(f"íŒ¨í„´ í•´ì‹œ ê³„ì‚° ì‹¤íŒ¨: {e}")
            # í˜„ì¬ ì‹œê°„ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ í´ë°± í•´ì‹œ
            return f"fallback_{int(time.time())}"

    def _compute_numbers_hash(self, numbers: List[int]) -> str:
        """
        ë²ˆí˜¸ ì¡°í•©ì˜ í•´ì‹œ ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

        Args:
            numbers: ë²ˆí˜¸ ì¡°í•© ë¦¬ìŠ¤íŠ¸

        Returns:
            ë²ˆí˜¸ ì¡°í•©ì˜ ê³ ìœ  í•´ì‹œ ê°’
        """
        try:
            # ì •ë ¬ëœ ë²ˆí˜¸ ëª©ë¡ì˜ ë¬¸ìì—´
            numbers_str = "_".join(map(str, sorted(numbers)))
            # SHA-256 í•´ì‹œ ê³„ì‚°
            return hashlib.sha256(numbers_str.encode()).hexdigest()
        except Exception as e:
            self.logger.warning(f"ë²ˆí˜¸ í•´ì‹œ ê³„ì‚° ì‹¤íŒ¨: {e}")
            # í˜„ì¬ ì‹œê°„ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ í´ë°± í•´ì‹œ
            return f"fallback_{int(time.time())}"

    def _create_cache_key(self, config_section: Dict[str, Any]) -> str:
        """
        config ì„¹ì…˜ì„ JSON ì§ë ¬í™”í•˜ì—¬ SHA1 í•´ì‹œ í‚¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        ë™ì¼ ì„¤ì •ì´ë©´ ë™ì¼ í‚¤, ì„¤ì •ì´ ë‹¤ë¥´ë©´ ë‹¤ë¥¸ ìºì‹œ í‚¤ ìƒì„±.

        Args:
            config_section: ì„¤ì • ì„¹ì…˜ ë”•ì…”ë„ˆë¦¬

        Returns:
            str: 10ìë¦¬ SHA1 í•´ì‹œ í‚¤
        """
        config_str = json.dumps(config_section, sort_keys=True)
        return hashlib.sha1(config_str.encode("utf-8")).hexdigest()[:10]

    def _load_cache(self) -> None:
        """ìºì‹œ íŒŒì¼ì—ì„œ ë²¡í„° ìºì‹œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            # íŒ¨í„´ ìºì‹œ ë¡œë“œ
            pattern_cache_path = self.cache_dir / "pattern_vector_cache.npy"
            if pattern_cache_path.exists():
                cached_data = np.load(pattern_cache_path, allow_pickle=True).item()
                if isinstance(cached_data, dict):
                    self._pattern_cache = cached_data
                    self.logger.info(
                        f"íŒ¨í„´ ë²¡í„° ìºì‹œ ë¡œë“œ ì™„ë£Œ: {len(self._pattern_cache)}ê°œ í•­ëª©"
                    )

            # ë²ˆí˜¸ ë²¡í„° ìºì‹œ ë¡œë“œ
            vector_cache_path = self.cache_dir / "number_vector_cache.npy"
            if vector_cache_path.exists():
                cached_data = np.load(vector_cache_path, allow_pickle=True).item()
                if isinstance(cached_data, dict):
                    self._vector_cache = cached_data
                    self.logger.info(
                        f"ë²ˆí˜¸ ë²¡í„° ìºì‹œ ë¡œë“œ ì™„ë£Œ: {len(self._vector_cache)}ê°œ í•­ëª©"
                    )

        except Exception as e:
            self.logger.warning(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self._pattern_cache = {}
            self._vector_cache = {}

    def _save_cache(self) -> None:
        """í˜„ì¬ ìºì‹œë¥¼ íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
        if not self.use_cache:
            return

        try:
            # íŒ¨í„´ ìºì‹œ ì €ì¥
            pattern_cache_path = self.cache_dir / "pattern_vector_cache.npy"
            np.save(
                pattern_cache_path, np.array([self._pattern_cache], dtype=object)[0]
            )

            # ë²ˆí˜¸ ë²¡í„° ìºì‹œ ì €ì¥
            vector_cache_path = self.cache_dir / "number_vector_cache.npy"
            np.save(vector_cache_path, np.array([self._vector_cache], dtype=object)[0])

            self.logger.info(
                f"ë²¡í„° ìºì‹œ ì €ì¥ ì™„ë£Œ (íŒ¨í„´: {len(self._pattern_cache)}ê°œ, ë²ˆí˜¸: {len(self._vector_cache)}ê°œ)"
            )
        except Exception as e:
            self.logger.warning(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

    def vectorize_pattern(
        self, pattern_data: Dict[str, Any], use_optimization: bool = True
    ) -> np.ndarray:
        """
        ìµœì í™”ëœ íŒ¨í„´ ë°ì´í„° ë²¡í„° ë³€í™˜

        Args:
            pattern_data: íŒ¨í„´ ë¶„ì„ ê²°ê³¼ ë°ì´í„°
            use_optimization: ìµœì í™” ì‚¬ìš© ì—¬ë¶€

        Returns:
            íŠ¹ì„± ë²¡í„°
        """
        if not use_optimization:
            return self._standard_vectorize_pattern(pattern_data)

        # ğŸ§  ë©”ëª¨ë¦¬ ê´€ë¦¬ ìŠ¤ì½”í”„ ì ìš©
        if self.memory_manager:
            with self.memory_manager.allocation_scope():
                return self._optimized_vectorize_pattern(pattern_data)
        else:
            return self._standard_vectorize_pattern(pattern_data)

    def _optimized_vectorize_pattern(self, pattern_data: Dict[str, Any]) -> np.ndarray:
        """ìµœì í™”ëœ íŒ¨í„´ ë²¡í„°í™”"""
        # ğŸ“Š ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        with performance_monitor("pattern_vectorization_optimized"):

            # ìºì‹œ í™•ì¸
            if self.use_cache:
                pattern_hash = self._compute_pattern_hash(pattern_data)
                if pattern_hash in self._pattern_cache:
                    return self._pattern_cache[pattern_hash]

            # ğŸš€ GPU ê°€ì† ë²¡í„°í™” ì‹œë„
            if (
                self.cuda_optimizer
                and self.cuda_optimizer.is_available()
                and self._should_use_gpu_vectorization(pattern_data)
            ):
                try:
                    return self._gpu_vectorize_pattern(pattern_data)
                except Exception as e:
                    self.logger.warning(f"GPU ë²¡í„°í™” ì‹¤íŒ¨: {e}, CPUë¡œ í´ë°±")

            # CPU ìµœì í™” ë²¡í„°í™”
            return self._cpu_vectorize_pattern_optimized(pattern_data)

    def _standard_vectorize_pattern(self, pattern_data: Dict[str, Any]) -> np.ndarray:
        """í‘œì¤€ íŒ¨í„´ ë²¡í„°í™” (ê¸°ì¡´ ë¡œì§)"""
        # ìºì‹œ ì‚¬ìš© ì‹œ ì´ë¯¸ ê³„ì‚°ëœ ë²¡í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        if self.use_cache:
            pattern_hash = self._compute_pattern_hash(pattern_data)
            if pattern_hash in self._pattern_cache:
                return self._pattern_cache[pattern_hash]

        # ë²¡í„° ìš”ì†Œ ì´ˆê¸°í™”
        vector_elements = []

        # 1. ë§ˆì§€ë§‰ ë‹¹ì²¨ ë²ˆí˜¸ì˜ ë¶„í¬ íŠ¹ì„±
        if "number_distribution" in pattern_data:
            dist_data = pattern_data["number_distribution"]

            # ë²”ìœ„ë³„ ë¶„í¬ (0-9, 10-19, 20-29, 30-39, 40-45)
            range_dist = dist_data.get("range_distribution", [0.2, 0.2, 0.2, 0.2, 0.2])
            vector_elements.extend(range_dist)

            # í™€ì§ ë¶„í¬
            odd_even_ratio = dist_data.get("odd_even_ratio", 0.5)
            vector_elements.append(odd_even_ratio)

            # ê³ ì € ë¶„í¬ (1-22, 23-45)
            high_low_ratio = dist_data.get("high_low_ratio", 0.5)
            vector_elements.append(high_low_ratio)
        else:
            # ê¸°ë³¸ê°’ ì¶”ê°€
            vector_elements.extend([0.2, 0.2, 0.2, 0.2, 0.2, 0.5, 0.5])

        # 2. í•©ê³„ ê´€ë ¨ íŠ¹ì„±
        if "sum_analysis" in pattern_data:
            sum_data = pattern_data["sum_analysis"]

            # ì •ê·œí™”ëœ í•©ê³„ (0-1 ë²”ìœ„)
            normalized_sum = sum_data.get("normalized_sum", 0.5)
            vector_elements.append(normalized_sum)

            # í‰ê· ê³¼ì˜ í¸ì°¨ (ì •ê·œí™”)
            deviation = sum_data.get("deviation_from_mean", 0.0)
            normalized_deviation = max(min(deviation / 50.0 + 0.5, 1.0), 0.0)
            vector_elements.append(normalized_deviation)
        else:
            # ê¸°ë³¸ê°’ ì¶”ê°€
            vector_elements.extend([0.5, 0.5])

        # 3. ê°„ê²© ê´€ë ¨ íŠ¹ì„±
        if "number_gaps" in pattern_data:
            gap_data = pattern_data["number_gaps"]

            # í‰ê·  ê°„ê²© (ì •ê·œí™”)
            avg_gap = gap_data.get("avg_gap", 7.5)
            normalized_avg_gap = max(min(avg_gap / 15.0, 1.0), 0.0)
            vector_elements.append(normalized_avg_gap)

            # ìµœëŒ€ ê°„ê²© (ì •ê·œí™”)
            max_gap = gap_data.get("max_gap", 15)
            normalized_max_gap = max(min(max_gap / 30.0, 1.0), 0.0)
            vector_elements.append(normalized_max_gap)

            # ìµœì†Œ ê°„ê²© (ì •ê·œí™”)
            min_gap = gap_data.get("min_gap", 1)
            normalized_min_gap = max(min(min_gap / 10.0, 1.0), 0.0)
            vector_elements.append(normalized_min_gap)
        else:
            # ê¸°ë³¸ê°’ ì¶”ê°€
            vector_elements.extend([0.5, 0.5, 0.1])

        # 4. ì—°ì† ë²ˆí˜¸ ê´€ë ¨ íŠ¹ì„±
        if "consecutive_numbers" in pattern_data:
            consecutive_data = pattern_data["consecutive_numbers"]

            # ì—°ì† ë²ˆí˜¸ ìˆ˜ (ì •ê·œí™”)
            count = consecutive_data.get("count", 0)
            normalized_count = min(count / 5.0, 1.0)
            vector_elements.append(normalized_count)
        else:
            # ê¸°ë³¸ê°’ ì¶”ê°€
            vector_elements.append(0.0)

        # 5. ê³¼ê±° ë‹¹ì²¨ ë²ˆí˜¸ì™€ì˜ ì¼ì¹˜ ê´€ë ¨ íŠ¹ì„±
        if "historical_match" in pattern_data:
            match_data = pattern_data["historical_match"]

            # ìµœëŒ€ ì¼ì¹˜ ìˆ˜ (ì •ê·œí™”)
            max_match = match_data.get("max_match", 0)
            normalized_max_match = min(max_match / 6.0, 1.0)
            vector_elements.append(normalized_max_match)

            # í‰ê·  ì¼ì¹˜ ìˆ˜ (ì •ê·œí™”)
            avg_match = match_data.get("avg_match", 0.0)
            normalized_avg_match = min(avg_match / 3.0, 1.0)
            vector_elements.append(normalized_avg_match)
        else:
            # ê¸°ë³¸ê°’ ì¶”ê°€
            vector_elements.extend([0.0, 0.0])

        # ë²¡í„° ìƒì„±
        feature_vector = np.array(vector_elements, dtype=np.float32)

        # ìºì‹œì— ì €ì¥
        if self.use_cache:
            pattern_hash = self._compute_pattern_hash(pattern_data)
            self._pattern_cache[pattern_hash] = feature_vector

            # ì£¼ê¸°ì ìœ¼ë¡œ ìºì‹œ ì €ì¥ (íŒ¨í„´ ìºì‹œê°€ 100ê°œ ì´ìƒ ëŠ˜ì–´ë‚  ë•Œë§ˆë‹¤)
            if len(self._pattern_cache) % 100 == 0:
                self._save_cache()

        return feature_vector

    def vectorize_number_combination(
        self, numbers: List[int], pattern_data: Dict[str, Any]
    ) -> np.ndarray:
        """
        ë²ˆí˜¸ ì¡°í•©ê³¼ íŒ¨í„´ ë°ì´í„°ë¥¼ ê²°í•©í•˜ì—¬ íŠ¹ì„± ë²¡í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            numbers: ë²ˆí˜¸ ì¡°í•© (6ê°œ ë²ˆí˜¸)
            pattern_data: íŒ¨í„´ ë¶„ì„ ê²°ê³¼ ë°ì´í„°

        Returns:
            íŠ¹ì„± ë²¡í„°
        """
        # ìºì‹œ ì‚¬ìš© ì‹œ ì´ë¯¸ ê³„ì‚°ëœ ë²¡í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        if self.use_cache:
            numbers_hash = self._compute_numbers_hash(numbers)
            if numbers_hash in self._vector_cache:
                return self._vector_cache[numbers_hash]

        # ê¸°ë³¸ íŒ¨í„´ ë²¡í„° ìƒì„±
        pattern_vector = self.vectorize_pattern(pattern_data)

        # ë²ˆí˜¸ë³„ íŠ¹ì„± ìƒì„± (ì •ê·œí™”ëœ ë²ˆí˜¸)
        normalized_numbers = [n / 45.0 for n in sorted(numbers)]

        # ë²¡í„° ê²°í•©
        combined_vector = np.concatenate([pattern_vector, normalized_numbers])

        # ìºì‹œì— ì €ì¥
        if self.use_cache:
            numbers_hash = self._compute_numbers_hash(numbers)
            self._vector_cache[numbers_hash] = combined_vector

            # ì£¼ê¸°ì ìœ¼ë¡œ ìºì‹œ ì €ì¥ (ë²ˆí˜¸ ìºì‹œê°€ 1000ê°œ ì´ìƒ ëŠ˜ì–´ë‚  ë•Œë§ˆë‹¤)
            if len(self._vector_cache) % 1000 == 0:
                self._save_cache()

        return combined_vector

    def _should_use_gpu_vectorization(self, pattern_data: Dict[str, Any]) -> bool:
        """GPU ë²¡í„°í™” ì‚¬ìš© ì—¬ë¶€ ê²°ì •"""
        # ë°ì´í„° í¬ê¸°ê°€ ì¶©ë¶„íˆ í´ ë•Œë§Œ GPU ì‚¬ìš©
        data_size = len(str(pattern_data))  # ëŒ€ëµì ì¸ ë°ì´í„° í¬ê¸°
        return data_size > 1000  # 1KB ì´ìƒì¼ ë•Œ GPU ì‚¬ìš©

    def _gpu_vectorize_pattern(self, pattern_data: Dict[str, Any]) -> np.ndarray:
        """GPU ê°€ì† íŒ¨í„´ ë²¡í„°í™”"""
        try:
            if self.cuda_optimizer and hasattr(self.cuda_optimizer, "device_context"):
                with self.cuda_optimizer.device_context():
                    # í˜„ì¬ëŠ” ê¸°ë³¸ ë²¡í„°í™”ë¡œ í´ë°± (í–¥í›„ CUDA êµ¬í˜„ í™•ì¥ ê°€ëŠ¥)
                    return self._cpu_vectorize_pattern_optimized(pattern_data)
            else:
                # GPU ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ CPUë¡œ ì²˜ë¦¬
                return self._cpu_vectorize_pattern_optimized(pattern_data)
        except Exception as e:
            self.logger.error(f"GPU ë²¡í„°í™” ì‹¤íŒ¨: {e}")
            return self._cpu_vectorize_pattern_optimized(pattern_data)

    def _cpu_vectorize_pattern_optimized(
        self, pattern_data: Dict[str, Any]
    ) -> np.ndarray:
        """ìµœì í™”ëœ CPU íŒ¨í„´ ë²¡í„°í™”"""
        # ğŸ§  ë©”ëª¨ë¦¬ í’€ì—ì„œ ë°°ì—´ í• ë‹¹ (ê°€ëŠ¥í•œ ê²½ìš°)
        if self.memory_manager:
            try:
                # ì˜ˆìƒ ë²¡í„° í¬ê¸°ë¡œ ë©”ëª¨ë¦¬ í• ë‹¹
                estimated_size = self._estimate_vector_size(pattern_data)
                vector_array = self.memory_manager.get_optimized_array(
                    shape=(estimated_size,), dtype=np.float32
                )

                # ê¸°ì¡´ ë¡œì§ìœ¼ë¡œ ë²¡í„° ê³„ì‚°
                result = self._standard_vectorize_pattern(pattern_data)

                # í¬ê¸° ì¡°ì •
                if len(result) <= len(vector_array):
                    vector_array[: len(result)] = result
                    return vector_array[: len(result)]
                else:
                    return result

            except Exception as e:
                self.logger.warning(f"ë©”ëª¨ë¦¬ ìµœì í™” ë²¡í„°í™” ì‹¤íŒ¨: {e}")

        # í´ë°±: í‘œì¤€ ë²¡í„°í™”
        return self._standard_vectorize_pattern(pattern_data)

    def _estimate_vector_size(self, pattern_data: Dict[str, Any]) -> int:
        """ë²¡í„° í¬ê¸° ì¶”ì •"""
        # ê¸°ë³¸ ë²¡í„° í¬ê¸° ì¶”ì •
        base_size = 15  # ê¸°ë³¸ íŠ¹ì„± ìˆ˜

        # ë°ì´í„° ë³µì¡ë„ì— ë”°ë¥¸ ì¶”ê°€ í¬ê¸°
        if "number_distribution" in pattern_data:
            base_size += 7
        if "sum_analysis" in pattern_data:
            base_size += 2
        if "number_gaps" in pattern_data:
            base_size += 3

        return base_size

    def set_optimizers(self, **optimizers):
        """ì™¸ë¶€ì—ì„œ ìµœì í™” ì‹œìŠ¤í…œ ì£¼ì…"""
        if "memory_manager" in optimizers:
            self.memory_manager = optimizers["memory_manager"]
        if "cuda_optimizer" in optimizers:
            self.cuda_optimizer = optimizers["cuda_optimizer"]

        self.logger.info("PatternVectorizer ì™¸ë¶€ ìµœì í™” ì‹œìŠ¤í…œ ì£¼ì… ì™„ë£Œ")

    def clear_cache(self) -> None:
        """ìºì‹œë¥¼ ëª¨ë‘ ë¹„ì›ë‹ˆë‹¤."""
        self._pattern_cache = {}
        self._vector_cache = {}

        # ìºì‹œ íŒŒì¼ ì‚­ì œ
        try:
            pattern_cache_path = self.cache_dir / "pattern_vector_cache.npy"
            if pattern_cache_path.exists():
                os.remove(pattern_cache_path)

            vector_cache_path = self.cache_dir / "number_vector_cache.npy"
            if vector_cache_path.exists():
                os.remove(vector_cache_path)

            self.logger.info("ë²¡í„° ìºì‹œê°€ ëª¨ë‘ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            self.logger.warning(f"ìºì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")

    def vectorize_full_analysis(self, full_analysis: Dict[str, Any]) -> np.ndarray:
        """
        ì „ì²´ ë¶„ì„ ë°ì´í„°ë¥¼ ë²¡í„°ë¡œ ë³€í™˜

        Args:
            full_analysis: í†µí•© ë¶„ì„ ê²°ê³¼

        Returns:
            ë³€í™˜ëœ ë²¡í„° (numpy ë°°ì—´)
        """
        # ì„±ëŠ¥ ì¶”ì  ì‹œì‘
        self.performance_tracker.start_tracking("vectorize_full_analysis")

        # ë©”ëª¨ë¦¬ ì •ë¦¬ ë³€ìˆ˜ë“¤
        temp_vectors = []
        large_arrays = []

        try:
            # ë²¡í„° ì„¤ì •ì—ì„œ ìºì‹œ í‚¤ ìƒì„±
            vector_settings = {}
            try:
                if isinstance(self.config, dict) and "vector_settings" in self.config:
                    vector_settings = self.config["vector_settings"]
                elif hasattr(self.config, "get") and callable(
                    getattr(self.config, "get")
                ):
                    vector_settings = self.config.get("vector_settings", {})
            except Exception as e:
                self.logger.warning(f"ë²¡í„° ì„¤ì • ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")

            # ìºì‹œ í‚¤ ìƒì„±
            cache_key = self._create_cache_key(vector_settings)
            self.logger.info(f"ë²¡í„° ì„¤ì • ê¸°ë°˜ ìºì‹œ í‚¤ ìƒì„±: {cache_key}")

            # ìºì‹œ íŒŒì¼ ê²½ë¡œì— í•´ì‹œ í‚¤ í¬í•¨
            cache_file = Path(self.cache_dir) / f"feature_vector_{cache_key}.npy"
            feature_names_file = (
                Path(self.cache_dir) / f"feature_vector_{cache_key}.names.json"
            )

            # ìºì‹œ í™•ì¸
            if cache_file.exists():
                try:
                    self.logger.info(f"ìºì‹œëœ ë²¡í„° ë°ì´í„° ì‚¬ìš©: {cache_file}")
                    # íŠ¹ì„± ì´ë¦„ë„ ë¡œë“œ
                    if feature_names_file.exists():
                        with open(feature_names_file, "r", encoding="utf-8") as f:
                            self.feature_names = json.load(f)

                    # ì„±ëŠ¥ ì¶”ì  ì¢…ë£Œ
                    self.performance_tracker.stop_tracking("vectorize_full_analysis")
                    return np.load(cache_file)
                except Exception as e:
                    self.logger.warning(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")

            # ë²¡í„° íŠ¹ì„± ì´ˆê¸°í™”
            vector_features = {}
            # íŠ¹ì„± ì´ë¦„ ì´ˆê¸°í™”
            self.feature_names = []
            # íŠ¹ì„± ê·¸ë£¹ë³„ ì´ë¦„ ì €ì¥
            feature_names_by_group = {}

            # 1. 10êµ¬ê°„ ë¹ˆë„ (10ê°œ ê°’)
            if "segment_10_frequency" in full_analysis:
                segment_10_vector = self._extract_segment_frequency(
                    full_analysis["segment_10_frequency"], 10
                )
                vector_features["segment_10"] = segment_10_vector
                feature_names_by_group["segment_10"] = [
                    f"segment_10_freq_{i+1}" for i in range(10)
                ]
                temp_vectors.append(segment_10_vector)

            # 2. 5êµ¬ê°„ ë¹ˆë„ (5ê°œ ê°’)
            if "segment_5_frequency" in full_analysis:
                segment_5_vector = self._extract_segment_frequency(
                    full_analysis["segment_5_frequency"], 5
                )
                vector_features["segment_5"] = segment_5_vector
                feature_names_by_group["segment_5"] = [
                    f"segment_5_freq_{i+1}" for i in range(5)
                ]
                temp_vectors.append(segment_5_vector)

            # ... ì¤‘ê°„ ë²¡í„° ì²˜ë¦¬ ê³¼ì •ì—ì„œ ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬ ...

            # ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬ (ë²¡í„° 5ê°œë§ˆë‹¤)
            if len(temp_vectors) >= 5:
                # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì‹¤í–‰
                gc.collect()
                # ì„ì‹œ ë²¡í„° ëª©ë¡ ì •ë¦¬
                temp_vectors.clear()
                self.logger.debug("ì¤‘ê°„ ë²¡í„° ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")

            # ... existing code for other vector processing ...

            # ìµœì¢… ë²¡í„° ê²°í•©
            combined_vector = self._combine_vectors(vector_features)
            large_arrays.append(combined_vector)

            # ë²¡í„° ê²€ì¦
            self._validate_final_vector(combined_vector)

            # ë²¡í„°ì™€ íŠ¹ì„± ì´ë¦„ ì €ì¥ (ìºì‹œ í‚¤ í¬í•¨ ê²½ë¡œ ì‚¬ìš©)
            self.save_vector_to_file(combined_vector, f"feature_vector_{cache_key}.npy")
            self.save_names_to_file(
                self.feature_names, f"feature_vector_{cache_key}.names.json"
            )

            # í˜¸í™˜ì„±ì„ ìœ„í•´ feature_vector_full.npyë„ í•¨ê»˜ ì €ì¥
            compat_file = Path(self.cache_dir) / "feature_vector_full.npy"
            compat_names_file = Path(self.cache_dir) / "feature_vector_full.names.json"
            np.save(compat_file, combined_vector)
            with open(compat_names_file, "w", encoding="utf-8") as f:
                json.dump(self.feature_names, f, ensure_ascii=False, indent=2)

            # ì„±ëŠ¥ ì¶”ì  ì¢…ë£Œ
            self.performance_tracker.stop_tracking("vectorize_full_analysis")

            return combined_vector

        except Exception as e:
            # ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ ì„±ëŠ¥ ì¶”ì  ì¢…ë£Œë¥¼ ë³´ì¥
            self.performance_tracker.stop_tracking("vectorize_full_analysis")
            raise e
        finally:
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            try:
                # ì„ì‹œ ë²¡í„°ë“¤ ì •ë¦¬
                for vec in temp_vectors:
                    if vec is not None:
                        del vec
                temp_vectors.clear()

                # ëŒ€í˜• ë°°ì—´ë“¤ ì •ë¦¬ (ìµœì¢… ê²°ê³¼ ì œì™¸)
                for arr in large_arrays[:-1]:  # ë§ˆì§€ë§‰ ê²°ê³¼ëŠ” ë³´ì¡´
                    if arr is not None:
                        del arr

                # ë²¡í„° íŠ¹ì„± ë”•ì…”ë„ˆë¦¬ ì •ë¦¬
                if "vector_features" in locals():
                    for key, vec in vector_features.items():
                        if vec is not None and key != "final_result":
                            del vec
                    vector_features.clear()

                # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì‹¤í–‰
                gc.collect()

                # CUDA ë©”ëª¨ë¦¬ ì •ë¦¬ (í•„ìš”ì‹œ)
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()

                self.logger.debug("ë²¡í„°í™” ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")

            except Exception as cleanup_error:
                self.logger.warning(f"ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(cleanup_error)}")

    def _extract_segment_frequency(
        self, segment_freq: Dict[str, Any], num_segments: int
    ) -> np.ndarray:
        """
        êµ¬ê°„ë³„ ë¹ˆë„ ë²¡í„° ì¶”ì¶œ

        Args:
            segment_freq: êµ¬ê°„ë³„ ë¹ˆë„ ë°ì´í„°
            num_segments: êµ¬ê°„ ìˆ˜ (5 ë˜ëŠ” 10)

        Returns:
            êµ¬ê°„ë³„ ë¹ˆë„ ë²¡í„°
        """
        # ê¸°ë³¸ ë²¡í„° ì´ˆê¸°í™”
        vector = np.zeros(num_segments, dtype=np.float32)

        # ê° êµ¬ê°„ë³„ ë¹ˆë„ ì„¤ì •
        for segment_idx, freq in segment_freq.items():
            try:
                idx = (
                    int(segment_idx) - 1
                )  # êµ¬ê°„ ë²ˆí˜¸ëŠ” 1ë¶€í„° ì‹œì‘í•˜ë¯€ë¡œ 0-ì¸ë±ìŠ¤ë¡œ ë³€í™˜
                if 0 <= idx < num_segments:
                    vector[idx] = float(freq)
            except (ValueError, TypeError):
                # êµ¬ê°„ ì¸ë±ìŠ¤ê°€ ìˆ«ìê°€ ì•„ë‹Œ ê²½ìš° ë¬´ì‹œ
                continue

        # ë²¡í„° ì •ê·œí™” (í•©ì´ 1ì´ ë˜ë„ë¡)
        total = np.sum(vector)
        if total > 0:
            vector = vector / total

        return vector

    def _extract_segment_centrality(
        self, segment_centrality: Dict[str, Any]
    ) -> np.ndarray:
        """
        ì„¸ê·¸ë¨¼íŠ¸ ì¤‘ì‹¬ì„± ë°ì´í„°ë¥¼ ë²¡í„°ë¡œ ë³€í™˜

        Args:
            segment_centrality: ì„¸ê·¸ë¨¼íŠ¸ ì¤‘ì‹¬ì„± ë°ì´í„° (9ê°œ ì„¸ê·¸ë¨¼íŠ¸ì— ëŒ€í•œ ê°’)

        Returns:
            ì„¸ê·¸ë¨¼íŠ¸ ì¤‘ì‹¬ì„± ë²¡í„° (9ì°¨ì›)
        """
        # ê¸°ë³¸ ë²¡í„° ì´ˆê¸°í™” (9ê°œ ì„¸ê·¸ë¨¼íŠ¸)
        vector = np.zeros(9, dtype=np.float32)

        if not segment_centrality:
            return vector

        # ì„¸ê·¸ë¨¼íŠ¸ ì¤‘ì‹¬ì„± ê°’ ì„¤ì •
        for i in range(9):
            segment_start = i * 5 + 1
            segment_end = (i + 1) * 5
            segment_key = f"{segment_start}~{segment_end}"

            # ì„¸ê·¸ë¨¼íŠ¸ í‚¤ê°€ ë‹¤ë¥¸ í˜•ì‹ì¼ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ ëŒ€ì²´ í‚¤ ì‹œë„
            alt_keys = [
                f"{segment_start}~{segment_end}",
                f"{segment_start}-{segment_end}",
                f"{segment_start}_{segment_end}",
            ]

            centrality_value = None
            for key in alt_keys:
                if key in segment_centrality:
                    if isinstance(segment_centrality[key], dict):
                        # ê°’ì´ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° (eigenvector, degree ë“±ì´ í¬í•¨ëœ ê²½ìš°)
                        if "eigenvector" in segment_centrality[key]:
                            centrality_value = float(
                                segment_centrality[key]["eigenvector"]
                            )
                        elif "degree" in segment_centrality[key]:
                            centrality_value = float(segment_centrality[key]["degree"])
                    else:
                        # ê°’ì´ ìˆ«ìì¸ ê²½ìš°
                        centrality_value = float(segment_centrality[key])
                    break

            if centrality_value is not None and i < len(vector):
                vector[i] = centrality_value

        # ë²¡í„° ì •ê·œí™” (0-1 ë²”ìœ„ë¡œ)
        max_val = np.max(vector)
        if max_val > 0:
            vector = vector / max_val

        return vector

    def _extract_segment_consecutive(
        self, segment_consecutive_patterns: Dict[str, Dict[str, int]]
    ) -> np.ndarray:
        """
        ì„¸ê·¸ë¨¼íŠ¸ ì—°ì† íŒ¨í„´ ë°ì´í„°ë¥¼ ë²¡í„°ë¡œ ë³€í™˜

        Args:
            segment_consecutive_patterns: ì„¸ê·¸ë¨¼íŠ¸ë³„ ì—°ì† íŒ¨í„´ í†µê³„ ë°ì´í„°
                {
                    "1~5": {"count_2": 37, "count_3": 12, "count_4+": 3},
                    "6~10": {"count_2": 25, "count_3": 8, "count_4+": 1},
                    ...
                }

        Returns:
            ì„¸ê·¸ë¨¼íŠ¸ ì—°ì† íŒ¨í„´ ë²¡í„° (9ê°œ ì„¸ê·¸ë¨¼íŠ¸ x 3ê°œ ì¹´ìš´íŠ¸ = 27ì°¨ì›)
        """
        # ê¸°ë³¸ ë²¡í„° ì´ˆê¸°í™” (9ê°œ ì„¸ê·¸ë¨¼íŠ¸ x 3ê°œ ì¹´ìš´íŠ¸ = 27)
        vector = np.zeros(27, dtype=np.float32)

        if not segment_consecutive_patterns:
            return vector

        # ì„¸ê·¸ë¨¼íŠ¸ë³„ ì—°ì† íŒ¨í„´ ì¹´ìš´íŠ¸ ì¶”ì¶œ
        for i in range(9):
            segment_start = i * 5 + 1
            segment_end = (i + 1) * 5

            # ì„¸ê·¸ë¨¼íŠ¸ í‚¤ê°€ ë‹¤ë¥¸ í˜•ì‹ì¼ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ ëŒ€ì²´ í‚¤ ì‹œë„
            segment_keys = [
                f"{segment_start}~{segment_end}",
                f"{segment_start}-{segment_end}",
                f"segment_{i+1}",
            ]

            segment_data = None
            for key in segment_keys:
                if key in segment_consecutive_patterns:
                    segment_data = segment_consecutive_patterns[key]
                    break

            if segment_data and isinstance(segment_data, dict):
                # ê° ì—°ì† íŒ¨í„´ ì¹´ìš´íŠ¸ ì¶”ì¶œ (2ê°œ ì—°ì†, 3ê°œ ì—°ì†, 4ê°œ ì´ìƒ ì—°ì†)
                base_idx = i * 3  # ê° ì„¸ê·¸ë¨¼íŠ¸ë§ˆë‹¤ 3ê°œ ì¹´ìš´íŠ¸

                # 2ê°œ ì—°ì† ì¹´ìš´íŠ¸
                if "count_2" in segment_data:
                    vector[base_idx] = float(segment_data["count_2"])

                # 3ê°œ ì—°ì† ì¹´ìš´íŠ¸
                if "count_3" in segment_data:
                    vector[base_idx + 1] = float(segment_data["count_3"])

                # 4ê°œ ì´ìƒ ì—°ì† ì¹´ìš´íŠ¸
                if "count_4+" in segment_data:
                    vector[base_idx + 2] = float(segment_data["count_4+"])

        # ë²¡í„° ì •ê·œí™” (0-1 ë²”ìœ„ë¡œ)
        max_val = np.max(vector)
        if max_val > 0:
            vector = vector / max_val

        return vector

    def _extract_pattern_reappearance(self, pattern_data: Dict[str, Any]) -> np.ndarray:
        """
        íŒ¨í„´ ì¬ì¶œí˜„ ê°„ê²© ë°ì´í„°ë¥¼ ë²¡í„°ë¡œ ë³€í™˜

        Args:
            pattern_data: íŒ¨í„´ ì¬ì¶œí˜„ ê°„ê²© ë°ì´í„°

        Returns:
            íŒ¨í„´ ì¬ì¶œí˜„ ê°„ê²© ë²¡í„°
        """
        # ì¬ì¶œí˜„ ê°„ê²© íŠ¹ì„± ì¶”ì¶œ (í‰ê· , í‘œì¤€í¸ì°¨, ìµœì†Œ, ìµœëŒ€)
        features = np.zeros(4, dtype=np.float32)

        all_intervals = []
        for pattern_type, intervals in pattern_data.items():
            if isinstance(intervals, list):
                all_intervals.extend(intervals)
            elif isinstance(intervals, dict):
                for _, values in intervals.items():
                    if isinstance(values, dict) and "intervals" in values:
                        if isinstance(values["intervals"], list):
                            all_intervals.extend(values["intervals"])
                    elif isinstance(values, list):
                        all_intervals.extend(values)

        if all_intervals:
            # í‰ê· , í‘œì¤€í¸ì°¨, ìµœì†Œ, ìµœëŒ€ ê³„ì‚°
            features[0] = np.mean(all_intervals)
            features[1] = np.std(all_intervals)
            features[2] = np.min(all_intervals)
            features[3] = np.max(all_intervals)

        return features

    def _extract_recent_gaps(self, gap_data: Dict[str, Any]) -> np.ndarray:
        """
        ë²ˆí˜¸ë³„ ìµœê·¼ ì¬ì¶œí˜„ ê°„ê²© ë°ì´í„°ë¥¼ ë²¡í„°ë¡œ ë³€í™˜

        Args:
            gap_data: ë²ˆí˜¸ë³„ ìµœê·¼ ì¬ì¶œí˜„ ê°„ê²© ë°ì´í„°

        Returns:
            ë²ˆí˜¸ë³„ ìµœê·¼ ì¬ì¶œí˜„ ê°„ê²© ë²¡í„° (45ê°œ ê°’)
        """
        vector = np.zeros(45, dtype=np.float32)

        # ë²ˆí˜¸ë³„ ê°„ê²© ì„¤ì •
        for num_str, gap in gap_data.items():
            try:
                num = int(num_str)
                if 1 <= num <= 45:
                    vector[num - 1] = float(gap)
            except (ValueError, TypeError):
                # ë²ˆí˜¸ê°€ ìˆ«ìê°€ ì•„ë‹Œ ê²½ìš° ë¬´ì‹œ
                continue

        # ìµœëŒ€ê°’ìœ¼ë¡œ ì •ê·œí™” (0-1 ë²”ìœ„ë¡œ)
        max_gap = np.max(vector)
        if max_gap > 0:
            vector = vector / max_gap

        return vector

    def _combine_vectors(self, vector_features: Dict[str, np.ndarray]) -> np.ndarray:
        """
        ì—¬ëŸ¬ íŠ¹ì„± ë²¡í„°ë“¤ì„ ê²°í•©í•˜ì—¬ í•˜ë‚˜ì˜ ë²¡í„°ë¡œ ë§Œë“­ë‹ˆë‹¤.

        Args:
            vector_features: íŠ¹ì„± ê·¸ë£¹ë³„ ë²¡í„° ì‚¬ì „

        Returns:
            ê²°í•©ëœ ë²¡í„°
        """
        total_dimensions = sum(
            vector.shape[0] if vector.ndim == 1 else vector.shape[1]
            for vector in vector_features.values()
        )

        # ê²°í•©ëœ ë²¡í„° ì¤€ë¹„
        combined_vector = np.zeros(total_dimensions, dtype=np.float32)
        self.feature_names = []  # íŠ¹ì„± ì´ë¦„ ëª©ë¡ ì´ˆê¸°í™”

        # ê·¸ë£¹ë³„ ë²¡í„° ì°¨ì› ì¶”ì 
        feature_group_dimensions = {}

        # í˜„ì¬ ìœ„ì¹˜ ì¸ë±ìŠ¤
        current_idx = 0

        # ê° ê·¸ë£¹ë³„ ë²¡í„° ì¶”ê°€
        for group_name, vector in vector_features.items():
            # ë²¡í„°ê°€ ë¹„ì–´ìˆìœ¼ë©´ ê±´ë„ˆë›°ê¸°
            if vector is None or vector.size == 0:
                continue

            vector_dim = vector.shape[0] if vector.ndim == 1 else vector.shape[1]

            # ê·¸ë£¹ ì°¨ì› ê¸°ë¡
            feature_group_dimensions[group_name] = vector_dim

            # ë²¡í„° ë°ì´í„° ë³µì‚¬
            if vector.ndim == 1:
                combined_vector[current_idx : current_idx + vector_dim] = vector
            else:
                combined_vector[current_idx : current_idx + vector_dim] = (
                    vector.flatten()[:vector_dim]
                )

            # íŠ¹ì„± ì´ë¦„ ì¶”ê°€
            # feature_names_by_groupì— ì´ë¦„ì´ ìˆìœ¼ë©´ í•´ë‹¹ ì´ë¦„ì„ ì‚¬ìš©, ì—†ìœ¼ë©´ ìë™ ìƒì„±
            if (
                hasattr(self, "feature_names_by_group")
                and group_name in self.feature_names_by_group
            ):
                group_feature_names = self.feature_names_by_group[group_name]
                # ì´ë¦„ ê°œìˆ˜ê°€ ì°¨ì›ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ì¡°ì •
                if len(group_feature_names) == vector_dim:
                    self.feature_names.extend(group_feature_names)
                else:
                    # ì´ë¦„ ê°œìˆ˜ê°€ ë‹¤ë¥¸ ê²½ìš° ê²½ê³  ë¡œê·¸ ì¶œë ¥ í›„ ìë™ ìƒì„± ì´ë¦„ ì‚¬ìš©
                    self.logger.warning(
                        f"ê·¸ë£¹ '{group_name}'ì˜ íŠ¹ì„± ì´ë¦„ ê°œìˆ˜({len(group_feature_names)})ì™€ "
                        f"ë²¡í„° ì°¨ì›({vector_dim})ì´ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìë™ ìƒì„± ì´ë¦„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
                    )
                    self.feature_names.extend(
                        [f"{group_name}_{i}" for i in range(vector_dim)]
                    )
            else:
                # ì´ë¦„ì´ ì—†ëŠ” ê²½ìš° ìë™ ìƒì„±
                self.feature_names.extend(
                    [f"{group_name}_{i}" for i in range(vector_dim)]
                )

            # í˜„ì¬ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
            current_idx += vector_dim

        # ê° ê·¸ë£¹ì˜ ì°¨ì› ë¡œê¹…
        for group_name, dim in feature_group_dimensions.items():
            self.logger.info(f"ë²¡í„° ê·¸ë£¹ '{group_name}' ì°¨ì›: ({dim},)")

        # íŠ¹ì„± ì´ë¦„ê³¼ ë²¡í„° ì°¨ì›ì´ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
        if len(self.feature_names) != combined_vector.shape[0]:
            self.logger.warning(
                f"íŠ¹ì„± ì´ë¦„ ê°œìˆ˜({len(self.feature_names)})ì™€ ë²¡í„° í¬ê¸°({combined_vector.shape[0]})ê°€ "
                f"ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê¸°ë³¸ ì´ë¦„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
            )

            # íŠ¹ì„± ì´ë¦„ ê¸¸ì´ ì¡°ì •
            if len(self.feature_names) < combined_vector.shape[0]:
                # ë¶€ì¡±í•œ íŠ¹ì„± ì´ë¦„ ì¶”ê°€
                self.feature_names.extend(
                    [
                        f"feature_{i+len(self.feature_names)}"
                        for i in range(
                            combined_vector.shape[0] - len(self.feature_names)
                        )
                    ]
                )
            elif len(self.feature_names) > combined_vector.shape[0]:
                # ì´ˆê³¼ëœ íŠ¹ì„± ì´ë¦„ ì œê±°
                self.feature_names = self.feature_names[: combined_vector.shape[0]]

        return combined_vector

    def normalize_vector(self, vector: np.ndarray) -> np.ndarray:
        """
        íŠ¹ì„± ë²¡í„°ë¥¼ ì •ê·œí™”í•©ë‹ˆë‹¤.

        Args:
            vector: ì›ë³¸ íŠ¹ì„± ë²¡í„°

        Returns:
            ì •ê·œí™”ëœ íŠ¹ì„± ë²¡í„°
        """
        with performance_monitor("normalize_vector"):
            # NaN ê°’ í™•ì¸ ë° ì²˜ë¦¬
            has_nan = np.isnan(vector).any()
            if has_nan:
                self.logger.warning(f"ë²¡í„°ì— NaN ê°’ì´ ìˆì–´ 0ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                vector = np.nan_to_num(vector, nan=0.0)

            # ë¬´í•œëŒ€ ê°’ í™•ì¸ ë° ì²˜ë¦¬
            has_inf = np.isinf(vector).any()
            if has_inf:
                self.logger.warning(f"ë²¡í„°ì— ë¬´í•œëŒ€ ê°’ì´ ìˆì–´ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
                vector = np.nan_to_num(vector, posinf=1.0, neginf=0.0)

            # ê°’ ë²”ìœ„ ê²€ì¦ ë° ì¡°ì • (0-1 ë²”ìœ„)
            out_of_range = np.logical_or(vector < 0, vector > 1).any()
            if out_of_range:
                self.logger.warning(f"ë²¡í„°ì— [0,1] ë²”ìœ„ë¥¼ ë²—ì–´ë‚œ ê°’ì´ ìˆì–´ ì¡°ì •í•©ë‹ˆë‹¤.")
                vector = np.clip(vector, 0.0, 1.0)

            return vector

    def _process_float_conversion(self, value: Any) -> float:
        """
        ë‹¤ì–‘í•œ íƒ€ì…ì˜ ê°’ì„ ì•ˆì „í•˜ê²Œ floatë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        ê¸°ë³¸ êµ¬í˜„ ë©”ì„œë“œ

        Args:
            value: ë³€í™˜í•  ê°’ (dictionary, list, ìˆ«ì, ë¬¸ìì—´ ë“±)

        Returns:
            float: ë³€í™˜ëœ float ê°’
        """
        try:
            # dict íƒ€ì…ì¸ ê²½ìš° ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ê°’ ì¶”ì¶œ ì‹œë„
            if isinstance(value, dict):
                # ìš°ì„ ìˆœìœ„: score > value > avg > mean > ì²« ë²ˆì§¸ ê°’
                for key in ["score", "value", "avg", "mean", "total"]:
                    if key in value and value[key] is not None:
                        return self._process_float_conversion(value[key])

                # í‚¤ê°€ ì—†ëŠ” ê²½ìš° ì²« ë²ˆì§¸ ê°’ ì‚¬ìš©
                if value:
                    return self._process_float_conversion(list(value.values())[0])
                return 0.0

            # list ë˜ëŠ” tupleì¸ ê²½ìš° ì²« ë²ˆì§¸ ìš”ì†Œ ì‚¬ìš©
            elif isinstance(value, (list, tuple)) and value:
                if len(value) == 1:
                    return self._process_float_conversion(value[0])
                # ëª¨ë“  ê°’ì˜ í‰ê·  ê³„ì‚° ì‹œë„
                try:
                    return float(
                        sum(self._process_float_conversion(v) for v in value)
                        / len(value)
                    )
                except:
                    return self._process_float_conversion(value[0])

            # bool íƒ€ì…ì¸ ê²½ìš° 1.0 ë˜ëŠ” 0.0ìœ¼ë¡œ ë³€í™˜
            elif isinstance(value, bool):
                return 1.0 if value else 0.0

            # None ê°’ ì²˜ë¦¬
            elif value is None:
                return 0.0

            # ë‚˜ë¨¸ì§€ ê²½ìš° float ë³€í™˜ ì‹œë„
            return float(value)
        except (ValueError, TypeError, IndexError, KeyError) as e:
            self.logger.warning(
                f"ê°’ '{value}' ({type(value)})ë¥¼ floatë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}"
            )
            return 0.0  # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜

    def safe_float_conversion(self, value: Any) -> float:
        """
        ë‹¤ì–‘í•œ íƒ€ì…ì˜ ê°’ì„ ì•ˆì „í•˜ê²Œ floatë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        ì™¸ë¶€ì—ì„œ ì£¼ì…ëœ í•¨ìˆ˜ê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©í•˜ê³ ,
        ì—†ìœ¼ë©´ ê¸°ë³¸ êµ¬í˜„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

        Args:
            value: ë³€í™˜í•  ê°’ (dictionary, list, ìˆ«ì, ë¬¸ìì—´ ë“±)

        Returns:
            float: ë³€í™˜ëœ float ê°’
        """
        # ì™¸ë¶€ì—ì„œ ì£¼ì…ëœ safe_float_conversion í•¨ìˆ˜ê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©
        if hasattr(self, "_external_float_conversion") and callable(
            self._external_float_conversion
        ):
            return self._external_float_conversion(value)

        # ì—†ìœ¼ë©´ ê¸°ë³¸ êµ¬í˜„ ì‚¬ìš©
        return self._process_float_conversion(value)

    @property
    def external_float_conversion(self):
        """ì™¸ë¶€ì—ì„œ ì£¼ì…ëœ float ë³€í™˜ í•¨ìˆ˜ë¥¼ ë°˜í™˜"""
        return (
            self._external_float_conversion
            if hasattr(self, "_external_float_conversion")
            else None
        )

    @external_float_conversion.setter
    def external_float_conversion(self, func):
        """ì™¸ë¶€ float ë³€í™˜ í•¨ìˆ˜ ì„¤ì •"""
        self._external_float_conversion = func

    def save_vector_to_file(
        self, vector: np.ndarray, filename: str = "feature_vector_full.npy"
    ) -> str:
        """
        íŠ¹ì„± ë²¡í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.

        Args:
            vector: ì €ì¥í•  íŠ¹ì„± ë²¡í„°
            filename: ì €ì¥í•  íŒŒì¼ëª…

        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ

        Raises:
            ValueError: ë²¡í„° ì°¨ì›ê³¼ íŠ¹ì„± ì´ë¦„ ìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ” ê²½ìš°
        """
        # ìºì‹œ ë””ë ‰í† ë¦¬ í™•ì¸
        try:
            cache_dir = self.config["paths"]["cache_dir"]
        except (KeyError, TypeError):
            cache_dir = "data/cache"

        cache_path = Path(cache_dir)
        cache_path.mkdir(parents=True, exist_ok=True)

        # íŠ¹ì„± ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
        feature_names = self.get_feature_names()

        # ë²¡í„° ì°¨ì›ê³¼ íŠ¹ì„± ì´ë¦„ ìˆ˜ ì¼ì¹˜ í™•ì¸
        vector_dim = vector.shape[1] if len(vector.shape) > 1 else vector.shape[0]
        names_count = len(feature_names)

        # í•„ìˆ˜ íŠ¹ì„± ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        essential_features = []
        try:
            if (
                isinstance(self.config, dict)
                and "validation" in self.config
                and "essential_features" in self.config["validation"]
            ):
                essential_features = self.config["validation"]["essential_features"]
            elif hasattr(self.config, "get") and callable(getattr(self.config, "get")):
                validation_config = self.config.get("validation", {})
                if hasattr(validation_config, "get") and callable(
                    getattr(validation_config, "get")
                ):
                    essential_features = validation_config.get("essential_features", [])
        except Exception as e:
            self.logger.warning(f"ì„¤ì •ì—ì„œ í•„ìˆ˜ íŠ¹ì„± ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜: {str(e)}")

        # í•„ìˆ˜ íŠ¹ì„± ëª©ë¡ì´ ë¹„ì–´ìˆìœ¼ë©´ ê¸°ë³¸ í•„ìˆ˜ íŠ¹ì„± ì‚¬ìš©
        if not essential_features:
            # í•„ìˆ˜ íŠ¹ì„±ì„ ì§ì ‘ ì •ì˜ (ëª¨ë“ˆì´ ì—†ìœ¼ë¯€ë¡œ)
            essential_features = [
                "gap_stddev",
                "pair_centrality",
                "hot_cold_mix_score",
                "segment_entropy",
                "roi_group_score",
                "duplicate_flag",
                "max_overlap_with_past",
                "combination_recency_score",
                "position_entropy_1",
                "position_entropy_2",
                "position_entropy_3",
                "position_entropy_4",
                "position_entropy_5",
                "position_entropy_6",
                "position_std_1",
                "position_std_2",
                "position_std_3",
                "position_std_4",
                "position_std_5",
                "position_std_6",
                "position_variance_avg",
                "position_bias_score",
            ]

        # í•„ìˆ˜ íŠ¹ì„± ì¤‘ ëˆ„ë½ëœ íŠ¹ì„± í™•ì¸
        missing_essential = [f for f in essential_features if f not in feature_names]

        # ì°¨ì› ë¶ˆì¼ì¹˜ í™•ì¸
        if vector_dim != names_count:
            self.logger.warning(
                f"ë²¡í„° ì°¨ì›({vector_dim})ê³¼ íŠ¹ì„± ì´ë¦„ ìˆ˜({names_count})ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"
            )

            # ì¶”ê°€ ì •ë³´ ë¡œê¹…: ì°¨ì› ë¶ˆì¼ì¹˜ ì •ë„
            dimension_diff = abs(vector_dim - names_count)
            percent_diff = (dimension_diff / max(vector_dim, names_count)) * 100
            self.logger.warning(
                f"ì°¨ì› ë¶ˆì¼ì¹˜ ì •ë„: {dimension_diff}ê°œ ({percent_diff:.2f}%)"
            )

            # ë²¡í„° ì°¨ì›ê³¼ íŠ¹ì„± ì´ë¦„ ìˆ˜ë¥¼ ì¼ì¹˜ì‹œí‚¤ê¸° ìœ„í•œ ì²˜ë¦¬
            if vector_dim > names_count:
                self.logger.warning(
                    f"ë²¡í„° ì°¨ì›ì´ íŠ¹ì„± ì´ë¦„ ìˆ˜ë³´ë‹¤ {vector_dim - names_count}ê°œ ë” ë§ìŠµë‹ˆë‹¤. ì´ë¦„ ëª©ë¡ì„ í™•ì¥í•©ë‹ˆë‹¤."
                )
                # íŠ¹ì„± ì´ë¦„ ëª©ë¡ í™•ì¥
                for i in range(names_count, vector_dim):
                    feature_names.append(f"feature_{i}")
            else:
                self.logger.warning(
                    f"íŠ¹ì„± ì´ë¦„ ìˆ˜ê°€ ë²¡í„° ì°¨ì›ë³´ë‹¤ {names_count - vector_dim}ê°œ ë” ë§ìŠµë‹ˆë‹¤. ë²¡í„°ë¥¼ í™•ì¥í•©ë‹ˆë‹¤."
                )
                # ë²¡í„°ë¥¼ í™•ì¥í•˜ëŠ” ì½”ë“œ
                if len(vector.shape) > 1:
                    extended_vector = np.zeros(
                        (vector.shape[0], names_count), dtype=vector.dtype
                    )
                    extended_vector[:, :vector_dim] = vector
                    vector = extended_vector
                else:
                    extended_vector = np.zeros(names_count, dtype=vector.dtype)
                    extended_vector[:vector_dim] = vector
                    vector = extended_vector

                # ì°¨ì› ì—…ë°ì´íŠ¸
                vector_dim = names_count

        # í•„ìˆ˜ íŠ¹ì„± ì¶”ê°€
        if missing_essential:
            self.logger.warning(f"ë‹¤ìŒ í•„ìˆ˜ íŠ¹ì„±ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {missing_essential}")

            # ëˆ„ë½ëœ íŠ¹ì„±ì„ feature_namesì— ì¶”ê°€
            feature_names.extend(missing_essential)

            # ë²¡í„°ë„ í™•ì¥
            new_dim = len(feature_names)
            if len(vector.shape) > 1:
                extended_vector = np.zeros(
                    (vector.shape[0], new_dim), dtype=vector.dtype
                )
                extended_vector[:, :vector_dim] = vector
                vector = extended_vector
            else:
                extended_vector = np.zeros(new_dim, dtype=vector.dtype)
                extended_vector[:vector_dim] = vector
                vector = extended_vector

            # ì¶”ê°€ëœ íŠ¹ì„±ì— ê¸°ë³¸ê°’ ì„¤ì • (0.5 ë˜ëŠ” íŠ¹ì • ê¸°ë³¸ê°’)
            for i, name in enumerate(feature_names):
                if i >= vector_dim and name in missing_essential:
                    # íŠ¹ì„±ë³„ ê¸°ë³¸ê°’ ì„¤ì •
                    if "position_entropy" in name or "segment_entropy" in name:
                        vector[..., i] = 0.5
                    elif "stddev" in name or "std_" in name:
                        vector[..., i] = 0.1
                    elif "score" in name:
                        vector[..., i] = 0.5
                    elif "flag" in name:
                        vector[..., i] = 0.0
                    elif "silhouette_score" in name:
                        vector[..., i] = 0.3
                    else:
                        vector[..., i] = 0.5

            self.logger.info(
                f"í•„ìˆ˜ íŠ¹ì„± {len(missing_essential)}ê°œê°€ ìë™ìœ¼ë¡œ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤."
            )

            # ì°¨ì› ì—…ë°ì´íŠ¸
            vector_dim = new_dim

        # ìµœì¢… ê²€ì¦
        names_count = len(feature_names)
        if vector_dim != names_count:
            error_msg = f"[ERROR] ì²˜ë¦¬ í›„ì—ë„ ë²¡í„° ì°¨ì›({vector_dim})ê³¼ íŠ¹ì„± ì´ë¦„ ìˆ˜({names_count})ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"
            self.logger.error(error_msg)
            raise ValueError(error_msg)

        # ë²¡í„° ë²”ìœ„ ê²€ì¦
        min_val = np.min(vector)
        max_val = np.max(vector)
        if min_val < 0 or max_val > 1:
            self.logger.warning(
                f"ë²¡í„°ì— [0,1] ë²”ìœ„ë¥¼ ë²—ì–´ë‚œ ê°’ì´ ìˆìŠµë‹ˆë‹¤: ìµœì†Œ={min_val}, ìµœëŒ€={max_val}"
            )
            # ê°’ ë²”ìœ„ë¥¼ 0-1ë¡œ í´ë¦¬í•‘
            vector = np.clip(vector, 0, 1)
            self.logger.info("ë²¡í„° ê°’ì„ [0,1] ë²”ìœ„ë¡œ í´ë¦¬í•‘í–ˆìŠµë‹ˆë‹¤")

        # íŒŒì¼ ê²½ë¡œ ìƒì„±
        file_path = cache_path / filename

        # NumPy ë°°ì—´ë¡œ ì €ì¥
        np.save(file_path, vector)
        self.logger.info(f"íŠ¹ì„± ë²¡í„° ì €ì¥ ì™„ë£Œ: {file_path}")

        # íŠ¹ì„± ì´ë¦„ ì €ì¥
        if feature_names:
            # í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ ë²¡í„°ì¸ ê²½ìš°, í•´ë‹¹ íŒŒì¼ì—ë§Œ ê´€ë ¨ ì§€í‘œ ì´ë¦„ ì‚¬ìš©
            if "cluster_quality" in filename:
                # í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ ê´€ë ¨ ì´ë¦„ë§Œ í•„í„°ë§
                relevant_names = []
                for name in feature_names:
                    if any(
                        metric in name
                        for metric in [
                            "silhouette",
                            "balance",
                            "cluster",
                            "cohesiveness",
                            "distance_between",
                            "entropy",
                            "quality",
                        ]
                    ):
                        relevant_names.append(name)

                # ë²¡í„°ì˜ í¬ê¸°ì— ë§ê²Œ ì´ë¦„ ëª©ë¡ ì¡°ì •
                if len(relevant_names) < vector_dim:
                    for i in range(len(relevant_names), vector_dim):
                        relevant_names.append(f"cluster_feature_{i}")
                elif len(relevant_names) > vector_dim:
                    relevant_names = relevant_names[:vector_dim]

                names_to_save = relevant_names
            else:
                names_to_save = feature_names

            names_file = cache_path / f"{Path(filename).stem}.names.json"
            with open(names_file, "w", encoding="utf-8") as f:
                json.dump(names_to_save, f, indent=2, ensure_ascii=False)
            self.logger.info(f"íŠ¹ì„± ì´ë¦„ ì €ì¥ ì™„ë£Œ: {names_file}")

            # íŠ¹ì„± ë²¡í„° ê²€ì¦
            # from ..utils.feature_vector_validator import check_vector_dimensions
            # ë²¡í„° ê²€ì¦ì€ ìƒëµ (ëª¨ë“ˆì´ ì—†ìŒ)

            try:
                # check_vector_dimensions(str(file_path), str(names_file))
                self.logger.info("ë²¡í„° ì°¨ì› ê²€ì¦ ìƒëµ (ëª¨ë“ˆ ì—†ìŒ)")
            except Exception as e:
                self.logger.error(f"íŠ¹ì„± ë²¡í„° ê²€ì¦ ì‹¤íŒ¨: {e}")
                # raise  # ê²€ì¦ ì‹¤íŒ¨ë¥¼ ë¬´ì‹œ

        return str(file_path)

    def save_names_to_file(
        self, names: List[str], filename: str = "feature_vector_full.names.json"
    ) -> str:
        """
        íŠ¹ì„± ì´ë¦„ ëª©ë¡ì„ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.

        Args:
            names: ì €ì¥í•  íŠ¹ì„± ì´ë¦„ ëª©ë¡
            filename: ì €ì¥í•  íŒŒì¼ëª…

        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        # ìºì‹œ ë””ë ‰í† ë¦¬ í™•ì¸
        try:
            cache_dir = self.config["paths"]["cache_dir"]
        except (KeyError, TypeError):
            cache_dir = "data/cache"

        cache_path = Path(cache_dir)
        cache_path.mkdir(parents=True, exist_ok=True)

        # íŒŒì¼ ê²½ë¡œ ìƒì„±
        file_path = cache_path / filename

        # ì´ë¦„ ëª©ë¡ ì €ì¥
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(names, f, indent=2, ensure_ascii=False)

        self.logger.info(f"íŠ¹ì„± ì´ë¦„ ì €ì¥ ì™„ë£Œ: {file_path} ({len(names)}ê°œ ì´ë¦„)")

        return str(file_path)

    def save_segment_specialized_vectors(
        self,
        segment_centrality_vector: np.ndarray = None,
        segment_consecutive_vector: np.ndarray = None,
        segment_entropy_vector: np.ndarray = None,
    ) -> None:
        """
        ì„¸ê·¸ë¨¼íŠ¸ íŠ¹ìˆ˜ ë²¡í„°ë“¤ì„ ê°œë³„ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.

        Args:
            segment_centrality_vector: ì„¸ê·¸ë¨¼íŠ¸ ì¤‘ì‹¬ì„± ë²¡í„°
            segment_consecutive_vector: ì„¸ê·¸ë¨¼íŠ¸ ì—°ì† íŒ¨í„´ ë²¡í„°
            segment_entropy_vector: ì„¸ê·¸ë¨¼íŠ¸ ì—”íŠ¸ë¡œí”¼ ë²¡í„°
        """
        try:
            cache_dir = self.config["paths"]["cache_dir"]
        except (KeyError, TypeError):
            cache_dir = "data/cache"

        cache_path = Path(cache_dir)
        cache_path.mkdir(parents=True, exist_ok=True)

        # ì„¸ê·¸ë¨¼íŠ¸ ì¤‘ì‹¬ì„± ë²¡í„° ì €ì¥
        if segment_centrality_vector is not None:
            centrality_path = cache_path / "segment_centrality_vector.npy"
            np.save(centrality_path, segment_centrality_vector)
            self.logger.info(f"íŠ¹ì„± ë²¡í„° ì €ì¥ ì™„ë£Œ: {centrality_path}")

        # ì„¸ê·¸ë¨¼íŠ¸ ì—°ì† íŒ¨í„´ ë²¡í„° ì €ì¥
        if segment_consecutive_vector is not None:
            consecutive_path = cache_path / "segment_consecutive_vector.npy"
            np.save(consecutive_path, segment_consecutive_vector)
            self.logger.info(f"íŠ¹ì„± ë²¡í„° ì €ì¥ ì™„ë£Œ: {consecutive_path}")

        # ì„¸ê·¸ë¨¼íŠ¸ ì—”íŠ¸ë¡œí”¼ ë²¡í„° ì €ì¥
        if segment_entropy_vector is not None:
            entropy_path = cache_path / "segment_entropy_vector.npy"
            np.save(entropy_path, segment_entropy_vector)
            self.logger.info(f"íŠ¹ì„± ë²¡í„° ì €ì¥ ì™„ë£Œ: {entropy_path}")

    def save_segment_history_to_numpy(
        self,
        segment_10_history: Dict[str, List[int]],
        segment_5_history: Dict[str, List[int]],
    ) -> Tuple[str, str]:
        """
        ì„¸ê·¸ë¨¼íŠ¸ íˆìŠ¤í† ë¦¬ë¥¼ NumPy íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.

        Args:
            segment_10_history: 10êµ¬ê°„ íˆìŠ¤í† ë¦¬ ë°ì´í„°
            segment_5_history: 5êµ¬ê°„ íˆìŠ¤í† ë¦¬ ë°ì´í„°

        Returns:
            10êµ¬ê°„ ì €ì¥ ê²½ë¡œ, 5êµ¬ê°„ ì €ì¥ ê²½ë¡œ
        """
        # ìºì‹œ ë””ë ‰í† ë¦¬ í™•ì¸
        try:
            cache_dir = self.config["paths"]["cache_dir"]
        except (KeyError, TypeError):
            cache_dir = "data/cache"

        cache_path = Path(cache_dir)
        cache_path.mkdir(parents=True, exist_ok=True)

        # ì„¸ê·¸ë¨¼íŠ¸ ë²¡í„° ë³€í™˜
        segment_10_matrix = []
        segment_5_matrix = []

        # 10êµ¬ê°„ íˆìŠ¤í† ë¦¬ ì²˜ë¦¬
        for segment, history in segment_10_history.items():
            # ìµœê·¼ 50íšŒì°¨ë§Œ ì‚¬ìš©
            recent_history = history[-50:] if len(history) > 50 else history
            # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
            max_val = max(recent_history) if recent_history else 1
            normalized = [h / max_val for h in recent_history]
            segment_10_matrix.append(normalized)

        # 5êµ¬ê°„ íˆìŠ¤í† ë¦¬ ì²˜ë¦¬
        for segment, history in segment_5_history.items():
            # ìµœê·¼ 50íšŒì°¨ë§Œ ì‚¬ìš©
            recent_history = history[-50:] if len(history) > 50 else history
            # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
            max_val = max(recent_history) if recent_history else 1
            normalized = [h / max_val for h in recent_history]
            segment_5_matrix.append(normalized)

        # NumPy ë°°ì—´ë¡œ ë³€í™˜
        segment_10_array = np.array(segment_10_matrix, dtype=np.float32)
        segment_5_array = np.array(segment_5_matrix, dtype=np.float32)

        # ì €ì¥
        segment_10_path = cache_path / "segment_10_history.npy"
        segment_5_path = cache_path / "segment_5_history.npy"

        np.save(segment_10_path, segment_10_array)
        np.save(segment_5_path, segment_5_array)

        self.logger.info(f"10êµ¬ê°„ íˆìŠ¤í† ë¦¬ ì €ì¥ ì™„ë£Œ: {segment_10_path}")
        self.logger.info(f"5êµ¬ê°„ íˆìŠ¤í† ë¦¬ ì €ì¥ ì™„ë£Œ: {segment_5_path}")

        return str(segment_10_path), str(segment_5_path)

    def _extract_identical_draw_feature(
        self, identical_draw_data: Dict[str, Any]
    ) -> np.ndarray:
        """
        ë™ì¼í•œ ë‹¹ì²¨ ë²ˆí˜¸ ì¡°í•© ë¶„ì„ ê²°ê³¼ë¥¼ íŠ¹ì„± ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

        Args:
            identical_draw_data: ë™ì¼ ì¡°í•© ë¶„ì„ ê²°ê³¼

        Returns:
            np.ndarray: ë³€í™˜ëœ íŠ¹ì„± ë²¡í„°
        """
        if not identical_draw_data or not isinstance(identical_draw_data, dict):
            # ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ë°˜í™˜
            return np.zeros(3, dtype=np.float32)

        # ì¤‘ë³µ ì¡°í•© ì—¬ë¶€ íŠ¹ì„± (0 ë˜ëŠ” 1)
        duplicate_flag = (
            1.0 if identical_draw_data.get("exact_match_in_history", False) else 0.0
        )

        # ê³¼ê±° ì¡°í•©ê³¼ì˜ ìµœëŒ€ ì¤‘ì²© ê°œìˆ˜ (0~6 â†’ 0~1ë¡œ ì •ê·œí™”)
        num_overlap = identical_draw_data.get("num_overlap_with_past_max", 0)
        max_overlap_with_past = float(num_overlap) / 6.0

        # ì¡°í•© ìµœê·¼ì„± ì ìˆ˜ (ì´ë¯¸ 0~1 ì‚¬ì´)
        combination_recency_score = identical_draw_data.get(
            "combination_recency_score", 0.0
        )

        # íŠ¹ì„± ì´ë¦„ ì—…ë°ì´íŠ¸
        if (
            self.feature_names is not None
            and "duplicate_flag" not in self.feature_names
        ):
            self.feature_names.append("duplicate_flag")
            self.feature_names.append("max_overlap_with_past")
            self.feature_names.append("combination_recency_score")

        # íŠ¹ì„± ë²¡í„° ìƒì„±
        feature_vector = np.array(
            [duplicate_flag, max_overlap_with_past, combination_recency_score],
            dtype=np.float32,
        )

        return feature_vector

    def vectorize_pattern_features(
        self, input_data: Union[Dict[str, Any], List[int]]
    ) -> np.ndarray:
        """
        íŒ¨í„´ íŠ¹ì„±ì„ ë²¡í„°í™”í•©ë‹ˆë‹¤.

        Args:
            input_data: íŒ¨í„´ íŠ¹ì„± ë°ì´í„° ë˜ëŠ” ë²ˆí˜¸ ì¡°í•©

        Returns:
            np.ndarray: ë²¡í„°í™”ëœ íŒ¨í„´ íŠ¹ì„±
        """
        with performance_monitor("vectorize_pattern_features"):
            # ì…ë ¥ ë°ì´í„°ê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° íŒ¨í„´ íŠ¹ì„± ì¶”ì¶œ
            if isinstance(input_data, list):
                # ì—¬ê¸°ì—ì„œ ë²¡í„°í™”í•˜ê¸° ìœ„í•œ íŠ¹ì„±ë“¤ì„ ê³„ì‚°
                from ..analysis.pattern_analyzer import PatternAnalyzer

                pattern_analyzer = PatternAnalyzer(self.config)
                try:
                    # ë¦¬ìŠ¤íŠ¸ë¥¼ ë²ˆí˜¸ ì¡°í•©ìœ¼ë¡œ ê°„ì£¼í•˜ê³  íŠ¹ì„± ì¶”ì¶œ
                    input_data = pattern_analyzer.extract_pattern_features(
                        input_data, []
                    )
                except Exception as e:
                    self.logger.error(f"íŒ¨í„´ íŠ¹ì„± ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                    return np.random.uniform(0.2, 0.8, 20).astype(
                        np.float32
                    )  # ì˜ë¯¸ìˆëŠ” ê¸°ë³¸ê°’ ë°˜í™˜

            # í•´ì‹œ ê¸°ë°˜ ìºì‹± (ì…ë ¥ ë°ì´í„°ê°€ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš°)
            if isinstance(input_data, dict):
                # ìºì‹œ í‚¤ ìƒì„±
                hash_key = self._compute_pattern_hash(input_data)
                # ìºì‹œì—ì„œ ê²°ê³¼ í™•ì¸
                if hash_key in self._pattern_cache and self.use_cache:
                    return self._pattern_cache[hash_key]

                # ë”•ì…”ë„ˆë¦¬ì—ì„œ ë²¡í„°í™”
                feature_vector = self._vectorize_from_dict(input_data)

                # ìºì‹œì— ì €ì¥
                if self.use_cache:
                    self._pattern_cache[hash_key] = feature_vector

                # ì €ë¶„ì‚° íŠ¹ì„± ì œê±° (ë²¡í„°í™” í›„ í•„í„°ë§)
                if self.remove_low_variance:
                    feature_vector = self._filter_low_variance_features(feature_vector)

                return feature_vector
            else:
                self.logger.warning(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ì…ë ¥ íƒ€ì…: {type(input_data)}")
                return np.random.uniform(0.2, 0.8, 20).astype(
                    np.float32
                )  # ì˜ë¯¸ìˆëŠ” ê¸°ë³¸ê°’ ë°˜í™˜

    def _filter_low_variance_features(
        self, feature_vector: np.ndarray, feature_names: Optional[List[str]] = None
    ) -> Tuple[np.ndarray, List[str]]:
        """
        ì €ë¶„ì‚° íŠ¹ì„±ì„ ì œê±°í•©ë‹ˆë‹¤. ë‹¨, í•„ìˆ˜ íŠ¹ì„±ì€ ë³´ì¡´í•©ë‹ˆë‹¤.

        Args:
            feature_vector: ì…ë ¥ íŠ¹ì„± ë²¡í„°
            feature_names: íŠ¹ì„± ì´ë¦„ ëª©ë¡ (ì„ íƒ ì‚¬í•­)

        Returns:
            Tuple[np.ndarray, List[str]]: í•„í„°ë§ëœ íŠ¹ì„± ë²¡í„°ì™€ í•„í„°ë§ëœ íŠ¹ì„± ì´ë¦„ ëª©ë¡
        """
        try:
            # íŠ¹ì„± ì´ë¦„ì´ ì œê³µë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ ì´ë¦„ ìƒì„±
            if feature_names is None or len(feature_names) == 0:
                feature_names = [
                    f"feature_{i}"
                    for i in range(
                        feature_vector.shape[0]
                        if feature_vector.ndim == 1
                        else feature_vector.shape[1]
                    )
                ]

            # ë²¡í„°ê°€ 1ì°¨ì›ì¸ ê²½ìš° 2Dë¡œ ë³€í™˜ (sklearnì˜ VarianceThresholdëŠ” 2D ì…ë ¥ í•„ìš”)
            is_1d = feature_vector.ndim == 1
            if is_1d:
                X = feature_vector.reshape(1, -1)
            else:
                X = feature_vector

            # íŠ¹ì„± ì´ë¦„ ê¸¸ì´ í™•ì¸ ë° ì¡°ì •
            if len(feature_names) != X.shape[1]:
                self.logger.warning(
                    f"íŠ¹ì„± ì´ë¦„ ê°œìˆ˜({len(feature_names)})ì™€ ë²¡í„° í¬ê¸°({X.shape[1]})ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                )
                # íŠ¹ì„± ì´ë¦„ì´ ë” ì§§ìœ¼ë©´ í™•ì¥
                if len(feature_names) < X.shape[1]:
                    feature_names.extend(
                        [
                            f"feature_{i+len(feature_names)}"
                            for i in range(X.shape[1] - len(feature_names))
                        ]
                    )
                # íŠ¹ì„± ì´ë¦„ì´ ë” ê¸¸ë©´ ìë¥´ê¸°
                elif len(feature_names) > X.shape[1]:
                    feature_names = feature_names[: X.shape[1]]

            # ì„¤ì •ì—ì„œ ì €ë¶„ì‚° íŠ¹ì„± í•„í„°ë§ í™œì„±í™” ì—¬ë¶€ í™•ì¸
            remove_low_variance = self._get_config_value(
                "filtering.remove_low_variance_features", False
            )

            # ì €ë¶„ì‚° íŠ¹ì„± í•„í„°ë§ì´ ë¹„í™œì„±í™”ëœ ê²½ìš° ì›ë³¸ ë°˜í™˜
            if not remove_low_variance:
                return feature_vector, feature_names

            # ë¨¼ì € ìºì‹œ ë””ë ‰í† ë¦¬ì—ì„œ low_variance_features.json íŒŒì¼ í™•ì¸
            try:
                from pathlib import Path
                import json

                # ìºì‹œ ë””ë ‰í† ë¦¬ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
                cache_dir = Path(
                    self._get_config_value("paths.cache_dir", "data/cache")
                )

                # ì €ë¶„ì‚° íŠ¹ì„± íŒŒì¼ ê²½ë¡œ
                low_var_path = cache_dir / "low_variance_features.json"

                # íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ ë¡œë“œ
                if low_var_path.exists():
                    with open(low_var_path, "r", encoding="utf-8") as f:
                        low_var_info = json.load(f)

                    # ì œê±°í•  íŠ¹ì„± ì¸ë±ìŠ¤ ê°€ì ¸ì˜¤ê¸°
                    removed_indices = low_var_info.get("removed_feature_indices", [])

                    # ì œê±°í•  íŠ¹ì„± ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
                    removed_feature_names = low_var_info.get(
                        "removed_feature_names", []
                    )

                    # ì •ë³´ ë¡œê¹…
                    if removed_indices:
                        self.logger.info(
                            f"ì €ë¶„ì‚° íŠ¹ì„± íŒŒì¼ì—ì„œ {len(removed_indices)}ê°œ íŠ¹ì„± í•„í„°ë§ (threshold: {low_var_info.get('threshold', 0.005)})"
                        )

                    # ì €ì¥ëœ ì •ë³´ê°€ ìˆìœ¼ë©´ í•„í„°ë§ ìˆ˜í–‰
                    if removed_indices:
                        # ìœ íš¨í•œ ì¸ë±ìŠ¤ë§Œ ì„ íƒ (ë²¡í„° í¬ê¸°ë¥¼ ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡)
                        valid_indices = [
                            idx for idx in removed_indices if idx < X.shape[1]
                        ]

                        if valid_indices:
                            # ìœ ì§€í•  ì¸ë±ìŠ¤ ìƒì„± (ì œê±°í•  ì¸ë±ìŠ¤ì˜ ë³´ìˆ˜)
                            keep_indices = [
                                i for i in range(X.shape[1]) if i not in valid_indices
                            ]

                            # íŠ¹ì„± ì´ë¦„ í•„í„°ë§ (ì œê±°í•  ì´ë¦„ ê¸°ì¤€)
                            if removed_feature_names:
                                filtered_names = [
                                    name
                                    for name in feature_names
                                    if name not in removed_feature_names
                                ]
                            else:
                                filtered_names = [
                                    feature_names[i]
                                    for i in keep_indices
                                    if i < len(feature_names)
                                ]

                            # ë²¡í„° í•„í„°ë§
                            filtered_vector = X[:, keep_indices]

                            # ì›ë˜ ì°¨ì›ìœ¼ë¡œ ë³µì›
                            if is_1d:
                                filtered_vector = filtered_vector.flatten()

                            self.logger.info(
                                f"ì €ë¶„ì‚° íŠ¹ì„± {len(valid_indices)}ê°œ ì œê±°ë¨ (feature_vector_validator.py ê¸°ì¤€)"
                            )
                            self.removed_low_variance_features = removed_feature_names
                            return filtered_vector, filtered_names

            except Exception as e:
                self.logger.warning(f"ì €ë¶„ì‚° íŠ¹ì„± íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ê³„ì† ì§„í–‰

            # ë¶„ì‚° ì„ê³„ê°’ ì„¤ì •
            variance_threshold = self.variance_threshold

            # í•„ìˆ˜ íŠ¹ì„± ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            essential_features = []
            try:
                if isinstance(self.config, dict) and "validation" in self.config:
                    essential_features = self.config["validation"].get(
                        "essential_features", []
                    )
                elif hasattr(self.config, "get") and callable(
                    getattr(self.config, "get")
                ):
                    validation_config = self.config.get("validation")
                    if (
                        validation_config is not None
                        and hasattr(validation_config, "get")
                        and callable(getattr(validation_config, "get"))
                    ):
                        essential_features = validation_config.get(
                            "essential_features", []
                        )
            except Exception as e:
                self.logger.warning(
                    f"ì„¤ì •ì—ì„œ í•„ìˆ˜ íŠ¹ì„± ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜: {str(e)}"
                )

            # í•„ìˆ˜ íŠ¹ì„± ëª©ë¡ì´ ë¹„ì–´ìˆìœ¼ë©´ ê¸°ë³¸ í•„ìˆ˜ íŠ¹ì„± ì‚¬ìš©
            if not essential_features:
                # í•„ìˆ˜ íŠ¹ì„±ì„ ì§ì ‘ ì •ì˜ (ëª¨ë“ˆì´ ì—†ìœ¼ë¯€ë¡œ)
                essential_features = [
                    "gap_stddev",
                    "pair_centrality",
                    "hot_cold_mix_score",
                    "segment_entropy",
                    "roi_group_score",
                    "duplicate_flag",
                    "max_overlap_with_past",
                    "combination_recency_score",
                    "position_entropy_1",
                    "position_entropy_2",
                    "position_entropy_3",
                    "position_entropy_4",
                    "position_entropy_5",
                    "position_entropy_6",
                    "position_std_1",
                    "position_std_2",
                    "position_std_3",
                    "position_std_4",
                    "position_std_5",
                    "position_std_6",
                    "position_variance_avg",
                    "position_bias_score",
                ]

            # íŠ¹ì„±ë³„ ë¶„ì‚° ê³„ì‚°
            variances = np.var(X, axis=0)

            # ë³´ì¡´í•  íŠ¹ì„± ì¸ë±ìŠ¤ (ì„ê³„ê°’ë³´ë‹¤ í° ë¶„ì‚° ë˜ëŠ” í•„ìˆ˜ íŠ¹ì„±)
            keep_indices = []

            # í•„ìˆ˜ íŠ¹ì„± ì¸ë±ìŠ¤ì™€ ê³ ë¶„ì‚° íŠ¹ì„± ì¸ë±ìŠ¤ ì°¾ê¸°
            for i, name in enumerate(feature_names):
                if name in essential_features:
                    keep_indices.append(i)  # í•„ìˆ˜ íŠ¹ì„± ë³´ì¡´
                elif variances[i] > variance_threshold:
                    keep_indices.append(i)  # ê³ ë¶„ì‚° íŠ¹ì„± ë³´ì¡´

            keep_indices = sorted(list(set(keep_indices)))  # ì¤‘ë³µ ì œê±° ë° ì •ë ¬

            # ë³´ì¡´í•˜ì§€ ì•Šì„ íŠ¹ì„± ì¸ë±ìŠ¤ (ì €ë¶„ì‚°ì´ë©´ì„œ í•„ìˆ˜ê°€ ì•„ë‹Œ íŠ¹ì„±)
            remove_indices = [i for i in range(X.shape[1]) if i not in keep_indices]

            # ì œê±°ëœ íŠ¹ì„± ìˆ˜ ë¡œê¹…
            if remove_indices:
                self.logger.info(
                    f"ì €ë¶„ì‚° íŠ¹ì„± {len(remove_indices)}ê°œ ì œê±°ë¨ (ì„ê³„ê°’: {variance_threshold})"
                )

                # ì œê±°ëœ íŠ¹ì„± ì´ë¦„ ì €ì¥
                removed_features = [feature_names[i] for i in remove_indices]
                self.removed_low_variance_features = removed_features
                self.logger.info(f"ì œê±°ëœ ì €ë¶„ì‚° íŠ¹ì„±: {removed_features}")

            # ë³´ì¡´ëœ í•„ìˆ˜ íŠ¹ì„± ë¡œê¹…
            preserved_essential = [
                feature_names[i]
                for i in keep_indices
                if feature_names[i] in essential_features
            ]
            if preserved_essential:
                self.logger.info(f"ë³´ì¡´ëœ í•„ìˆ˜ íŠ¹ì„±: {len(preserved_essential)}ê°œ")

            # í•„í„°ë§ëœ ë²¡í„° ìƒì„±
            if keep_indices:
                filtered_vector = X[:, keep_indices]
                filtered_names = [feature_names[i] for i in keep_indices]
            else:
                # ëª¨ë“  íŠ¹ì„±ì´ ì œê±°ë˜ëŠ” ê²ƒì„ ë°©ì§€
                self.logger.warning(
                    "ëª¨ë“  íŠ¹ì„±ì´ ì €ë¶„ì‚°ìœ¼ë¡œ ì œê±°ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì›ë³¸ ë²¡í„°ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤."
                )
                filtered_vector = X
                filtered_names = feature_names

            # ì›ë˜ ì°¨ì›ìœ¼ë¡œ ë³µì›
            if is_1d:
                filtered_vector = filtered_vector.flatten()

            return filtered_vector, filtered_names

        except Exception as e:
            self.logger.error(f"ì €ë¶„ì‚° íŠ¹ì„± ì œê±° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì›ë³¸ ë²¡í„°ì™€ ì´ë¦„ ê·¸ëŒ€ë¡œ ë°˜í™˜
            return feature_vector, feature_names if feature_names else []

    def get_feature_names(self) -> List[str]:
        """
        í˜„ì¬ ë²¡í„°ì— ì‚¬ìš©ëœ íŠ¹ì„± ì´ë¦„ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

        Returns:
            íŠ¹ì„± ì´ë¦„ ëª©ë¡
        """
        return self.feature_names

    def vectorize_number(
        self, number: int, historical_data: Optional[List[LotteryNumber]] = None
    ) -> np.ndarray:
        """
        ë‹¨ì¼ ë²ˆí˜¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ML ëª¨ë¸ìš© íŠ¹ì„± ë²¡í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            number: ë²¡í„°í™”í•  ë²ˆí˜¸ (1~45)
            historical_data: ê³¼ê±° ë‹¹ì²¨ ë²ˆí˜¸ ë°ì´í„° (ì„ íƒì )

        Returns:
            ë²ˆí˜¸ íŠ¹ì„± ë²¡í„° (numpy ë°°ì—´)
        """
        # ìœ íš¨í•œ ë²ˆí˜¸ ë²”ìœ„ ê²€ì‚¬
        if not 1 <= number <= 45:
            self.logger.warning(
                f"ìœ íš¨í•˜ì§€ ì•Šì€ ë²ˆí˜¸: {number}, ë²”ìœ„ëŠ” 1~45ì—¬ì•¼ í•©ë‹ˆë‹¤."
            )
            number = max(1, min(number, 45))

        # ìºì‹œ í‚¤ ìƒì„±
        cache_key = f"num_{number}"

        # ìºì‹œ í™•ì¸
        if self.use_cache and hasattr(self.vector_cache, "get"):
            try:
                cached_vector = self.vector_cache.get(cache_key)
                if cached_vector is not None:
                    return cached_vector
            except Exception as e:
                self.logger.warning(f"ìºì‹œ ì ‘ê·¼ ì˜¤ë¥˜: {str(e)}")

        # 1. ê¸°ë³¸ íŠ¹ì„± ì´ˆê¸°í™”
        feature_vector = np.zeros(10, dtype=np.float32)

        # 2. ë²ˆí˜¸ë¥¼ 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”
        feature_vector[0] = number / 45.0

        # 3. ë²ˆí˜¸ì˜ ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´ (5ê°œ êµ¬ê°„)
        segment_idx = (number - 1) // 9  # 0-4 ë²”ìœ„
        feature_vector[1] = segment_idx / 4.0  # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”

        # 4. í™€ì§ íŠ¹ì„±
        feature_vector[2] = 1.0 if number % 2 == 0 else 0.0

        # 5. êµ¬ê°„ë³„ íŠ¹ì„± (1-10, 11-20, 21-30, 31-40, 41-45)
        if 1 <= number <= 10:
            feature_vector[3] = 1.0
        elif 11 <= number <= 20:
            feature_vector[4] = 1.0
        elif 21 <= number <= 30:
            feature_vector[5] = 1.0
        elif 31 <= number <= 40:
            feature_vector[6] = 1.0
        else:  # 41-45
            feature_vector[7] = 1.0

        # 6. ì†Œìˆ˜/í•©ì„±ìˆ˜ íŠ¹ì„±
        is_prime = (
            all(number % i != 0 for i in range(2, int(number**0.5) + 1))
            if number > 1
            else False
        )
        feature_vector[8] = 1.0 if is_prime else 0.0

        # 7. ì œê³±ìˆ˜ íŠ¹ì„±
        is_perfect_square = int(number**0.5) ** 2 == number
        feature_vector[9] = 1.0 if is_perfect_square else 0.0

        # 8. ê³¼ê±° ë°ì´í„° ê¸°ë°˜ ì¶”ê°€ íŠ¹ì„± (ì œê³µëœ ê²½ìš°)
        if historical_data and len(historical_data) > 0:
            # ê³¼ê±° ì¶œí˜„ ë¹ˆë„ ê³„ì‚°
            appearances = sum(1 for draw in historical_data if number in draw.numbers)
            frequency = appearances / len(historical_data)

            # ì¶œí˜„ ë¹ˆë„ íŠ¹ì„± ì¶”ê°€
            frequency_feature = np.array([frequency], dtype=np.float32)
            feature_vector = np.concatenate([feature_vector, frequency_feature])

        # 9. ì •ê·œí™” ì ìš©
        feature_vector = self.normalize_vector(feature_vector)

        # 10. ìºì‹œì— ì €ì¥
        if self.use_cache and hasattr(self.vector_cache, "set"):
            try:
                self.vector_cache.set(cache_key, feature_vector)
            except Exception as e:
                self.logger.warning(f"ìºì‹œ ì €ì¥ ì˜¤ë¥˜: {str(e)}")

        return feature_vector

    def check_feature_distribution(
        self, vectors: np.ndarray, feature_names: Optional[List[str]] = None
    ) -> Dict[str, Dict[str, float]]:
        """
        íŠ¹ì„± ë²¡í„°ì˜ ë¶„í¬ë¥¼ ë¶„ì„í•˜ì—¬ í†µê³„ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

        Args:
            vectors: ë¶„ì„í•  íŠ¹ì„± ë²¡í„° (ë°°ì—´ ë˜ëŠ” í–‰ë ¬)
            feature_names: íŠ¹ì„± ì´ë¦„ ëª©ë¡ (ì œê³µë˜ì§€ ì•Šìœ¼ë©´ ì¸ë±ìŠ¤ ì‚¬ìš©)

        Returns:
            íŠ¹ì„±ë³„ í†µê³„ ì •ë³´
        """
        if len(vectors.shape) == 1:
            # ë‹¨ì¼ ë²¡í„°ë¥¼ í–‰ë ¬ë¡œ ë³€í™˜
            vectors = vectors.reshape(1, -1)

        # íŠ¹ì„± ìˆ˜ í™•ì¸
        n_features = vectors.shape[1]

        # íŠ¹ì„± ì´ë¦„ ìƒì„±
        if feature_names is None:
            feature_names = [f"feature_{i}" for i in range(n_features)]

        # íŠ¹ì„± ì´ë¦„ ê¸¸ì´ ì¡°ì •
        if len(feature_names) < n_features:
            feature_names.extend(
                [f"feature_{i}" for i in range(len(feature_names), n_features)]
            )
        elif len(feature_names) > n_features:
            feature_names = feature_names[:n_features]

        # íŠ¹ì„±ë³„ í†µê³„ ê³„ì‚°
        stats = {}
        for i, name in enumerate(feature_names):
            col = vectors[:, i]
            unique_values = np.unique(col)

            # ê¸°ë³¸ í†µê³„ ê³„ì‚°
            stats[name] = {
                "mean": float(np.mean(col)),
                "std": float(np.std(col)),
                "min": float(np.min(col)),
                "max": float(np.max(col)),
                "unique_count": len(unique_values),
            }

            # ë¬¸ì œê°€ ìˆëŠ” íŠ¹ì„± í™•ì¸
            if stats[name]["std"] < 1e-6:  # í‘œì¤€í¸ì°¨ê°€ ê±°ì˜ 0ì¸ ê²½ìš°
                stats[name]["warning"] = "std_zero"
                self.logger.warning(
                    f"íŠ¹ì„± '{name}'ì˜ í‘œì¤€í¸ì°¨ê°€ ê±°ì˜ 0ì…ë‹ˆë‹¤: {stats[name]['std']}"
                )

            if len(unique_values) == 1:  # ê³ ìœ ê°’ì´ 1ê°œì¸ ê²½ìš°
                stats[name]["warning"] = "single_value"
                self.logger.warning(
                    f"íŠ¹ì„± '{name}'ì—ëŠ” í•˜ë‚˜ì˜ ê³ ìœ ê°’ë§Œ ìˆìŠµë‹ˆë‹¤: {unique_values[0]}"
                )

        return stats

    def vectorize_pattern_features_with_diagnostics(
        self, input_data: Union[Dict[str, Any], List[int]]
    ) -> Tuple[np.ndarray, Dict[str, Dict[str, float]]]:
        """
        íŒ¨í„´ íŠ¹ì„±ì„ ë²¡í„°ë¡œ ë³€í™˜í•˜ê³  íŠ¹ì„± ë¶„í¬ ì§„ë‹¨ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

        Args:
            input_data: íŒ¨í„´ íŠ¹ì„± ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” ë²ˆí˜¸ ëª©ë¡

        Returns:
            íŠ¹ì„± ë²¡í„°ì™€ íŠ¹ì„± ë¶„í¬ í†µê³„
        """
        # íŠ¹ì„± ë²¡í„° ìƒì„±
        vector = self.vectorize_pattern_features(input_data)

        # ê¸°ë³¸ íŠ¹ì„± ì´ë¦„ ìƒì„±
        feature_names = [
            "consecutive_length",
            "total_sum",
            "odd_count",
            "even_count",
            "gap_avg",
            "gap_std",
            "high_low_ratio",
            "repeat_pattern",
            "cluster_overlap",
            "frequent_pair",
            "roi_weight",
            "trend_score",
            "risk_score",
        ]

        # ë¶„í¬ ì§„ë‹¨ ìˆ˜í–‰
        stats = self.check_feature_distribution(vector, feature_names)

        return vector, stats

    def _vectorize_from_dict(self, feature_dict: Dict[str, Any]) -> np.ndarray:
        """
        ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ íŒ¨í„´ íŠ¹ì„±ì„ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

        Args:
            feature_dict: íŒ¨í„´ íŠ¹ì„± ë”•ì…”ë„ˆë¦¬

        Returns:
            np.ndarray: ë³€í™˜ëœ íŠ¹ì„± ë²¡í„°
        """
        # ê¸°ë³¸ ë²¡í„° í¬ê¸° ì„¤ì • (í™•ì¥ ê°€ëŠ¥)
        vector_size = 20
        feature_vector = np.zeros(vector_size, dtype=np.float32)

        # ì¸ë±ìŠ¤ ê³„ì‚°
        idx = 0

        # ê¸°ë³¸ íŠ¹ì„± ì²˜ë¦¬
        # 1. í™€ìˆ˜/ì§ìˆ˜ ë¹„ìœ¨
        if "odd_even_ratio" in feature_dict:
            feature_vector[idx] = float(feature_dict["odd_even_ratio"])
            idx += 1

        # 2. í•©ê³„ ê´€ë ¨ íŠ¹ì„±
        if "total_sum" in feature_dict:
            # ì •ê·œí™”ëœ í•©ê³„ (0-1 ë²”ìœ„)
            total_sum = float(feature_dict["total_sum"])
            # í•©ê³„ ë²”ìœ„ (ê¸°ëŒ€ ë²”ìœ„: 21-255)
            normalized_sum = max(min((total_sum - 21) / (255 - 21), 1.0), 0.0)
            feature_vector[idx] = normalized_sum
            idx += 1

        # 3. ì—°ì† ë²ˆí˜¸ ê´€ë ¨
        if "max_consecutive" in feature_dict:
            feature_vector[idx] = min(float(feature_dict["max_consecutive"]) / 5.0, 1.0)
            idx += 1

        # 4. í´ëŸ¬ìŠ¤í„° ê´€ë ¨
        if "cluster_score" in feature_dict:
            feature_vector[idx] = float(feature_dict["cluster_score"])
            idx += 1

        # 5. ìµœê·¼ ë“±ì¥ ì ìˆ˜
        if "recent_hit_score" in feature_dict:
            feature_vector[idx] = float(feature_dict["recent_hit_score"])
            idx += 1

        # 6. ë²”ìœ„ ë°€ë„
        if "range_density" in feature_dict:
            if isinstance(feature_dict["range_density"], dict):
                density_vals = list(feature_dict["range_density"].values())
                for val in density_vals[:5]:  # ìµœëŒ€ 5ê°œ ê°’ë§Œ ì‚¬ìš©
                    feature_vector[idx] = float(val)
                    idx += 1
            elif isinstance(feature_dict["range_density"], list):
                for val in feature_dict["range_density"][:5]:  # ìµœëŒ€ 5ê°œ ê°’ë§Œ ì‚¬ìš©
                    feature_vector[idx] = float(val)
                    idx += 1

        # 7. ë²ˆí˜¸ ê°„ ê±°ë¦¬
        if "pairwise_distance" in feature_dict:
            # í‰ê·  ê±°ë¦¬
            if (
                isinstance(feature_dict["pairwise_distance"], dict)
                and "avg" in feature_dict["pairwise_distance"]
            ):
                feature_vector[idx] = (
                    float(feature_dict["pairwise_distance"]["avg"]) / 22.0
                )  # ìµœëŒ€ ê±°ë¦¬ë¡œ ì •ê·œí™”
                idx += 1

        # 8. í•«/ì½œë“œ í˜¼í•© ì ìˆ˜
        if "hot_cold_mix_score" in feature_dict:
            feature_vector[idx] = float(feature_dict["hot_cold_mix_score"])
            idx += 1

        # 9. ìœ„í—˜ë„ ì ìˆ˜ (ìˆëŠ” ê²½ìš°)
        if "risk_score" in feature_dict:
            feature_vector[idx] = float(feature_dict["risk_score"])
            idx += 1

        # 10. íŠ¸ë Œë“œ ì ìˆ˜ (ìˆëŠ” ê²½ìš°)
        if "trend_score" in feature_dict:
            feature_vector[idx] = float(feature_dict["trend_score"])
            idx += 1

        # ë‚˜ë¨¸ì§€ íŠ¹ì„± ì²˜ë¦¬ - ì‚¬ì „ì— ì •ì˜ëœ ì´ë¦„ì´ ì•„ë‹Œ ë‹¤ë¥¸ íŠ¹ì„±ë“¤
        for key, value in feature_dict.items():
            if (
                key
                not in [
                    "odd_even_ratio",
                    "total_sum",
                    "max_consecutive",
                    "cluster_score",
                    "recent_hit_score",
                    "range_density",
                    "pairwise_distance",
                    "hot_cold_mix_score",
                    "risk_score",
                    "trend_score",
                ]
                and idx < vector_size
            ):
                # ìˆ«ì ê°’ë§Œ ì²˜ë¦¬
                if isinstance(value, (int, float)) and not isinstance(value, bool):
                    # ê°’ ì •ê·œí™” ê°€ì • (0-1 ë²”ìœ„)
                    feature_vector[idx] = min(max(float(value), 0.0), 1.0)
                    idx += 1

        # íŠ¹ì„± ì´ë¦„ ëª©ë¡ ì—…ë°ì´íŠ¸ (ì²« í˜¸ì¶œ ì‹œì—ë§Œ)
        if not self.feature_names:
            self.feature_names = ["feature_" + str(i) for i in range(vector_size)]

        return self.normalize_vector(feature_vector)

    def vectorize_enhanced_analysis(
        self, analysis_data: Dict[str, Any], pair_data: Optional[Dict[str, Any]] = None
    ) -> np.ndarray:
        """
        íŒ¨í„´ ë¶„ì„ ê²°ê³¼ë¥¼ í™•ì¥ëœ íŠ¹ì„± ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

        Args:
            analysis_data: íŒ¨í„´ ë¶„ì„ ê²°ê³¼ ë°ì´í„°
            pair_data: ë²ˆí˜¸ ìŒ ë¶„ì„ ê²°ê³¼ ë°ì´í„° (ì„ íƒ ì‚¬í•­)

        Returns:
            np.ndarray: í™•ì¥ëœ íŠ¹ì„± ë²¡í„°
        """
        # ì„±ëŠ¥ ì¶”ì  ì‹œì‘
        self.performance_tracker.start_tracking("vectorize_enhanced_analysis")
        self.logger.info("íŒ¨í„´ ë¶„ì„ ê²°ê³¼ í™•ì¥ ë²¡í„°í™” ì‹œì‘")

        # ê¸°ë³¸ ë²¡í„° íŠ¹ì„±
        vector_features = {}
        feature_names_by_group = {}

        # íŠ¹ì„± ì´ë¦„ ì´ˆê¸°í™”
        self.feature_names = []

        # ë°˜ë“œì‹œ í¬í•¨ë˜ì–´ì•¼ í•˜ëŠ” í•„ìˆ˜ ë¶„ì„ í•„ë“œ ëª©ë¡
        essential_fields = [
            "gap_stddev",
            "pair_centrality",
            "hot_cold_mix_score",
            "segment_entropy",
            "roi_group_score",
            "duplicate_flag",
        ]

        # ë¶„ì„ ê²°ê³¼ì—ì„œ ëˆ„ë½ëœ í•„ìˆ˜ í•„ë“œ í™•ì¸
        for field in essential_fields:
            if field not in analysis_data and field not in (pair_data or {}):
                self.logger.warning(f"ë¶„ì„ ê²°ê³¼ì—ì„œ '{field}' í•„ë“œê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤")

        try:
            # 1. ê¸°ë³¸ íŒ¨í„´ ë¶„ì„ ê²°ê³¼ ë²¡í„°í™”
            pattern_vector = self.vectorize_full_analysis(analysis_data)
            vector_features["pattern"] = pattern_vector

            # í•„ìˆ˜ í•„ë“œ ì§ì ‘ ì¶”ì¶œí•˜ì—¬ ë²¡í„°ì— ì¶”ê°€
            essential_vector = []
            essential_names = []

            # 1. gap_stddev - ê°­ í‘œì¤€í¸ì°¨
            if "gap_stddev" in analysis_data:
                gap_stddev = self.safe_float_conversion(analysis_data["gap_stddev"])
                # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™” (ì¼ë°˜ì ì¸ ë²”ìœ„: 0-2)
                normalized_gap_stddev = min(gap_stddev / 2.0, 1.0)
                essential_vector.append(normalized_gap_stddev)
                essential_names.append("gap_stddev")
            else:
                # gap_statisticsì—ì„œ ì¶”ì¶œ ì‹œë„
                if (
                    "gap_statistics" in analysis_data
                    and "std" in analysis_data["gap_statistics"]
                ):
                    gap_stddev = self.safe_float_conversion(
                        analysis_data["gap_statistics"]["std"]
                    )
                    normalized_gap_stddev = min(gap_stddev / 2.0, 1.0)
                    essential_vector.append(normalized_gap_stddev)
                    essential_names.append("gap_stddev")
                else:
                    essential_vector.append(0.5)  # ê¸°ë³¸ê°’
                    essential_names.append("gap_stddev")

            # 2. pair_centrality - ìŒ ì¤‘ì‹¬ì„±
            if "pair_centrality" in analysis_data:
                # ì´ë¯¸ êµ¬í˜„ëœ ë²¡í„°í™” ë©”ì„œë“œ ì‚¬ìš©
                pair_centrality_vector = self._vectorize_pair_centrality(
                    analysis_data["pair_centrality"]
                )
                # í‰ê· ê°’ ì¶”ì¶œí•˜ì—¬ ì‚¬ìš©
                pair_centrality_avg = np.mean(pair_centrality_vector)
                essential_vector.append(pair_centrality_avg)
                essential_names.append("pair_centrality")
            else:
                # segment_centralityë¡œ ëŒ€ì²´ ì‹œë„
                if "segment_centrality" in analysis_data:
                    segment_centrality_vector = self._extract_segment_centrality(
                        analysis_data["segment_centrality"]
                    )
                    segment_centrality_avg = np.mean(segment_centrality_vector)
                    essential_vector.append(segment_centrality_avg)
                    essential_names.append("pair_centrality")
                else:
                    essential_vector.append(0.5)  # ê¸°ë³¸ê°’
                    essential_names.append("pair_centrality")

            # 3. hot_cold_mix_score - í•«/ì½œë“œ ë¯¹ìŠ¤ ì ìˆ˜
            if "hot_cold_mix_score" in analysis_data:
                hot_cold_mix = self.safe_float_conversion(
                    analysis_data["hot_cold_mix_score"]
                )
                essential_vector.append(hot_cold_mix)
                essential_names.append("hot_cold_mix_score")
            else:
                essential_vector.append(0.5)  # ê¸°ë³¸ê°’
                essential_names.append("hot_cold_mix_score")

            # 4. segment_entropy - ì„¸ê·¸ë¨¼íŠ¸ ì—”íŠ¸ë¡œí”¼
            if "segment_entropy" in analysis_data:
                if (
                    isinstance(analysis_data["segment_entropy"], dict)
                    and "total_entropy" in analysis_data["segment_entropy"]
                ):
                    segment_entropy = self.safe_float_conversion(
                        analysis_data["segment_entropy"]["total_entropy"]
                    )
                    # ì¼ë°˜ì ì¸ ì—”íŠ¸ë¡œí”¼ ë²”ìœ„: 0-2.5
                    normalized_entropy = min(segment_entropy / 2.5, 1.0)
                    essential_vector.append(normalized_entropy)
                    essential_names.append("segment_entropy")
                else:
                    segment_entropy = self.safe_float_conversion(
                        analysis_data["segment_entropy"]
                    )
                    essential_vector.append(segment_entropy)
                    essential_names.append("segment_entropy")
            else:
                essential_vector.append(0.5)  # ê¸°ë³¸ê°’
                essential_names.append("segment_entropy")

            # 5. roi_group_score - ROI ê·¸ë£¹ ì ìˆ˜
            if "roi_group_score" in analysis_data:
                roi_group_score = self.safe_float_conversion(
                    analysis_data["roi_group_score"]
                )
                essential_vector.append(roi_group_score)
                essential_names.append("roi_group_score")
            else:
                # roi_pattern_groupsì—ì„œ ì¶”ì¶œ ì‹œë„
                if "roi_pattern_groups" in analysis_data and isinstance(
                    analysis_data["roi_pattern_groups"], dict
                ):
                    roi_groups = analysis_data["roi_pattern_groups"]
                    if "group_scores" in roi_groups and isinstance(
                        roi_groups["group_scores"], dict
                    ):
                        group_scores = roi_groups["group_scores"]
                        group_values = [
                            self.safe_float_conversion(v) for v in group_scores.values()
                        ]
                        avg_score = sum(group_values) / max(1, len(group_values))
                        essential_vector.append(avg_score)
                        essential_names.append("roi_group_score")
                    else:
                        essential_vector.append(0.5)  # ê¸°ë³¸ê°’
                        essential_names.append("roi_group_score")
                else:
                    essential_vector.append(0.5)  # ê¸°ë³¸ê°’
                    essential_names.append("roi_group_score")

            # 6. duplicate_flag - ì¤‘ë³µ í”Œë˜ê·¸
            if "duplicate_flag" in analysis_data:
                duplicate_flag = (
                    1.0
                    if self.safe_float_conversion(analysis_data["duplicate_flag"]) > 0
                    else 0.0
                )
                essential_vector.append(duplicate_flag)
                essential_names.append("duplicate_flag")
            else:
                # identical_drawsì—ì„œ ì¶”ì¶œ ì‹œë„
                if "identical_draws" in analysis_data and isinstance(
                    analysis_data["identical_draws"], dict
                ):
                    duplicate_count = self.safe_float_conversion(
                        analysis_data["identical_draws"].get("duplicate_count", 0)
                    )
                    duplicate_flag = 1.0 if duplicate_count > 0 else 0.0
                    essential_vector.append(duplicate_flag)
                    essential_names.append("duplicate_flag")
                else:
                    essential_vector.append(0.0)  # ê¸°ë³¸ê°’: ì¤‘ë³µ ì—†ìŒ
                    essential_names.append("duplicate_flag")

            # í•„ìˆ˜ í•„ë“œ ë²¡í„° ì¶”ê°€
            vector_features["essential"] = np.array(essential_vector, dtype=np.float32)
            feature_names_by_group["essential"] = essential_names

            # 2. ìœ„ì¹˜ ê¸°ë°˜ ë¹ˆë„ ë¶„ì„ (position_frequency - shape: [6, 45])
            if "position_frequency" in analysis_data:
                try:
                    position_matrix = analysis_data["position_frequency"]
                    position_vector = self._vectorize_position_frequency(
                        position_matrix
                    )
                    vector_features["position_frequency"] = position_vector
                    feature_names_by_group["position_frequency"] = [
                        f"position_freq_{i}" for i in range(len(position_vector))
                    ]
                except Exception as e:
                    self.logger.error(f"ìœ„ì¹˜ ë¹ˆë„ ë²¡í„°í™” ì¤‘ ì˜¤ë¥˜: {e}")

            # 3. ìœ„ì¹˜ë³„ ì—”íŠ¸ë¡œí”¼ ë° í‘œì¤€í¸ì°¨ íŠ¹ì„± ì¶”ì¶œ
            try:
                (
                    position_entropy_vector,
                    position_std_vector,
                    position_entropy_names,
                    position_std_names,
                ) = self._extract_position_entropy_std_features(analysis_data)

                # ì—”íŠ¸ë¡œí”¼ ë²¡í„° ì¶”ê°€
                if len(position_entropy_vector) > 0:
                    vector_features["position_entropy"] = position_entropy_vector
                    feature_names_by_group["position_entropy"] = position_entropy_names

                # í‘œì¤€í¸ì°¨ ë²¡í„° ì¶”ê°€
                if len(position_std_vector) > 0:
                    vector_features["position_std"] = position_std_vector
                    feature_names_by_group["position_std"] = position_std_names
            except Exception as e:
                self.logger.error(f"ìœ„ì¹˜ë³„ ì—”íŠ¸ë¡œí”¼/í‘œì¤€í¸ì°¨ íŠ¹ì„± ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")

            # 3. ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì„¸ ë¶„ì„ (segment_trend_history)
            if "segment_trend_history" in analysis_data:
                try:
                    segment_trend_matrix = analysis_data["segment_trend_history"]

                    # list ê°ì²´ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
                    if isinstance(segment_trend_matrix, list):
                        try:
                            segment_trend_matrix = np.array(
                                segment_trend_matrix, dtype=np.float32
                            )
                        except Exception as e:
                            self.logger.error(f"ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì„¸ í–‰ë ¬ ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {e}")
                            segment_trend_matrix = np.zeros((5, 10), dtype=np.float32)

                    segment_trend_vector = self._vectorize_segment_trend(
                        segment_trend_matrix
                    )
                    vector_features["segment_trend"] = segment_trend_vector
                    feature_names_by_group["segment_trend"] = [
                        f"segment_{i+1}_trend_{j}" for i in range(5) for j in range(3)
                    ]
                except Exception as e:
                    self.logger.error(f"ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì„¸ ë²¡í„°í™” ì¤‘ ì˜¤ë¥˜: {e}")
                    # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ ë²¡í„° ìƒì„±
                    vector_features["segment_trend"] = np.zeros(15, dtype=np.float32)
                    feature_names_by_group["segment_trend"] = [
                        f"segment_{i+1}_trend_{j}" for i in range(5) for j in range(3)
                    ]

            # 5. ê°­ í¸ì°¨ ì ìˆ˜ ë¶„ì„ (gap_deviation_score)
            if "gap_deviation_score" in analysis_data:
                try:
                    gap_deviation_vector = self._vectorize_gap_deviation(
                        analysis_data["gap_deviation_score"]
                    )
                    vector_features["gap_deviation"] = gap_deviation_vector
                    feature_names_by_group["gap_deviation"] = [
                        "gap_dev_avg",
                        "gap_dev_std",
                        "gap_dev_high_ratio",
                        "gap_dev_low_ratio",
                        "gap_dev_range",
                    ]
                except Exception as e:
                    self.logger.error(f"ê°­ í¸ì°¨ ì ìˆ˜ ë²¡í„°í™” ì¤‘ ì˜¤ë¥˜: {e}")

            # 6. ì¡°í•© ë‹¤ì–‘ì„± ì ìˆ˜ ë¶„ì„ (combination_diversity)
            if "combination_diversity" in analysis_data:
                try:
                    diversity_vector = self._vectorize_combination_diversity(
                        analysis_data["combination_diversity"]
                    )
                    vector_features["combination_diversity"] = diversity_vector
                    feature_names_by_group["combination_diversity"] = [
                        "total_diversity",
                        "segment_diversity_avg",
                        "segment_diversity_std",
                        "segment_diversity_min",
                        "segment_diversity_max",
                    ]
                except Exception as e:
                    self.logger.error(f"ì¡°í•© ë‹¤ì–‘ì„± ì ìˆ˜ ë²¡í„°í™” ì¤‘ ì˜¤ë¥˜: {e}")

            # 7. ROI íŠ¸ë Œë“œ ë¶„ì„ (roi_trend_by_pattern)
            if "roi_trend_by_pattern" in analysis_data:
                try:
                    roi_trend_vector = self._vectorize_roi_trend_by_pattern(
                        analysis_data["roi_trend_by_pattern"]
                    )
                    vector_features["roi_trend"] = roi_trend_vector
                    feature_names_by_group["roi_trend"] = [
                        f"roi_trend_{i}" for i in range(len(roi_trend_vector))
                    ]
                except Exception as e:
                    self.logger.error(f"ROI íŠ¸ë Œë“œ ë²¡í„°í™” ì¤‘ ì˜¤ë¥˜: {e}")

            # ìŒ ë¶„ì„ ê²°ê³¼ ì¶”ê°€ (ì„ íƒì )
            if pair_data:
                # ìŒ ì¤‘ì‹¬ì„± ë²¡í„°í™” (pair_centrality)
                if "pair_centrality" in pair_data:
                    try:
                        pair_centrality_vector = self._vectorize_pair_centrality(
                            pair_data["pair_centrality"]
                        )
                        vector_features["pair_centrality"] = pair_centrality_vector
                        feature_names_by_group["pair_centrality"] = [
                            f"pair_centrality_{i}"
                            for i in range(len(pair_centrality_vector))
                        ]
                    except Exception as e:
                        self.logger.error(f"ìŒ ì¤‘ì‹¬ì„± ë²¡í„°í™” ì¤‘ ì˜¤ë¥˜: {e}")

                # ìŒ ROI ì ìˆ˜ ë²¡í„°í™” (pair_roi_score)
                if "pair_roi_score" in pair_data:
                    try:
                        pair_roi_vector = self._vectorize_pair_roi_scores(
                            pair_data["pair_roi_score"]
                        )
                        vector_features["pair_roi"] = pair_roi_vector
                        feature_names_by_group["pair_roi"] = [
                            f"pair_roi_{i}" for i in range(len(pair_roi_vector))
                        ]
                    except Exception as e:
                        self.logger.error(f"ìŒ ROI ì ìˆ˜ ë²¡í„°í™” ì¤‘ ì˜¤ë¥˜: {e}")

                # ìì£¼ ë“±ì¥í•˜ëŠ” íŠ¸ë¦¬í”Œ ë²¡í„°í™” (frequent_triples)
                if "frequent_triples" in pair_data:
                    try:
                        triples_vector = self._vectorize_frequent_triples(
                            pair_data["frequent_triples"]
                        )
                        vector_features["frequent_triples"] = triples_vector
                        feature_names_by_group["frequent_triples"] = [
                            f"triple_{i}" for i in range(len(triples_vector))
                        ]
                    except Exception as e:
                        self.logger.error(f"ìì£¼ ë“±ì¥í•˜ëŠ” íŠ¸ë¦¬í”Œ ë²¡í„°í™” ì¤‘ ì˜¤ë¥˜: {e}")

            # 8. ì¤‘ë³µ ë‹¹ì²¨ ë²ˆí˜¸ ë²¡í„° (2ê°œ ê°’) - ìƒˆë¡œ ì¶”ê°€
            if "identical_draw_check" in analysis_data:
                duplicate_vector = self._extract_identical_draw_feature(
                    analysis_data["identical_draw_check"]
                )
                vector_features["duplicate_combination"] = duplicate_vector
                feature_names_by_group["duplicate_combination"] = [
                    "has_duplicates",
                    "total_duplicates_norm",
                ]

                # ì´ ë²¡í„°ëŠ” ë³„ë„ íŒŒì¼ë¡œë„ ì €ì¥
                self.save_vector_to_file(
                    duplicate_vector, "duplicate_combination_vector.npy"
                )

            # 9-10. ìœ„ì¹˜ë³„ ì—”íŠ¸ë¡œí”¼ì™€ í‘œì¤€í¸ì°¨ íŠ¹ì„± ì¶”ì¶œ
            (
                position_entropy_vector,
                position_std_vector,
                position_entropy_names,
                position_std_names,
            ) = self._extract_position_entropy_std_features(analysis_data)

            # ì—”íŠ¸ë¡œí”¼ ë²¡í„° ì¶”ê°€
            if len(position_entropy_vector) > 0:
                vector_features["position_entropy"] = position_entropy_vector
                feature_names_by_group["position_entropy"] = position_entropy_names
                self.logger.info(
                    f"ìœ„ì¹˜ë³„ ì—”íŠ¸ë¡œí”¼ ë²¡í„° ì¶”ê°€: {len(position_entropy_names)}ê°œ íŠ¹ì„±"
                )

            # í‘œì¤€í¸ì°¨ ë²¡í„° ì¶”ê°€
            if len(position_std_vector) > 0:
                vector_features["position_std"] = position_std_vector
                feature_names_by_group["position_std"] = position_std_names
                self.logger.info(
                    f"ìœ„ì¹˜ë³„ í‘œì¤€í¸ì°¨ ë²¡í„° ì¶”ê°€: {len(position_std_names)}ê°œ íŠ¹ì„±"
                )

            # ëª¨ë“  ë²¡í„°ë¥¼ í•˜ë‚˜ë¡œ ì—°ê²°
            final_vector = self._combine_vectors(vector_features)

            # ê²°í•©ëœ íŠ¹ì„± ì´ë¦„ ì—…ë°ì´íŠ¸
            self.feature_names = []
            for group, names in feature_names_by_group.items():
                self.feature_names.extend(names)

            # í•„ìš”í•œ ê²½ìš° ì¶”ê°€ ì´ë¦„ ìƒì„±
            if len(self.feature_names) < len(final_vector):
                self.feature_names.extend(
                    [
                        f"feature_{i}"
                        for i in range(len(self.feature_names), len(final_vector))
                    ]
                )

            # ì €ë¶„ì‚° íŠ¹ì„± ì œê±° ì˜µì…˜ ì ìš©
            if self.remove_low_variance:
                final_vector, self.feature_names = self._filter_low_variance_features(
                    final_vector, self.feature_names
                )

            # ë²¡í„° ì •ê·œí™”
            normalized_vector = self.normalize_vector(final_vector)

            # ì„±ëŠ¥ ì¶”ì  ì¢…ë£Œ ë° ë¡œê¹…
            self.performance_tracker.stop_tracking("vectorize_enhanced_analysis")
            self.logger.info(f"í™•ì¥ ë²¡í„°í™” ì™„ë£Œ: íŠ¹ì„± ìˆ˜ {len(normalized_vector)}")
            return normalized_vector

        except Exception as e:
            # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ì„±ëŠ¥ ì¶”ì  ì¢…ë£Œ ë³´ì¥
            self.performance_tracker.stop_tracking("vectorize_enhanced_analysis")
            self.logger.error(f"í™•ì¥ íŠ¹ì„± ë²¡í„°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ë²¡í„° ë°˜í™˜
            return np.zeros(100, dtype=np.float32)

    def _detect_outliers(
        self, vectors: np.ndarray, z_threshold: float = 2.5
    ) -> np.ndarray:
        """
        Z-ì ìˆ˜ ê¸°ë°˜ ì´ìƒì¹˜ íƒì§€

        Args:
            vectors: ì…ë ¥ íŠ¹ì„± ë²¡í„° (1D ë˜ëŠ” 2D ë°°ì—´)
            z_threshold: Z-ì ìˆ˜ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 2.5)

        Returns:
            np.ndarray: ì´ìƒì¹˜ í”Œë˜ê·¸ ë²¡í„° (1: ì´ìƒì¹˜, 0: ì •ìƒ)
        """
        # ì…ë ¥ ë²¡í„°ê°€ 1Dì¸ì§€ 2Dì¸ì§€ í™•ì¸
        is_1d = len(vectors.shape) == 1
        if is_1d:
            # 1D ë°°ì—´ì˜ ê²½ìš° í¬ê¸° 1ì¸ 2D ë°°ì—´ë¡œ ë³€í™˜
            vectors = vectors.reshape(1, -1)

        # ì›ë³¸ shape ì €ì¥
        original_shape = vectors.shape[0]
        outlier_flags = np.zeros(original_shape, dtype=np.bool_)

        # ê° íŠ¹ì„±ì— ëŒ€í•´ Z-ì ìˆ˜ ê³„ì‚°
        for j in range(vectors.shape[1]):
            feature_values = vectors[:, j]
            mean = np.mean(feature_values)
            std = np.std(feature_values)

            if std > 0:  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
                z_scores = np.abs((feature_values - mean) / std)
                # íŠ¹ì„±ë³„ ì´ìƒì¹˜ íƒì§€
                feature_outliers = z_scores > z_threshold
                # ì „ì²´ ì´ìƒì¹˜ í”Œë˜ê·¸ ì—…ë°ì´íŠ¸ (í•˜ë‚˜ë¼ë„ ì´ìƒì¹˜ë©´ 1)
                outlier_flags = np.logical_or(outlier_flags, feature_outliers)

        # ì´ìƒì¹˜ í†µê³„ ë¡œê¹…
        outlier_count = np.sum(outlier_flags)
        outlier_ratio = (
            outlier_count / len(outlier_flags) if len(outlier_flags) > 0 else 0
        )
        self.logger.info(f"ì´ìƒì¹˜ íƒì§€ ì™„ë£Œ: {outlier_count}ê°œ ({outlier_ratio:.2%})")

        return outlier_flags

    def _save_outlier_mask(self, outlier_flags: np.ndarray) -> None:
        """
        ì´ìƒì¹˜ ë§ˆìŠ¤í¬ ë° ì¸ë±ìŠ¤ ì €ì¥

        Args:
            outlier_flags: ì´ìƒì¹˜ í”Œë˜ê·¸ ë°°ì—´ (1: ì´ìƒì¹˜, 0: ì •ìƒ)
        """
        try:
            # ìºì‹œ ë””ë ‰í† ë¦¬ í™•ì¸
            cache_dir = Path(self.cache_dir)
            cache_dir.mkdir(parents=True, exist_ok=True)

            # ì´ìƒì¹˜ ë§ˆìŠ¤í¬ ì €ì¥
            mask_file = cache_dir / "outlier_vector_mask.npy"
            np.save(mask_file, outlier_flags)

            # ì´ìƒì¹˜ ì¸ë±ìŠ¤ ì €ì¥
            outlier_indices = np.where(outlier_flags)[0].tolist()
            indices_file = cache_dir / "outlier_vector_indices.json"
            with open(indices_file, "w", encoding="utf-8") as f:
                json.dump(outlier_indices, f)

            self.logger.info(
                f"ì´ìƒì¹˜ ë§ˆìŠ¤í¬ ì €ì¥ ì™„ë£Œ: {mask_file}, ì´ìƒì¹˜ ê°œìˆ˜: {len(outlier_indices)}"
            )
        except Exception as e:
            self.logger.error(f"ì´ìƒì¹˜ ë§ˆìŠ¤í¬ ì €ì¥ ì‹¤íŒ¨: {e}")

    def _update_feature_names_with_enhanced_features(self) -> None:
        """
        í™•ì¥ëœ íŠ¹ì„± ì´ë¦„ ëª©ë¡ ì—…ë°ì´íŠ¸
        """
        # í˜„ì¬ íŠ¹ì„± ëª©ë¡ì´ ë¹„ì–´ìˆê±°ë‚˜ í™•ì¥ì´ í•„ìš”í•œ ê²½ìš°
        if not self.feature_names or len(self.feature_names) < 20:
            # í™•ì¥ëœ íŠ¹ì„± ëª©ë¡ ì •ì˜
            enhanced_features = [
                # ê¸°ë³¸ íŠ¹ì„±
                "frequency",
                "recency",
                # ì„¸ê·¸ë¨¼íŠ¸ íŠ¹ì„±
                "segment_10_freq",
                "segment_5_freq",
                # ê°„ê²© íŠ¹ì„±
                "gap_avg",
                "gap_std",
                "gap_min",
                "gap_max",
                # íŒ¨í„´ ì¬ì¶œí˜„ íŠ¹ì„±
                "pattern_reappearance",
                # ìµœê·¼ ì¬ì¶œí˜„ íŠ¹ì„±
                "recent_gaps",
                # ì¤‘ë³µ ë‹¹ì²¨ íŠ¹ì„±
                "identical_draw",
                # ìŒ ì¤‘ì‹¬ì„± íŠ¹ì„±
                "pair_centrality",
                "pair_centrality_avg",
                # ìŒ ROI íŠ¹ì„±
                "pair_roi_score",
                "roi_score_avg",
                # ë¹ˆë²ˆí•œ 3ì¤‘ ì¡°í•© íŠ¹ì„±
                "frequent_triples",
                # ê·¸ë˜í”„ í†µê³„ íŠ¹ì„±
                "graph_avg_weight",
                "graph_max_weight",
                # ìœ„ì¹˜ë³„ íŠ¹ì„±
                "position_freq_1",
                "position_freq_2",
                "position_freq_3",
                "position_freq_4",
                "position_freq_5",
                "position_freq_6",
                # ì„¸ê·¸ë¨¼íŠ¸ íŠ¸ë Œë“œ íŠ¹ì„±
                "segment_trend",
                "segment_trend_score",
                # ê°„ê²© í¸ì°¨ íŠ¹ì„±
                "gap_stddev",
                "gap_deviation",
                # ì¡°í•© ë‹¤ì–‘ì„± íŠ¹ì„±
                "combination_diversity",
                # ROI ì¶”ì„¸ íŠ¹ì„±
                "roi_trend_by_pattern",
                # í•«/ì½œë“œ íŠ¹ì„±
                "hot_ratio",
                "cold_ratio",
                "hot_cold_mix_score",
            ]

            # ì¤‘ë³µ ì—†ì´ íŠ¹ì„± ì´ë¦„ ì—…ë°ì´íŠ¸
            self.feature_names = list(dict.fromkeys(enhanced_features))

        # ìµœì¢… íŠ¹ì„± ê°œìˆ˜ í™•ì¸
        self.logger.info(f"ì—…ë°ì´íŠ¸ëœ íŠ¹ì„± ì´ë¦„ ëª©ë¡: {len(self.feature_names)}ê°œ")

    def _vectorize_position_frequency(self, position_matrix: np.ndarray) -> np.ndarray:
        """
        ìœ„ì¹˜ë³„ ë¹ˆë„ í–‰ë ¬ì„ íŠ¹ì„± ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

        Args:
            position_matrix: ìœ„ì¹˜ë³„ ë¹ˆë„ í–‰ë ¬ (shape: [6, 45])

        Returns:
            np.ndarray: ë³€í™˜ëœ íŠ¹ì„± ë²¡í„°
        """
        if position_matrix is None or position_matrix.size == 0:
            self.logger.warning("ìœ„ì¹˜ë³„ ë¹ˆë„ í–‰ë ¬ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤")
            return np.zeros(12)  # ê¸°ë³¸ 0 ë²¡í„° ë°˜í™˜ (ìœ„ì¹˜ ì—”íŠ¸ë¡œí”¼ 6ê°œ + í‘œì¤€í¸ì°¨ 6ê°œ)

        # í–‰ë ¬ ì •ê·œí™” (ê° ìœ„ì¹˜ë³„ë¡œ í•©ì´ 1ì´ ë˜ë„ë¡)
        normalized_matrix = position_matrix.copy()
        row_sums = normalized_matrix.sum(axis=1, keepdims=True)
        row_sums[row_sums == 0] = 1  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
        normalized_matrix = normalized_matrix / row_sums

        # ê° ìœ„ì¹˜ë³„ ì—”íŠ¸ë¡œí”¼ ê³„ì‚°
        position_entropy = np.zeros(6)
        position_std = np.zeros(6)
        position_top_freq_ratio = np.zeros(6)

        for i in range(6):
            # ì—”íŠ¸ë¡œí”¼ ê³„ì‚° (0 ê°’ì€ ê±´ë„ˆë›°ê¸°)
            probs = normalized_matrix[i]
            non_zero_probs = probs[probs > 0]
            entropy = -np.sum(non_zero_probs * np.log2(non_zero_probs))
            # ìµœëŒ€ ê°€ëŠ¥ ì—”íŠ¸ë¡œí”¼ë¡œ ì •ê·œí™” (log2(45) = 5.49...)
            max_entropy = np.log2(45)
            position_entropy[i] = entropy / max_entropy

            # í‘œì¤€í¸ì°¨ ê³„ì‚° (ë³€ë™ì„± ì§€í‘œ)
            position_std[i] = (
                np.std(normalized_matrix[i]) * 10
            )  # ì •ê·œí™”ë¥¼ ìœ„í•´ ìŠ¤ì¼€ì¼ ì¡°ì •

            # ìµœëŒ€ ë¹ˆë„ ë¹„ìœ¨ ê³„ì‚° (í•œ ë²ˆí˜¸ì— ì§‘ì¤‘ë„)
            if np.max(normalized_matrix[i]) > 0:
                position_top_freq_ratio[i] = np.max(normalized_matrix[i]) / (1.0 / 45)
            else:
                position_top_freq_ratio[i] = 1.0

        # ìœ„ì¹˜ë³„ ë¶„ì‚° í‰ê·  ë° í¸í–¥ ì ìˆ˜ ê³„ì‚°
        position_variance_avg = np.mean(position_std)

        # ìœ„ì¹˜ ê°„ ê· ì¼ì„± ê³„ì‚° (ìœ„ì¹˜ë³„ í‘œì¤€í¸ì°¨ì˜ í‘œì¤€í¸ì°¨)
        position_bias_score = np.std(position_std)

        # íŠ¹ì„± ì´ë¦„ ì—…ë°ì´íŠ¸
        if self.feature_names is not None:
            # ì´ë¯¸ ì¶”ê°€ëœ íŠ¹ì„±ì¸ì§€ í™•ì¸í•˜ì—¬ ì¤‘ë³µ ë°©ì§€
            if "position_entropy_1" not in self.feature_names:
                for i in range(6):
                    self.feature_names.append(f"position_entropy_{i+1}")
                    self.feature_names.append(f"position_std_{i+1}")
                    self.feature_names.append(f"position_top_freq_{i+1}")
                self.feature_names.append("position_variance_avg")
                self.feature_names.append("position_bias_score")

        # ëª¨ë“  íŠ¹ì„± ë²¡í„° ê²°í•©
        result_vector = np.concatenate(
            [
                position_entropy,  # 6
                position_std,  # 6
                position_top_freq_ratio,  # 6
                np.array([position_variance_avg, position_bias_score]),  # 2
            ]
        )

        # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
        result_vector = np.clip(result_vector, 0, 1)

        return result_vector

    def _vectorize_segment_trend(self, segment_trend_matrix: np.ndarray) -> np.ndarray:
        """
        ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì„¸ í–‰ë ¬ì„ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

        Args:
            segment_trend_matrix: ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì„¸ í–‰ë ¬ (í˜•íƒœ: (n_segments, n_turns))

        Returns:
            np.ndarray: ë²¡í„°í™”ëœ ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì„¸ íŠ¹ì„±
        """
        # ë¹ˆ ë°ì´í„° ì²˜ë¦¬
        if segment_trend_matrix is None:
            return np.zeros(15, dtype=np.float32)

        # list ê°ì²´ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
        if isinstance(segment_trend_matrix, list):
            try:
                segment_trend_matrix = np.array(segment_trend_matrix, dtype=np.float32)
            except Exception as e:
                self.logger.error(f"ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì„¸ í–‰ë ¬ ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {e}")
                return np.zeros(15, dtype=np.float32)

        # í¬ê¸°ê°€ 0ì¸ ë°°ì—´ ì²˜ë¦¬
        if (
            not isinstance(segment_trend_matrix, np.ndarray)
            or segment_trend_matrix.size == 0
        ):
            return np.zeros(15, dtype=np.float32)

        # ë°°ì—´ì´ 1ì°¨ì›ì´ë©´ 2ì°¨ì›ìœ¼ë¡œ ë³€í™˜ (ë‹¨ì¼ ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬)
        if len(segment_trend_matrix.shape) == 1:
            segment_trend_matrix = segment_trend_matrix.reshape(1, -1)

        # ê²°ê³¼ ì €ì¥í•  íŠ¹ì„± ëª©ë¡
        trend_features = []

        # 1. ê° ì„¸ê·¸ë¨¼íŠ¸ì˜ ìµœê·¼ ì¶”ì„¸ (5ê°œ íŠ¹ì„±)
        # ë§ˆì§€ë§‰ 10íšŒ í‰ê· ì„ ì‚¬ìš©
        segment_count = segment_trend_matrix.shape[0]
        turn_count = segment_trend_matrix.shape[1]

        # ë§ˆì§€ë§‰ íšŒì°¨ë¶€í„° ìµœëŒ€ 10íšŒì°¨ ë°ì´í„° ì‚¬ìš©
        recent_window = min(10, turn_count)
        recent_trends = []

        for i in range(segment_count):
            segment_data = segment_trend_matrix[i, -recent_window:]
            avg_trend = np.mean(segment_data)
            recent_trends.append(float(avg_trend))

        # ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜ê°€ 5ê°œë³´ë‹¤ ì ì„ ê²½ìš° 0ìœ¼ë¡œ ì±„ì›€
        while len(recent_trends) < 5:
            recent_trends.append(0.0)

        # 5ê°œ ì„¸ê·¸ë¨¼íŠ¸ë§Œ ì‚¬ìš©
        recent_trends = recent_trends[:5]
        trend_features.extend(recent_trends)

        # 2. ê° ì„¸ê·¸ë¨¼íŠ¸ì˜ ë³€ë™ì„± (5ê°œ íŠ¹ì„±)
        segment_volatility = []
        for i in range(segment_count):
            segment_data = segment_trend_matrix[i, -recent_window:]
            # ë³€ë™ ê³„ìˆ˜ (CV) = í‘œì¤€í¸ì°¨ / í‰ê· 
            mean_val = np.mean(segment_data)
            std_val = np.std(segment_data)
            cv = std_val / mean_val if mean_val > 0 else 0.0
            segment_volatility.append(float(cv))

        # ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜ê°€ 5ê°œë³´ë‹¤ ì ì„ ê²½ìš° 0ìœ¼ë¡œ ì±„ì›€
        while len(segment_volatility) < 5:
            segment_volatility.append(0.0)

        # 5ê°œ ì„¸ê·¸ë¨¼íŠ¸ë§Œ ì‚¬ìš©
        segment_volatility = segment_volatility[:5]
        trend_features.extend(segment_volatility)

        # 3. ì„¸ê·¸ë¨¼íŠ¸ ê°„ ìƒê´€ê´€ê³„ (5ê°œ íŠ¹ì„±)
        segment_correlations = []
        if segment_count >= 2:
            # ì¸ì ‘ ì„¸ê·¸ë¨¼íŠ¸ ê°„ ìƒê´€ê´€ê³„ ê³„ì‚°
            for i in range(segment_count - 1):
                seg1 = segment_trend_matrix[i, -recent_window:]
                seg2 = segment_trend_matrix[i + 1, -recent_window:]
                corr = np.corrcoef(seg1, seg2)[0, 1]
                segment_correlations.append(float(corr))

            # ì²« ë²ˆì§¸ì™€ ë§ˆì§€ë§‰ ì„¸ê·¸ë¨¼íŠ¸ ìƒê´€ê´€ê³„ ì¶”ê°€ (ì›í˜• êµ¬ì¡° ê³ ë ¤)
            if segment_count >= 3:
                seg1 = segment_trend_matrix[0, -recent_window:]
                seg2 = segment_trend_matrix[-1, -recent_window:]
                corr = np.corrcoef(seg1, seg2)[0, 1]
                segment_correlations.append(float(corr))

        # NaN ê°’ 0ìœ¼ë¡œ ëŒ€ì²´
        segment_correlations = [0.0 if np.isnan(x) else x for x in segment_correlations]

        # ìƒê´€ê´€ê³„ ìˆ˜ê°€ 5ê°œë³´ë‹¤ ì ì„ ê²½ìš° 0ìœ¼ë¡œ ì±„ì›€
        while len(segment_correlations) < 5:
            segment_correlations.append(0.0)

        # 5ê°œ ìƒê´€ê´€ê³„ë§Œ ì‚¬ìš©
        segment_correlations = segment_correlations[:5]
        trend_features.extend(segment_correlations)

        # ì´ 15ê°œ íŠ¹ì„± ë°˜í™˜
        return np.array(trend_features, dtype=np.float32)

    def _vectorize_pair_centrality(self, pair_centrality: Dict[str, Any]) -> np.ndarray:
        """
        ìŒ ì¤‘ì‹¬ì„± ë°ì´í„°ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

        Args:
            pair_centrality: ìŒ ì¤‘ì‹¬ì„± ë°ì´í„°

        Returns:
            np.ndarray: ë²¡í„°í™”ëœ ìŒ ì¤‘ì‹¬ì„± íŠ¹ì„±
        """
        # ì´ˆê¸° ë²¡í„° í¬ê¸° ì„¤ì • (10ê°œ íŠ¹ì„±)
        vector_size = 10
        centrality_vector = np.zeros(vector_size, dtype=np.float32)

        # ìŒ ì¤‘ì‹¬ì„± ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ë¹ˆ ë²¡í„° ë°˜í™˜
        if not pair_centrality:
            return centrality_vector

        # ì¤‘ì‹¬ì„± ê°’ ì¶”ì¶œ
        centrality_values = []
        for key, value in pair_centrality.items():
            try:
                # ë¬¸ìì—´ í˜•íƒœì˜ í‚¤ì—ì„œ ë‘ ë²ˆí˜¸ ì¶”ì¶œ (ì˜ˆ: "1_34")
                if isinstance(value, (int, float)):
                    centrality_values.append(float(value))
            except (ValueError, TypeError):
                continue

        # ì¤‘ì‹¬ì„± ê°’ì´ ì—†ëŠ” ê²½ìš° ë¹ˆ ë²¡í„° ë°˜í™˜
        if not centrality_values:
            return centrality_vector

        # ì£¼ìš” í†µê³„ê°’ ê³„ì‚°
        centrality_array = np.array(centrality_values)
        centrality_vector[0] = np.mean(centrality_array)  # í‰ê· 
        centrality_vector[1] = np.std(centrality_array)  # í‘œì¤€ í¸ì°¨
        centrality_vector[2] = np.max(centrality_array)  # ìµœëŒ€ê°’
        centrality_vector[3] = np.min(centrality_array)  # ìµœì†Œê°’

        # ìƒìœ„ nê°œ ê°’ì˜ í‰ê· 
        n_values = min(5, len(centrality_array))
        if n_values > 0:
            top_values = np.sort(centrality_array)[-n_values:]
            centrality_vector[4] = np.mean(top_values)

        # ì¤‘ì‹¬ì„± ë¶„í¬ íŠ¹ì„±
        if len(centrality_array) > 0:
            # ì‚¬ë¶„ìœ„ìˆ˜ ê³„ì‚°
            q1 = np.percentile(centrality_array, 25)
            q2 = np.percentile(centrality_array, 50)  # ì¤‘ì•™ê°’
            q3 = np.percentile(centrality_array, 75)

            centrality_vector[5] = q2  # ì¤‘ì•™ê°’
            centrality_vector[6] = q3 - q1  # ì‚¬ë¶„ìœ„ ë²”ìœ„

            # ë¶„í¬ì˜ ë¹„ëŒ€ì¹­ë„
            if np.std(centrality_array) > 0:
                skewness = np.mean(
                    (
                        (centrality_array - np.mean(centrality_array))
                        / np.std(centrality_array)
                    )
                    ** 3
                )
                centrality_vector[7] = skewness

            # 0.5 ì´ìƒì˜ ì¤‘ì‹¬ì„± ê°’ ë¹„ìœ¨
            high_centrality_ratio = np.sum(centrality_array > 0.5) / len(
                centrality_array
            )
            centrality_vector[8] = high_centrality_ratio

            # ì¤‘ì‹¬ì„± ê°’ë“¤ì˜ ì—”íŠ¸ë¡œí”¼ (ê°’ ë²”ìœ„ ë‹¤ì–‘ì„±)
            bins = np.linspace(0, 1, 10)  # 0-1 ë²”ìœ„ë¥¼ 10ê°œ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ”
            hist, _ = np.histogram(centrality_array, bins=bins, density=True)
            hist = hist[hist > 0]  # 0ì¸ ê°’ ì œê±°
            if len(hist) > 0:
                entropy = -np.sum(hist * np.log(hist))
                centrality_vector[9] = entropy

        return centrality_vector

    def _vectorize_pair_roi_scores(self, pair_roi_score: Dict[str, Any]) -> np.ndarray:
        """
        ìŒ ROI ì ìˆ˜ ë°ì´í„°ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

        Args:
            pair_roi_score: ìŒ ROI ì ìˆ˜ ë°ì´í„°

        Returns:
            np.ndarray: ë²¡í„°í™”ëœ ìŒ ROI ì ìˆ˜ íŠ¹ì„±
        """
        # ì´ˆê¸° ë²¡í„° í¬ê¸° ì„¤ì • (10ê°œ íŠ¹ì„±)
        vector_size = 10
        roi_vector = np.zeros(vector_size, dtype=np.float32)

        # ROI ì ìˆ˜ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ë¹ˆ ë²¡í„° ë°˜í™˜
        if not pair_roi_score:
            return roi_vector

        # ROI ê°’ ì¶”ì¶œ
        roi_values = []
        for key, value in pair_roi_score.items():
            try:
                if isinstance(value, (int, float)):
                    roi_values.append(float(value))
            except (ValueError, TypeError):
                continue

        # ROI ê°’ì´ ì—†ëŠ” ê²½ìš° ë¹ˆ ë²¡í„° ë°˜í™˜
        if not roi_values:
            return roi_vector

        # ì£¼ìš” í†µê³„ê°’ ê³„ì‚°
        roi_array = np.array(roi_values)
        roi_vector[0] = np.mean(roi_array)  # í‰ê· 
        roi_vector[1] = np.std(roi_array)  # í‘œì¤€ í¸ì°¨
        roi_vector[2] = np.max(roi_array)  # ìµœëŒ€ê°’
        roi_vector[3] = np.min(roi_array)  # ìµœì†Œê°’

        # ìƒìœ„ nê°œ ê°’ì˜ í‰ê· 
        n_values = min(5, len(roi_array))
        if n_values > 0:
            top_values = np.sort(roi_array)[-n_values:]
            roi_vector[4] = np.mean(top_values)

        # ROI ê°’ ë¶„í¬ íŠ¹ì„±
        if len(roi_array) > 0:
            # ì–‘ìˆ˜ ROI ê°’ ë¹„ìœ¨
            positive_roi_ratio = np.sum(roi_array > 0) / len(roi_array)
            roi_vector[5] = positive_roi_ratio

            # ë†’ì€ ROI ê°’ (1.5 ì´ìƒ) ë¹„ìœ¨
            high_roi_ratio = np.sum(roi_array > 1.5) / len(roi_array)
            roi_vector[6] = high_roi_ratio

            # ROI ê°’ë“¤ì˜ í¸ì°¨ (ë³€ë™ì„± ì§€í‘œ)
            if np.mean(roi_array) > 0:
                roi_vector[7] = np.std(roi_array) / np.mean(roi_array)  # ë³€ë™ ê³„ìˆ˜

            # ì¤‘ì•™ê°’
            roi_vector[8] = np.median(roi_array)

            # ë¶„í¬ ë²”ìœ„ (ìµœëŒ€-ìµœì†Œ)
            roi_vector[9] = (
                roi_array.max() - roi_array.min() if len(roi_array) > 1 else 0
            )

        return roi_vector

    def _vectorize_frequent_triples(
        self, frequent_triples: Dict[str, Any]
    ) -> np.ndarray:
        """
        ë¹ˆë²ˆí•œ 3ì¤‘ ì¡°í•© ë°ì´í„°ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

        Args:
            frequent_triples: ë¹ˆë²ˆí•œ 3ì¤‘ ì¡°í•© ë°ì´í„°

        Returns:
            np.ndarray: ë²¡í„°í™”ëœ ë¹ˆë²ˆí•œ 3ì¤‘ ì¡°í•© íŠ¹ì„±
        """
        # ì´ˆê¸° ë²¡í„° í¬ê¸° ì„¤ì • (5ê°œ íŠ¹ì„±)
        vector_size = 5
        triples_vector = np.zeros(vector_size, dtype=np.float32)

        # 3ì¤‘ ì¡°í•© ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ë¹ˆ ë²¡í„° ë°˜í™˜
        if not frequent_triples or "triples" not in frequent_triples:
            return triples_vector

        # 3ì¤‘ ì¡°í•© ëª©ë¡ ì¶”ì¶œ
        triples_list = frequent_triples.get("triples", [])
        if not triples_list:
            return triples_vector

        # 3ì¤‘ ì¡°í•© ê°œìˆ˜
        triples_vector[0] = min(len(triples_list) / 50.0, 1.0)  # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”

        # 3ì¤‘ ì¡°í•©ì˜ í‰ê·  ì¶œí˜„ ë¹ˆë„
        if "average_frequency" in frequent_triples:
            triples_vector[1] = (
                frequent_triples["average_frequency"] / 10.0
            )  # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”

        # 3ì¤‘ ì¡°í•©ì˜ í‰ê·  ROI (ìˆëŠ” ê²½ìš°)
        if "average_roi" in frequent_triples:
            triples_vector[2] = min(
                frequent_triples["average_roi"] / 2.0, 1.0
            )  # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”

        # ìƒìœ„ 3ì¤‘ ì¡°í•©ì˜ ë¹ˆë„ ì°¨ì´ (ìˆëŠ” ê²½ìš°)
        if len(triples_list) >= 2 and all(
            isinstance(t, dict) and "frequency" in t for t in triples_list[:2]
        ):
            top_diff = abs(triples_list[0]["frequency"] - triples_list[1]["frequency"])
            triples_vector[3] = min(top_diff / 5.0, 1.0)  # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”

        # 3ì¤‘ ì¡°í•©ì— í¬í•¨ëœ ê³ ìœ  ë²ˆí˜¸ ìˆ˜ (ìˆëŠ” ê²½ìš°)
        unique_numbers = set()
        for triple in triples_list:
            if isinstance(triple, dict) and "numbers" in triple:
                unique_numbers.update(triple["numbers"])

        if unique_numbers:
            triples_vector[4] = min(
                len(unique_numbers) / 45.0, 1.0
            )  # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”

        return triples_vector

    def _visualize_features_with_tsne(self, feature_matrix: np.ndarray) -> str:
        """
        t-SNEë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì„± í–‰ë ¬ì„ ì‹œê°í™”í•©ë‹ˆë‹¤.

        Args:
            feature_matrix: ì‹œê°í™”í•  íŠ¹ì„± í–‰ë ¬

        Returns:
            str: ì €ì¥ëœ ì‹œê°í™” íŒŒì¼ ê²½ë¡œ
        """
        try:
            from sklearn.manifold import TSNE
            import matplotlib.pyplot as plt
            from matplotlib.colors import LinearSegmentedColormap

            # t-SNE ë§¤ê°œë³€ìˆ˜ ìˆ˜ì • - 3ì°¨ì› ì‹œê°í™”, í•™ìŠµë¥  ìë™, ì´ˆê¸°í™” ë¬´ì‘ìœ„, í¼í”Œë ‰ì‹œí‹° 30
            tsne = TSNE(
                n_components=3,
                learning_rate="auto",
                init="random",
                perplexity=30,
                random_state=42,
            )

            # ì°¨ì› ì¶•ì†Œ ìˆ˜í–‰
            self.logger.info(
                f"t-SNE ì°¨ì› ì¶•ì†Œ ìˆ˜í–‰ ì¤‘... (ì…ë ¥ í˜•íƒœ: {feature_matrix.shape})"
            )

            # ë°ì´í„°ê°€ ì¶©ë¶„í•œ ê²½ìš°ì—ë§Œ ìˆ˜í–‰
            if len(feature_matrix) < 10:
                self.logger.warning("ë°ì´í„°ê°€ ë„ˆë¬´ ì ì–´ t-SNE ì‹œê°í™”ë¥¼ ìƒëµí•©ë‹ˆë‹¤.")
                return ""

            tsne_result = tsne.fit_transform(feature_matrix)

            # 3D í”Œë¡¯ ìƒì„±
            fig = plt.figure(figsize=(10, 8))
            ax = fig.add_subplot(111, projection="3d")

            # ì»¬ëŸ¬ë§µ ì„¤ì • (blue -> red ê·¸ë¼ë°ì´ì…˜)
            cmap = LinearSegmentedColormap.from_list(
                "custom_cmap", ["blue", "cyan", "green", "yellow", "red"]
            )

            # í”Œë¡¯íŒ…
            scatter = ax.scatter(
                tsne_result[:, 0],
                tsne_result[:, 1],
                tsne_result[:, 2],
                c=np.arange(len(tsne_result)),  # ì—°ì†ì ì¸ ìƒ‰ìƒ í• ë‹¹
                cmap=cmap,
                alpha=0.8,
                s=40,
            )

            # ê·¸ë˜í”„ ì œëª© ë° ë ˆì´ë¸” ì„¤ì •
            ax.set_title("t-SNE ì‹œê°í™” (3D)", fontsize=14)
            ax.set_xlabel("t-SNE 1")
            ax.set_ylabel("t-SNE 2")
            ax.set_zlabel("t-SNE 3")

            # ì»¬ëŸ¬ë°” ì¶”ê°€
            plt.colorbar(scatter, ax=ax, label="ë°ì´í„° í¬ì¸íŠ¸ ì¸ë±ìŠ¤")

            # ê·¸ë˜í”„ ì €ì¥
            output_dir = Path(self.cache_dir)
            output_dir.mkdir(parents=True, exist_ok=True)
            output_path = output_dir / "tsne_visualization_3d.png"
            plt.savefig(output_path, dpi=150, bbox_inches="tight")
            plt.close()

            self.logger.info(f"t-SNE ì‹œê°í™” ì €ì¥ ì™„ë£Œ: {output_path}")
            return str(output_path)

        except Exception as e:
            self.logger.error(f"t-SNE ì‹œê°í™” ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return ""

    def _vectorize_gap_deviation(
        self, gap_deviation_score: Dict[str, Any]
    ) -> np.ndarray:
        """
        ê°„ê²© í¸ì°¨ ì ìˆ˜ ë°ì´í„°ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

        Args:
            gap_deviation_score: ê°„ê²© í¸ì°¨ ì ìˆ˜ ë°ì´í„°

        Returns:
            np.ndarray: ë²¡í„°í™”ëœ ê°„ê²© í¸ì°¨ ì ìˆ˜ íŠ¹ì„±
        """
        # ì´ˆê¸° ë²¡í„° í¬ê¸° ì„¤ì • (5ê°œ íŠ¹ì„±)
        vector_size = 5
        gap_dev_vector = np.zeros(vector_size, dtype=np.float32)

        # ê°„ê²© í¸ì°¨ ì ìˆ˜ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ë¹ˆ ë²¡í„° ë°˜í™˜
        if not gap_deviation_score:
            return gap_dev_vector

        # ê°„ê²© í¸ì°¨ ì ìˆ˜ ì¶”ì¶œ
        deviation_values = []
        for num_str, score in gap_deviation_score.items():
            try:
                num = int(num_str) if isinstance(num_str, str) else num_str
                if 1 <= num <= 45 and isinstance(score, (int, float)):
                    deviation_values.append((num, float(score)))
            except (ValueError, TypeError):
                continue

        # ê°„ê²© í¸ì°¨ ê°’ì´ ì—†ëŠ” ê²½ìš° ë¹ˆ ë²¡í„° ë°˜í™˜
        if not deviation_values:
            return gap_dev_vector

        # ë²ˆí˜¸ë³„ ê°„ê²© í¸ì°¨ ì ìˆ˜ë¥¼ ë°°ì—´ë¡œ ë³€í™˜
        nums, scores = zip(*deviation_values)
        deviation_array = np.array(scores)

        # ì£¼ìš” í†µê³„ê°’ ê³„ì‚°
        gap_dev_vector[0] = np.mean(deviation_array)  # í‰ê·  í¸ì°¨ ì ìˆ˜
        gap_dev_vector[1] = np.std(deviation_array)  # í¸ì°¨ ì ìˆ˜ì˜ í‘œì¤€í¸ì°¨

        # ë†’ì€ í¸ì°¨ ì ìˆ˜(0.7 ì´ìƒ)ë¥¼ ê°€ì§„ ë²ˆí˜¸ì˜ ë¹„ìœ¨
        high_dev_ratio = (
            np.sum(deviation_array > 0.7) / len(deviation_array)
            if len(deviation_array) > 0
            else 0
        )
        gap_dev_vector[2] = high_dev_ratio

        # ë‚®ì€ í¸ì°¨ ì ìˆ˜(0.3 ì´í•˜)ë¥¼ ê°€ì§„ ë²ˆí˜¸ì˜ ë¹„ìœ¨
        low_dev_ratio = (
            np.sum(deviation_array < 0.3) / len(deviation_array)
            if len(deviation_array) > 0
            else 0
        )
        gap_dev_vector[3] = low_dev_ratio

        # í¸ì°¨ ì ìˆ˜ì˜ ë²”ìœ„ (ìµœëŒ€-ìµœì†Œ)
        if len(deviation_array) > 1:
            gap_dev_vector[4] = np.max(deviation_array) - np.min(deviation_array)

        return gap_dev_vector

    def _vectorize_combination_diversity(
        self, diversity_score: Dict[str, Any]
    ) -> np.ndarray:
        """
        ì¡°í•© ë‹¤ì–‘ì„± ì ìˆ˜ ë°ì´í„°ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

        Args:
            diversity_score: ì¡°í•© ë‹¤ì–‘ì„± ì ìˆ˜ ë°ì´í„°

        Returns:
            np.ndarray: ë²¡í„°í™”ëœ ì¡°í•© ë‹¤ì–‘ì„± ì ìˆ˜ íŠ¹ì„±
        """
        # ì´ˆê¸° ë²¡í„° í¬ê¸° ì„¤ì • (5ê°œ íŠ¹ì„±)
        vector_size = 5
        diversity_vector = np.zeros(vector_size, dtype=np.float32)

        # ë‹¤ì–‘ì„± ì ìˆ˜ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ë¹ˆ ë²¡í„° ë°˜í™˜
        if not diversity_score:
            return diversity_vector

        # ì „ì²´ ë‹¤ì–‘ì„± ì ìˆ˜
        if "total_diversity" in diversity_score:
            diversity_vector[0] = float(diversity_score["total_diversity"])

        # ì„¸ê·¸ë¨¼íŠ¸ë³„ ë‹¤ì–‘ì„± ì ìˆ˜
        if "segment_diversity" in diversity_score and isinstance(
            diversity_score["segment_diversity"], dict
        ):
            segment_div = diversity_score["segment_diversity"]
            segment_scores = []

            # 5ê°œ ì„¸ê·¸ë¨¼íŠ¸ì˜ ë‹¤ì–‘ì„± ì ìˆ˜ ì¶”ì¶œ
            for i in range(1, 6):
                seg_key = f"segment_{i}"
                if seg_key in segment_div and isinstance(
                    segment_div[seg_key], (int, float)
                ):
                    segment_scores.append(float(segment_div[seg_key]))

            # ì„¸ê·¸ë¨¼íŠ¸ ë‹¤ì–‘ì„± ì ìˆ˜ì˜ í‰ê· ê³¼ í‘œì¤€í¸ì°¨
            if segment_scores:
                diversity_vector[1] = np.mean(segment_scores)  # í‰ê· 
                diversity_vector[2] = np.std(segment_scores)  # í‘œì¤€í¸ì°¨

        # ë‹¤ì–‘ì„± ìˆœìœ„
        if "diversity_rank" in diversity_score:
            rank = float(diversity_score["diversity_rank"])
            diversity_vector[3] = rank / 1000.0  # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™” (ì´ íšŒì°¨ìˆ˜ ê¸°ì¤€)

        # ë‹¤ì–‘ì„± ë¶„ì‚°ë„
        if "diversity_variance" in diversity_score:
            diversity_vector[4] = float(diversity_score["diversity_variance"])

        return diversity_vector

    def _vectorize_roi_trend_by_pattern(
        self, roi_trend_data: Dict[str, Any]
    ) -> np.ndarray:
        """
        íŒ¨í„´ë³„ ROI ì¶”ì„¸ ë°ì´í„°ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

        Args:
            roi_trend_data: íŒ¨í„´ë³„ ROI ì¶”ì„¸ ë°ì´í„°

        Returns:
            np.ndarray: ë²¡í„°í™”ëœ íŒ¨í„´ë³„ ROI ì¶”ì„¸ íŠ¹ì„±
        """
        # ì´ˆê¸° ë²¡í„° í¬ê¸° ì„¤ì • (10ê°œ íŠ¹ì„±)
        vector_size = 10
        roi_trend_vector = np.zeros(vector_size, dtype=np.float32)

        # ROI ì¶”ì„¸ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ë¹ˆ ë²¡í„° ë°˜í™˜
        if not roi_trend_data:
            return roi_trend_vector

        # íŒ¨í„´ë³„ ROI ì¶”ì„¸ ë°ì´í„° ì¶”ì¶œ
        pattern_trends = []
        for pattern_name, trend_data in roi_trend_data.items():
            if isinstance(trend_data, dict) and "trend" in trend_data:
                trend_value = float(trend_data["trend"])
                pattern_trends.append((pattern_name, trend_value))

        # íŒ¨í„´ë³„ ROI ì¶”ì„¸ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ë¹ˆ ë²¡í„° ë°˜í™˜
        if not pattern_trends:
            return roi_trend_vector

        # íŒ¨í„´ë³„ ì¶”ì„¸ ê°’ ì¶”ì¶œ
        _, trend_values = zip(*pattern_trends)
        trend_array = np.array(trend_values)

        # ì¶”ì„¸ í†µê³„ ê³„ì‚°
        roi_trend_vector[0] = np.mean(trend_array)  # í‰ê·  ì¶”ì„¸
        roi_trend_vector[1] = np.std(trend_array)  # ì¶”ì„¸ì˜ í‘œì¤€í¸ì°¨
        roi_trend_vector[2] = np.max(trend_array)  # ìµœëŒ€ ì¶”ì„¸
        roi_trend_vector[3] = np.min(trend_array)  # ìµœì†Œ ì¶”ì„¸

        # ì–‘ìˆ˜/ìŒìˆ˜ ì¶”ì„¸ ë¹„ìœ¨
        if len(trend_array) > 0:
            roi_trend_vector[4] = np.sum(trend_array > 0) / len(
                trend_array
            )  # ì–‘ìˆ˜ ì¶”ì„¸ ë¹„ìœ¨
            roi_trend_vector[5] = np.sum(trend_array < 0) / len(
                trend_array
            )  # ìŒìˆ˜ ì¶”ì„¸ ë¹„ìœ¨

        # ì¶”ì„¸ ë³€ë™ì„± (ì ˆëŒ€ê°’ í‰ê· )
        roi_trend_vector[6] = np.mean(np.abs(trend_array))

        # ìƒìœ„ 3ê°œ ì¶”ì„¸ í‰ê· 
        top_n = min(3, len(trend_array))
        if top_n > 0:
            top_trends = np.sort(trend_array)[-top_n:]
            roi_trend_vector[7] = np.mean(top_trends)

        # í•˜ìœ„ 3ê°œ ì¶”ì„¸ í‰ê· 
        if top_n > 0:
            bottom_trends = np.sort(trend_array)[:top_n]
            roi_trend_vector[8] = np.mean(bottom_trends)

        # ì¶”ì„¸ ì¤‘ì•™ê°’
        roi_trend_vector[9] = np.median(trend_array)

        return roi_trend_vector

    def _vectorize_physical_structure_features(
        self, physical_structure_data: Dict[str, Any]
    ) -> np.ndarray:
        """
        ë¡œë˜ ì¶”ì²¨ê¸°ì˜ ë¬¼ë¦¬ì  êµ¬ì¡° íŠ¹ì„± ë°ì´í„°ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

        Args:
            physical_structure_data: ë¬¼ë¦¬ì  êµ¬ì¡° íŠ¹ì„± ë°ì´í„°

        Returns:
            np.ndarray: ë²¡í„°í™”ëœ ë¬¼ë¦¬ì  êµ¬ì¡° íŠ¹ì„±
        """
        # ì´ˆê¸° ë²¡í„° í¬ê¸° ì„¤ì • (15ê°œ íŠ¹ì„±)
        vector_size = 15
        physical_vector = np.zeros(vector_size, dtype=np.float32)

        # ë¬¼ë¦¬ì  êµ¬ì¡° íŠ¹ì„± ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ë¹ˆ ë²¡í„° ë°˜í™˜
        if not physical_structure_data:
            return physical_vector

        try:
            # 1. ê±°ë¦¬ ë¶„ì‚° (distance_variance) íŠ¹ì„±
            if "distance_variance" in physical_structure_data:
                dist_variance = physical_structure_data["distance_variance"]
                if isinstance(dist_variance, dict):
                    # í‰ê·  ê±°ë¦¬ ë¶„ì‚°
                    if "average" in dist_variance:
                        physical_vector[0] = self.safe_float_conversion(
                            dist_variance["average"]
                        )
                    # í‘œì¤€í¸ì°¨
                    if "std" in dist_variance:
                        physical_vector[1] = self.safe_float_conversion(
                            dist_variance["std"]
                        )
                    # ìµœëŒ€ ê±°ë¦¬ ë¶„ì‚°
                    if "max" in dist_variance:
                        physical_vector[2] = self.safe_float_conversion(
                            dist_variance["max"]
                        )
                    # ìµœì†Œ ê±°ë¦¬ ë¶„ì‚°
                    if "min" in dist_variance:
                        physical_vector[3] = self.safe_float_conversion(
                            dist_variance["min"]
                        )

            # 2. ì—°ì† ìŒ ë¹„ìœ¨ (sequential_pair_rate) íŠ¹ì„±
            if "sequential_pair_rate" in physical_structure_data:
                seq_pair_rate = physical_structure_data["sequential_pair_rate"]
                if isinstance(seq_pair_rate, dict):
                    # í‰ê·  ì—°ì† ìŒ ë¹„ìœ¨
                    if "avg_rate" in seq_pair_rate:
                        physical_vector[4] = self.safe_float_conversion(
                            seq_pair_rate["avg_rate"]
                        )
                    # ìµœëŒ€ ì—°ì† ìŒ ë¹„ìœ¨
                    if "max_rate" in seq_pair_rate:
                        physical_vector[5] = self.safe_float_conversion(
                            seq_pair_rate["max_rate"]
                        )
                    # ì—°ì† ìŒ ë¹„ìœ¨ í‘œì¤€í¸ì°¨
                    if "std_rate" in seq_pair_rate:
                        physical_vector[6] = self.safe_float_conversion(
                            seq_pair_rate["std_rate"]
                        )

            # 3. ìœ„ì¹˜ë³„ Z-ì ìˆ˜ (zscore_num1 ~ zscore_num6)
            for i in range(6):
                z_score_key = f"zscore_num{i+1}"
                if z_score_key in physical_structure_data:
                    physical_vector[7 + i] = self.safe_float_conversion(
                        physical_structure_data[z_score_key]
                    )

            # 4. ì´í•­ë¶„í¬ ë§¤ì¹­ ì ìˆ˜ (binomial_match_score)
            if "binomial_match_score" in physical_structure_data:
                physical_vector[13] = self.safe_float_conversion(
                    physical_structure_data["binomial_match_score"]
                )

            # 5. ë²ˆí˜¸ í‘œì¤€í¸ì°¨ ì ìˆ˜ (number_std_score)
            if "number_std_score" in physical_structure_data:
                physical_vector[14] = self.safe_float_conversion(
                    physical_structure_data["number_std_score"]
                )

            return physical_vector

        except Exception as e:
            self.logger.error(f"ë¬¼ë¦¬ì  êµ¬ì¡° íŠ¹ì„± ë²¡í„°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return np.zeros(vector_size, dtype=np.float32)

    def vectorize_extended_features(
        self, analysis_data: Dict[str, Any], pair_data: Optional[Dict[str, Any]] = None
    ) -> Tuple[np.ndarray, List[str]]:
        """
        ì²­ì‚¬ì§„ ì‹œìŠ¤í…œì„ ì‚¬ìš©í•œ ì•ˆì „í•œ í™•ì¥ íŠ¹ì„± ë²¡í„°í™”
        ê° ê·¸ë£¹ë³„ë¡œ ê³ ì • ì°¨ì›ì„ ë³´ì¥í•˜ì—¬ ì°¨ì› ë¶ˆì¼ì¹˜ ë¬¸ì œ í•´ê²°

        Args:
            analysis_data: íŒ¨í„´ ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
            pair_data: ìŒ ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ (ì„ íƒì )

        Returns:
            Tuple[np.ndarray, List[str]]: 95ì°¨ì› ê³ ì • ë²¡í„°ì™€ íŠ¹ì„± ì´ë¦„
        """
        self.logger.info("ì²­ì‚¬ì§„ ê¸°ë°˜ í™•ì¥ íŠ¹ì„± ë²¡í„°í™” ì‹œì‘...")

        try:
            # ê·¸ë£¹ë³„ ë²¡í„° ìˆ˜ì§‘
            group_vectors = {}
            all_feature_names = []

            # 1. ê¸°ë³¸ íŒ¨í„´ ë¶„ì„ (25ì°¨ì›)
            pattern_vec, pattern_names = self._vectorize_group_safe(
                "pattern_analysis",
                analysis_data.get("pattern_analysis", {}),
                lambda data: self._extract_basic_pattern_features(data),
            )
            group_vectors["pattern_analysis"] = pattern_vec
            all_feature_names.extend(pattern_names)

            # 2. ë¶„í¬ íŒ¨í„´ (10ì°¨ì›)
            dist_vec, dist_names = self._vectorize_group_safe(
                "distribution_pattern",
                analysis_data.get("distribution_pattern", {}),
                lambda data: self._extract_distribution_features(data),
            )
            group_vectors["distribution_pattern"] = dist_vec
            all_feature_names.extend(dist_names)

            # 3. ì„¸ê·¸ë¨¼íŠ¸ ë¹ˆë„ (15ì°¨ì›: 10êµ¬ê°„ + 5êµ¬ê°„)
            seg_vec, seg_names = self._vectorize_group_safe(
                "segment_frequency",
                {
                    "segment_10": analysis_data.get("segment_10_frequency", {}),
                    "segment_5": analysis_data.get("segment_5_frequency", {}),
                },
                lambda data: self._extract_segment_frequency_features(data),
            )
            group_vectors["segment_frequency"] = seg_vec
            all_feature_names.extend(seg_names)

            # 4. ì¤‘ì‹¬ì„± ë° ì—°ì†ì„± (12ì°¨ì›)
            cent_vec, cent_names = self._vectorize_group_safe(
                "centrality_consecutive",
                {
                    "centrality": analysis_data.get("segment_centrality", {}),
                    "consecutive": analysis_data.get(
                        "segment_consecutive_patterns", {}
                    ),
                },
                lambda data: self._extract_centrality_consecutive_features(data),
            )
            group_vectors["centrality_consecutive"] = cent_vec
            all_feature_names.extend(cent_names)

            # 5. ê°­ í†µê³„ ë° ì¬ì¶œí˜„ (8ì°¨ì›)
            gap_vec, gap_names = self._vectorize_group_safe(
                "gap_reappearance",
                {
                    "gap_stats": analysis_data.get("gap_statistics", {}),
                    "reappearance": analysis_data.get("pattern_reappearance", {}),
                },
                lambda data: self._extract_gap_reappearance_features(data),
            )
            group_vectors["gap_reappearance"] = gap_vec
            all_feature_names.extend(gap_names)

            # 6. ROI íŠ¹ì„± (15ì°¨ì›)
            roi_vec, roi_names = self._vectorize_group_safe(
                "roi_features",
                analysis_data.get("roi_features", {}),
                lambda data: self.extract_roi_features({"roi_features": data}),
            )
            group_vectors["roi_features"] = roi_vec
            all_feature_names.extend(roi_names)

            # 7. í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ (10ì°¨ì›)
            cluster_vec, cluster_names = self._vectorize_group_safe(
                "cluster_features",
                analysis_data.get("cluster_quality", {}),
                lambda data: self.extract_cluster_features({"cluster_quality": data}),
            )
            group_vectors["cluster_features"] = cluster_vec
            all_feature_names.extend(cluster_names)

            # 8. ì¤‘ë³µ íŒ¨í„´ íŠ¹ì„± (20ì°¨ì›)
            overlap_vec, overlap_names = self._vectorize_group_safe(
                "overlap_patterns",
                analysis_data.get("overlap", {}),
                lambda data: self.extract_overlap_pattern_features({"overlap": data}),
            )
            group_vectors["overlap_patterns"] = overlap_vec
            all_feature_names.extend(overlap_names)

            # 9. ë¬¼ë¦¬ì  êµ¬ì¡° íŠ¹ì„± (11ì°¨ì›)
            phys_vec, phys_names = self._vectorize_group_safe(
                "physical_structure",
                analysis_data.get("physical_structure_features", {}),
                lambda data: self._extract_physical_structure_features(data),
            )
            group_vectors["physical_structure"] = phys_vec
            all_feature_names.extend(phys_names)

            # 10. ìŒ ê·¸ë˜í”„ ì••ì¶• ë²¡í„° (20ì°¨ì›)
            pair_vec, pair_names = self._vectorize_group_safe(
                "pair_graph_vector",
                analysis_data.get("pair_graph_compressed_vector", []),
                lambda data: self._extract_pair_graph_features(data),
            )
            group_vectors["pair_graph_vector"] = pair_vec
            all_feature_names.extend(pair_names)

            # ëª¨ë“  ê·¸ë£¹ ë²¡í„° ê²°í•©
            combined_vector = np.concatenate(list(group_vectors.values()))

            # ìµœì¢… ê²€ì¦
            if not self.validate_vector_integrity(combined_vector, all_feature_names):
                raise ValueError("ë²¡í„° ë¬´ê²°ì„± ê²€ì¦ ì‹¤íŒ¨")

            self.logger.info(
                f"ì²­ì‚¬ì§„ ê¸°ë°˜ ë²¡í„°í™” ì™„ë£Œ: {len(combined_vector)}ì°¨ì›, {len(all_feature_names)}ê°œ íŠ¹ì„±"
            )
            return combined_vector, all_feature_names

        except Exception as e:
            self.logger.error(f"í™•ì¥ íŠ¹ì„± ë²¡í„°í™” ì¤‘ ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ ì²­ì‚¬ì§„ ë²¡í„° ë°˜í™˜ - ì˜ë¯¸ìˆëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©
            default_vector = np.random.uniform(
                0.2, 0.8, self.total_expected_dims
            ).astype(np.float32)
            default_names = []
            for group_name, dims in self.vector_blueprint.items():
                default_names.extend(self.feature_name_templates[group_name][:dims])

            return default_vector, default_names

    def _extract_basic_pattern_features(
        self, pattern_data: Dict[str, Any]
    ) -> Tuple[np.ndarray, List[str]]:
        """ê¸°ë³¸ íŒ¨í„´ íŠ¹ì„± ì¶”ì¶œ (25ì°¨ì›)"""
        features = []
        names = []

        # ê¸°ë³¸ í†µê³„ íŠ¹ì„±ë“¤ (10ê°œ) - ì˜ë¯¸ìˆëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©
        basic_stats = {
            "frequency_sum": 0.5,  # ì¤‘ê°„ê°’
            "frequency_mean": 0.5,  # ì¤‘ê°„ê°’
            "frequency_std": 0.1,  # ë‚®ì€ ë¶„ì‚°
            "frequency_max": 0.8,  # ë†’ì€ ìµœëŒ€ê°’
            "frequency_min": 0.2,  # ë‚®ì€ ìµœì†Œê°’
            "gap_mean": 0.5,  # ì¤‘ê°„ ê°­
            "gap_std": 0.3,  # ì¤‘ê°„ ë¶„ì‚°
            "gap_max": 0.9,  # ë†’ì€ ìµœëŒ€ ê°­
            "gap_min": 0.1,  # ë‚®ì€ ìµœì†Œ ê°­
            "total_draws": 0.6,  # ì¤‘ê°„ ì¶”ì²¨ ìˆ˜
        }

        for stat, default_val in basic_stats.items():
            value = self.safe_float_conversion(pattern_data.get(stat, default_val))
            # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
            if stat == "total_draws":
                value = min(1.0, value / 1200.0)  # ì´ ì¶”ì²¨ ìˆ˜ ê¸°ì¤€ ì •ê·œí™”
            elif "frequency" in stat:
                value = min(1.0, max(0.0, value))  # ë¹ˆë„ëŠ” ì´ë¯¸ ì •ê·œí™”ë¨
            else:
                value = min(1.0, max(0.0, value / 100.0))  # ê°­ ê´€ë ¨ ì •ê·œí™”

            features.append(value)
            names.append(f"pattern_{stat}")

        # ì¶”ê°€ íŒ¨í„´ íŠ¹ì„±ë“¤ (15ê°œ) - ë‹¤ì–‘í•œ ê¸°ë³¸ê°’ ì‚¬ìš©
        extra_defaults = [
            0.3,
            0.7,
            0.2,
            0.8,
            0.4,
            0.6,
            0.1,
            0.9,
            0.35,
            0.65,
            0.25,
            0.75,
            0.15,
            0.85,
            0.45,
        ]

        for i, default_val in enumerate(extra_defaults):
            # ì‹¤ì œ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’
            key = f"extra_{i+1}"
            value = self.safe_float_conversion(pattern_data.get(key, default_val))
            features.append(min(1.0, max(0.0, value)))
            names.append(f"pattern_extra_{i+1}")

        return np.array(features, dtype=np.float32), names

    def _extract_distribution_features(
        self, dist_data: Dict[str, Any]
    ) -> Tuple[np.ndarray, List[str]]:
        """ë¶„í¬ íŒ¨í„´ íŠ¹ì„± ì¶”ì¶œ (10ì°¨ì›)"""
        features = []
        names = []

        # ë¶„í¬ íŠ¹ì„±ë“¤ - ì˜ë¯¸ìˆëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©
        dist_stats = {
            "entropy": 0.7,  # ë†’ì€ ì—”íŠ¸ë¡œí”¼ (ë‹¤ì–‘ì„±)
            "balance_score": 0.5,  # ì¤‘ê°„ ê· í˜•
            "diversity": 0.6,  # ì¤‘ê°„ ë‹¤ì–‘ì„±
            "uniformity": 0.4,  # ì¤‘ê°„ ê· ë“±ì„±
            "skewness": 0.0,  # ëŒ€ì¹­ ë¶„í¬
            "kurtosis": 0.3,  # ì •ê·œ ë¶„í¬ì— ê°€ê¹Œì›€
            "range_ratio": 0.8,  # ë†’ì€ ë²”ìœ„ ë¹„ìœ¨
            "concentration": 0.3,  # ë‚®ì€ ì§‘ì¤‘ë„
            "dispersion": 0.7,  # ë†’ì€ ë¶„ì‚°
            "variance": 0.5,  # ì¤‘ê°„ ë¶„ì‚°
        }

        for stat, default_val in dist_stats.items():
            value = self.safe_float_conversion(dist_data.get(stat, default_val))

            # í†µê³„ë³„ ì •ê·œí™”
            if stat in ["skewness", "kurtosis"]:
                # ì™œë„/ì²¨ë„ëŠ” -2~2 ë²”ìœ„ë¥¼ 0~1ë¡œ ì •ê·œí™”
                value = (value + 2) / 4.0
            elif stat == "entropy":
                # ì—”íŠ¸ë¡œí”¼ëŠ” ë³´í†µ 0~log(n) ë²”ìœ„
                value = min(1.0, max(0.0, value / 5.0))
            else:
                # ë‚˜ë¨¸ì§€ëŠ” 0~1 ë²”ìœ„ë¡œ í´ë¦¬í•‘
                value = min(1.0, max(0.0, value))

            features.append(value)
            names.append(f"dist_{stat}")

        return np.array(features, dtype=np.float32), names

    def _extract_segment_frequency_features(
        self, seg_data: Dict[str, Any]
    ) -> Tuple[np.ndarray, List[str]]:
        """ì„¸ê·¸ë¨¼íŠ¸ ë¹ˆë„ íŠ¹ì„± ì¶”ì¶œ (15ì°¨ì›)"""
        features = []
        names = []

        # 10êµ¬ê°„ ë¹ˆë„ (10ì°¨ì›) - ê· ë“± ë¶„í¬ ê¸°ë³¸ê°’
        seg_10 = seg_data.get("segment_10", {})
        uniform_freq_10 = 1.0 / 10  # ê· ë“± ë¶„í¬ ì‹œ ê° êµ¬ê°„ ë¹ˆë„

        for i in range(1, 11):
            value = self.safe_float_conversion(seg_10.get(str(i), uniform_freq_10))
            # ë¹ˆë„ëŠ” ì´ë¯¸ ë¹„ìœ¨ì´ë¯€ë¡œ 0~1 ë²”ìœ„ í™•ì¸ë§Œ
            value = min(1.0, max(0.0, value))
            features.append(value)
            names.append(f"seg10_{i}")

        # 5êµ¬ê°„ ë¹ˆë„ (5ì°¨ì›) - ê· ë“± ë¶„í¬ ê¸°ë³¸ê°’
        seg_5 = seg_data.get("segment_5", {})
        uniform_freq_5 = 1.0 / 5  # ê· ë“± ë¶„í¬ ì‹œ ê° êµ¬ê°„ ë¹ˆë„

        for i in range(1, 6):
            value = self.safe_float_conversion(seg_5.get(str(i), uniform_freq_5))
            # ë¹ˆë„ëŠ” ì´ë¯¸ ë¹„ìœ¨ì´ë¯€ë¡œ 0~1 ë²”ìœ„ í™•ì¸ë§Œ
            value = min(1.0, max(0.0, value))
            features.append(value)
            names.append(f"seg5_{i}")

        return np.array(features, dtype=np.float32), names

    def _extract_centrality_consecutive_features(
        self, cent_data: Dict[str, Any]
    ) -> Tuple[np.ndarray, List[str]]:
        """ì¤‘ì‹¬ì„± ë° ì—°ì†ì„± íŠ¹ì„± ì¶”ì¶œ (12ì°¨ì›)"""
        features = []
        names = []

        # ì¤‘ì‹¬ì„± íŠ¹ì„± (6ì°¨ì›)
        centrality = cent_data.get("centrality", {})
        cent_stats = ["mean", "std", "max", "min", "median", "range"]
        for stat in cent_stats:
            value = self.safe_float_conversion(centrality.get(stat, 0.0))
            features.append(value)
            names.append(f"centrality_{stat}")

        # ì—°ì†ì„± íŠ¹ì„± (6ì°¨ì›)
        consecutive = cent_data.get("consecutive", {})
        cons_stats = [
            "count",
            "avg_length",
            "max_length",
            "frequency",
            "density",
            "ratio",
        ]
        for stat in cons_stats:
            value = self.safe_float_conversion(consecutive.get(stat, 0.0))
            features.append(value)
            names.append(f"consecutive_{stat}")

        return np.array(features, dtype=np.float32), names

    def _extract_gap_reappearance_features(
        self, gap_data: Dict[str, Any]
    ) -> Tuple[np.ndarray, List[str]]:
        """ê°­ í†µê³„ ë° ì¬ì¶œí˜„ íŠ¹ì„± ì¶”ì¶œ (8ì°¨ì›)"""
        features = []
        names = []

        # ê°­ í†µê³„ (4ì°¨ì›)
        gap_stats = gap_data.get("gap_stats", {})
        gap_metrics = ["mean", "std", "max", "min"]
        for metric in gap_metrics:
            value = self.safe_float_conversion(gap_stats.get(metric, 0.0))
            features.append(value)
            names.append(f"gap_{metric}")

        # ì¬ì¶œí˜„ íŠ¹ì„± (4ì°¨ì›)
        reappearance = gap_data.get("reappearance", {})
        reapp_metrics = ["frequency", "interval", "consistency", "trend"]
        for metric in reapp_metrics:
            value = self.safe_float_conversion(reappearance.get(metric, 0.0))
            features.append(value)
            names.append(f"reappear_{metric}")

        return np.array(features, dtype=np.float32), names

    def _extract_physical_structure_features(
        self, phys_data: Dict[str, Any]
    ) -> Tuple[np.ndarray, List[str]]:
        """ë¬¼ë¦¬ì  êµ¬ì¡° íŠ¹ì„± ì¶”ì¶œ (11ì°¨ì›)"""
        features = []
        names = []

        # ë¬¼ë¦¬ì  íŠ¹ì„±ë“¤ - ì˜ë¯¸ìˆëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©
        phys_stats = {
            "distance_variance_avg": 0.4,  # ì¤‘ê°„ ê±°ë¦¬ ë¶„ì‚°
            "distance_variance_std": 0.2,  # ë‚®ì€ í‘œì¤€í¸ì°¨
            "sequential_pair_avg": 0.3,  # ë‚®ì€ ì—°ì† ìŒ ë¹„ìœ¨
            "zscore_mean": 0.5,  # ì¤‘ê°„ Zì ìˆ˜ (ì •ê·œí™”ë¨)
            "zscore_std": 0.3,  # ì¤‘ê°„ Zì ìˆ˜ ë¶„ì‚°
            "binomial_match_score": 0.5,  # ì¤‘ê°„ ì´í•­ ë§¤ì¹˜ ì ìˆ˜
            "number_std_score": 0.4,  # ì¤‘ê°„ ë²ˆí˜¸ í‘œì¤€í¸ì°¨ ì ìˆ˜
            "balance_score": 0.5,  # ì¤‘ê°„ ê· í˜• ì ìˆ˜
            "uniformity": 0.4,  # ì¤‘ê°„ ê· ë“±ì„±
            "concentration": 0.3,  # ë‚®ì€ ì§‘ì¤‘ë„
            "dispersion": 0.7,  # ë†’ì€ ë¶„ì‚°
        }

        for stat, default_val in phys_stats.items():
            value = self.safe_float_conversion(phys_data.get(stat, default_val))

            # í†µê³„ë³„ ì •ê·œí™”
            if "zscore" in stat:
                # Zì ìˆ˜ëŠ” -3~3 ë²”ìœ„ë¥¼ 0~1ë¡œ ì •ê·œí™”
                value = (value + 3) / 6.0
                value = min(1.0, max(0.0, value))
            elif stat == "distance_variance_avg":
                # ê±°ë¦¬ ë¶„ì‚°ì€ 0~ìµœëŒ€ê°’ ë²”ìœ„
                value = min(1.0, max(0.0, value / 10.0))  # ì ì ˆí•œ ìŠ¤ì¼€ì¼ë§
            elif stat == "sequential_pair_avg":
                # ì—°ì† ìŒ ë¹„ìœ¨ì€ ì´ë¯¸ 0~1 ë²”ìœ„
                value = min(1.0, max(0.0, value))
            else:
                # ë‚˜ë¨¸ì§€ ì ìˆ˜ë“¤ì€ 0~1 ë²”ìœ„ë¡œ í´ë¦¬í•‘
                value = min(1.0, max(0.0, value))

            features.append(value)
            names.append(f"physical_{stat}")

        return np.array(features, dtype=np.float32), names

    def _extract_pair_graph_features(
        self, pair_data: Any
    ) -> Tuple[np.ndarray, List[str]]:
        """ìŒ ê·¸ë˜í”„ íŠ¹ì„± ì¶”ì¶œ (20ì°¨ì›)"""
        features = []
        names = []

        if isinstance(pair_data, (list, np.ndarray)) and len(pair_data) > 0:
            # ê¸°ì¡´ ì••ì¶• ë²¡í„° ì‚¬ìš©
            pair_vector = np.array(pair_data, dtype=np.float32)
            # 20ì°¨ì›ìœ¼ë¡œ ì¡°ì •
            if len(pair_vector) >= 20:
                features = pair_vector[:20].tolist()
            else:
                features = pair_vector.tolist() + [0.0] * (20 - len(pair_vector))
        else:
            # ê¸°ë³¸ê°’
            features = [0.0] * 20

        # íŠ¹ì„± ì´ë¦„ ìƒì„±
        names = [f"pair_graph_{i+1}" for i in range(20)]

        return np.array(features, dtype=np.float32), names

    def _extract_position_entropy_std_features(
        self, analysis_data: Dict[str, Any]
    ) -> Tuple[np.ndarray, np.ndarray, List[str], List[str]]:
        """
        ìœ„ì¹˜ë³„ ì—”íŠ¸ë¡œí”¼ ë° í‘œì¤€í¸ì°¨ íŠ¹ì„±ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.

        Args:
            analysis_data: ë¶„ì„ ë°ì´í„°

        Returns:
            Tuple[np.ndarray, np.ndarray, List[str], List[str]]:
                (ì—”íŠ¸ë¡œí”¼ ë²¡í„°, í‘œì¤€í¸ì°¨ ë²¡í„°, ì—”íŠ¸ë¡œí”¼ íŠ¹ì„±ëª…, í‘œì¤€í¸ì°¨ íŠ¹ì„±ëª…)
        """
        # ì—”íŠ¸ë¡œí”¼ íŠ¹ì„± ì¶”ì¶œ
        position_entropy_values = []
        position_entropy_names = []
        all_entropy_present = True

        for i in range(1, 7):
            key = f"position_entropy_{i}"
            if key in analysis_data:
                position_entropy_values.append(analysis_data[key])
                position_entropy_names.append(key)
            else:
                all_entropy_present = False
                self.logger.warning(f"ìœ„ì¹˜ë³„ ì—”íŠ¸ë¡œí”¼ íŠ¹ì„±ì´ ëˆ„ë½ë¨: {key}")

        if all_entropy_present:
            position_entropy_vector = np.array(
                position_entropy_values, dtype=np.float32
            )
            self.logger.info(
                f"ìœ„ì¹˜ë³„ ì—”íŠ¸ë¡œí”¼ ë²¡í„° ì¶”ê°€: {len(position_entropy_names)}ê°œ íŠ¹ì„±"
            )
        else:
            position_entropy_vector = np.zeros(6, dtype=np.float32)

        # í‘œì¤€í¸ì°¨ íŠ¹ì„± ì¶”ì¶œ
        position_std_values = []
        position_std_names = []
        all_std_present = True

        for i in range(1, 7):
            key = f"position_std_{i}"
            if key in analysis_data:
                position_std_values.append(analysis_data[key])
                position_std_names.append(key)
            else:
                all_std_present = False
                self.logger.warning(f"ìœ„ì¹˜ë³„ í‘œì¤€í¸ì°¨ íŠ¹ì„±ì´ ëˆ„ë½ë¨: {key}")

        if all_std_present:
            position_std_vector = np.array(position_std_values, dtype=np.float32)
            self.logger.info(
                f"ìœ„ì¹˜ë³„ í‘œì¤€í¸ì°¨ ë²¡í„° ì¶”ê°€: {len(position_std_names)}ê°œ íŠ¹ì„±"
            )
        else:
            position_std_vector = np.zeros(6, dtype=np.float32)

        return (
            position_entropy_vector,
            position_std_vector,
            position_entropy_names,
            position_std_names,
        )

    def _vectorize_roi_features(
        self, roi_data: Dict[str, Any]
    ) -> Tuple[np.ndarray, List[str]]:
        """
        ROI ê¸°ë°˜ íŒ¨í„´ íŠ¹ì„±ì„ ë²¡í„°í™”í•©ë‹ˆë‹¤.

        ì´ ë©”ì„œë“œëŠ” ROI ë¶„ì„ ê²°ê³¼ë¥¼ ê°•í™”í•™ìŠµ, ë©”íƒ€ëŸ¬ë‹ ë° ì ìˆ˜ ëª¨ë¸ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ”
        íŠ¹ì„± ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ì£¼ìš” íŠ¹ì„±ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

        1. roi_group_score: ê·¸ë£¹ ID(0,1,2) ë¶„í¬ - íšŒì°¨ë³„ ROI ê·¸ë£¹ í†µê³„
        2. roi_cluster_score: í´ëŸ¬ìŠ¤í„° ID ë¶„í¬ - í´ëŸ¬ìŠ¤í„° ë¶„ì„ ê¸°ë°˜ ROI íŒ¨í„´
        3. low_risk_bonus_flag: ì €ìœ„í—˜ ë³´ë„ˆìŠ¤ í”Œë˜ê·¸ ë¹„ìœ¨ - ì•ˆì •ì  ìˆ˜ìµ ì§€í‘œ
        4. roi_pattern_group_id: íŒ¨í„´ë³„ ROI ê·¸ë£¹ ID - íŒ¨í„´ ê¸°ë°˜ ìˆ˜ìµì„± ì§€í‘œ

        Args:
            roi_data: ROI ë¶„ì„ ë°ì´í„°

        Returns:
            Tuple[np.ndarray, List[str]]: (ë²¡í„°í™”ëœ ROI íŠ¹ì„±, íŠ¹ì„± ì´ë¦„ ëª©ë¡)
        """
        self.logger.info("ROI ê¸°ë°˜ íŒ¨í„´ íŠ¹ì„± ë²¡í„°í™” ì¤‘...")

        # ê²°ê³¼ ë²¡í„°ì™€ íŠ¹ì„± ì´ë¦„ ëª©ë¡ ì´ˆê¸°í™”
        feature_vectors = []
        feature_names = []

        # 1. roi_group_score (íšŒì°¨ë³„ ê·¸ë£¹ ID) ì²˜ë¦¬
        roi_group_score = roi_data.get("roi_group_score", {})
        if roi_group_score:
            # ê·¸ë£¹ ID(0,1,2) ë¶„í¬ ê³„ì‚°
            from collections import Counter

            group_ids = list(roi_group_score.values())
            group_dist = Counter(group_ids)
            group_vector = np.array(
                [group_dist.get(i, 0) for i in range(3)], dtype=np.float32
            )

            # ì •ê·œí™”
            if group_vector.sum() > 0:
                group_vector = group_vector / group_vector.sum()

            feature_vectors.append(group_vector)
            feature_names.extend([f"roi_group_{i}" for i in range(3)])
            self.logger.info(f"ROI ê·¸ë£¹ ì ìˆ˜ ë²¡í„° ìƒì„±: {group_vector}")
        else:
            # ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° 0ìœ¼ë¡œ ì±„ìš´ ë²¡í„° ì¶”ê°€
            feature_vectors.append(np.zeros(3, dtype=np.float32))
            feature_names.extend([f"roi_group_{i}" for i in range(3)])
            self.logger.warning("ROI ê·¸ë£¹ ì ìˆ˜ ë°ì´í„°ê°€ ì—†ì–´ 0ìœ¼ë¡œ ì´ˆê¸°í™”ë¨")

        # 2. roi_cluster_score (í´ëŸ¬ìŠ¤í„° í• ë‹¹) ì²˜ë¦¬
        roi_cluster_data = roi_data.get("roi_cluster_score", {})
        cluster_assignments = roi_cluster_data.get("cluster_assignments", {})

        if cluster_assignments:
            # í´ëŸ¬ìŠ¤í„° ID ë¶„í¬ ê³„ì‚° (ìµœëŒ€ 5ê°œ í´ëŸ¬ìŠ¤í„°)
            cluster_ids = list(cluster_assignments.values())
            cluster_dist = Counter(cluster_ids)
            # ê³ ì • ê¸¸ì´(5)ë¡œ ìƒì„±
            cluster_vector = np.array(
                [cluster_dist.get(i, 0) for i in range(5)], dtype=np.float32
            )

            # ì •ê·œí™”
            if cluster_vector.sum() > 0:
                cluster_vector = cluster_vector / cluster_vector.sum()

            feature_vectors.append(cluster_vector)
            feature_names.extend([f"roi_cluster_{i}" for i in range(5)])
            self.logger.info(f"ROI í´ëŸ¬ìŠ¤í„° ë²¡í„° ìƒì„±: {cluster_vector}")
        else:
            # ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° 0ìœ¼ë¡œ ì±„ìš´ ë²¡í„° ì¶”ê°€
            feature_vectors.append(np.zeros(5, dtype=np.float32))
            feature_names.extend([f"roi_cluster_{i}" for i in range(5)])
            self.logger.warning("ROI í´ëŸ¬ìŠ¤í„° ë°ì´í„°ê°€ ì—†ì–´ 0ìœ¼ë¡œ ì´ˆê¸°í™”ë¨")

        # 3. low_risk_bonus_flag (ì €ìœ„í—˜ ë³´ë„ˆìŠ¤ í”Œë˜ê·¸) ì²˜ë¦¬
        low_risk_flags_data = roi_data.get("low_risk_bonus_flag", {})
        if low_risk_flags_data:
            # ì¤‘ì²© êµ¬ì¡° ì²˜ë¦¬ (low_risk_bonus_flag í•„ë“œê°€ ì¤‘ì²©ë˜ì–´ ìˆëŠ” ê²½ìš°)
            if "low_risk_bonus_flag" in low_risk_flags_data:
                low_risk_flags = low_risk_flags_data.get("low_risk_bonus_flag", {})
            else:
                low_risk_flags = low_risk_flags_data

            # True ê°’ì˜ ë¹„ìœ¨ ê³„ì‚°
            if isinstance(low_risk_flags, dict) and low_risk_flags:
                low_risk_ratio = sum(1 for v in low_risk_flags.values() if v) / max(
                    len(low_risk_flags), 1
                )
            elif isinstance(low_risk_flags, (list, tuple)) and low_risk_flags:
                low_risk_ratio = sum(1 for v in low_risk_flags if v) / max(
                    len(low_risk_flags), 1
                )
            else:
                # ì ì ˆí•œ í˜•ì‹ì´ ì•„ë‹Œ ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
                low_risk_ratio = 0.0
                self.logger.warning(
                    f"ì €ìœ„í—˜ ë³´ë„ˆìŠ¤ í”Œë˜ê·¸ ë°ì´í„° í˜•ì‹ ì˜¤ë¥˜: {type(low_risk_flags)}"
                )

            low_risk_vector = np.array([low_risk_ratio], dtype=np.float32)
            feature_vectors.append(low_risk_vector)
            feature_names.append("low_risk_ratio")
            self.logger.info(f"ì €ìœ„í—˜ ë³´ë„ˆìŠ¤ í”Œë˜ê·¸ ë¹„ìœ¨: {low_risk_ratio:.4f}")
        else:
            # ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° 0ìœ¼ë¡œ ì´ˆê¸°í™”
            feature_vectors.append(np.array([0.0], dtype=np.float32))
            feature_names.append("low_risk_ratio")
            self.logger.warning("ì €ìœ„í—˜ ë³´ë„ˆìŠ¤ í”Œë˜ê·¸ ë°ì´í„°ê°€ ì—†ì–´ 0ìœ¼ë¡œ ì´ˆê¸°í™”ë¨")

        # 4. roi_pattern_group_id (íŒ¨í„´ í‚¤ë³„ ê·¸ë£¹ ID) ì²˜ë¦¬
        roi_pattern_group_id = roi_data.get("roi_pattern_group_id", {})
        if roi_pattern_group_id:
            # ìµœëŒ€ 10ê°œ ê·¸ë£¹ìœ¼ë¡œ ì œí•œ
            pattern_group_ids = list(roi_pattern_group_id.values())
            pattern_group_dist = Counter(pattern_group_ids)
            # ê³ ì • ê¸¸ì´(10)ë¡œ ìƒì„±
            max_groups = 10
            pattern_group_vector = np.array(
                [pattern_group_dist.get(i, 0) for i in range(max_groups)],
                dtype=np.float32,
            )

            # ì •ê·œí™”
            if pattern_group_vector.sum() > 0:
                pattern_group_vector = pattern_group_vector / pattern_group_vector.sum()

            feature_vectors.append(pattern_group_vector)
            feature_names.extend([f"roi_pattern_group_{i}" for i in range(max_groups)])
            self.logger.info(
                f"ROI íŒ¨í„´ ê·¸ë£¹ ë²¡í„° ìƒì„±: ê¸¸ì´={len(pattern_group_vector)}"
            )
        else:
            # ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° 0ìœ¼ë¡œ ì±„ìš´ ë²¡í„° ì¶”ê°€
            feature_vectors.append(np.zeros(10, dtype=np.float32))
            feature_names.extend([f"roi_pattern_group_{i}" for i in range(10)])
            self.logger.warning("ROI íŒ¨í„´ ê·¸ë£¹ ID ë°ì´í„°ê°€ ì—†ì–´ 0ìœ¼ë¡œ ì´ˆê¸°í™”ë¨")

        # ìµœì¢… ë²¡í„° ê²°í•©
        final_vector = np.concatenate(feature_vectors)
        self.logger.info(f"ROI íŠ¹ì„± ë²¡í„° ìƒì„± ì™„ë£Œ: ì°¨ì›={len(final_vector)}")

        # ë³„ë„ íŒŒì¼ë¡œ ì €ì¥
        self.save_vector_to_file(final_vector, "roi_features_vector.npy")

        return final_vector, feature_names

    def extract_cluster_features(
        self, full_analysis: Dict[str, Any]
    ) -> Tuple[np.ndarray, List[str]]:
        """
        í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ ë¶„ì„ ê²°ê³¼ë¥¼ ì •ê·œí™”í•˜ì—¬ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

        Args:
            full_analysis: í†µí•© ë¶„ì„ ê²°ê³¼

        Returns:
            Tuple of (vector array, feature name list)
        """
        metrics = full_analysis.get("cluster_analysis", {}).get("quality_metrics", {})

        # ì •ê·œí™” ê¸°ì¤€
        cluster_count = metrics.get("cluster_count", 1)
        sample_size = metrics.get("sample_size", 1)

        def norm(value, base):
            return min(1.0, max(0.0, float(value) / float(base))) if base else 0.0

        vector = [
            float(metrics.get("silhouette_score", 0.0)),
            float(metrics.get("cohesiveness_score", 0.0)),
            norm(metrics.get("avg_distance_between_clusters", 0.0), 10.0),
            norm(metrics.get("balance_score", 0.0), 1.0),
            norm(metrics.get("largest_cluster_size", 0.0), sample_size),
            norm(cluster_count - 1, 9.0),  # ìµœëŒ€ 10ê°œ ê¸°ì¤€
            float(metrics.get("cluster_entropy_score", 0.0)),
        ]

        feature_names = [
            "silhouette_score",
            "cohesiveness_score",
            "avg_distance_between_clusters",
            "balance_score",
            "largest_cluster_size_norm",
            "cluster_count_norm",
            "cluster_entropy_score",
        ]

        return np.array(vector, dtype=np.float32), feature_names

    def extract_roi_features(
        self, full_analysis: Dict[str, Any]
    ) -> Tuple[np.ndarray, List[str]]:
        """
        ROI ë¶„ì„ê¸°ì—ì„œ ê³„ì‚°í•œ ì£¼ìš” ROI ê´€ë ¨ í”¼ì²˜ë“¤ì„ ì •ê·œí™”í•˜ì—¬ ë²¡í„°ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.

        Args:
            full_analysis: í†µí•© ë¶„ì„ ê²°ê³¼ ë°ì´í„°

        Returns:
            Tuple[np.ndarray, List[str]]: ë²¡í„° ê°’ê³¼ í•´ë‹¹ íŠ¹ì„± ì´ë¦„ ëª©ë¡
        """
        roi_data = full_analysis.get("roi_analysis", {})

        # ì •ê·œí™” í•¨ìˆ˜
        def norm(value, base=1.0):
            try:
                return min(1.0, max(0.0, float(value) / float(base)))
            except:
                return 0.0

        vector = []
        names = []

        # 1. roi_group_score ë¶„í¬ (0~2 ê·¸ë£¹ â†’ 3ì°¨ì›)
        group_dist = roi_data.get("roi_group_score_distribution", {})
        for i in range(3):
            val = group_dist.get(str(i), 0)
            vector.append(norm(val, 1.0))  # ì´ë¯¸ ë¹„ìœ¨ í˜•íƒœì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
            names.append(f"roi_group_ratio_{i}")

        # 2. roi_cluster_score ë¶„í¬ (ìµœëŒ€ 5ê°œ í´ëŸ¬ìŠ¤í„°)
        cluster_dist = roi_data.get("roi_cluster_distribution", {})
        for i in range(5):
            val = cluster_dist.get(str(i), 0)
            vector.append(norm(val, 1.0))
            names.append(f"roi_cluster_ratio_{i}")

        # 3. low_risk_bonus_flag â†’ ë¹„ìœ¨ë¡œ ë³€í™˜
        low_risk_ratio = roi_data.get("low_risk_bonus_ratio", 0.0)
        vector.append(norm(low_risk_ratio, 1.0))
        names.append("low_risk_bonus_ratio")

        # 4. roi_pattern_group_id ë¶„í¬ (ìµœëŒ€ 10ê°œ ê·¸ë£¹ ê°€ì •)
        pattern_group_dist = roi_data.get("roi_pattern_group_distribution", {})
        for i in range(10):
            val = pattern_group_dist.get(str(i), 0)
            vector.append(norm(val, 1.0))
            names.append(f"roi_pattern_group_{i}")

        return np.array(vector, dtype=np.float32), names

    def get_gnn_vector(
        self,
        vector_path: str = "data/cache/pair_graph_compressed_vector.npy",
        names_path: str = "data/cache/pair_graph_compressed_vector.names.json",
    ) -> Tuple[np.ndarray, List[str]]:
        """
        GNN ë²¡í„°ì™€ íŠ¹ì„± ì´ë¦„ì„ ë¡œë“œí•©ë‹ˆë‹¤.

        Args:
            vector_path: GNN ë²¡í„° íŒŒì¼ ê²½ë¡œ
            names_path: GNN íŠ¹ì„± ì´ë¦„ íŒŒì¼ ê²½ë¡œ

        Returns:
            Tuple[np.ndarray, List[str]]: GNN ë²¡í„°ì™€ íŠ¹ì„± ì´ë¦„
        """
        try:
            # ë²¡í„° ë¡œë“œ
            if os.path.exists(vector_path):
                gnn_vector = np.load(vector_path)
            else:
                self.logger.warning(f"GNN ë²¡í„° íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {vector_path}")
                return np.array([]), []

            # íŠ¹ì„± ì´ë¦„ ë¡œë“œ
            if os.path.exists(names_path):
                with open(names_path, "r", encoding="utf-8") as f:
                    feature_names = json.load(f)
            else:
                self.logger.warning(
                    f"GNN íŠ¹ì„± ì´ë¦„ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {names_path}"
                )
                feature_names = [f"gnn_feature_{i}" for i in range(len(gnn_vector))]

            return gnn_vector, feature_names

        except Exception as e:
            self.logger.error(f"GNN ë²¡í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return np.array([]), []

    def extract_overlap_pattern_features(
        self, full_analysis: Dict[str, Any]
    ) -> Tuple[np.ndarray, List[str]]:
        """
        ì¤‘ë³µ íŒ¨í„´ ë¶„ì„ ê²°ê³¼ë¥¼ ë²¡í„°í™”

        Args:
            full_analysis: ì „ì²´ ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬

        Returns:
            Tuple[np.ndarray, List[str]]: ì¤‘ë³µ íŒ¨í„´ ë²¡í„°ì™€ íŠ¹ì„± ì´ë¦„
        """
        overlap_features = []
        feature_names = []

        try:
            # overlap ë¶„ì„ ê²°ê³¼ ì¶”ì¶œ
            overlap_data = full_analysis.get("overlap", {})

            # 1. 3ìë¦¬ ì¤‘ë³µ íŒ¨í„´ íŠ¹ì„±
            overlap_3_4_data = overlap_data.get("overlap_3_4_digit_patterns", {})

            # 3ìë¦¬ íŒ¨í„´ ë¹ˆë„ ì ìˆ˜
            overlap_3_patterns = overlap_3_4_data.get("overlap_3_patterns", {})
            overlap_3_frequency_score = self.safe_float_conversion(
                overlap_3_patterns.get("avg_frequency", 0.0)
            )
            overlap_features.append(overlap_3_frequency_score)
            feature_names.append("overlap_3_frequency_score")

            # 3ìë¦¬ íŒ¨í„´ ì´ ê°œìˆ˜ (ì •ê·œí™”)
            total_3_patterns = self.safe_float_conversion(
                overlap_3_patterns.get("total_patterns", 0)
            )
            overlap_3_pattern_count_norm = min(
                1.0, total_3_patterns / 100.0
            )  # ìµœëŒ€ 100ê°œë¡œ ì •ê·œí™”
            overlap_features.append(overlap_3_pattern_count_norm)
            feature_names.append("overlap_3_pattern_count_norm")

            # 2. 4ìë¦¬ ì¤‘ë³µ íŒ¨í„´ íŠ¹ì„± (í¬ê·€ë„)
            overlap_4_patterns = overlap_3_4_data.get("overlap_4_patterns", {})
            overlap_4_rarity_score = self.safe_float_conversion(
                overlap_4_patterns.get("rarity_score", 0.0)
            )
            overlap_features.append(overlap_4_rarity_score)
            feature_names.append("overlap_4_rarity_score")

            # 4ìë¦¬ íŒ¨í„´ ì´ ê°œìˆ˜ (ì •ê·œí™”)
            total_4_patterns = self.safe_float_conversion(
                overlap_4_patterns.get("total_patterns", 0)
            )
            overlap_4_pattern_count_norm = min(
                1.0, total_4_patterns / 50.0
            )  # ìµœëŒ€ 50ê°œë¡œ ì •ê·œí™”
            overlap_features.append(overlap_4_pattern_count_norm)
            feature_names.append("overlap_4_pattern_count_norm")

            # 3. ì‹œê°„ ê°„ê²© ë¶„ì„ íŠ¹ì„±
            time_gap_data = overlap_3_4_data.get("overlap_time_gap_analysis", {})

            # 3ìë¦¬ íŒ¨í„´ ì‹œê°„ ê°„ê²© ë¶„ì‚° (ì •ê·œí™”)
            gap_3_std = self.safe_float_conversion(time_gap_data.get("gap_3_std", 0.0))
            overlap_time_gap_variance_3 = min(
                1.0, gap_3_std / 100.0
            )  # ìµœëŒ€ 100íšŒì°¨ë¡œ ì •ê·œí™”
            overlap_features.append(overlap_time_gap_variance_3)
            feature_names.append("overlap_time_gap_variance_3")

            # 4ìë¦¬ íŒ¨í„´ ì‹œê°„ ê°„ê²© ë¶„ì‚° (ì •ê·œí™”)
            gap_4_std = self.safe_float_conversion(time_gap_data.get("gap_4_std", 0.0))
            overlap_time_gap_variance_4 = min(
                1.0, gap_4_std / 100.0
            )  # ìµœëŒ€ 100íšŒì°¨ë¡œ ì •ê·œí™”
            overlap_features.append(overlap_time_gap_variance_4)
            feature_names.append("overlap_time_gap_variance_4")

            # 4. íŒ¨í„´ ì¼ê´€ì„± íŠ¹ì„±
            pattern_consistency = overlap_3_4_data.get("pattern_consistency", {})

            # 3ìë¦¬ íŒ¨í„´ ì¼ê´€ì„±
            overlap_pattern_consistency_3 = self.safe_float_conversion(
                pattern_consistency.get("consistency_3", 0.0)
            )
            overlap_features.append(overlap_pattern_consistency_3)
            feature_names.append("overlap_pattern_consistency_3")

            # 4ìë¦¬ íŒ¨í„´ ì¼ê´€ì„±
            overlap_pattern_consistency_4 = self.safe_float_conversion(
                pattern_consistency.get("consistency_4", 0.0)
            )
            overlap_features.append(overlap_pattern_consistency_4)
            feature_names.append("overlap_pattern_consistency_4")

            # 5. ROI ìƒê´€ê´€ê³„ íŠ¹ì„±
            roi_correlation_data = overlap_data.get("overlap_roi_correlation", {})

            # 3ìë¦¬ ì¤‘ë³µ íŒ¨í„´ ROI ìƒê´€ê´€ê³„
            overlap_3_roi_correlation = self.safe_float_conversion(
                roi_correlation_data.get("overlap_3_roi_correlation", 0.0)
            )
            overlap_features.append(overlap_3_roi_correlation)
            feature_names.append("overlap_3_roi_correlation")

            # 4ìë¦¬ ì¤‘ë³µ íŒ¨í„´ ROI ìƒê´€ê´€ê³„
            overlap_4_roi_correlation = self.safe_float_conversion(
                roi_correlation_data.get("overlap_4_roi_correlation", 0.0)
            )
            overlap_features.append(overlap_4_roi_correlation)
            feature_names.append("overlap_4_roi_correlation")

            # 6. ROI ê¸°ëŒ€ê°’ íŠ¹ì„±
            overlap_3_roi_stats = roi_correlation_data.get("overlap_3_roi_stats", {})
            overlap_4_roi_stats = roi_correlation_data.get("overlap_4_roi_stats", {})

            # 3ìë¦¬ íŒ¨í„´ í‰ê·  ROI
            overlap_3_avg_roi = self.safe_float_conversion(
                overlap_3_roi_stats.get("avg_roi", 0.0)
            )
            overlap_features.append(overlap_3_avg_roi)
            feature_names.append("overlap_3_avg_roi")

            # 4ìë¦¬ íŒ¨í„´ í‰ê·  ROI
            overlap_4_avg_roi = self.safe_float_conversion(
                overlap_4_roi_stats.get("avg_roi", 0.0)
            )
            overlap_features.append(overlap_4_avg_roi)
            feature_names.append("overlap_4_avg_roi")

            # 7. ROI ê¸ì • ë¹„ìœ¨ íŠ¹ì„±
            overlap_3_positive_ratio = self.safe_float_conversion(
                overlap_3_roi_stats.get("positive_roi_ratio", 0.0)
            )
            overlap_features.append(overlap_3_positive_ratio)
            feature_names.append("overlap_3_positive_roi_ratio")

            overlap_4_positive_ratio = self.safe_float_conversion(
                overlap_4_roi_stats.get("positive_roi_ratio", 0.0)
            )
            overlap_features.append(overlap_4_positive_ratio)
            feature_names.append("overlap_4_positive_roi_ratio")

            # 8. í†µí•© ROI ê¸°ëŒ€ê°’ (3ìë¦¬ì™€ 4ìë¦¬ íŒ¨í„´ì˜ ê°€ì¤‘ í‰ê· )
            weight_3 = max(1, overlap_3_roi_stats.get("sample_count", 1))
            weight_4 = max(1, overlap_4_roi_stats.get("sample_count", 1))
            total_weight = weight_3 + weight_4

            overlap_roi_expectation = (
                overlap_3_avg_roi * weight_3 + overlap_4_avg_roi * weight_4
            ) / total_weight
            overlap_features.append(overlap_roi_expectation)
            feature_names.append("overlap_roi_expectation")

            # 9. ìƒ˜í”Œ ì‹ ë¢°ë„ (ìƒ˜í”Œ ìˆ˜ ê¸°ë°˜)
            overlap_3_sample_confidence = min(
                1.0, weight_3 / 50.0
            )  # ìµœëŒ€ 50ê°œ ìƒ˜í”Œë¡œ ì •ê·œí™”
            overlap_features.append(overlap_3_sample_confidence)
            feature_names.append("overlap_3_sample_confidence")

            overlap_4_sample_confidence = min(
                1.0, weight_4 / 20.0
            )  # ìµœëŒ€ 20ê°œ ìƒ˜í”Œë¡œ ì •ê·œí™”
            overlap_features.append(overlap_4_sample_confidence)
            feature_names.append("overlap_4_sample_confidence")

            # 10. ì¶”ê°€ ì¤‘ë³µ íŠ¹ì„±ë“¤ (ê¸°ì¡´ overlap ë¶„ì„ì—ì„œ)
            # ìµœëŒ€ ì¤‘ë³µë„
            max_overlap_data = overlap_data.get("max_overlap_with_past", {})
            max_overlap_avg = self.safe_float_conversion(
                max_overlap_data.get("max_overlap_avg", 0.0)
            )
            overlap_features.append(max_overlap_avg / 6.0)  # 6ê°œ ë²ˆí˜¸ë¡œ ì •ê·œí™”
            feature_names.append("max_overlap_avg_norm")

            # ì¤‘ë³µ í”Œë˜ê·¸
            duplicate_flag = overlap_data.get("duplicate_flag", {})
            duplicate_ratio = self.safe_float_conversion(
                duplicate_flag.get("duplicate_ratio", 0.0)
            )
            overlap_features.append(duplicate_ratio)
            feature_names.append("duplicate_ratio")

            # ìµœê·¼ì„± ì ìˆ˜
            recency_score = self.safe_float_conversion(
                overlap_data.get("recency_score", {}).get("avg_recency", 0.0)
            )
            overlap_features.append(recency_score)
            feature_names.append("recency_score")

            # ë²¡í„° ë³€í™˜ ë° NaN ì²˜ë¦¬
            overlap_vector = np.array(overlap_features, dtype=np.float32)
            overlap_vector = np.nan_to_num(
                overlap_vector, nan=0.0, posinf=1.0, neginf=0.0
            )

            self.logger.info(
                f"ì¤‘ë³µ íŒ¨í„´ íŠ¹ì„± ë²¡í„° ìƒì„± ì™„ë£Œ: {len(feature_names)}ê°œ íŠ¹ì„±"
            )
            self.logger.info(
                f"ì¤‘ë³µ íŒ¨í„´ ë²¡í„° í†µê³„ - í‰ê· : {np.mean(overlap_vector):.4f}, ìµœëŒ€: {np.max(overlap_vector):.4f}, ìµœì†Œ: {np.min(overlap_vector):.4f}"
            )

            return overlap_vector, feature_names

        except Exception as e:
            self.logger.error(f"ì¤‘ë³µ íŒ¨í„´ íŠ¹ì„± ë²¡í„° ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ë²¡í„° ë°˜í™˜ (20ê°œ íŠ¹ì„±) - ì˜ë¯¸ìˆëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©
            default_values = [
                0.3,
                0.5,
                0.2,
                0.7,
                0.4,
                0.6,
                0.25,
                0.75,
                0.35,
                0.65,
                0.15,
                0.85,
                0.45,
                0.55,
                0.3,
                0.7,
                0.2,
                0.8,
                0.4,
                0.6,
            ]
            default_vector = np.array(default_values, dtype=np.float32)
            default_names = [f"overlap_feature_{i+1}" for i in range(20)]
            return default_vector, default_names

    def extract_position_bias_features(
        self, full_analysis: Dict[str, Any]
    ) -> Tuple[np.ndarray, List[str]]:
        """
        ì¶”ì²¨ ìˆœì„œ í¸í–¥ ë¶„ì„ ê²°ê³¼ë¥¼ ë²¡í„°í™”í•©ë‹ˆë‹¤.

        Args:
            full_analysis: ì „ì²´ ë¶„ì„ ê²°ê³¼

        Returns:
            Tuple[np.ndarray, List[str]]: ì¶”ì²¨ ìˆœì„œ í¸í–¥ ë²¡í„°ì™€ íŠ¹ì„± ì´ë¦„
        """
        try:
            position_bias_data = full_analysis.get("position_bias_features", {})

            if not position_bias_data:
                self.logger.warning(
                    "ì¶”ì²¨ ìˆœì„œ í¸í–¥ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤."
                )
                position_bias_data = {}

            # ì¶”ì²¨ ìˆœì„œ í¸í–¥ íŠ¹ì„± ì¶”ì¶œ (5ê°œ)
            features = []
            feature_names = []

            # 1. ìœ„ì¹˜ë³„ í‰ê· ê°’ íŠ¹ì„±
            position_min_value_mean = position_bias_data.get(
                "position_min_value_mean", 0.0
            )
            position_max_value_mean = position_bias_data.get(
                "position_max_value_mean", 0.0
            )
            position_gap_mean = position_bias_data.get("position_gap_mean", 0.0)

            features.extend(
                [
                    np.clip(position_min_value_mean / 45.0, 0, 1),  # ì •ê·œí™”
                    np.clip(position_max_value_mean / 45.0, 0, 1),  # ì •ê·œí™”
                    np.clip(position_gap_mean / 40.0, 0, 1),  # ìµœëŒ€ ê°„ê²© 40ìœ¼ë¡œ ì •ê·œí™”
                ]
            )

            feature_names.extend(
                [
                    "position_min_value_mean",
                    "position_max_value_mean",
                    "position_gap_mean",
                ]
            )

            # 2. í™€ì§ ë° êµ¬ê°„ ë¹„ìœ¨ íŠ¹ì„±
            position_even_odd_ratio = position_bias_data.get(
                "position_even_odd_ratio", 0.5
            )
            position_low_high_ratio = position_bias_data.get(
                "position_low_high_ratio", 0.5
            )

            features.extend(
                [
                    np.clip(position_even_odd_ratio, 0, 1),
                    np.clip(position_low_high_ratio, 0, 1),
                ]
            )

            feature_names.extend(["position_even_odd_ratio", "position_low_high_ratio"])

            # NaN/Inf ì²´í¬ ë° ìˆ˜ì •
            features = [self.safe_float_conversion(f) for f in features]
            position_vector = np.array(features, dtype=np.float32)

            self.logger.info(f"ì¶”ì²¨ ìˆœì„œ í¸í–¥ ë²¡í„°í™” ì™„ë£Œ: {len(features)}ê°œ íŠ¹ì„±")
            return position_vector, feature_names

        except Exception as e:
            self.logger.error(f"ì¶”ì²¨ ìˆœì„œ í¸í–¥ ë²¡í„°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            # ê¸°ë³¸ê°’ ë°˜í™˜ - ì˜ë¯¸ìˆëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©
            default_vector = np.array([0.5, 0.7, 0.6, 0.5, 0.5], dtype=np.float32)
            default_names = [
                "position_min_value_mean",
                "position_max_value_mean",
                "position_gap_mean",
                "position_even_odd_ratio",
                "position_low_high_ratio",
            ]
            return default_vector, default_names

    def extract_overlap_time_gap_features(
        self, full_analysis: Dict[str, Any]
    ) -> Tuple[np.ndarray, List[str]]:
        """
        ì¤‘ë³µ íŒ¨í„´ ì‹œê°„ì  ì£¼ê¸°ì„± ë¶„ì„ ê²°ê³¼ë¥¼ ë²¡í„°í™”í•©ë‹ˆë‹¤.

        Args:
            full_analysis: ì „ì²´ ë¶„ì„ ê²°ê³¼

        Returns:
            Tuple[np.ndarray, List[str]]: ì‹œê°„ì  ì£¼ê¸°ì„± ë²¡í„°ì™€ íŠ¹ì„± ì´ë¦„
        """
        try:
            overlap_time_data = full_analysis.get("overlap_time_gaps", {})

            if not overlap_time_data:
                self.logger.warning(
                    "ì¤‘ë³µ íŒ¨í„´ ì‹œê°„ì  ì£¼ê¸°ì„± ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤."
                )
                overlap_time_data = {}

            # ì‹œê°„ì  ì£¼ê¸°ì„± íŠ¹ì„± ì¶”ì¶œ (5ê°œ)
            features = []
            feature_names = []

            # 1. 3ë§¤ì¹˜/4ë§¤ì¹˜ í‰ê·  ê°„ê²©
            overlap_3_time_gap_mean = overlap_time_data.get(
                "overlap_3_time_gap_mean", 0.0
            )
            overlap_4_time_gap_mean = overlap_time_data.get(
                "overlap_4_time_gap_mean", 0.0
            )

            # ê°„ê²©ì„ 100íšŒì°¨ë¡œ ì •ê·œí™” (ë³´í†µ 10-50íšŒì°¨ ê°„ê²©)
            features.extend(
                [
                    np.clip(overlap_3_time_gap_mean / 100.0, 0, 1),
                    np.clip(overlap_4_time_gap_mean / 100.0, 0, 1),
                ]
            )

            feature_names.extend(["overlap_3_time_gap_mean", "overlap_4_time_gap_mean"])

            # 2. ê°„ê²© í‘œì¤€í¸ì°¨
            overlap_time_gap_stddev = overlap_time_data.get(
                "overlap_time_gap_stddev", 0.0
            )
            features.append(
                np.clip(overlap_time_gap_stddev / 50.0, 0, 1)
            )  # í‘œì¤€í¸ì°¨ 50ìœ¼ë¡œ ì •ê·œí™”
            feature_names.append("overlap_time_gap_stddev")

            # 3. ìµœê·¼ ì¤‘ë³µ ë°œìƒ íšŸìˆ˜
            recent_overlap_3_count = overlap_time_data.get("recent_overlap_3_count", 0)
            recent_overlap_4_count = overlap_time_data.get("recent_overlap_4_count", 0)

            features.extend(
                [
                    np.clip(recent_overlap_3_count / 10.0, 0, 1),  # ìµœëŒ€ 10íšŒë¡œ ì •ê·œí™”
                    np.clip(recent_overlap_4_count / 5.0, 0, 1),  # ìµœëŒ€ 5íšŒë¡œ ì •ê·œí™”
                ]
            )

            feature_names.extend(["recent_overlap_3_count", "recent_overlap_4_count"])

            # NaN/Inf ì²´í¬ ë° ìˆ˜ì •
            features = [self.safe_float_conversion(f) for f in features]
            time_gap_vector = np.array(features, dtype=np.float32)

            self.logger.info(
                f"ì¤‘ë³µ íŒ¨í„´ ì‹œê°„ì  ì£¼ê¸°ì„± ë²¡í„°í™” ì™„ë£Œ: {len(features)}ê°œ íŠ¹ì„±"
            )
            return time_gap_vector, feature_names

        except Exception as e:
            self.logger.error(f"ì¤‘ë³µ íŒ¨í„´ ì‹œê°„ì  ì£¼ê¸°ì„± ë²¡í„°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            # ê¸°ë³¸ê°’ ë°˜í™˜ - ì˜ë¯¸ìˆëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©
            default_vector = np.array([0.3, 0.2, 0.4, 0.1, 0.05], dtype=np.float32)
            default_names = [
                "overlap_3_time_gap_mean",
                "overlap_4_time_gap_mean",
                "overlap_time_gap_stddev",
                "recent_overlap_3_count",
                "recent_overlap_4_count",
            ]
            return default_vector, default_names

    def extract_conditional_interaction_features(
        self, full_analysis: Dict[str, Any]
    ) -> Tuple[np.ndarray, List[str]]:
        """
        ë²ˆí˜¸ ê°„ ì¡°ê±´ë¶€ ìƒí˜¸ì‘ìš© ë¶„ì„ ê²°ê³¼ë¥¼ ë²¡í„°í™”í•©ë‹ˆë‹¤.

        Args:
            full_analysis: ì „ì²´ ë¶„ì„ ê²°ê³¼

        Returns:
            Tuple[np.ndarray, List[str]]: ì¡°ê±´ë¶€ ìƒí˜¸ì‘ìš© ë²¡í„°ì™€ íŠ¹ì„± ì´ë¦„
        """
        try:
            conditional_data = full_analysis.get("conditional_interaction_features", {})

            if not conditional_data:
                self.logger.warning(
                    "ë²ˆí˜¸ ê°„ ì¡°ê±´ë¶€ ìƒí˜¸ì‘ìš© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤."
                )
                conditional_data = {}

            # ì¡°ê±´ë¶€ ìƒí˜¸ì‘ìš© íŠ¹ì„± ì¶”ì¶œ (3ê°œ)
            features = []
            feature_names = []

            # 1. ëŒë¦¼ ì ìˆ˜ (Attraction Score)
            number_attraction_score = conditional_data.get(
                "number_attraction_score", 0.0
            )
            features.append(np.clip(number_attraction_score, 0, 1))
            feature_names.append("number_attraction_score")

            # 2. íšŒí”¼ ì ìˆ˜ (Repulsion Score)
            number_repulsion_score = conditional_data.get("number_repulsion_score", 0.0)
            features.append(np.clip(number_repulsion_score, 0, 1))
            feature_names.append("number_repulsion_score")

            # 3. ì¡°ê±´ë¶€ ì˜ì¡´ì„± ê°•ë„
            conditional_dependency_strength = conditional_data.get(
                "conditional_dependency_strength", 0.0
            )
            features.append(np.clip(conditional_dependency_strength, 0, 1))
            feature_names.append("conditional_dependency_strength")

            # NaN/Inf ì²´í¬ ë° ìˆ˜ì •
            features = [self.safe_float_conversion(f) for f in features]
            conditional_vector = np.array(features, dtype=np.float32)

            self.logger.info(
                f"ë²ˆí˜¸ ê°„ ì¡°ê±´ë¶€ ìƒí˜¸ì‘ìš© ë²¡í„°í™” ì™„ë£Œ: {len(features)}ê°œ íŠ¹ì„±"
            )
            return conditional_vector, feature_names

        except Exception as e:
            self.logger.error(f"ë²ˆí˜¸ ê°„ ì¡°ê±´ë¶€ ìƒí˜¸ì‘ìš© ë²¡í„°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            # ê¸°ë³¸ê°’ ë°˜í™˜
            default_vector = np.zeros(3, dtype=np.float32)
            default_names = [
                "number_attraction_score",
                "number_repulsion_score",
                "conditional_dependency_strength",
            ]
            return default_vector, default_names

    def extract_micro_bias_features(
        self, full_analysis: Dict[str, Any]
    ) -> Tuple[np.ndarray, List[str]]:
        """
        í™€ì§ ë° êµ¬ê°„ë³„ ë¯¸ì„¸ í¸í–¥ì„± ë¶„ì„ ê²°ê³¼ë¥¼ ë²¡í„°í™”í•©ë‹ˆë‹¤.

        Args:
            full_analysis: ì „ì²´ ë¶„ì„ ê²°ê³¼

        Returns:
            Tuple[np.ndarray, List[str]]: ë¯¸ì„¸ í¸í–¥ì„± ë²¡í„°ì™€ íŠ¹ì„± ì´ë¦„
        """
        try:
            odd_even_data = full_analysis.get("odd_even_micro_bias", {})
            range_data = full_analysis.get("range_micro_bias", {})

            if not odd_even_data:
                self.logger.warning(
                    "í™€ì§ ë¯¸ì„¸ í¸í–¥ì„± ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤."
                )
                odd_even_data = {}

            if not range_data:
                self.logger.warning(
                    "êµ¬ê°„ë³„ ë¯¸ì„¸ í¸í–¥ì„± ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤."
                )
                range_data = {}

            # ë¯¸ì„¸ í¸í–¥ì„± íŠ¹ì„± ì¶”ì¶œ (4ê°œ)
            features = []
            feature_names = []

            # 1. í™€ì§ í¸í–¥ ì ìˆ˜
            odd_even_bias_score = odd_even_data.get("odd_even_bias_score", 0.0)
            features.append(np.clip(odd_even_bias_score, 0, 1))
            feature_names.append("odd_even_bias_score")

            # 2. êµ¬ê°„ ê· í˜• í¸í–¥ ì ìˆ˜
            segment_balance_bias_score = range_data.get(
                "segment_balance_bias_score", 0.0
            )
            features.append(np.clip(segment_balance_bias_score, 0, 1))
            feature_names.append("segment_balance_bias_score")

            # 3. êµ¬ê°„ í¸í–¥ ì´ë™ í‰ê· 
            range_bias_moving_avg = range_data.get("range_bias_moving_avg", 0.0)
            features.append(np.clip(range_bias_moving_avg, 0, 1))
            feature_names.append("range_bias_moving_avg")

            # 4. í™€ì§ ë³€í™”ìœ¨ (ìµœê·¼ vs ê³¼ê±°)
            odd_ratio_change_rate = odd_even_data.get("odd_ratio_change_rate", 0.0)
            # ë³€í™”ìœ¨ì„ [-0.5, 0.5] ë²”ìœ„ë¡œ ê°€ì •í•˜ê³  [0, 1]ë¡œ ì •ê·œí™”
            features.append(np.clip((odd_ratio_change_rate + 0.5), 0, 1))
            feature_names.append("odd_ratio_change_rate")

            # NaN/Inf ì²´í¬ ë° ìˆ˜ì •
            features = [self.safe_float_conversion(f) for f in features]
            micro_bias_vector = np.array(features, dtype=np.float32)

            self.logger.info(f"ë¯¸ì„¸ í¸í–¥ì„± ë²¡í„°í™” ì™„ë£Œ: {len(features)}ê°œ íŠ¹ì„±")
            return micro_bias_vector, feature_names

        except Exception as e:
            self.logger.error(f"ë¯¸ì„¸ í¸í–¥ì„± ë²¡í„°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            # ê¸°ë³¸ê°’ ë°˜í™˜
            default_vector = np.zeros(4, dtype=np.float32)
            default_names = [
                "odd_even_bias_score",
                "segment_balance_bias_score",
                "range_bias_moving_avg",
                "odd_ratio_change_rate",
            ]
            return default_vector, default_names
