"""
íŒ¨í„´ ë²¡í„°í™” ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ íŒ¨í„´ ë¶„ì„ ê²°ê³¼ë¥¼ ë²¡í„°í™”í•˜ì—¬ ML/DL ëª¨ë¸ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.
"""

import numpy as np
import os
import logging
import hashlib
import json
import time
from pathlib import Path
from typing import Dict, List, Any, Optional, Union, Tuple, Set
from datetime import datetime
from collections import Counter
import gc
import torch

from ..utils.error_handler_refactored import get_logger
from ..utils.unified_performance import performance_monitor
from ..utils.unified_config import ConfigProxy
from ..utils.normalizer import Normalizer
from ..shared.types import LotteryNumber, PatternAnalysis, PatternFeatures
from ..shared.graph_utils import calculate_segment_entropy

# type: ignore ì£¼ì„ ì¶”ê°€
# ë¡œê±° ì„¤ì •
logger = get_logger(__name__)


class PatternVectorizer:
    """
    íŒ¨í„´ íŠ¹ì„±ì„ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” í´ë˜ìŠ¤

    ë‹¤ì–‘í•œ íŒ¨í„´ ë¶„ì„ ê²°ê³¼ë¥¼ ë¨¸ì‹ ëŸ¬ë‹/ë”¥ëŸ¬ë‹ ëª¨ë¸ì´ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ”
    íŠ¹ì„± ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """

    def __init__(self, config: Optional[Union[Dict[str, Any], ConfigProxy]] = None):
        """
        ì´ˆê¸°í™”

        Args:
            config: ì„¤ì • ê°ì²´
        """
        self.config = config if config is not None else {}
        self.logger = get_logger(__name__)

        # ë°ì´í„° ë¡œë” ì´ˆê¸°í™” (ìºì‹œëœ ë°ì´í„° ì ‘ê·¼ìš©)
        self._data_loader = None
        self._latest_draw_count = None  # ìºì‹œëœ ìµœì‹  íšŒì°¨ ìˆ˜

        # ğŸš€ ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        try:
            from ..utils.memory_manager import MemoryManager, MemoryConfig
            from ..utils.cuda_optimizers import get_cuda_optimizer, CudaConfig

            # ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™”
            memory_config = MemoryConfig(
                max_memory_usage=0.8,
                use_memory_pooling=True,
                pool_size=32,
            )
            self.memory_manager = MemoryManager(memory_config)

            # CUDA ìµœì í™” ì´ˆê¸°í™” (ë²¡í„°í™” ì‘ì—…ì— íŠ¹í™”)
            cuda_config = CudaConfig(
                use_amp=False,  # ë²¡í„°í™”ì—ì„œëŠ” ì •í™•ë„ ìš°ì„ 
                batch_size=32,
                use_cudnn=True,
            )
            self.cuda_optimizer = get_cuda_optimizer(cuda_config)

            self.logger.info("âœ… PatternVectorizer ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")

        except Exception as e:
            self.logger.warning(f"ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.memory_manager = None
            self.cuda_optimizer = None

        # ìºì‹œ ì„¤ì •
        try:
            self.use_cache = self.config["vectorizer"]["use_cache"]
        except (KeyError, TypeError):
            self.logger.warning(
                "ì„¤ì •ì—ì„œ 'vectorizer.use_cache'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’(True)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
            )
            self.use_cache = True

        try:
            self.cache_dir = Path(self.config["paths"]["cache_dir"])
        except (KeyError, TypeError):
            self.logger.warning(
                "ì„¤ì •ì—ì„œ 'paths.cache_dir'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’('data/cache')ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
            )
            self.cache_dir = Path("data/cache")

        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # ì €ë¶„ì‚° íŠ¹ì„± ì œê±° ì„¤ì •
        try:
            self.remove_low_variance = self.config["filtering"][
                "remove_low_variance_features"
            ]
        except (KeyError, TypeError):
            self.logger.warning(
                "ì„¤ì •ì—ì„œ 'filtering.remove_low_variance_features'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’(False)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
            )
            self.remove_low_variance = False

        try:
            self.variance_threshold = self.config["filtering"]["variance_threshold"]
        except (KeyError, TypeError):
            self.logger.warning(
                "ì„¤ì •ì—ì„œ 'filtering.variance_threshold'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’(0.01)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
            )
            self.variance_threshold = 0.01

        # ì œê±°ëœ ì €ë¶„ì‚° íŠ¹ì„± ì´ë¦„ ì €ì¥
        self.removed_low_variance_features = []

        # ë²¡í„° ìºì‹œ
        self._pattern_cache = {}
        self._vector_cache = {}

        # íŠ¹ì„± ì´ë¦„ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
        self.feature_names = []

        # ìºì‹œ ì ì¬ ì‹œë„
        if self.use_cache:
            self._load_cache()

        logger.info(
            f"PatternVectorizer ì´ˆê¸°í™” ì™„ë£Œ (ìºì‹œ ì‚¬ìš©: {self.use_cache}, ì €ë¶„ì‚° íŠ¹ì„± ì œê±°: {self.remove_low_variance})"
        )

        # ì •ê·œí™” ìœ í‹¸ë¦¬í‹° ì´ˆê¸°í™”
        self.normalizer = Normalizer(self.config)

        # íŠ¹ì„± ë²¡í„° ìºì‹œ ì´ˆê¸°í™”
        from ..utils.state_vector_cache import get_cache

        self.vector_cache = get_cache(self.config)

        # ë²¡í„° ì°¨ì› ì²­ì‚¬ì§„ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self._initialize_vector_blueprint()

        # ì„±ëŠ¥ ì¶”ì ê¸° ì´ˆê¸°í™”
        from ..utils.unified_performance import get_performance_manager

        self.performance_tracker = get_performance_manager()

        logger.info(
            f"ë²¡í„° ì²­ì‚¬ì§„ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ: ì´ {self.total_expected_dims}ì°¨ì›"
        )

    def _get_latest_draw_count(self) -> int:
        """
        ìºì‹œëœ ë°ì´í„°ì—ì„œ ìµœì‹  íšŒì°¨ ìˆ˜ë¥¼ ë™ì ìœ¼ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤.

        Returns:
            int: ìµœì‹  íšŒì°¨ ìˆ˜
        """
        try:
            # ë°ì´í„° ë¡œë”ê°€ ì—†ìœ¼ë©´ ì´ˆê¸°í™”
            if self._data_loader is None:
                from ..utils.data_loader import DataLoader

                # ConfigProxy ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•´ ê°„ë‹¨í•œ ì„¤ì •ë§Œ ì „ë‹¬
                simple_config = {}
                try:
                    # ConfigProxyë‚˜ dict íƒ€ì…ì— ê´€ê³„ì—†ì´ ì•ˆì „í•˜ê²Œ ì ‘ê·¼
                    if hasattr(self.config, "__getitem__"):
                        data_config = (
                            self.config.get("data")
                            if hasattr(self.config, "get")
                            else self.config.get("data", None)
                        )
                        if data_config:
                            simple_config = {"data": data_config}
                except (TypeError, AttributeError, KeyError):
                    pass  # ì„¤ì • ì ‘ê·¼ ì‹¤íŒ¨ ì‹œ ë¹ˆ ì„¤ì • ì‚¬ìš©
                self._data_loader = DataLoader(simple_config)

            # ìºì‹œëœ ìµœì‹  íšŒì°¨ ìˆ˜ê°€ ì—†ê±°ë‚˜ ì˜¤ë˜ëœ ê²½ìš° ê°±ì‹ 
            if self._latest_draw_count is None:
                all_data = self._data_loader.get_all_data()
                if all_data:
                    self._latest_draw_count = len(all_data)
                    self.logger.info(
                        f"ìµœì‹  íšŒì°¨ ìˆ˜ ìºì‹œ ê°±ì‹ : {self._latest_draw_count}íšŒ"
                    )
                else:
                    self._latest_draw_count = 1172  # ê¸°ë³¸ê°’ (ì•ˆì „ì¥ì¹˜)
                    self.logger.warning("ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: 1172íšŒ")

            return self._latest_draw_count

        except Exception as e:
            self.logger.error(f"ìµœì‹  íšŒì°¨ ìˆ˜ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            # ì•ˆì „ì¥ì¹˜: ì§ì ‘ CSV íŒŒì¼ì—ì„œ ë¼ì¸ ìˆ˜ í™•ì¸
            try:
                from pathlib import Path

                csv_path = (
                    Path(__file__).parent.parent.parent / "data" / "raw" / "lottery.csv"
                )
                if csv_path.exists():
                    with open(csv_path, "r", encoding="utf-8") as f:
                        line_count = sum(1 for _ in f) - 1  # í—¤ë” ì œì™¸
                    self.logger.info(f"CSV íŒŒì¼ì—ì„œ ì§ì ‘ íšŒì°¨ ìˆ˜ í™•ì¸: {line_count}íšŒ")
                    return line_count
            except Exception as csv_error:
                self.logger.error(f"CSV íŒŒì¼ ì§ì ‘ ì½ê¸° ì‹¤íŒ¨: {csv_error}")

            return 1172  # ìµœì¢… ê¸°ë³¸ê°’ ë°˜í™˜

    def refresh_draw_count_cache(self) -> None:
        """
        íšŒì°¨ ìˆ˜ ìºì‹œë¥¼ ê°•ì œë¡œ ê°±ì‹ í•©ë‹ˆë‹¤.
        ìƒˆë¡œìš´ íšŒì°¨ ë°ì´í„°ê°€ ì¶”ê°€ë˜ì—ˆì„ ë•Œ í˜¸ì¶œí•˜ì„¸ìš”.
        """
        try:
            self._latest_draw_count = None  # ìºì‹œ ë¬´íš¨í™”
            new_count = self._get_latest_draw_count()  # ìƒˆë¡œ ë¡œë“œ
            self.logger.info(f"íšŒì°¨ ìˆ˜ ìºì‹œ ê°•ì œ ê°±ì‹  ì™„ë£Œ: {new_count}íšŒ")
        except Exception as e:
            self.logger.error(f"íšŒì°¨ ìˆ˜ ìºì‹œ ê°±ì‹  ì‹¤íŒ¨: {e}")

    def _initialize_vector_blueprint(self):
        """
        ë²¡í„° ì°¨ì› ì²­ì‚¬ì§„ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        ê° ê·¸ë£¹ë³„ ê³ ì • ì°¨ì›ì„ ì‚¬ì „ ì •ì˜í•˜ì—¬ ì°¨ì› ë¶ˆì¼ì¹˜ ë¬¸ì œ í•´ê²°
        """
        # ê·¸ë£¹ë³„ ê³ ì • ì°¨ì› ì •ì˜ (ì´ 95ì°¨ì›ìœ¼ë¡œ í‘œì¤€í™”)
        self.vector_blueprint = {
            # ê¸°ë³¸ íŒ¨í„´ ë¶„ì„ (25ì°¨ì›)
            "pattern_analysis": 25,
            # ë¶„í¬ íŒ¨í„´ (10ì°¨ì›)
            "distribution_pattern": 10,
            # ì„¸ê·¸ë¨¼íŠ¸ ë¹ˆë„ (15ì°¨ì›: 10êµ¬ê°„ + 5êµ¬ê°„)
            "segment_frequency": 15,
            # ì¤‘ì‹¬ì„± ë° ì—°ì†ì„± (12ì°¨ì›)
            "centrality_consecutive": 12,
            # ê°­ í†µê³„ ë° ì¬ì¶œí˜„ (8ì°¨ì›)
            "gap_reappearance": 8,
            # ROI íŠ¹ì„± (15ì°¨ì›)
            "roi_features": 15,
            # í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ (10ì°¨ì›)
            "cluster_features": 10,
            # ì¤‘ë³µ íŒ¨í„´ íŠ¹ì„± (20ì°¨ì›)
            "overlap_patterns": 20,
            # ë¬¼ë¦¬ì  êµ¬ì¡° íŠ¹ì„± (11ì°¨ì›)
            "physical_structure": 11,
            # ìŒ ê·¸ë˜í”„ ì••ì¶• ë²¡í„° (ìµœëŒ€ 20ì°¨ì›)
            "pair_graph_vector": 20,
        }

        # ì´ ì˜ˆìƒ ì°¨ì› ê³„ì‚°
        self.total_expected_dims = sum(self.vector_blueprint.values())

        # ê·¸ë£¹ë³„ íŠ¹ì„± ì´ë¦„ í…œí”Œë¦¿
        self.feature_name_templates = {
            "pattern_analysis": [f"pattern_{i+1}" for i in range(25)],
            "distribution_pattern": [f"dist_{i+1}" for i in range(10)],
            "segment_frequency": [f"seg10_{i+1}" for i in range(10)]
            + [f"seg5_{i+1}" for i in range(5)],
            "centrality_consecutive": [f"centrality_{i+1}" for i in range(6)]
            + [f"consecutive_{i+1}" for i in range(6)],
            "gap_reappearance": [f"gap_{i+1}" for i in range(4)]
            + [f"reappear_{i+1}" for i in range(4)],
            "roi_features": [f"roi_{i+1}" for i in range(15)],
            "cluster_features": [f"cluster_{i+1}" for i in range(10)],
            "overlap_patterns": [f"overlap_{i+1}" for i in range(20)],
            "physical_structure": [f"physical_{i+1}" for i in range(11)],
            "pair_graph_vector": [f"pair_graph_{i+1}" for i in range(20)],
        }

        self.logger.info(
            f"ë²¡í„° ì²­ì‚¬ì§„ ì •ì˜ ì™„ë£Œ: {len(self.vector_blueprint)}ê°œ ê·¸ë£¹, ì´ {self.total_expected_dims}ì°¨ì›"
        )

    def _vectorize_group_safe(
        self, group_name: str, data: Any, vectorize_func
    ) -> Tuple[np.ndarray, List[str]]:
        """
        ê·¸ë£¹ë³„ ì•ˆì „í•œ ë²¡í„°í™” ìˆ˜í–‰
        ì˜ˆìƒ ì°¨ì›ê³¼ ì¼ì¹˜í•˜ì§€ ì•Šìœ¼ë©´ íŒ¨ë”© ë˜ëŠ” ì ˆë‹¨í•˜ì—¬ ì°¨ì› ë³´ì¥

        Args:
            group_name: ê·¸ë£¹ ì´ë¦„
            data: ë²¡í„°í™”í•  ë°ì´í„°
            vectorize_func: ë²¡í„°í™” í•¨ìˆ˜

        Returns:
            Tuple[np.ndarray, List[str]]: ì°¨ì›ì´ ë³´ì¥ëœ ë²¡í„°ì™€ íŠ¹ì„± ì´ë¦„
        """
        try:
            # ë²¡í„°í™” ìˆ˜í–‰
            if vectorize_func is None:
                # ê¸°ë³¸ ë²¡í„°í™” (ëª¨ë“  ê°’ì„ 0ìœ¼ë¡œ)
                vector = np.zeros(self.vector_blueprint[group_name], dtype=np.float32)
                feature_names = self.feature_name_templates[group_name][
                    : self.vector_blueprint[group_name]
                ]
            else:
                result = vectorize_func(data)
                if isinstance(result, tuple) and len(result) == 2:
                    vector, feature_names = result
                elif isinstance(result, np.ndarray):
                    vector = result
                    feature_names = [f"{group_name}_{i+1}" for i in range(len(vector))]
                else:
                    # ì˜ˆìƒì¹˜ ëª»í•œ ë°˜í™˜ í˜•íƒœ
                    self.logger.warning(
                        f"ê·¸ë£¹ '{group_name}': ì˜ˆìƒì¹˜ ëª»í•œ ë°˜í™˜ í˜•íƒœ {type(result)}"
                    )
                    vector = np.zeros(
                        self.vector_blueprint[group_name], dtype=np.float32
                    )
                    feature_names = self.feature_name_templates[group_name][
                        : self.vector_blueprint[group_name]
                    ]

            expected_dims = self.vector_blueprint.get(group_name, len(vector))

            # ì°¨ì› ì¡°ì •
            vector, feature_names = self._pad_or_truncate_vector(
                vector, feature_names, expected_dims, group_name
            )

            # NaN/Inf ê²€ì¦ ë° ì²˜ë¦¬
            vector = self._sanitize_vector(vector, group_name)

            self.logger.debug(f"ê·¸ë£¹ '{group_name}' ë²¡í„°í™” ì™„ë£Œ: {len(vector)}ì°¨ì›")
            return vector, feature_names

        except Exception as e:
            self.logger.warning(f"ê·¸ë£¹ '{group_name}' ë²¡í„°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ ë²¡í„° ë°˜í™˜
            expected_dims = self.vector_blueprint.get(group_name, 10)
            vector = np.zeros(expected_dims, dtype=np.float32)
            feature_names = self.feature_name_templates.get(
                group_name, [f"{group_name}_{i+1}" for i in range(expected_dims)]
            )[:expected_dims]
            return vector, feature_names

    def _pad_or_truncate_vector(
        self,
        vector: np.ndarray,
        feature_names: List[str],
        expected_dims: int,
        group_name: str,
    ) -> Tuple[np.ndarray, List[str]]:
        """
        ë²¡í„°ë¥¼ ì˜ˆìƒ ì°¨ì›ì— ë§ê²Œ íŒ¨ë”© ë˜ëŠ” ì ˆë‹¨

        Args:
            vector: ì›ë³¸ ë²¡í„°
            feature_names: ì›ë³¸ íŠ¹ì„± ì´ë¦„
            expected_dims: ì˜ˆìƒ ì°¨ì›
            group_name: ê·¸ë£¹ ì´ë¦„

        Returns:
            Tuple[np.ndarray, List[str]]: ì¡°ì •ëœ ë²¡í„°ì™€ íŠ¹ì„± ì´ë¦„
        """
        current_dims = len(vector)

        if current_dims == expected_dims:
            return vector, feature_names
        elif current_dims < expected_dims:
            # íŒ¨ë”© (ë¶€ì¡±í•œ ì°¨ì›ì„ 0ìœ¼ë¡œ ì±„ì›€)
            padding_size = expected_dims - current_dims
            padded_vector = np.pad(
                vector, (0, padding_size), mode="constant", constant_values=0.0
            )

            # íŠ¹ì„± ì´ë¦„ë„ íŒ¨ë”©
            template_names = self.feature_name_templates.get(
                group_name, [f"{group_name}_{i+1}" for i in range(expected_dims)]
            )
            padded_names = feature_names + template_names[current_dims:expected_dims]

            self.logger.debug(
                f"ê·¸ë£¹ '{group_name}': {current_dims}ì°¨ì› â†’ {expected_dims}ì°¨ì›ìœ¼ë¡œ íŒ¨ë”©"
            )
            return padded_vector.astype(np.float32), padded_names
        else:
            # ì ˆë‹¨ (ì´ˆê³¼ ì°¨ì› ì œê±°)
            truncated_vector = vector[:expected_dims]
            truncated_names = feature_names[:expected_dims]

            self.logger.debug(
                f"ê·¸ë£¹ '{group_name}': {current_dims}ì°¨ì› â†’ {expected_dims}ì°¨ì›ìœ¼ë¡œ ì ˆë‹¨"
            )
            return truncated_vector.astype(np.float32), truncated_names

    def _sanitize_vector(self, vector: np.ndarray, group_name: str) -> np.ndarray:
        """
        ë²¡í„°ì˜ NaN/Inf ê°’ì„ ì²˜ë¦¬í•˜ì—¬ ì•ˆì „í•œ ë²¡í„°ë¡œ ë³€í™˜

        Args:
            vector: ì›ë³¸ ë²¡í„°
            group_name: ê·¸ë£¹ ì´ë¦„

        Returns:
            np.ndarray: ì •ì œëœ ë²¡í„°
        """
        # NaNì„ 0ìœ¼ë¡œ ëŒ€ì²´
        nan_count = np.isnan(vector).sum()
        if nan_count > 0:
            vector = np.nan_to_num(vector, nan=0.0)
            self.logger.debug(f"ê·¸ë£¹ '{group_name}': {nan_count}ê°œ NaN ê°’ì„ 0ìœ¼ë¡œ ëŒ€ì²´")

        # Infë¥¼ ìœ í•œí•œ ê°’ìœ¼ë¡œ ëŒ€ì²´
        inf_count = np.isinf(vector).sum()
        if inf_count > 0:
            vector = np.nan_to_num(vector, posinf=1.0, neginf=-1.0)
            self.logger.debug(
                f"ê·¸ë£¹ '{group_name}': {inf_count}ê°œ Inf ê°’ì„ ìœ í•œê°’ìœ¼ë¡œ ëŒ€ì²´"
            )

        # ê°’ ë²”ìœ„ ì œí•œ (-10 ~ 10)
        vector = np.clip(vector, -10.0, 10.0)

        return vector.astype(np.float32)

    def validate_vector_integrity(
        self, vector: np.ndarray, feature_names: List[str]
    ) -> bool:
        """
        ë²¡í„° ë¬´ê²°ì„± ê²€ì¦

        Args:
            vector: ê²€ì¦í•  ë²¡í„°
            feature_names: íŠ¹ì„± ì´ë¦„ ë¦¬ìŠ¤íŠ¸

        Returns:
            bool: ê²€ì¦ í†µê³¼ ì—¬ë¶€
        """
        try:
            # ì°¨ì› ì¼ì¹˜ í™•ì¸ (ê²½ê³ ë§Œ ì¶œë ¥, ì‹¤íŒ¨ë¡œ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ)
            if len(vector) != len(feature_names):
                self.logger.warning(
                    f"ë²¡í„° ì°¨ì›({len(vector)})ê³¼ íŠ¹ì„± ì´ë¦„ ìˆ˜({len(feature_names)})ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"
                )
                if len(feature_names) == 0:
                    self.logger.warning(
                        "íŠ¹ì„± ì´ë¦„ì´ ì „í˜€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì´ë¦„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
                    )
                elif len(vector) > len(feature_names):
                    diff = len(vector) - len(feature_names)
                    self.logger.warning(
                        f"ì°¨ì› ë¶ˆì¼ì¹˜ ì •ë„: {diff}ê°œ ({diff/len(vector)*100:.2f}%)"
                    )
                    self.logger.warning(
                        f"ë²¡í„° ì°¨ì›ì´ íŠ¹ì„± ì´ë¦„ ìˆ˜ë³´ë‹¤ {diff}ê°œ ë” ë§ìŠµë‹ˆë‹¤. ì´ë¦„ ëª©ë¡ì„ í™•ì¥í•©ë‹ˆë‹¤."
                    )

            # ì˜ˆìƒ ì´ ì°¨ì›ê³¼ ì¼ì¹˜ í™•ì¸
            if len(vector) != self.total_expected_dims:
                self.logger.error(
                    f"ì´ ì°¨ì› ë¶ˆì¼ì¹˜: ì˜ˆìƒ {self.total_expected_dims}ì°¨ì› vs ì‹¤ì œ {len(vector)}ì°¨ì›"
                )
                return False

            # NaN/Inf ë¹„ìœ¨ í™•ì¸ (1% ì´ˆê³¼ ì‹œ ì‹¤íŒ¨)
            nan_inf_ratio = (np.isnan(vector).sum() + np.isinf(vector).sum()) / len(
                vector
            )
            if nan_inf_ratio > 0.01:
                self.logger.error(f"NaN/Inf ë¹„ìœ¨ ì´ˆê³¼: {nan_inf_ratio:.4f} > 0.01")
                return False

            # 0 ê°’ ë¹„ìœ¨ ê²€ì¦
            zero_ratio = np.sum(vector == 0) / len(vector)
            if zero_ratio > 0.8:
                self.logger.warning(
                    f"ë²¡í„°ì˜ {zero_ratio:.1%}ê°€ 0ê°’ì…ë‹ˆë‹¤. íŠ¹ì„± ì¶”ì¶œì„ ê°œì„ í•´ì•¼ í•©ë‹ˆë‹¤."
                )

            # íŠ¹ì„± ë‹¤ì–‘ì„± ì—”íŠ¸ë¡œí”¼ ê³„ì‚° (ì •ë³´ í’ˆì§ˆ ê²€ì¦)
            entropy_score = self._calculate_feature_entropy(vector)
            if entropy_score < 1.0:
                self.logger.info(
                    f"íŠ¹ì„± ë‹¤ì–‘ì„± ë‚®ìŒ: ì—”íŠ¸ë¡œí”¼ {entropy_score:.4f} (ê°œì„  ê¶Œì¥)"
                )
            else:
                self.logger.info(f"íŠ¹ì„± ë‹¤ì–‘ì„± ì–‘í˜¸: ì—”íŠ¸ë¡œí”¼ {entropy_score:.4f}")

            self.logger.info(
                f"ë²¡í„° ë¬´ê²°ì„± ê²€ì¦ í†µê³¼: {len(vector)}ì°¨ì›, 0ê°’ ë¹„ìœ¨: {zero_ratio:.1%}"
            )
            return True

        except Exception as e:
            self.logger.error(f"ë²¡í„° ë¬´ê²°ì„± ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def _calculate_feature_entropy(self, vector: np.ndarray) -> float:
        """
        íŠ¹ì„± ë²¡í„°ì˜ ì •ë³´ ì—”íŠ¸ë¡œí”¼ ê³„ì‚°

        Args:
            vector: íŠ¹ì„± ë²¡í„°

        Returns:
            float: ì—”íŠ¸ë¡œí”¼ ì ìˆ˜ (0~1)
        """
        try:
            # ë²¡í„°ë¥¼ íˆìŠ¤í† ê·¸ë¨ìœ¼ë¡œ ë³€í™˜
            hist, _ = np.histogram(vector, bins=50, density=True)

            # 0ì´ ì•„ë‹Œ í™•ë¥ ë§Œ ì‚¬ìš©
            hist = hist[hist > 0]

            if len(hist) == 0:
                return 0.0

            # ì—”íŠ¸ë¡œí”¼ ê³„ì‚°
            entropy = -np.sum(hist * np.log2(hist + 1e-10))

            # 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”
            max_entropy = np.log2(len(hist))
            normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0.0

            return float(normalized_entropy)

        except Exception as e:
            self.logger.warning(f"ì—”íŠ¸ë¡œí”¼ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
            return 0.0

    def _get_config_value(self, key_path: str, default_value: Any) -> Any:
        """ì„¤ì • ê°’ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        # ConfigProxy ê°ì²´ì¸ ê²½ìš°
        if hasattr(self.config, "get") or hasattr(self.config, "__getitem__"):
            try:
                # key_pathë¥¼ ë¶„í•´í•˜ì—¬ ì§ì ‘ ì ‘ê·¼
                keys = key_path.split(".")
                value = self.config
                for k in keys:
                    value = value[k]
                return value
            except (KeyError, TypeError):
                self.logger.warning(
                    f"ì„¤ì •ì—ì„œ '{key_path}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
                )
                return default_value
        # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš°
        elif isinstance(self.config, dict):
            # ì¤‘ì²©ëœ í‚¤ ì²˜ë¦¬
            keys = key_path.split(".")
            value = self.config
            for k in keys:
                if isinstance(value, dict) and k in value:
                    value = value[k]
                else:
                    return default_value
            return value
        # ê¸°íƒ€ ê²½ìš°
        return default_value

    def _compute_pattern_hash(self, pattern_data: Dict[str, Any]) -> str:
        """
        íŒ¨í„´ ë°ì´í„°ì˜ í•´ì‹œ ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

        Args:
            pattern_data: íŒ¨í„´ ë°ì´í„° ë”•ì…”ë„ˆë¦¬

        Returns:
            íŒ¨í„´ ë°ì´í„°ì˜ ê³ ìœ  í•´ì‹œ ê°’
        """
        try:
            # ì •ë ¬ëœ í‚¤ë¡œ JSON ë¬¸ìì—´ ìƒì„±
            pattern_json = json.dumps(pattern_data, sort_keys=True)
            # SHA-256 í•´ì‹œ ê³„ì‚°
            return hashlib.sha256(pattern_json.encode()).hexdigest()
        except Exception as e:
            self.logger.warning(f"íŒ¨í„´ í•´ì‹œ ê³„ì‚° ì‹¤íŒ¨: {e}")
            # í˜„ì¬ ì‹œê°„ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ í´ë°± í•´ì‹œ
            return f"fallback_{int(time.time())}"

    def _compute_numbers_hash(self, numbers: List[int]) -> str:
        """
        ë²ˆí˜¸ ì¡°í•©ì˜ í•´ì‹œ ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

        Args:
            numbers: ë²ˆí˜¸ ì¡°í•© ë¦¬ìŠ¤íŠ¸

        Returns:
            ë²ˆí˜¸ ì¡°í•©ì˜ ê³ ìœ  í•´ì‹œ ê°’
        """
        try:
            # ì •ë ¬ëœ ë²ˆí˜¸ ëª©ë¡ì˜ ë¬¸ìì—´
            numbers_str = "_".join(map(str, sorted(numbers)))
            # SHA-256 í•´ì‹œ ê³„ì‚°
            return hashlib.sha256(numbers_str.encode()).hexdigest()
        except Exception as e:
            self.logger.warning(f"ë²ˆí˜¸ í•´ì‹œ ê³„ì‚° ì‹¤íŒ¨: {e}")
            # í˜„ì¬ ì‹œê°„ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ í´ë°± í•´ì‹œ
            return f"fallback_{int(time.time())}"

    def _create_cache_key(self, config_section: Dict[str, Any]) -> str:
        """
        config ì„¹ì…˜ì„ JSON ì§ë ¬í™”í•˜ì—¬ SHA1 í•´ì‹œ í‚¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        ë™ì¼ ì„¤ì •ì´ë©´ ë™ì¼ í‚¤, ì„¤ì •ì´ ë‹¤ë¥´ë©´ ë‹¤ë¥¸ ìºì‹œ í‚¤ ìƒì„±.

        Args:
            config_section: ì„¤ì • ì„¹ì…˜ ë”•ì…”ë„ˆë¦¬

        Returns:
            str: 10ìë¦¬ SHA1 í•´ì‹œ í‚¤
        """
        config_str = json.dumps(config_section, sort_keys=True)
        return hashlib.sha1(config_str.encode("utf-8")).hexdigest()[:10]

    def _load_cache(self) -> None:
        """ìºì‹œ íŒŒì¼ì—ì„œ ë²¡í„° ìºì‹œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            # íŒ¨í„´ ìºì‹œ ë¡œë“œ
            pattern_cache_path = self.cache_dir / "pattern_vector_cache.npy"
            if pattern_cache_path.exists():
                cached_data = np.load(pattern_cache_path, allow_pickle=True).item()
                if isinstance(cached_data, dict):
                    self._pattern_cache = cached_data
                    self.logger.info(
                        f"íŒ¨í„´ ë²¡í„° ìºì‹œ ë¡œë“œ ì™„ë£Œ: {len(self._pattern_cache)}ê°œ í•­ëª©"
                    )

            # ë²ˆí˜¸ ë²¡í„° ìºì‹œ ë¡œë“œ
            vector_cache_path = self.cache_dir / "number_vector_cache.npy"
            if vector_cache_path.exists():
                cached_data = np.load(vector_cache_path, allow_pickle=True).item()
                if isinstance(cached_data, dict):
                    self._vector_cache = cached_data
                    self.logger.info(
                        f"ë²ˆí˜¸ ë²¡í„° ìºì‹œ ë¡œë“œ ì™„ë£Œ: {len(self._vector_cache)}ê°œ í•­ëª©"
                    )

        except Exception as e:
            self.logger.warning(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self._pattern_cache = {}
            self._vector_cache = {}

    def _save_cache(self) -> None:
        """í˜„ì¬ ìºì‹œë¥¼ íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
        if not self.use_cache:
            return

        try:
            # íŒ¨í„´ ìºì‹œ ì €ì¥
            pattern_cache_path = self.cache_dir / "pattern_vector_cache.npy"
            np.save(
                pattern_cache_path, np.array([self._pattern_cache], dtype=object)[0]
            )

            # ë²ˆí˜¸ ë²¡í„° ìºì‹œ ì €ì¥
            vector_cache_path = self.cache_dir / "number_vector_cache.npy"
            np.save(vector_cache_path, np.array([self._vector_cache], dtype=object)[0])

            self.logger.info(
                f"ë²¡í„° ìºì‹œ ì €ì¥ ì™„ë£Œ (íŒ¨í„´: {len(self._pattern_cache)}ê°œ, ë²ˆí˜¸: {len(self._vector_cache)}ê°œ)"
            )
        except Exception as e:
            self.logger.warning(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

    def vectorize_pattern(
        self, pattern_data: Dict[str, Any], use_optimization: bool = True
    ) -> np.ndarray:
        """
        ìµœì í™”ëœ íŒ¨í„´ ë°ì´í„° ë²¡í„° ë³€í™˜

        Args:
            pattern_data: íŒ¨í„´ ë¶„ì„ ê²°ê³¼ ë°ì´í„°
            use_optimization: ìµœì í™” ì‚¬ìš© ì—¬ë¶€

        Returns:
            íŠ¹ì„± ë²¡í„°
        """
        if not use_optimization:
            return self._standard_vectorize_pattern(pattern_data)

        # ğŸ§  ë©”ëª¨ë¦¬ ê´€ë¦¬ ìŠ¤ì½”í”„ ì ìš©
        if self.memory_manager:
            with self.memory_manager.allocation_scope():
                return self._optimized_vectorize_pattern(pattern_data)
        else:
            return self._standard_vectorize_pattern(pattern_data)

    def _optimized_vectorize_pattern(self, pattern_data: Dict[str, Any]) -> np.ndarray:
        """ìµœì í™”ëœ íŒ¨í„´ ë²¡í„°í™”"""
        # ğŸ“Š ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        with performance_monitor("pattern_vectorization_optimized"):

            # ìºì‹œ í™•ì¸
            if self.use_cache:
                pattern_hash = self._compute_pattern_hash(pattern_data)
                if pattern_hash in self._pattern_cache:
                    return self._pattern_cache[pattern_hash]

            # ğŸš€ GPU ê°€ì† ë²¡í„°í™” ì‹œë„
            if (
                self.cuda_optimizer
                and self.cuda_optimizer.is_available()
                and self._should_use_gpu_vectorization(pattern_data)
            ):
                try:
                    return self._gpu_vectorize_pattern(pattern_data)
                except Exception as e:
                    self.logger.warning(f"GPU ë²¡í„°í™” ì‹¤íŒ¨: {e}, CPUë¡œ í´ë°±")

            # CPU ìµœì í™” ë²¡í„°í™”
            return self._cpu_vectorize_pattern_optimized(pattern_data)

    def _standard_vectorize_pattern(self, pattern_data: Dict[str, Any]) -> np.ndarray:
        """í‘œì¤€ íŒ¨í„´ ë²¡í„°í™” (ê¸°ì¡´ ë¡œì§)"""
        # ìºì‹œ ì‚¬ìš© ì‹œ ì´ë¯¸ ê³„ì‚°ëœ ë²¡í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        if self.use_cache:
            pattern_hash = self._compute_pattern_hash(pattern_data)
            if pattern_hash in self._pattern_cache:
                return self._pattern_cache[pattern_hash]

        # ë²¡í„° ìš”ì†Œ ì´ˆê¸°í™”
        vector_elements = []

        # 1. ë§ˆì§€ë§‰ ë‹¹ì²¨ ë²ˆí˜¸ì˜ ë¶„í¬ íŠ¹ì„±
        if "number_distribution" in pattern_data:
            dist_data = pattern_data["number_distribution"]

            # ë²”ìœ„ë³„ ë¶„í¬ (0-9, 10-19, 20-29, 30-39, 40-45)
            range_dist = dist_data.get("range_distribution", [0.2, 0.2, 0.2, 0.2, 0.2])
            vector_elements.extend(range_dist)

            # í™€ì§ ë¶„í¬
            odd_even_ratio = dist_data.get("odd_even_ratio", 0.5)
            vector_elements.append(odd_even_ratio)

            # ê³ ì € ë¶„í¬ (1-22, 23-45)
            high_low_ratio = dist_data.get("high_low_ratio", 0.5)
            vector_elements.append(high_low_ratio)
        else:
            # ê¸°ë³¸ê°’ ì¶”ê°€
            vector_elements.extend([0.2, 0.2, 0.2, 0.2, 0.2, 0.5, 0.5])

        # 2. í•©ê³„ ê´€ë ¨ íŠ¹ì„±
        if "sum_analysis" in pattern_data:
            sum_data = pattern_data["sum_analysis"]

            # ì •ê·œí™”ëœ í•©ê³„ (0-1 ë²”ìœ„)
            normalized_sum = sum_data.get("normalized_sum", 0.5)
            vector_elements.append(normalized_sum)

            # í‰ê· ê³¼ì˜ í¸ì°¨ (ì •ê·œí™”)
            deviation = sum_data.get("deviation_from_mean", 0.0)
            normalized_deviation = max(min(deviation / 50.0 + 0.5, 1.0), 0.0)
            vector_elements.append(normalized_deviation)
        else:
            # ê¸°ë³¸ê°’ ì¶”ê°€
            vector_elements.extend([0.5, 0.5])

        # 3. ê°„ê²© ê´€ë ¨ íŠ¹ì„±
        if "number_gaps" in pattern_data:
            gap_data = pattern_data["number_gaps"]

            # í‰ê·  ê°„ê²© (ì •ê·œí™”)
            avg_gap = gap_data.get("avg_gap", 7.5)
            normalized_avg_gap = max(min(avg_gap / 15.0, 1.0), 0.0)
            vector_elements.append(normalized_avg_gap)

            # ìµœëŒ€ ê°„ê²© (ì •ê·œí™”)
            max_gap = gap_data.get("max_gap", 15)
            normalized_max_gap = max(min(max_gap / 30.0, 1.0), 0.0)
            vector_elements.append(normalized_max_gap)

            # ìµœì†Œ ê°„ê²© (ì •ê·œí™”)
            min_gap = gap_data.get("min_gap", 1)
            normalized_min_gap = max(min(min_gap / 10.0, 1.0), 0.0)
            vector_elements.append(normalized_min_gap)
        else:
            # ê¸°ë³¸ê°’ ì¶”ê°€
            vector_elements.extend([0.5, 0.5, 0.1])

        # 4. ì—°ì† ë²ˆí˜¸ ê´€ë ¨ íŠ¹ì„±
        if "consecutive_numbers" in pattern_data:
            consecutive_data = pattern_data["consecutive_numbers"]

            # ì—°ì† ë²ˆí˜¸ ìˆ˜ (ì •ê·œí™”)
            count = consecutive_data.get("count", 0)
            normalized_count = min(count / 5.0, 1.0)
            vector_elements.append(normalized_count)
        else:
            # ê¸°ë³¸ê°’ ì¶”ê°€
            vector_elements.append(0.0)

        # 5. ê³¼ê±° ë‹¹ì²¨ ë²ˆí˜¸ì™€ì˜ ì¼ì¹˜ ê´€ë ¨ íŠ¹ì„±
        if "historical_match" in pattern_data:
            match_data = pattern_data["historical_match"]

            # ìµœëŒ€ ì¼ì¹˜ ìˆ˜ (ì •ê·œí™”)
            max_match = match_data.get("max_match", 0)
            normalized_max_match = min(max_match / 6.0, 1.0)
            vector_elements.append(normalized_max_match)

            # í‰ê·  ì¼ì¹˜ ìˆ˜ (ì •ê·œí™”)
            avg_match = match_data.get("avg_match", 0.0)
            normalized_avg_match = min(avg_match / 3.0, 1.0)
            vector_elements.append(normalized_avg_match)
        else:
            # ê¸°ë³¸ê°’ ì¶”ê°€
            vector_elements.extend([0.0, 0.0])

        # ë²¡í„° ìƒì„±
        feature_vector = np.array(vector_elements, dtype=np.float32)

        # ìºì‹œì— ì €ì¥
        if self.use_cache:
            pattern_hash = self._compute_pattern_hash(pattern_data)
            self._pattern_cache[pattern_hash] = feature_vector

            # ì£¼ê¸°ì ìœ¼ë¡œ ìºì‹œ ì €ì¥ (íŒ¨í„´ ìºì‹œê°€ 100ê°œ ì´ìƒ ëŠ˜ì–´ë‚  ë•Œë§ˆë‹¤)
            if len(self._pattern_cache) % 100 == 0:
                self._save_cache()

        return feature_vector

    def vectorize_number_combination(
        self, numbers: List[int], pattern_data: Dict[str, Any]
    ) -> np.ndarray:
        """
        ë²ˆí˜¸ ì¡°í•©ê³¼ íŒ¨í„´ ë°ì´í„°ë¥¼ ê²°í•©í•˜ì—¬ íŠ¹ì„± ë²¡í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            numbers: ë²ˆí˜¸ ì¡°í•© (6ê°œ ë²ˆí˜¸)
            pattern_data: íŒ¨í„´ ë¶„ì„ ê²°ê³¼ ë°ì´í„°

        Returns:
            íŠ¹ì„± ë²¡í„°
        """
        # ìºì‹œ ì‚¬ìš© ì‹œ ì´ë¯¸ ê³„ì‚°ëœ ë²¡í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        if self.use_cache:
            numbers_hash = self._compute_numbers_hash(numbers)
            if numbers_hash in self._vector_cache:
                return self._vector_cache[numbers_hash]

        # ê¸°ë³¸ íŒ¨í„´ ë²¡í„° ìƒì„±
        pattern_vector = self.vectorize_pattern(pattern_data)

        # ë²ˆí˜¸ë³„ íŠ¹ì„± ìƒì„± (ì •ê·œí™”ëœ ë²ˆí˜¸)
        normalized_numbers = [n / 45.0 for n in sorted(numbers)]

        # ë²¡í„° ê²°í•©
        combined_vector = np.concatenate([pattern_vector, normalized_numbers])

        # ìºì‹œì— ì €ì¥
        if self.use_cache:
            numbers_hash = self._compute_numbers_hash(numbers)
            self._vector_cache[numbers_hash] = combined_vector

            # ì£¼ê¸°ì ìœ¼ë¡œ ìºì‹œ ì €ì¥ (ë²ˆí˜¸ ìºì‹œê°€ 1000ê°œ ì´ìƒ ëŠ˜ì–´ë‚  ë•Œë§ˆë‹¤)
            if len(self._vector_cache) % 1000 == 0:
                self._save_cache()

        return combined_vector

    def _should_use_gpu_vectorization(self, pattern_data: Dict[str, Any]) -> bool:
        """GPU ë²¡í„°í™” ì‚¬ìš© ì—¬ë¶€ ê²°ì •"""
        # ë°ì´í„° í¬ê¸°ê°€ ì¶©ë¶„íˆ í´ ë•Œë§Œ GPU ì‚¬ìš©
        data_size = len(str(pattern_data))  # ëŒ€ëµì ì¸ ë°ì´í„° í¬ê¸°
        return data_size > 1000  # 1KB ì´ìƒì¼ ë•Œ GPU ì‚¬ìš©

    def _gpu_vectorize_pattern(self, pattern_data: Dict[str, Any]) -> np.ndarray:
        """GPU ê°€ì† íŒ¨í„´ ë²¡í„°í™”"""
        try:
            if self.cuda_optimizer and hasattr(self.cuda_optimizer, "device_context"):
                with self.cuda_optimizer.device_context():
                    # í˜„ì¬ëŠ” ê¸°ë³¸ ë²¡í„°í™”ë¡œ í´ë°± (í–¥í›„ CUDA êµ¬í˜„ í™•ì¥ ê°€ëŠ¥)
                    return self._cpu_vectorize_pattern_optimized(pattern_data)
            else:
                # GPU ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ CPUë¡œ ì²˜ë¦¬
                return self._cpu_vectorize_pattern_optimized(pattern_data)
        except Exception as e:
            self.logger.error(f"GPU ë²¡í„°í™” ì‹¤íŒ¨: {e}")
            return self._cpu_vectorize_pattern_optimized(pattern_data)

    def _cpu_vectorize_pattern_optimized(
        self, pattern_data: Dict[str, Any]
    ) -> np.ndarray:
        """ìµœì í™”ëœ CPU íŒ¨í„´ ë²¡í„°í™”"""
        # ğŸ§  ë©”ëª¨ë¦¬ í’€ì—ì„œ ë°°ì—´ í• ë‹¹ (ê°€ëŠ¥í•œ ê²½ìš°)
        if self.memory_manager:
            try:
                # ì˜ˆìƒ ë²¡í„° í¬ê¸°ë¡œ ë©”ëª¨ë¦¬ í• ë‹¹
                estimated_size = self._estimate_vector_size(pattern_data)
                vector_array = self.memory_manager.get_optimized_array(
                    shape=(estimated_size,), dtype=np.float32
                )

                # ê¸°ì¡´ ë¡œì§ìœ¼ë¡œ ë²¡í„° ê³„ì‚°
                result = self._standard_vectorize_pattern(pattern_data)

                # í¬ê¸° ì¡°ì •
                if len(result) <= len(vector_array):
                    vector_array[: len(result)] = result
                    return vector_array[: len(result)]
                else:
                    return result

            except Exception as e:
                self.logger.warning(f"ë©”ëª¨ë¦¬ ìµœì í™” ë²¡í„°í™” ì‹¤íŒ¨: {e}")

        # í´ë°±: í‘œì¤€ ë²¡í„°í™”
        return self._standard_vectorize_pattern(pattern_data)

    def _estimate_vector_size(self, pattern_data: Dict[str, Any]) -> int:
        """ë²¡í„° í¬ê¸° ì¶”ì •"""
        # ê¸°ë³¸ ë²¡í„° í¬ê¸° ì¶”ì •
        base_size = 15  # ê¸°ë³¸ íŠ¹ì„± ìˆ˜

        # ë°ì´í„° ë³µì¡ë„ì— ë”°ë¥¸ ì¶”ê°€ í¬ê¸°
        if "number_distribution" in pattern_data:
            base_size += 7
        if "sum_analysis" in pattern_data:
            base_size += 2
        if "number_gaps" in pattern_data:
            base_size += 3

        return base_size

    def set_optimizers(self, **optimizers):
        """ì™¸ë¶€ì—ì„œ ìµœì í™” ì‹œìŠ¤í…œ ì£¼ì…"""
        if "memory_manager" in optimizers:
            self.memory_manager = optimizers["memory_manager"]
        if "cuda_optimizer" in optimizers:
            self.cuda_optimizer = optimizers["cuda_optimizer"]

        self.logger.info("PatternVectorizer ì™¸ë¶€ ìµœì í™” ì‹œìŠ¤í…œ ì£¼ì… ì™„ë£Œ")

    def clear_cache(self) -> None:
        """ìºì‹œë¥¼ ëª¨ë‘ ë¹„ì›ë‹ˆë‹¤."""
        self._pattern_cache = {}
        self._vector_cache = {}

        # ìºì‹œ íŒŒì¼ ì‚­ì œ
        try:
            pattern_cache_path = self.cache_dir / "pattern_vector_cache.npy"
            if pattern_cache_path.exists():
                os.remove(pattern_cache_path)

            vector_cache_path = self.cache_dir / "number_vector_cache.npy"
            if vector_cache_path.exists():
                os.remove(vector_cache_path)

            self.logger.info("ë²¡í„° ìºì‹œê°€ ëª¨ë‘ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            self.logger.warning(f"ìºì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")

    def vectorize_full_analysis(self, full_analysis: Dict[str, Any]) -> np.ndarray:
        """
        ì „ì²´ ë¶„ì„ ë°ì´í„°ë¥¼ ë²¡í„°ë¡œ ë³€í™˜

        Args:
            full_analysis: í†µí•© ë¶„ì„ ê²°ê³¼

        Returns:
            ë³€í™˜ëœ ë²¡í„° (numpy ë°°ì—´)
        """
        # ì„±ëŠ¥ ì¶”ì  ì‹œì‘
        self.performance_tracker.start_tracking("vectorize_full_analysis")

        # ë©”ëª¨ë¦¬ ì •ë¦¬ ë³€ìˆ˜ë“¤
        temp_vectors = []
        large_arrays = []

        try:
            # ë²¡í„° ì„¤ì •ì—ì„œ ìºì‹œ í‚¤ ìƒì„±
            vector_settings = {}
            try:
                if isinstance(self.config, dict) and "vector_settings" in self.config:
                    vector_settings = self.config["vector_settings"]
                elif hasattr(self.config, "get") and callable(
                    getattr(self.config, "get")
                ):
                    vector_settings = self.config.get("vector_settings", {})
            except Exception as e:
                self.logger.warning(f"ë²¡í„° ì„¤ì • ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")

            # ìºì‹œ í‚¤ ìƒì„±
            cache_key = self._create_cache_key(vector_settings)
            self.logger.info(f"ë²¡í„° ì„¤ì • ê¸°ë°˜ ìºì‹œ í‚¤ ìƒì„±: {cache_key}")

            # ìºì‹œ íŒŒì¼ ê²½ë¡œì— í•´ì‹œ í‚¤ í¬í•¨
            cache_file = Path(self.cache_dir) / f"feature_vector_{cache_key}.npy"
            feature_names_file = (
                Path(self.cache_dir) / f"feature_vector_{cache_key}.names.json"
            )

            # ìºì‹œ í™•ì¸
            if cache_file.exists():
                try:
                    self.logger.info(f"ìºì‹œëœ ë²¡í„° ë°ì´í„° ì‚¬ìš©: {cache_file}")
                    # íŠ¹ì„± ì´ë¦„ë„ ë¡œë“œ
                    if feature_names_file.exists():
                        with open(feature_names_file, "r", encoding="utf-8") as f:
                            self.feature_names = json.load(f)

                    # ì„±ëŠ¥ ì¶”ì  ì¢…ë£Œ
                    self.performance_tracker.stop_tracking("vectorize_full_analysis")
                    return np.load(cache_file)
                except Exception as e:
                    self.logger.warning(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")

            # ë²¡í„° íŠ¹ì„± ì´ˆê¸°í™”
            vector_features = {}
            # íŠ¹ì„± ì´ë¦„ ì´ˆê¸°í™”
            self.feature_names = []
            # íŠ¹ì„± ê·¸ë£¹ë³„ ì´ë¦„ ì €ì¥
            feature_names_by_group = {}

            # 1. 10êµ¬ê°„ ë¹ˆë„ (10ê°œ ê°’)
            if "segment_10_frequency" in full_analysis:
                segment_10_vector = self._extract_segment_frequency(
                    full_analysis["segment_10_frequency"], 10
                )
                vector_features["segment_10"] = segment_10_vector
                feature_names_by_group["segment_10"] = [
                    f"segment_10_freq_{i+1}" for i in range(10)
                ]
                temp_vectors.append(segment_10_vector)

            # 2. 5êµ¬ê°„ ë¹ˆë„ (5ê°œ ê°’)
            if "segment_5_frequency" in full_analysis:
                segment_5_vector = self._extract_segment_frequency(
                    full_analysis["segment_5_frequency"], 5
                )
                vector_features["segment_5"] = segment_5_vector
                feature_names_by_group["segment_5"] = [
                    f"segment_5_freq_{i+1}" for i in range(5)
                ]
                temp_vectors.append(segment_5_vector)

            # ... ì¤‘ê°„ ë²¡í„° ì²˜ë¦¬ ê³¼ì •ì—ì„œ ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬ ...

            # ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬ (ë²¡í„° 5ê°œë§ˆë‹¤)
            if len(temp_vectors) >= 5:
                # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì‹¤í–‰
                gc.collect()
                # ì„ì‹œ ë²¡í„° ëª©ë¡ ì •ë¦¬
                temp_vectors.clear()
                self.logger.debug("ì¤‘ê°„ ë²¡í„° ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")

            # ... existing code for other vector processing ...

            # ìµœì¢… ë²¡í„° ê²°í•©
            combined_vector = self._combine_vectors(vector_features)
            large_arrays.append(combined_vector)

            # ë²¡í„° ê²€ì¦
            self._validate_final_vector(combined_vector, self.feature_names)

            # ë²¡í„°ì™€ íŠ¹ì„± ì´ë¦„ ì €ì¥ (ìºì‹œ í‚¤ í¬í•¨ ê²½ë¡œ ì‚¬ìš©)
            self.save_vector_to_file(combined_vector, f"feature_vector_{cache_key}.npy")
            self.save_names_to_file(
                self.feature_names, f"feature_vector_{cache_key}.names.json"
            )

            # í˜¸í™˜ì„±ì„ ìœ„í•´ feature_vector_full.npyë„ í•¨ê»˜ ì €ì¥
            compat_file = Path(self.cache_dir) / "feature_vector_full.npy"
            compat_names_file = Path(self.cache_dir) / "feature_vector_full.names.json"
            np.save(compat_file, combined_vector)
            with open(compat_names_file, "w", encoding="utf-8") as f:
                json.dump(self.feature_names, f, ensure_ascii=False, indent=2)

            # ì„±ëŠ¥ ì¶”ì  ì¢…ë£Œ
            self.performance_tracker.stop_tracking("vectorize_full_analysis")

            return combined_vector

        except Exception as e:
            # ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ ì„±ëŠ¥ ì¶”ì  ì¢…ë£Œë¥¼ ë³´ì¥
            self.performance_tracker.stop_tracking("vectorize_full_analysis")
            raise e
        finally:
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            try:
                # ì„ì‹œ ë²¡í„°ë“¤ ì •ë¦¬
                for vec in temp_vectors:
                    if vec is not None:
                        del vec
                temp_vectors.clear()

                # ëŒ€í˜• ë°°ì—´ë“¤ ì •ë¦¬ (ìµœì¢… ê²°ê³¼ ì œì™¸)
                for arr in large_arrays[:-1]:  # ë§ˆì§€ë§‰ ê²°ê³¼ëŠ” ë³´ì¡´
                    if arr is not None:
                        del arr

                # ë²¡í„° íŠ¹ì„± ë”•ì…”ë„ˆë¦¬ ì •ë¦¬
                if "vector_features" in locals():
                    for key, vec in vector_features.items():
                        if vec is not None and key != "final_result":
                            del vec
                    vector_features.clear()

                # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì‹¤í–‰
                gc.collect()

                # CUDA ë©”ëª¨ë¦¬ ì •ë¦¬ (í•„ìš”ì‹œ)
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()

                self.logger.debug("ë²¡í„°í™” ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")

            except Exception as cleanup_error:
                self.logger.warning(f"ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(cleanup_error)}")

    def _extract_segment_frequency(
        self, segment_freq: Dict[str, Any], num_segments: int
    ) -> np.ndarray:
        """
        êµ¬ê°„ë³„ ë¹ˆë„ ë²¡í„° ì¶”ì¶œ

        Args:
            segment_freq: êµ¬ê°„ë³„ ë¹ˆë„ ë°ì´í„°
            num_segments: êµ¬ê°„ ìˆ˜ (5 ë˜ëŠ” 10)

        Returns:
            êµ¬ê°„ë³„ ë¹ˆë„ ë²¡í„°
        """
        # ê¸°ë³¸ ë²¡í„° ì´ˆê¸°í™”
        vector = np.zeros(num_segments, dtype=np.float32)

        # ê° êµ¬ê°„ë³„ ë¹ˆë„ ì„¤ì •
        for segment_idx, freq in segment_freq.items():
            try:
                idx = (
                    int(segment_idx) - 1
                )  # êµ¬ê°„ ë²ˆí˜¸ëŠ” 1ë¶€í„° ì‹œì‘í•˜ë¯€ë¡œ 0-ì¸ë±ìŠ¤ë¡œ ë³€í™˜
                if 0 <= idx < num_segments:
                    vector[idx] = float(freq)
            except (ValueError, TypeError):
                # êµ¬ê°„ ì¸ë±ìŠ¤ê°€ ìˆ«ìê°€ ì•„ë‹Œ ê²½ìš° ë¬´ì‹œ
                continue

        # ë²¡í„° ì •ê·œí™” (í•©ì´ 1ì´ ë˜ë„ë¡)
        total = np.sum(vector)
        if total > 0:
            vector = vector / total

        return vector

    def _extract_segment_centrality(
        self, segment_centrality: Dict[str, Any]
    ) -> np.ndarray:
        """
        ì„¸ê·¸ë¨¼íŠ¸ ì¤‘ì‹¬ì„± ë°ì´í„°ë¥¼ ë²¡í„°ë¡œ ë³€í™˜

        Args:
            segment_centrality: ì„¸ê·¸ë¨¼íŠ¸ ì¤‘ì‹¬ì„± ë°ì´í„° (9ê°œ ì„¸ê·¸ë¨¼íŠ¸ì— ëŒ€í•œ ê°’)

        Returns:
            ì„¸ê·¸ë¨¼íŠ¸ ì¤‘ì‹¬ì„± ë²¡í„° (9ì°¨ì›)
        """
        # ê¸°ë³¸ ë²¡í„° ì´ˆê¸°í™” (9ê°œ ì„¸ê·¸ë¨¼íŠ¸)
        vector = np.zeros(9, dtype=np.float32)

        if not segment_centrality:
            return vector

        # ì„¸ê·¸ë¨¼íŠ¸ ì¤‘ì‹¬ì„± ê°’ ì„¤ì •
        for i in range(9):
            segment_start = i * 5 + 1
            segment_end = (i + 1) * 5
            segment_key = f"{segment_start}~{segment_end}"

            # ì„¸ê·¸ë¨¼íŠ¸ í‚¤ê°€ ë‹¤ë¥¸ í˜•ì‹ì¼ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ ëŒ€ì²´ í‚¤ ì‹œë„
            alt_keys = [
                f"{segment_start}~{segment_end}",
                f"{segment_start}-{segment_end}",
                f"{segment_start}_{segment_end}",
            ]

            centrality_value = None
            for key in alt_keys:
                if key in segment_centrality:
                    if isinstance(segment_centrality[key], dict):
                        # ê°’ì´ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° (eigenvector, degree ë“±ì´ í¬í•¨ëœ ê²½ìš°)
                        if "eigenvector" in segment_centrality[key]:
                            centrality_value = float(
                                segment_centrality[key]["eigenvector"]
                            )
                        elif "degree" in segment_centrality[key]:
                            centrality_value = float(segment_centrality[key]["degree"])
                    else:
                        # ê°’ì´ ìˆ«ìì¸ ê²½ìš°
                        centrality_value = float(segment_centrality[key])
                    break

            if centrality_value is not None and i < len(vector):
                vector[i] = centrality_value

        # ë²¡í„° ì •ê·œí™” (0-1 ë²”ìœ„ë¡œ)
        max_val = np.max(vector)
        if max_val > 0:
            vector = vector / max_val

        return vector

    def _extract_segment_consecutive(
        self, segment_consecutive_patterns: Dict[str, Dict[str, int]]
    ) -> np.ndarray:
        """
        ì„¸ê·¸ë¨¼íŠ¸ ì—°ì† íŒ¨í„´ ë°ì´í„°ë¥¼ ë²¡í„°ë¡œ ë³€í™˜

        Args:
            segment_consecutive_patterns: ì„¸ê·¸ë¨¼íŠ¸ë³„ ì—°ì† íŒ¨í„´ í†µê³„ ë°ì´í„°
                {
                    "1~5": {"count_2": 37, "count_3": 12, "count_4+": 3},
                    "6~10": {"count_2": 25, "count_3": 8, "count_4+": 1},
                    ...
                }

        Returns:
            ì„¸ê·¸ë¨¼íŠ¸ ì—°ì† íŒ¨í„´ ë²¡í„° (9ê°œ ì„¸ê·¸ë¨¼íŠ¸ x 3ê°œ ì¹´ìš´íŠ¸ = 27ì°¨ì›)
        """
        # ê¸°ë³¸ ë²¡í„° ì´ˆê¸°í™” (9ê°œ ì„¸ê·¸ë¨¼íŠ¸ x 3ê°œ ì¹´ìš´íŠ¸ = 27)
        vector = np.zeros(27, dtype=np.float32)

        if not segment_consecutive_patterns:
            return vector

        # ì„¸ê·¸ë¨¼íŠ¸ë³„ ì—°ì† íŒ¨í„´ ì¹´ìš´íŠ¸ ì¶”ì¶œ
        for i in range(9):
            segment_start = i * 5 + 1
            segment_end = (i + 1) * 5

            # ì„¸ê·¸ë¨¼íŠ¸ í‚¤ê°€ ë‹¤ë¥¸ í˜•ì‹ì¼ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ ëŒ€ì²´ í‚¤ ì‹œë„
            segment_keys = [
                f"{segment_start}~{segment_end}",
                f"{segment_start}-{segment_end}",
                f"segment_{i+1}",
            ]

            segment_data = None
            for key in segment_keys:
                if key in segment_consecutive_patterns:
                    segment_data = segment_consecutive_patterns[key]
                    break

            if segment_data and isinstance(segment_data, dict):
                # ê° ì—°ì† íŒ¨í„´ ì¹´ìš´íŠ¸ ì¶”ì¶œ (2ê°œ ì—°ì†, 3ê°œ ì—°ì†, 4ê°œ ì´ìƒ ì—°ì†)
                base_idx = i * 3  # ê° ì„¸ê·¸ë¨¼íŠ¸ë§ˆë‹¤ 3ê°œ ì¹´ìš´íŠ¸

                # 2ê°œ ì—°ì† ì¹´ìš´íŠ¸
                if "count_2" in segment_data:
                    vector[base_idx] = float(segment_data["count_2"])

                # 3ê°œ ì—°ì† ì¹´ìš´íŠ¸
                if "count_3" in segment_data:
                    vector[base_idx + 1] = float(segment_data["count_3"])

                # 4ê°œ ì´ìƒ ì—°ì† ì¹´ìš´íŠ¸
                if "count_4+" in segment_data:
                    vector[base_idx + 2] = float(segment_data["count_4+"])

        # ë²¡í„° ì •ê·œí™” (0-1 ë²”ìœ„ë¡œ)
        max_val = np.max(vector)
        if max_val > 0:
            vector = vector / max_val

        return vector

    def _extract_pattern_reappearance(self, pattern_data: Dict[str, Any]) -> np.ndarray:
        """
        íŒ¨í„´ ì¬ì¶œí˜„ ê°„ê²© ë°ì´í„°ë¥¼ ë²¡í„°ë¡œ ë³€í™˜

        Args:
            pattern_data: íŒ¨í„´ ì¬ì¶œí˜„ ê°„ê²© ë°ì´í„°

        Returns:
            íŒ¨í„´ ì¬ì¶œí˜„ ê°„ê²© ë²¡í„°
        """
        # ì¬ì¶œí˜„ ê°„ê²© íŠ¹ì„± ì¶”ì¶œ (í‰ê· , í‘œì¤€í¸ì°¨, ìµœì†Œ, ìµœëŒ€)
        features = np.zeros(4, dtype=np.float32)

        all_intervals = []
        for pattern_type, intervals in pattern_data.items():
            if isinstance(intervals, list):
                all_intervals.extend(intervals)
            elif isinstance(intervals, dict):
                for _, values in intervals.items():
                    if isinstance(values, dict) and "intervals" in values:
                        if isinstance(values["intervals"], list):
                            all_intervals.extend(values["intervals"])
                    elif isinstance(values, list):
                        all_intervals.extend(values)

        if all_intervals:
            # í‰ê· , í‘œì¤€í¸ì°¨, ìµœì†Œ, ìµœëŒ€ ê³„ì‚°
            features[0] = np.mean(all_intervals)
            features[1] = np.std(all_intervals)
            features[2] = np.min(all_intervals)
            features[3] = np.max(all_intervals)

        return features

    def _extract_recent_gaps(self, gap_data: Dict[str, Any]) -> np.ndarray:
        """
        ë²ˆí˜¸ë³„ ìµœê·¼ ì¬ì¶œí˜„ ê°„ê²© ë°ì´í„°ë¥¼ ë²¡í„°ë¡œ ë³€í™˜

        Args:
            gap_data: ë²ˆí˜¸ë³„ ìµœê·¼ ì¬ì¶œí˜„ ê°„ê²© ë°ì´í„°

        Returns:
            ë²ˆí˜¸ë³„ ìµœê·¼ ì¬ì¶œí˜„ ê°„ê²© ë²¡í„° (45ê°œ ê°’)
        """
        vector = np.zeros(45, dtype=np.float32)

        # ë²ˆí˜¸ë³„ ê°„ê²© ì„¤ì •
        for num_str, gap in gap_data.items():
            try:
                num = int(num_str)
                if 1 <= num <= 45:
                    vector[num - 1] = float(gap)
            except (ValueError, TypeError):
                # ë²ˆí˜¸ê°€ ìˆ«ìê°€ ì•„ë‹Œ ê²½ìš° ë¬´ì‹œ
                continue

        # ìµœëŒ€ê°’ìœ¼ë¡œ ì •ê·œí™” (0-1 ë²”ìœ„ë¡œ)
        max_gap = np.max(vector)
        if max_gap > 0:
            vector = vector / max_gap

        return vector

    def _combine_vectors(self, vector_features: Dict[str, np.ndarray]) -> np.ndarray:
        """
        ğŸ”§ ì™„ì „íˆ ì¬êµ¬ì¶•ëœ ë²¡í„° ê²°í•© ì‹œìŠ¤í…œ - ë²¡í„°ì™€ ì´ë¦„ì˜ ì™„ë²½í•œ ë™ì‹œ ìƒì„±

        Args:
            vector_features: íŠ¹ì„± ê·¸ë£¹ë³„ ë²¡í„° ì‚¬ì „

        Returns:
            ê²°í•©ëœ ë²¡í„° (ì°¨ì›ê³¼ ì´ë¦„ì´ 100% ì¼ì¹˜ ë³´ì¥)
        """
        self.logger.info("ğŸš€ ë²¡í„°-ì´ë¦„ ë™ì‹œ ìƒì„± ì‹œìŠ¤í…œ ì‹œì‘")

        # ğŸ¯ Step 1: ìˆœì„œ ë³´ì¥ëœ ë²¡í„°+ì´ë¦„ ë™ì‹œ ìƒì„±
        combined_vector = []
        combined_names = []

        # ì²­ì‚¬ì§„ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬í•˜ì—¬ ìˆœì„œ ë³´ì¥
        for group_name in self.vector_blueprint.keys():
            if group_name in vector_features:
                vector = vector_features[group_name]

                # ë²¡í„°ê°€ ë¹„ì–´ìˆìœ¼ë©´ ê±´ë„ˆë›°ê¸°
                if vector is None or vector.size == 0:
                    self.logger.warning(f"ê·¸ë£¹ '{group_name}': ë¹ˆ ë²¡í„° ìŠ¤í‚µ")
                    continue

                # ë²¡í„° ì°¨ì› ì •ê·œí™”
                if vector.ndim > 1:
                    vector = vector.flatten()

                # ê·¸ë£¹ë³„ íŠ¹ì„± ì´ë¦„ ìƒì„±
                group_names = self._get_group_feature_names(group_name, len(vector))

                # ë™ì‹œ ì¶”ê°€ë¡œ ìˆœì„œ ë³´ì¥
                combined_vector.extend(vector.tolist())
                combined_names.extend(group_names)

                self.logger.debug(
                    f"ê·¸ë£¹ '{group_name}': {len(vector)}ì°¨ì› ë²¡í„°+ì´ë¦„ ì¶”ê°€"
                )

        # ğŸ” Step 2: ì‹¤ì‹œê°„ ê²€ì¦
        if len(combined_vector) != len(combined_names):
            error_msg = (
                f"âŒ ë²¡í„°({len(combined_vector)})ì™€ ì´ë¦„({len(combined_names)}) ë¶ˆì¼ì¹˜!"
            )
            self.logger.error(error_msg)
            raise ValueError(error_msg)

        # ğŸ¯ Step 3: í•„ìˆ˜ íŠ¹ì„± ì¶”ê°€ (ëˆ„ë½ëœ 22ê°œ íŠ¹ì„±)
        essential_features = self._get_essential_features()
        for feature_name, feature_value in essential_features.items():
            if feature_name not in combined_names:
                combined_vector.append(feature_value)
                combined_names.append(feature_name)
                self.logger.debug(f"í•„ìˆ˜ íŠ¹ì„± ì¶”ê°€: {feature_name} = {feature_value}")

        # ğŸ”§ Step 4: íŠ¹ì„± í’ˆì§ˆ ê°œì„  (0ê°’ 50% â†’ 30% ì´í•˜)
        combined_vector = self._improve_feature_diversity(
            combined_vector, combined_names
        )

        # ìµœì¢… ê²€ì¦
        assert len(combined_vector) == len(
            combined_names
        ), f"ìµœì¢… ê²€ì¦ ì‹¤íŒ¨: ë²¡í„°({len(combined_vector)}) != ì´ë¦„({len(combined_names)})"

        # íŠ¹ì„± ì´ë¦„ ì €ì¥
        self.feature_names = combined_names

        self.logger.info(
            f"âœ… ë²¡í„°-ì´ë¦„ ë™ì‹œ ìƒì„± ì™„ë£Œ: {len(combined_vector)}ì°¨ì› (100% ì¼ì¹˜)"
        )
        return np.array(combined_vector, dtype=np.float32)

    def _get_group_feature_names(
        self, group_name: str, vector_length: int
    ) -> List[str]:
        """ê·¸ë£¹ë³„ íŠ¹ì„± ì´ë¦„ ìƒì„±"""
        if (
            hasattr(self, "feature_name_templates")
            and group_name in self.feature_name_templates
        ):
            base_names = self.feature_name_templates[group_name]
            if len(base_names) >= vector_length:
                return base_names[:vector_length]
            else:
                # ë¶€ì¡±í•œ ì´ë¦„ ì¶”ê°€ ìƒì„±
                extended_names = base_names.copy()
                for i in range(len(base_names), vector_length):
                    extended_names.append(f"{group_name}_feature_{i}")
                return extended_names
        else:
            # í…œí”Œë¦¿ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ì´ë¦„ ìƒì„±
            return [f"{group_name}_feature_{i}" for i in range(vector_length)]

    def _get_essential_features(self) -> Dict[str, float]:
        """í•„ìˆ˜ íŠ¹ì„± 22ê°œ ë°˜í™˜"""
        return {
            "gap_stddev": 0.15,
            "pair_centrality": 0.5,
            "hot_cold_mix_score": 0.6,
            "segment_entropy": 1.2,
            "position_entropy_1": 1.0,
            "position_entropy_2": 1.0,
            "position_entropy_3": 1.0,
            "position_entropy_4": 1.0,
            "position_entropy_5": 1.0,
            "position_entropy_6": 1.0,
            "position_std_1": 5.0,
            "position_std_2": 5.0,
            "position_std_3": 5.0,
            "position_std_4": 5.0,
            "position_std_5": 5.0,
            "position_std_6": 5.0,
            "distance_variance": 0.25,
            "cohesiveness_score": 0.4,
            "sequential_pair_rate": 0.15,
            "number_spread": 0.35,
            "pattern_complexity": 0.55,
            "trend_strength": 0.3,
        }

    def _improve_feature_diversity(
        self, vector: np.ndarray, feature_names: List[str]
    ) -> np.ndarray:
        """
        ğŸ¯ íŠ¹ì„± ë‹¤ì–‘ì„± ê°œì„  ì•Œê³ ë¦¬ì¦˜

        0ê°’ ë¹„ìœ¨ì„ 50% â†’ 30% ì´í•˜ë¡œ ê°œì„ í•˜ê³  ì—”íŠ¸ë¡œí”¼ë¥¼ ì–‘ìˆ˜ë¡œ ë§Œë“­ë‹ˆë‹¤.

        Args:
            vector: ê°œì„ í•  ë²¡í„°
            feature_names: íŠ¹ì„± ì´ë¦„ ë¦¬ìŠ¤íŠ¸

        Returns:
            np.ndarray: ê°œì„ ëœ ë²¡í„°
        """
        try:
            # ìŠ¤ì¹¼ë¼ ë°°ì—´ ì²˜ë¦¬
            if vector.ndim == 0:
                vector = np.atleast_1d(vector)

            # Step 1: 0ê°’ íŠ¹ì„± ì‹¤ì œ ê³„ì‚°ìœ¼ë¡œ ëŒ€ì²´
            zero_indices = np.where(vector == 0.0)[0]
            essential_features = self._get_essential_features()

            for idx in zero_indices:
                if idx < len(feature_names):
                    feature_name = feature_names[idx]
                    # í•„ìˆ˜ íŠ¹ì„±ì— í•´ë‹¹í•˜ëŠ” ê²½ìš° ì‹¤ì œ ê°’ ì ìš©
                    for essential_name, essential_value in essential_features.items():
                        if essential_name in feature_name:
                            vector[idx] = essential_value
                            break
                    else:
                        # í•„ìˆ˜ íŠ¹ì„±ì´ ì•„ë‹Œ ê²½ìš° ëœë¤ ê°’ ì ìš© (0.1 ~ 0.9)
                        vector[idx] = np.random.uniform(0.1, 0.9)

            # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° numpy ë°°ì—´ë¡œ ë³€í™˜
            if isinstance(vector, list):
                vector = np.array(vector)

            # Step 2: íŠ¹ì„± ì •ê·œí™” ë° ë‹¤ì–‘ì„± ê°•í™”
            vector = self._enhance_feature_variance(vector)

            # Step 3: ì—”íŠ¸ë¡œí”¼ ê²€ì¦ ë° ë¶€ìŠ¤íŒ…
            entropy = self._calculate_vector_entropy(vector)
            if entropy <= 0:
                vector = self._boost_entropy(vector)

            return vector

        except Exception as e:
            self.logger.error(f"íŠ¹ì„± ë‹¤ì–‘ì„± ê°œì„  ì‹¤íŒ¨: {e}")
            return vector

    def _enhance_feature_variance(self, vector: np.ndarray) -> np.ndarray:
        """íŠ¹ì„± ë¶„ì‚° ê°•í™”"""
        try:
            # ë„ˆë¬´ ê· ë“±í•œ ê°’ë“¤ì„ ë‹¤ì–‘í™”
            unique_values = np.unique(vector)
            if len(unique_values) < len(vector) * 0.1:  # ê³ ìœ ê°’ì´ 10% ë¯¸ë§Œì¸ ê²½ìš°
                # ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ ì¶”ê°€
                noise = np.random.normal(0, 0.05, len(vector))
                vector = vector + noise
                # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
                vector = np.clip(vector, 0, 1)

            return vector
        except Exception as e:
            self.logger.error(f"íŠ¹ì„± ë¶„ì‚° ê°•í™” ì‹¤íŒ¨: {e}")
            return vector

    def _boost_entropy(self, vector: np.ndarray) -> np.ndarray:
        """ì—”íŠ¸ë¡œí”¼ ë¶€ìŠ¤íŒ…"""
        try:
            # íˆìŠ¤í† ê·¸ë¨ ê¸°ë°˜ ì—”íŠ¸ë¡œí”¼ ê°œì„ 
            hist, bins = np.histogram(vector, bins=20, range=(0, 1))

            # ë¹ˆ êµ¬ê°„ì— ê°’ ì¶”ê°€
            empty_bins = np.where(hist == 0)[0]
            if len(empty_bins) > 0:
                for bin_idx in empty_bins[:5]:  # ìµœëŒ€ 5ê°œ ë¹ˆ êµ¬ê°„ ì±„ìš°ê¸°
                    bin_center = (bins[bin_idx] + bins[bin_idx + 1]) / 2
                    # ê°€ì¥ ê°€ê¹Œìš´ 0ê°’ ì°¾ì•„ì„œ ëŒ€ì²´
                    zero_idx = np.where(vector == 0.0)[0]
                    if len(zero_idx) > 0:
                        vector[zero_idx[0]] = bin_center

            return vector
        except Exception as e:
            self.logger.error(f"ì—”íŠ¸ë¡œí”¼ ë¶€ìŠ¤íŒ… ì‹¤íŒ¨: {e}")
            return vector

    def normalize_vector(self, vector: np.ndarray) -> np.ndarray:
        """
        íŠ¹ì„± ë²¡í„°ë¥¼ ì •ê·œí™”í•©ë‹ˆë‹¤.

        Args:
            vector: ì›ë³¸ íŠ¹ì„± ë²¡í„°

        Returns:
            ì •ê·œí™”ëœ íŠ¹ì„± ë²¡í„°
        """
        with performance_monitor("normalize_vector"):
            # NaN ê°’ í™•ì¸ ë° ì²˜ë¦¬
            has_nan = np.isnan(vector).any()
            if has_nan:
                self.logger.warning(f"ë²¡í„°ì— NaN ê°’ì´ ìˆì–´ 0ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                vector = np.nan_to_num(vector, nan=0.0)

            # ë¬´í•œëŒ€ ê°’ í™•ì¸ ë° ì²˜ë¦¬
            has_inf = np.isinf(vector).any()
            if has_inf:
                self.logger.warning(f"ë²¡í„°ì— ë¬´í•œëŒ€ ê°’ì´ ìˆì–´ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
                vector = np.nan_to_num(vector, posinf=1.0, neginf=0.0)

            # ê°’ ë²”ìœ„ ê²€ì¦ ë° ì¡°ì • (0-1 ë²”ìœ„)
            out_of_range = np.logical_or(vector < 0, vector > 1).any()
            if out_of_range:
                self.logger.warning(f"ë²¡í„°ì— [0,1] ë²”ìœ„ë¥¼ ë²—ì–´ë‚œ ê°’ì´ ìˆì–´ ì¡°ì •í•©ë‹ˆë‹¤.")
                vector = np.clip(vector, 0.0, 1.0)

            return vector

    def _process_float_conversion(self, value: Any) -> float:
        """
        ë‹¤ì–‘í•œ íƒ€ì…ì˜ ê°’ì„ ì•ˆì „í•˜ê²Œ floatë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        ê¸°ë³¸ êµ¬í˜„ ë©”ì„œë“œ

        Args:
            value: ë³€í™˜í•  ê°’ (dictionary, list, ìˆ«ì, ë¬¸ìì—´ ë“±)

        Returns:
            float: ë³€í™˜ëœ float ê°’
        """
        try:
            # dict íƒ€ì…ì¸ ê²½ìš° ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ê°’ ì¶”ì¶œ ì‹œë„
            if isinstance(value, dict):
                # ìš°ì„ ìˆœìœ„: score > value > avg > mean > ì²« ë²ˆì§¸ ê°’
                for key in ["score", "value", "avg", "mean", "total"]:
                    if key in value and value[key] is not None:
                        return self._process_float_conversion(value[key])

                # í‚¤ê°€ ì—†ëŠ” ê²½ìš° ì²« ë²ˆì§¸ ê°’ ì‚¬ìš©
                if value:
                    return self._process_float_conversion(list(value.values())[0])
                return 0.0

            # list ë˜ëŠ” tupleì¸ ê²½ìš° ì²« ë²ˆì§¸ ìš”ì†Œ ì‚¬ìš©
            elif isinstance(value, (list, tuple)) and value:
                if len(value) == 1:
                    return self._process_float_conversion(value[0])
                # ëª¨ë“  ê°’ì˜ í‰ê·  ê³„ì‚° ì‹œë„
                try:
                    return float(
                        sum(self._process_float_conversion(v) for v in value)
                        / len(value)
                    )
                except:
                    return self._process_float_conversion(value[0])

            # bool íƒ€ì…ì¸ ê²½ìš° 1.0 ë˜ëŠ” 0.0ìœ¼ë¡œ ë³€í™˜
            elif isinstance(value, bool):
                return 1.0 if value else 0.0

            # None ê°’ ì²˜ë¦¬
            elif value is None:
                return 0.0

            # ë‚˜ë¨¸ì§€ ê²½ìš° float ë³€í™˜ ì‹œë„
            return float(value)
        except (ValueError, TypeError, IndexError, KeyError) as e:
            self.logger.warning(
                f"ê°’ '{value}' ({type(value)})ë¥¼ floatë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}"
            )
            return 0.0  # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜

    def safe_float_conversion(self, value: Any) -> float:
        """
        ë‹¤ì–‘í•œ íƒ€ì…ì˜ ê°’ì„ ì•ˆì „í•˜ê²Œ floatë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        ì™¸ë¶€ì—ì„œ ì£¼ì…ëœ í•¨ìˆ˜ê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©í•˜ê³ ,
        ì—†ìœ¼ë©´ ê¸°ë³¸ êµ¬í˜„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

        Args:
            value: ë³€í™˜í•  ê°’ (dictionary, list, ìˆ«ì, ë¬¸ìì—´ ë“±)

        Returns:
            float: ë³€í™˜ëœ float ê°’
        """
        # ì™¸ë¶€ì—ì„œ ì£¼ì…ëœ safe_float_conversion í•¨ìˆ˜ê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©
        if hasattr(self, "_external_float_conversion") and callable(
            self._external_float_conversion
        ):
            return self._external_float_conversion(value)

        # ì—†ìœ¼ë©´ ê¸°ë³¸ êµ¬í˜„ ì‚¬ìš©
        return self._process_float_conversion(value)

    @property
    def external_float_conversion(self):
        """ì™¸ë¶€ì—ì„œ ì£¼ì…ëœ float ë³€í™˜ í•¨ìˆ˜ë¥¼ ë°˜í™˜"""
        return (
            self._external_float_conversion
            if hasattr(self, "_external_float_conversion")
            else None
        )

    @external_float_conversion.setter
    def external_float_conversion(self, func):
        """ì™¸ë¶€ float ë³€í™˜ í•¨ìˆ˜ ì„¤ì •"""
        self._external_float_conversion = func

    def save_vector_to_file(
        self, vector: np.ndarray, filename: str = "feature_vector_full.npy"
    ) -> str:
        """
        íŠ¹ì„± ë²¡í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.

        Args:
            vector: ì €ì¥í•  íŠ¹ì„± ë²¡í„°
            filename: ì €ì¥í•  íŒŒì¼ëª…

        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ

        Raises:
            ValueError: ë²¡í„° ì°¨ì›ê³¼ íŠ¹ì„± ì´ë¦„ ìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ” ê²½ìš°
        """
        # ìºì‹œ ë””ë ‰í† ë¦¬ í™•ì¸
        try:
            cache_dir = self.config["paths"]["cache_dir"]
        except (KeyError, TypeError):
            cache_dir = "data/cache"

        cache_path = Path(cache_dir)
        cache_path.mkdir(parents=True, exist_ok=True)

        # íŠ¹ì„± ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
        feature_names = self.get_feature_names()

        # ë²¡í„° ì°¨ì›ê³¼ íŠ¹ì„± ì´ë¦„ ìˆ˜ ì¼ì¹˜ í™•ì¸
        vector_dim = vector.shape[1] if len(vector.shape) > 1 else vector.shape[0]
        names_count = len(feature_names)

        # í•„ìˆ˜ íŠ¹ì„± ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        essential_features = []
        try:
            if (
                isinstance(self.config, dict)
                and "validation" in self.config
                and "essential_features" in self.config["validation"]
            ):
                essential_features = self.config["validation"]["essential_features"]
            elif hasattr(self.config, "get") and callable(getattr(self.config, "get")):
                validation_config = self.config.get("validation", {})
                if hasattr(validation_config, "get") and callable(
                    getattr(validation_config, "get")
                ):
                    essential_features = validation_config.get("essential_features", [])
        except Exception as e:
            self.logger.warning(f"ì„¤ì •ì—ì„œ í•„ìˆ˜ íŠ¹ì„± ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜: {str(e)}")

        # í•„ìˆ˜ íŠ¹ì„± ëª©ë¡ì´ ë¹„ì–´ìˆìœ¼ë©´ ê¸°ë³¸ í•„ìˆ˜ íŠ¹ì„± ì‚¬ìš©
        if not essential_features:
            # í•„ìˆ˜ íŠ¹ì„±ì„ ì§ì ‘ ì •ì˜ (ëª¨ë“ˆì´ ì—†ìœ¼ë¯€ë¡œ)
            essential_features = [
                "gap_stddev",
                "pair_centrality",
                "hot_cold_mix_score",
                "segment_entropy",
                "roi_group_score",
                "duplicate_flag",
                "max_overlap_with_past",
                "combination_recency_score",
                "position_entropy_1",
                "position_entropy_2",
                "position_entropy_3",
                "position_entropy_4",
                "position_entropy_5",
                "position_entropy_6",
                "position_std_1",
                "position_std_2",
                "position_std_3",
                "position_std_4",
                "position_std_5",
                "position_std_6",
                "position_variance_avg",
                "position_bias_score",
            ]

        # í•„ìˆ˜ íŠ¹ì„± ì¤‘ ëˆ„ë½ëœ íŠ¹ì„± í™•ì¸
        missing_essential = [f for f in essential_features if f not in feature_names]

        # ì°¨ì› ë¶ˆì¼ì¹˜ í™•ì¸
        if vector_dim != names_count:
            self.logger.warning(
                f"ë²¡í„° ì°¨ì›({vector_dim})ê³¼ íŠ¹ì„± ì´ë¦„ ìˆ˜({names_count})ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"
            )

            # ì¶”ê°€ ì •ë³´ ë¡œê¹…: ì°¨ì› ë¶ˆì¼ì¹˜ ì •ë„
            dimension_diff = abs(vector_dim - names_count)
            percent_diff = (dimension_diff / max(vector_dim, names_count)) * 100
            self.logger.warning(
                f"ì°¨ì› ë¶ˆì¼ì¹˜ ì •ë„: {dimension_diff}ê°œ ({percent_diff:.2f}%)"
            )

            # ë²¡í„° ì°¨ì›ê³¼ íŠ¹ì„± ì´ë¦„ ìˆ˜ë¥¼ ì¼ì¹˜ì‹œí‚¤ê¸° ìœ„í•œ ì²˜ë¦¬
            if vector_dim > names_count:
                self.logger.warning(
                    f"ë²¡í„° ì°¨ì›ì´ íŠ¹ì„± ì´ë¦„ ìˆ˜ë³´ë‹¤ {vector_dim - names_count}ê°œ ë” ë§ìŠµë‹ˆë‹¤. ì´ë¦„ ëª©ë¡ì„ í™•ì¥í•©ë‹ˆë‹¤."
                )
                # íŠ¹ì„± ì´ë¦„ ëª©ë¡ í™•ì¥
                for i in range(names_count, vector_dim):
                    feature_names.append(f"feature_{i}")
            else:
                self.logger.warning(
                    f"íŠ¹ì„± ì´ë¦„ ìˆ˜ê°€ ë²¡í„° ì°¨ì›ë³´ë‹¤ {names_count - vector_dim}ê°œ ë” ë§ìŠµë‹ˆë‹¤. ë²¡í„°ë¥¼ í™•ì¥í•©ë‹ˆë‹¤."
                )
                # ë²¡í„°ë¥¼ í™•ì¥í•˜ëŠ” ì½”ë“œ
                if len(vector.shape) > 1:
                    extended_vector = np.zeros(
                        (vector.shape[0], names_count), dtype=vector.dtype
                    )
                    extended_vector[:, :vector_dim] = vector
                    vector = extended_vector
                else:
                    extended_vector = np.zeros(names_count, dtype=vector.dtype)
                    extended_vector[:vector_dim] = vector
                    vector = extended_vector

                # ì°¨ì› ì—…ë°ì´íŠ¸
                vector_dim = names_count

        # í•„ìˆ˜ íŠ¹ì„± ì¶”ê°€
        if missing_essential:
            self.logger.warning(f"ë‹¤ìŒ í•„ìˆ˜ íŠ¹ì„±ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {missing_essential}")

            # ëˆ„ë½ëœ íŠ¹ì„±ì„ feature_namesì— ì¶”ê°€
            feature_names.extend(missing_essential)

            # ë²¡í„°ë„ í™•ì¥
            new_dim = len(feature_names)
            if len(vector.shape) > 1:
                extended_vector = np.zeros(
                    (vector.shape[0], new_dim), dtype=vector.dtype
                )
                extended_vector[:, :vector_dim] = vector
                vector = extended_vector
            else:
                extended_vector = np.zeros(new_dim, dtype=vector.dtype)
                extended_vector[:vector_dim] = vector
                vector = extended_vector

            # ì¶”ê°€ëœ íŠ¹ì„±ì— ê¸°ë³¸ê°’ ì„¤ì • (0.5 ë˜ëŠ” íŠ¹ì • ê¸°ë³¸ê°’)
            for i, name in enumerate(feature_names):
                if i >= vector_dim and name in missing_essential:
                    # íŠ¹ì„±ë³„ ê¸°ë³¸ê°’ ì„¤ì •
                    if "position_entropy" in name or "segment_entropy" in name:
                        vector[..., i] = 0.5
                    elif "stddev" in name or "std_" in name:
                        vector[..., i] = 0.1
                    elif "score" in name:
                        vector[..., i] = 0.5
                    elif "flag" in name:
                        vector[..., i] = 0.0
                    elif "silhouette_score" in name:
                        vector[..., i] = 0.3
                    else:
                        vector[..., i] = 0.5

            self.logger.info(
                f"í•„ìˆ˜ íŠ¹ì„± {len(missing_essential)}ê°œê°€ ìë™ìœ¼ë¡œ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤."
            )

            # ì°¨ì› ì—…ë°ì´íŠ¸
            vector_dim = new_dim

    def _calculate_vector_entropy(self, vector: np.ndarray) -> float:
        """ë²¡í„°ì˜ ì—”íŠ¸ë¡œí”¼ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        try:
            # íˆìŠ¤í† ê·¸ë¨ ìƒì„±
            hist, _ = np.histogram(vector, bins=20, range=(0, 1))

            # í™•ë¥  ë¶„í¬ ê³„ì‚°
            hist = hist / np.sum(hist)

            # 0ì´ ì•„ë‹Œ ê°’ë§Œ ì‚¬ìš©
            hist = hist[hist > 0]

            # ì—”íŠ¸ë¡œí”¼ ê³„ì‚°
            if len(hist) > 0:
                entropy = -np.sum(hist * np.log2(hist))
                return float(entropy)
            else:
                return 0.0

        except Exception as e:
            self.logger.error(f"ì—”íŠ¸ë¡œí”¼ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0

    def _validate_final_vector(
        self, vector: np.ndarray, feature_names: List[str]
    ) -> bool:
        """ìµœì¢… ë²¡í„° ê²€ì¦"""
        try:
            # ì°¨ì› ì¼ì¹˜ì„± ê²€ì¦
            if len(vector) != len(feature_names):
                self.logger.error(
                    f"ë²¡í„° ì°¨ì› ë¶ˆì¼ì¹˜: {len(vector)} != {len(feature_names)}"
                )
                return False

            # NaN/Inf ê²€ì¦
            if np.isnan(vector).any() or np.isinf(vector).any():
                self.logger.error("ë²¡í„°ì— NaN ë˜ëŠ” Inf ê°’ì´ ìˆìŠµë‹ˆë‹¤")
                return False

            # 0ê°’ ë¹„ìœ¨ ê²€ì¦
            zero_ratio = np.sum(vector == 0) / len(vector)
            if zero_ratio > 0.7:  # 70% ì´ˆê³¼ì‹œ ê²½ê³ 
                self.logger.warning(f"0ê°’ ë¹„ìœ¨ì´ ë†’ìŠµë‹ˆë‹¤: {zero_ratio*100:.1f}%")

            # ì—”íŠ¸ë¡œí”¼ ê²€ì¦
            entropy = self._calculate_vector_entropy(vector)
            if entropy <= 0:
                self.logger.warning(f"ì—”íŠ¸ë¡œí”¼ê°€ ë‚®ìŠµë‹ˆë‹¤: {entropy:.3f}")

            self.logger.info(
                f"âœ… ë²¡í„° ê²€ì¦ í†µê³¼: {len(vector)}ì°¨ì›, 0ê°’ë¹„ìœ¨={zero_ratio*100:.1f}%, ì—”íŠ¸ë¡œí”¼={entropy:.3f}"
            )
            return True

        except Exception as e:
            self.logger.error(f"ë²¡í„° ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False

    def get_feature_names(self) -> List[str]:
        """íŠ¹ì„± ì´ë¦„ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        if hasattr(self, "feature_names") and self.feature_names:
            return self.feature_names.copy()
        else:
            # ê¸°ë³¸ íŠ¹ì„± ì´ë¦„ ìƒì„±
            return [f"feature_{i}" for i in range(146)]  # ê¸°ë³¸ 146ì°¨ì›

    def save_names_to_file(self, feature_names: List[str], filename: str) -> str:
        """íŠ¹ì„± ì´ë¦„ì„ JSON íŒŒì¼ë¡œ ì €ì¥"""
        try:
            names_path = Path(self.cache_dir) / filename

            with open(names_path, "w", encoding="utf-8") as f:
                json.dump(feature_names, f, ensure_ascii=False, indent=2)

            self.logger.info(f"íŠ¹ì„± ì´ë¦„ ì €ì¥ ì™„ë£Œ: {names_path}")
            return str(names_path)

        except Exception as e:
            self.logger.error(f"íŠ¹ì„± ì´ë¦„ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise
