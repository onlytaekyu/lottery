"""
ë©”íƒ€ íŠ¹ì„± ë¶„ì„ê¸°
- íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ (SHAP/LIME)
- íŠ¹ì„± ìƒí˜¸ì‘ìš© ë¶„ì„
- ì°¨ì› ì¶•ì†Œ ë° íŠ¹ì„± ì„ íƒ
"""

import numpy as np
from typing import Dict, Any, List, Tuple, Optional
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import PolynomialFeatures
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression
import json

# ê¸°ì¡´ ì‹œìŠ¤í…œ import
from ..utils.unified_logging import get_logger
from ..shared.types import LotteryNumber
from ..analysis.base_analyzer import BaseAnalyzer

logger = get_logger(__name__)


class MetaFeatureAnalyzer(BaseAnalyzer):
    """
    ë©”íƒ€ íŠ¹ì„± ë¶„ì„ê¸°
    - íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„
    - íŠ¹ì„± ìƒí˜¸ì‘ìš© íƒì§€
    - ì°¨ì› ì¶•ì†Œ
    - íŠ¹ì„± ì„ íƒ
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__(config or {}, "meta_feature")

        # ì„¤ì • ì´ˆê¸°í™”
        self.use_shap = self.config.get("use_shap", False)
        self.use_lime = self.config.get("use_lime", False)
        self.polynomial_degree = self.config.get("polynomial_degree", 2)
        self.pca_components = self.config.get("pca_components", 50)
        self.tsne_components = self.config.get("tsne_components", 2)
        self.select_k_best = self.config.get("select_k_best", 100)

        # ë‚´ë¶€ ìƒíƒœ
        self.feature_importance_scores = {}
        self.interaction_features = None
        self.dimension_reduction_models = {}

        logger.info("âœ… MetaFeatureAnalyzer ì´ˆê¸°í™” ì™„ë£Œ")

    def _analyze_impl(self, data: List[LotteryNumber]) -> Dict[str, Any]:
        """ë©”íƒ€ íŠ¹ì„± ë¶„ì„ ë©”ì¸ ë¡œì§"""
        try:
            logger.info(f"ğŸ” ë©”íƒ€ íŠ¹ì„± ë¶„ì„ ì‹œì‘: {len(data)}ê°œ íšŒì°¨")

            # ë°ì´í„°ê°€ ë¶€ì¡±í•œ ê²½ìš° ê¸°ë³¸ ë¶„ì„ë§Œ ìˆ˜í–‰
            if len(data) < 50:
                logger.warning("ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ê¸°ë³¸ ë¶„ì„ë§Œ ìˆ˜í–‰")
                return self._basic_meta_analysis(data)

            # 1. ê¸°ë³¸ íŠ¹ì„± í–‰ë ¬ ìƒì„±
            feature_matrix, target_vector = self._create_feature_matrix(data)

            # 2. íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„
            importance_analysis = self._analyze_feature_importance(
                feature_matrix, target_vector
            )

            # 3. íŠ¹ì„± ìƒí˜¸ì‘ìš© ë¶„ì„
            interaction_analysis = self._analyze_feature_interactions(
                feature_matrix, target_vector
            )

            # 4. ì°¨ì› ì¶•ì†Œ ë¶„ì„
            dimension_reduction = self._perform_dimension_reduction(feature_matrix)

            # 5. íŠ¹ì„± ì„ íƒ
            feature_selection = self._perform_feature_selection(
                feature_matrix, target_vector
            )

            # 6. ë©”íƒ€ íŠ¹ì„± í†µê³„
            meta_statistics = self._calculate_meta_statistics(
                feature_matrix, target_vector
            )

            result = {
                "importance_analysis": importance_analysis,
                "interaction_analysis": interaction_analysis,
                "dimension_reduction": dimension_reduction,
                "feature_selection": feature_selection,
                "meta_statistics": meta_statistics,
                "original_features": (
                    feature_matrix.shape[1] if feature_matrix is not None else 0
                ),
                "data_samples": len(data),
            }

            logger.info("âœ… ë©”íƒ€ íŠ¹ì„± ë¶„ì„ ì™„ë£Œ")
            return result

        except Exception as e:
            logger.error(f"ë©”íƒ€ íŠ¹ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
            return self._get_fallback_result()

    def _create_feature_matrix(
        self, data: List[LotteryNumber]
    ) -> Tuple[np.ndarray, np.ndarray]:
        """ê¸°ë³¸ íŠ¹ì„± í–‰ë ¬ ìƒì„±"""
        try:
            features = []
            targets = []

            for i, draw in enumerate(data):
                # ê¸°ë³¸ íŠ¹ì„± ì¶”ì¶œ
                feature_vector = self._extract_basic_features(draw)
                features.append(feature_vector)

                # íƒ€ê²Ÿ ë³€ìˆ˜ (ë‹¤ìŒ íšŒì°¨ ì˜ˆì¸¡ì„ ìœ„í•œ ê°„ë‹¨í•œ íƒ€ê²Ÿ)
                if i < len(data) - 1:
                    next_draw = data[i + 1]
                    target = self._create_target_variable(draw, next_draw)
                    targets.append(target)

            # ë§ˆì§€ë§‰ ìƒ˜í”Œ ì œê±° (íƒ€ê²Ÿì´ ì—†ìŒ)
            if features:
                features = features[:-1]

            if len(features) == 0 or len(targets) == 0:
                logger.warning("íŠ¹ì„± í–‰ë ¬ ìƒì„± ì‹¤íŒ¨: ë°ì´í„° ë¶€ì¡±")
                return None, None

            feature_matrix = np.array(features, dtype=np.float32)
            target_vector = np.array(targets, dtype=np.float32)

            logger.info(f"íŠ¹ì„± í–‰ë ¬ ìƒì„± ì™„ë£Œ: {feature_matrix.shape}")
            return feature_matrix, target_vector

        except Exception as e:
            logger.error(f"íŠ¹ì„± í–‰ë ¬ ìƒì„± ì‹¤íŒ¨: {e}")
            return None, None

    def _extract_basic_features(self, draw: LotteryNumber) -> List[float]:
        """ê¸°ë³¸ íŠ¹ì„± ì¶”ì¶œ"""
        try:
            features = []
            numbers = sorted(draw.numbers)

            # ê¸°ë³¸ í†µê³„
            features.extend(
                [
                    np.sum(numbers),  # í•©ê³„
                    np.mean(numbers),  # í‰ê· 
                    np.std(numbers),  # í‘œì¤€í¸ì°¨
                    np.min(numbers),  # ìµœì†Ÿê°’
                    np.max(numbers),  # ìµœëŒ“ê°’
                    np.median(numbers),  # ì¤‘ì•™ê°’
                    numbers[-1] - numbers[0],  # ë²”ìœ„
                ]
            )

            # ê°„ê²© ì •ë³´
            gaps = [numbers[i + 1] - numbers[i] for i in range(len(numbers) - 1)]
            features.extend(
                [
                    np.mean(gaps),  # í‰ê·  ê°„ê²©
                    np.std(gaps),  # ê°„ê²© í‘œì¤€í¸ì°¨
                    np.max(gaps),  # ìµœëŒ€ ê°„ê²©
                    np.min(gaps),  # ìµœì†Œ ê°„ê²©
                ]
            )

            # í™€ì§ ì •ë³´
            odd_count = sum(1 for n in numbers if n % 2 == 1)
            even_count = sum(1 for n in numbers if n % 2 == 0)
            features.extend(
                [
                    odd_count,  # í™€ìˆ˜ ê°œìˆ˜
                    even_count,  # ì§ìˆ˜ ê°œìˆ˜
                    odd_count / len(numbers),  # í™€ìˆ˜ ë¹„ìœ¨
                ]
            )

            # êµ¬ê°„ ë¶„í¬ (1-9, 10-18, 19-27, 28-36, 37-45)
            segments = [0, 0, 0, 0, 0]
            for num in numbers:
                if 1 <= num <= 9:
                    segments[0] += 1
                elif 10 <= num <= 18:
                    segments[1] += 1
                elif 19 <= num <= 27:
                    segments[2] += 1
                elif 28 <= num <= 36:
                    segments[3] += 1
                elif 37 <= num <= 45:
                    segments[4] += 1

            features.extend(segments)

            # ì—°ì† ë²ˆí˜¸ ì •ë³´
            consecutive_count = 0
            for i in range(len(numbers) - 1):
                if numbers[i + 1] - numbers[i] == 1:
                    consecutive_count += 1
            features.append(consecutive_count)

            # íŒ¨í„´ íŠ¹ì„± (ê°„ë‹¨í•œ ë²„ì „)
            features.extend(
                [
                    len(set(numbers)),  # ê³ ìœ  ë²ˆí˜¸ ìˆ˜ (í•­ìƒ 6ì´ì§€ë§Œ ì¼ê´€ì„± ìœ„í•´)
                    sum(numbers) % 10,  # í•©ê³„ì˜ ì¼ì˜ ìë¦¬
                    np.var(numbers),  # ë¶„ì‚°
                    np.sum(np.diff(numbers)),  # ì´ ê°„ê²© í•©
                ]
            )

            return features

        except Exception as e:
            logger.error(f"ê¸°ë³¸ íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return [0.0] * 25  # ê¸°ë³¸ íŠ¹ì„± 25ê°œ

    def _create_target_variable(
        self, current_draw: LotteryNumber, next_draw: LotteryNumber
    ) -> float:
        """íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± (ë‹¤ìŒ íšŒì°¨ì™€ì˜ ê´€ê³„)"""
        try:
            # ê°„ë‹¨í•œ íƒ€ê²Ÿ: ë‹¤ìŒ íšŒì°¨ì™€ í˜„ì¬ íšŒì°¨ì˜ ê³µí†µ ë²ˆí˜¸ ê°œìˆ˜
            current_set = set(current_draw.numbers)
            next_set = set(next_draw.numbers)
            overlap_count = len(current_set & next_set)

            # ì •ê·œí™” (0~1 ë²”ìœ„)
            return overlap_count / 6.0

        except Exception as e:
            logger.error(f"íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± ì‹¤íŒ¨: {e}")
            return 0.0

    def _analyze_feature_importance(
        self, feature_matrix: np.ndarray, target_vector: np.ndarray
    ) -> Dict[str, Any]:
        """íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„"""
        try:
            importance_result = {}

            # 1. Random Forest ê¸°ë°˜ íŠ¹ì„± ì¤‘ìš”ë„
            rf_importance = self._calculate_rf_importance(feature_matrix, target_vector)
            importance_result["random_forest"] = rf_importance

            # 2. ìƒê´€ê´€ê³„ ê¸°ë°˜ ì¤‘ìš”ë„
            correlation_importance = self._calculate_correlation_importance(
                feature_matrix, target_vector
            )
            importance_result["correlation"] = correlation_importance

            # 3. ìƒí˜¸ì •ë³´ ê¸°ë°˜ ì¤‘ìš”ë„
            mutual_info_importance = self._calculate_mutual_info_importance(
                feature_matrix, target_vector
            )
            importance_result["mutual_info"] = mutual_info_importance

            # 4. SHAP ê°’ (ì„ íƒì )
            if self.use_shap:
                shap_importance = self._calculate_shap_importance(
                    feature_matrix, target_vector
                )
                importance_result["shap"] = shap_importance

            # 5. LIME ì„¤ëª… (ì„ íƒì )
            if self.use_lime:
                lime_importance = self._calculate_lime_importance(
                    feature_matrix, target_vector
                )
                importance_result["lime"] = lime_importance

            # 6. ì¢…í•© ì¤‘ìš”ë„ ì ìˆ˜
            combined_importance = self._combine_importance_scores(importance_result)
            importance_result["combined"] = combined_importance

            return importance_result

        except Exception as e:
            logger.error(f"íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    def _calculate_rf_importance(
        self, X: np.ndarray, y: np.ndarray
    ) -> Dict[str, float]:
        """Random Forest ê¸°ë°˜ íŠ¹ì„± ì¤‘ìš”ë„"""
        try:
            rf = RandomForestRegressor(n_estimators=100, random_state=42)
            rf.fit(X, y)

            importance_scores = {}
            for i, importance in enumerate(rf.feature_importances_):
                importance_scores[f"feature_{i}"] = float(importance)

            return importance_scores

        except Exception as e:
            logger.error(f"RF ì¤‘ìš”ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {}

    def _calculate_correlation_importance(
        self, X: np.ndarray, y: np.ndarray
    ) -> Dict[str, float]:
        """ìƒê´€ê´€ê³„ ê¸°ë°˜ ì¤‘ìš”ë„"""
        try:
            importance_scores = {}

            for i in range(X.shape[1]):
                correlation = np.corrcoef(X[:, i], y)[0, 1]
                # NaN ì²˜ë¦¬
                if np.isnan(correlation):
                    correlation = 0.0
                importance_scores[f"feature_{i}"] = float(abs(correlation))

            return importance_scores

        except Exception as e:
            logger.error(f"ìƒê´€ê´€ê³„ ì¤‘ìš”ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {}

    def _calculate_mutual_info_importance(
        self, X: np.ndarray, y: np.ndarray
    ) -> Dict[str, float]:
        """ìƒí˜¸ì •ë³´ ê¸°ë°˜ ì¤‘ìš”ë„"""
        try:
            mutual_info_scores = mutual_info_regression(X, y)

            importance_scores = {}
            for i, score in enumerate(mutual_info_scores):
                importance_scores[f"feature_{i}"] = float(score)

            return importance_scores

        except Exception as e:
            logger.error(f"ìƒí˜¸ì •ë³´ ì¤‘ìš”ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {}

    def _calculate_shap_importance(
        self, X: np.ndarray, y: np.ndarray
    ) -> Dict[str, float]:
        """SHAP ê°’ ê¸°ë°˜ ì¤‘ìš”ë„"""
        try:
            import shap

            # ëª¨ë¸ í›ˆë ¨
            rf = RandomForestRegressor(n_estimators=50, random_state=42)
            rf.fit(X, y)

            # SHAP ê°’ ê³„ì‚°
            explainer = shap.TreeExplainer(rf)
            shap_values = explainer.shap_values(X)

            # í‰ê·  ì ˆëŒ“ê°’ìœ¼ë¡œ ì¤‘ìš”ë„ ê³„ì‚°
            importance_scores = {}
            for i in range(X.shape[1]):
                importance_scores[f"feature_{i}"] = float(
                    np.mean(np.abs(shap_values[:, i]))
                )

            return importance_scores

        except ImportError:
            logger.warning("SHAP ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ")
            return {}
        except Exception as e:
            logger.error(f"SHAP ì¤‘ìš”ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {}

    def _calculate_lime_importance(
        self, X: np.ndarray, y: np.ndarray
    ) -> Dict[str, float]:
        """LIME ê¸°ë°˜ ì¤‘ìš”ë„"""
        try:
            from lime import lime_tabular

            # ëª¨ë¸ í›ˆë ¨
            rf = RandomForestRegressor(n_estimators=50, random_state=42)
            rf.fit(X, y)

            # LIME ì„¤ëª…ê¸° ìƒì„±
            explainer = lime_tabular.LimeTabularExplainer(
                X,
                mode="regression",
                feature_names=[f"feature_{i}" for i in range(X.shape[1])],
            )

            # ìƒ˜í”Œ ì„¤ëª…
            importance_scores = {}
            sample_indices = np.random.choice(len(X), min(10, len(X)), replace=False)

            for idx in sample_indices:
                explanation = explainer.explain_instance(X[idx], rf.predict)
                for feature_name, importance in explanation.as_list():
                    if feature_name not in importance_scores:
                        importance_scores[feature_name] = []
                    importance_scores[feature_name].append(abs(importance))

            # í‰ê·  ì¤‘ìš”ë„ ê³„ì‚°
            for feature_name in importance_scores:
                importance_scores[feature_name] = float(
                    np.mean(importance_scores[feature_name])
                )

            return importance_scores

        except ImportError:
            logger.warning("LIME ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ")
            return {}
        except Exception as e:
            logger.error(f"LIME ì¤‘ìš”ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {}

    def _combine_importance_scores(
        self, importance_result: Dict[str, Any]
    ) -> Dict[str, float]:
        """ì—¬ëŸ¬ ì¤‘ìš”ë„ ì ìˆ˜ë¥¼ ì¢…í•©"""
        try:
            combined_scores = {}

            # ì‚¬ìš© ê°€ëŠ¥í•œ ì¤‘ìš”ë„ ë°©ë²•ë“¤
            methods = ["random_forest", "correlation", "mutual_info", "shap", "lime"]
            available_methods = [
                m for m in methods if m in importance_result and importance_result[m]
            ]

            if not available_methods:
                return {}

            # íŠ¹ì„± ì´ë¦„ ì¶”ì¶œ
            feature_names = set()
            for method in available_methods:
                feature_names.update(importance_result[method].keys())

            # ê° íŠ¹ì„±ë³„ ì¢…í•© ì ìˆ˜ ê³„ì‚°
            for feature_name in feature_names:
                scores = []
                for method in available_methods:
                    if feature_name in importance_result[method]:
                        scores.append(importance_result[method][feature_name])

                if scores:
                    combined_scores[feature_name] = float(np.mean(scores))

            return combined_scores

        except Exception as e:
            logger.error(f"ì¤‘ìš”ë„ ì ìˆ˜ ì¢…í•© ì‹¤íŒ¨: {e}")
            return {}

    def _analyze_feature_interactions(
        self, X: np.ndarray, y: np.ndarray
    ) -> Dict[str, Any]:
        """íŠ¹ì„± ìƒí˜¸ì‘ìš© ë¶„ì„"""
        try:
            interaction_result = {}

            # 1. ë‹¤í•­ì‹ íŠ¹ì„± ìƒì„±
            polynomial_features = self._generate_polynomial_features(X)
            interaction_result["polynomial"] = polynomial_features

            # 2. íŠ¹ì„± ê°„ ìƒê´€ê´€ê³„ í–‰ë ¬
            correlation_matrix = self._calculate_feature_correlations(X)
            interaction_result["correlation_matrix"] = correlation_matrix

            # 3. ìƒí˜¸ì‘ìš© ì¤‘ìš”ë„ (ë‹¤í•­ì‹ íŠ¹ì„± ê¸°ë°˜)
            if polynomial_features["success"]:
                interaction_importance = self._calculate_interaction_importance(
                    polynomial_features["features"], y
                )
                interaction_result["interaction_importance"] = interaction_importance

            return interaction_result

        except Exception as e:
            logger.error(f"íŠ¹ì„± ìƒí˜¸ì‘ìš© ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    def _generate_polynomial_features(self, X: np.ndarray) -> Dict[str, Any]:
        """ë‹¤í•­ì‹ íŠ¹ì„± ìƒì„±"""
        try:
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³ ë ¤í•˜ì—¬ íŠ¹ì„± ìˆ˜ ì œí•œ
            max_features = min(X.shape[1], 15)  # ìµœëŒ€ 15ê°œ íŠ¹ì„±ë§Œ ì‚¬ìš©
            X_subset = X[:, :max_features]

            poly = PolynomialFeatures(degree=self.polynomial_degree, include_bias=False)
            X_poly = poly.fit_transform(X_subset)

            # ì›ë³¸ íŠ¹ì„± ì œê±° (ìƒí˜¸ì‘ìš© í•­ë§Œ ì¶”ì¶œ)
            X_interactions = X_poly[:, max_features:]

            result = {
                "success": True,
                "features": X_interactions,
                "feature_names": poly.get_feature_names_out()[:max_features],
                "interaction_features": poly.get_feature_names_out()[max_features:],
                "original_features": max_features,
                "interaction_count": X_interactions.shape[1],
            }

            logger.info(
                f"ë‹¤í•­ì‹ íŠ¹ì„± ìƒì„± ì™„ë£Œ: {X_interactions.shape[1]}ê°œ ìƒí˜¸ì‘ìš© íŠ¹ì„±"
            )
            return result

        except Exception as e:
            logger.error(f"ë‹¤í•­ì‹ íŠ¹ì„± ìƒì„± ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}

    def _calculate_feature_correlations(self, X: np.ndarray) -> List[List[float]]:
        """íŠ¹ì„± ê°„ ìƒê´€ê´€ê³„ í–‰ë ¬"""
        try:
            correlation_matrix = np.corrcoef(X, rowvar=False)
            # NaN ì²˜ë¦¬
            correlation_matrix = np.nan_to_num(correlation_matrix, nan=0.0)
            return correlation_matrix.tolist()

        except Exception as e:
            logger.error(f"ìƒê´€ê´€ê³„ í–‰ë ¬ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return [[0.0] * X.shape[1] for _ in range(X.shape[1])]

    def _calculate_interaction_importance(
        self, X_interactions: np.ndarray, y: np.ndarray
    ) -> Dict[str, float]:
        """ìƒí˜¸ì‘ìš© íŠ¹ì„± ì¤‘ìš”ë„"""
        try:
            # Random Forestë¡œ ìƒí˜¸ì‘ìš© ì¤‘ìš”ë„ ê³„ì‚°
            rf = RandomForestRegressor(n_estimators=50, random_state=42)
            rf.fit(X_interactions, y)

            importance_scores = {}
            for i, importance in enumerate(rf.feature_importances_):
                importance_scores[f"interaction_{i}"] = float(importance)

            return importance_scores

        except Exception as e:
            logger.error(f"ìƒí˜¸ì‘ìš© ì¤‘ìš”ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {}

    def _perform_dimension_reduction(self, X: np.ndarray) -> Dict[str, Any]:
        """ì°¨ì› ì¶•ì†Œ ìˆ˜í–‰"""
        try:
            reduction_result = {}

            # 1. PCA
            pca_result = self._perform_pca(X)
            reduction_result["pca"] = pca_result

            # 2. t-SNE (ë°ì´í„°ê°€ ì¶©ë¶„í•œ ê²½ìš°ë§Œ)
            if X.shape[0] > 30:
                tsne_result = self._perform_tsne(X)
                reduction_result["tsne"] = tsne_result

            # 3. íŠ¹ì„± ì„ íƒì„ í†µí•œ ì°¨ì› ì¶•ì†Œ
            feature_selection_result = self._perform_univariate_selection(X)
            reduction_result["feature_selection"] = feature_selection_result

            return reduction_result

        except Exception as e:
            logger.error(f"ì°¨ì› ì¶•ì†Œ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    def _perform_pca(self, X: np.ndarray) -> Dict[str, Any]:
        """PCA ìˆ˜í–‰"""
        try:
            # ì»´í¬ë„ŒíŠ¸ ìˆ˜ ì¡°ì •
            n_components = min(self.pca_components, X.shape[1], X.shape[0] - 1)

            pca = PCA(n_components=n_components)
            X_pca = pca.fit_transform(X)

            result = {
                "success": True,
                "transformed_features": X_pca.tolist(),
                "explained_variance_ratio": pca.explained_variance_ratio_.tolist(),
                "cumulative_variance_ratio": np.cumsum(
                    pca.explained_variance_ratio_
                ).tolist(),
                "n_components": n_components,
                "total_variance_explained": float(
                    np.sum(pca.explained_variance_ratio_)
                ),
            }

            # ëª¨ë¸ ì €ì¥
            self.dimension_reduction_models["pca"] = pca

            return result

        except Exception as e:
            logger.error(f"PCA ìˆ˜í–‰ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}

    def _perform_tsne(self, X: np.ndarray) -> Dict[str, Any]:
        """t-SNE ìˆ˜í–‰"""
        try:
            # í¼í”Œë ‰ì‹œí‹° ì¡°ì •
            perplexity = min(30, X.shape[0] // 4)

            tsne = TSNE(
                n_components=self.tsne_components,
                perplexity=perplexity,
                random_state=42,
            )
            X_tsne = tsne.fit_transform(X)

            result = {
                "success": True,
                "transformed_features": X_tsne.tolist(),
                "n_components": self.tsne_components,
                "perplexity": perplexity,
            }

            return result

        except Exception as e:
            logger.error(f"t-SNE ìˆ˜í–‰ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}

    def _perform_univariate_selection(self, X: np.ndarray) -> Dict[str, Any]:
        """ë‹¨ë³€ëŸ‰ íŠ¹ì„± ì„ íƒ"""
        try:
            # k ê°’ ì¡°ì •
            k = min(self.select_k_best, X.shape[1])

            # F-score ê¸°ë°˜ ì„ íƒ
            selector = SelectKBest(score_func=f_regression, k=k)
            X_selected = selector.fit_transform(
                X, np.random.rand(X.shape[0])
            )  # ë”ë¯¸ íƒ€ê²Ÿ

            result = {
                "success": True,
                "selected_features": X_selected.tolist(),
                "selected_indices": selector.get_support(indices=True).tolist(),
                "scores": selector.scores_.tolist(),
                "k": k,
            }

            return result

        except Exception as e:
            logger.error(f"ë‹¨ë³€ëŸ‰ íŠ¹ì„± ì„ íƒ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}

    def _perform_feature_selection(
        self, X: np.ndarray, y: np.ndarray
    ) -> Dict[str, Any]:
        """íŠ¹ì„± ì„ íƒ ìˆ˜í–‰"""
        try:
            selection_result = {}

            # 1. ë¶„ì‚° ê¸°ë°˜ ì„ íƒ
            variance_selection = self._variance_based_selection(X)
            selection_result["variance_based"] = variance_selection

            # 2. ìƒê´€ê´€ê³„ ê¸°ë°˜ ì„ íƒ
            correlation_selection = self._correlation_based_selection(X, y)
            selection_result["correlation_based"] = correlation_selection

            # 3. ëª¨ë¸ ê¸°ë°˜ ì„ íƒ
            model_selection = self._model_based_selection(X, y)
            selection_result["model_based"] = model_selection

            return selection_result

        except Exception as e:
            logger.error(f"íŠ¹ì„± ì„ íƒ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    def _variance_based_selection(self, X: np.ndarray) -> Dict[str, Any]:
        """ë¶„ì‚° ê¸°ë°˜ íŠ¹ì„± ì„ íƒ"""
        try:
            from sklearn.feature_selection import VarianceThreshold

            # ë¶„ì‚° ì„ê³„ê°’ ì„¤ì •
            threshold = 0.01
            selector = VarianceThreshold(threshold=threshold)
            X_selected = selector.fit_transform(X)

            result = {
                "success": True,
                "selected_features": X_selected.tolist(),
                "selected_indices": selector.get_support(indices=True).tolist(),
                "threshold": threshold,
                "selected_count": X_selected.shape[1],
            }

            return result

        except Exception as e:
            logger.error(f"ë¶„ì‚° ê¸°ë°˜ ì„ íƒ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}

    def _correlation_based_selection(
        self, X: np.ndarray, y: np.ndarray
    ) -> Dict[str, Any]:
        """ìƒê´€ê´€ê³„ ê¸°ë°˜ íŠ¹ì„± ì„ íƒ"""
        try:
            # íƒ€ê²Ÿê³¼ì˜ ìƒê´€ê´€ê³„ ê³„ì‚°
            correlations = []
            for i in range(X.shape[1]):
                corr = np.corrcoef(X[:, i], y)[0, 1]
                correlations.append(abs(corr) if not np.isnan(corr) else 0.0)

            # ìƒìœ„ íŠ¹ì„± ì„ íƒ
            top_k = min(20, len(correlations))
            top_indices = np.argsort(correlations)[-top_k:]

            result = {
                "success": True,
                "correlations": correlations,
                "top_indices": top_indices.tolist(),
                "top_correlations": [correlations[i] for i in top_indices],
                "selected_count": top_k,
            }

            return result

        except Exception as e:
            logger.error(f"ìƒê´€ê´€ê³„ ê¸°ë°˜ ì„ íƒ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}

    def _model_based_selection(self, X: np.ndarray, y: np.ndarray) -> Dict[str, Any]:
        """ëª¨ë¸ ê¸°ë°˜ íŠ¹ì„± ì„ íƒ"""
        try:
            from sklearn.feature_selection import SelectFromModel

            # Random Forest ê¸°ë°˜ ì„ íƒ
            rf = RandomForestRegressor(n_estimators=50, random_state=42)
            selector = SelectFromModel(rf, threshold="median")
            X_selected = selector.fit_transform(X, y)

            result = {
                "success": True,
                "selected_features": X_selected.tolist(),
                "selected_indices": selector.get_support(indices=True).tolist(),
                "selected_count": X_selected.shape[1],
                "feature_importances": rf.feature_importances_.tolist(),
            }

            return result

        except Exception as e:
            logger.error(f"ëª¨ë¸ ê¸°ë°˜ ì„ íƒ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}

    def _calculate_meta_statistics(
        self, X: np.ndarray, y: np.ndarray
    ) -> Dict[str, Any]:
        """ë©”íƒ€ í†µê³„ ê³„ì‚°"""
        try:
            stats = {
                "data_shape": X.shape,
                "feature_statistics": {
                    "mean": np.mean(X, axis=0).tolist(),
                    "std": np.std(X, axis=0).tolist(),
                    "min": np.min(X, axis=0).tolist(),
                    "max": np.max(X, axis=0).tolist(),
                    "median": np.median(X, axis=0).tolist(),
                },
                "target_statistics": {
                    "mean": float(np.mean(y)),
                    "std": float(np.std(y)),
                    "min": float(np.min(y)),
                    "max": float(np.max(y)),
                    "median": float(np.median(y)),
                },
                "data_quality": {
                    "missing_values": int(np.sum(np.isnan(X))),
                    "infinite_values": int(np.sum(np.isinf(X))),
                    "zero_values": int(np.sum(X == 0)),
                    "constant_features": int(np.sum(np.std(X, axis=0) == 0)),
                },
            }

            return stats

        except Exception as e:
            logger.error(f"ë©”íƒ€ í†µê³„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    def _basic_meta_analysis(self, data: List[LotteryNumber]) -> Dict[str, Any]:
        """ê¸°ë³¸ ë©”íƒ€ ë¶„ì„ (ë°ì´í„° ë¶€ì¡± ì‹œ)"""
        try:
            basic_stats = {
                "data_count": len(data),
                "status": "basic_analysis_only",
                "reason": "insufficient_data",
                "min_required": 50,
                "basic_statistics": {},
            }

            if data:
                # ê¸°ë³¸ í†µê³„ë§Œ ê³„ì‚°
                all_numbers = []
                for draw in data:
                    all_numbers.extend(draw.numbers)

                basic_stats["basic_statistics"] = {
                    "number_count": len(all_numbers),
                    "unique_numbers": len(set(all_numbers)),
                    "mean_number": float(np.mean(all_numbers)),
                    "std_number": float(np.std(all_numbers)),
                    "min_number": int(np.min(all_numbers)),
                    "max_number": int(np.max(all_numbers)),
                }

            return basic_stats

        except Exception as e:
            logger.error(f"ê¸°ë³¸ ë©”íƒ€ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    def _get_fallback_result(self) -> Dict[str, Any]:
        """í´ë°± ê²°ê³¼ ë°˜í™˜"""
        return {
            "importance_analysis": {"error": "analysis_failed"},
            "interaction_analysis": {"error": "analysis_failed"},
            "dimension_reduction": {"error": "analysis_failed"},
            "feature_selection": {"error": "analysis_failed"},
            "meta_statistics": {"error": "analysis_failed"},
            "original_features": 0,
            "data_samples": 0,
        }

    def get_meta_features_vector(self, meta_analysis: Dict[str, Any]) -> np.ndarray:
        """ë©”íƒ€ ë¶„ì„ ê²°ê³¼ë¥¼ ë²¡í„°ë¡œ ë³€í™˜"""
        try:
            features = []

            # 1. íŠ¹ì„± ì¤‘ìš”ë„ ìš”ì•½
            importance_analysis = meta_analysis.get("importance_analysis", {})
            if "combined" in importance_analysis:
                importance_values = list(importance_analysis["combined"].values())
                if importance_values:
                    features.extend(
                        [
                            np.mean(importance_values),
                            np.std(importance_values),
                            np.max(importance_values),
                            np.min(importance_values),
                        ]
                    )
                else:
                    features.extend([0.0, 0.0, 0.0, 0.0])
            else:
                features.extend([0.0, 0.0, 0.0, 0.0])

            # 2. ì°¨ì› ì¶•ì†Œ ê²°ê³¼
            dimension_reduction = meta_analysis.get("dimension_reduction", {})
            if "pca" in dimension_reduction and dimension_reduction["pca"].get(
                "success", False
            ):
                pca_result = dimension_reduction["pca"]
                features.extend(
                    [
                        pca_result.get("total_variance_explained", 0.0),
                        pca_result.get("n_components", 0),
                        len(pca_result.get("explained_variance_ratio", [])),
                    ]
                )
            else:
                features.extend([0.0, 0.0, 0.0])

            # 3. íŠ¹ì„± ì„ íƒ ê²°ê³¼
            feature_selection = meta_analysis.get("feature_selection", {})
            if "model_based" in feature_selection and feature_selection[
                "model_based"
            ].get("success", False):
                model_result = feature_selection["model_based"]
                features.extend(
                    [
                        model_result.get("selected_count", 0),
                        meta_analysis.get("original_features", 0),
                        model_result.get("selected_count", 0)
                        / max(meta_analysis.get("original_features", 1), 1),
                    ]
                )
            else:
                features.extend([0.0, 0.0, 0.0])

            # 4. ë©”íƒ€ í†µê³„ ìš”ì•½
            meta_statistics = meta_analysis.get("meta_statistics", {})
            if "data_quality" in meta_statistics:
                data_quality = meta_statistics["data_quality"]
                features.extend(
                    [
                        data_quality.get("missing_values", 0),
                        data_quality.get("infinite_values", 0),
                        data_quality.get("zero_values", 0),
                        data_quality.get("constant_features", 0),
                    ]
                )
            else:
                features.extend([0.0, 0.0, 0.0, 0.0])

            # 5. ì¶”ê°€ ë©”íƒ€ íŠ¹ì„±
            features.extend(
                [
                    meta_analysis.get("data_samples", 0),
                    meta_analysis.get("original_features", 0),
                    len(importance_analysis.get("combined", {})),
                    1.0 if "error" not in meta_analysis else 0.0,  # ì„±ê³µ ì—¬ë¶€
                ]
            )

            return np.array(features, dtype=np.float32)

        except Exception as e:
            logger.error(f"ë©”íƒ€ íŠ¹ì„± ë²¡í„° ë³€í™˜ ì‹¤íŒ¨: {e}")
            return np.zeros(18, dtype=np.float32)  # ê¸°ë³¸ 18ì°¨ì› ë²¡í„°

    def save_meta_analysis_results(
        self, results: Dict[str, Any], filename: str = "meta_analysis_results.json"
    ) -> bool:
        """ë©”íƒ€ ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        try:
            from pathlib import Path

            # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
            results_dir = Path("data/analysis_results")
            results_dir.mkdir(parents=True, exist_ok=True)

            # ê²°ê³¼ ì €ì¥
            output_path = results_dir / filename
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(results, f, ensure_ascii=False, indent=2, default=str)

            logger.info(f"ë©”íƒ€ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}")
            return True

        except Exception as e:
            logger.error(f"ë©”íƒ€ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
