"""
ë¡œë˜ ë²ˆí˜¸ì˜ í´ëŸ¬ìŠ¤í„°ë¥¼ ë¶„ì„í•˜ê¸° ìœ„í•œ ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ ë¡œë˜ ë²ˆí˜¸ë“¤ ê°„ì˜ ê´€ê³„ë¥¼ ê·¸ë˜í”„ë¡œ ëª¨ë¸ë§í•˜ê³ 
í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆ˜í–‰í•˜ì—¬ íŒ¨í„´ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Dict, Tuple, Optional, Any
from ..utils.unified_logging import get_logger
from sklearn.metrics import silhouette_score
from sklearn.cluster import KMeans, DBSCAN
from .base_analyzer import BaseAnalyzer

# GPU ê°€ì† ë¼ì´ë¸ŒëŸ¬ë¦¬ (cuML, CuPy)
try:
    import cuml
    from cuml.cluster import KMeans as cuKMeans
    import cupy as cp
    CUML_AVAILABLE = True
except ImportError:
    cuml = None
    cuKMeans = None
    cp = None
    CUML_AVAILABLE = False

logger = get_logger(__name__)


def build_pair_graph(draw_history: List[Any]) -> np.ndarray:
    """
    ë¡œë˜ ë²ˆí˜¸ ì‚¬ì´ì˜ ë™ì‹œ ì¶œí˜„ ê´€ê³„ë¥¼ ì¸ì ‘ í–‰ë ¬ë¡œ êµ¬ì„±

    Args:
        draw_history: ë¡œë˜ ë²ˆí˜¸ ì´ë ¥

    Returns:
        ì¸ì ‘ í–‰ë ¬ (45x45)
    """
    # ì¸ì ‘ í–‰ë ¬ ì´ˆê¸°í™” (45x45)
    adjacency_matrix = np.zeros((45, 45))

    # ëª¨ë“  ë¡œë˜ ë²ˆí˜¸ ë°ì´í„° ì²˜ë¦¬
    for draw in draw_history:
        # ìˆ«ì ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (numbers ì†ì„±ì´ ìˆëŠ” ê²½ìš°)
        numbers = draw.numbers if hasattr(draw, "numbers") else draw

        # ëª¨ë“  ë²ˆí˜¸ ìŒì— ëŒ€í•´ ì—°ê²° ì¦ê°€
        for i in range(len(numbers)):
            for j in range(i + 1, len(numbers)):
                num1, num2 = numbers[i], numbers[j]
                if 1 <= num1 <= 45 and 1 <= num2 <= 45:
                    adjacency_matrix[num1 - 1, num2 - 1] += 1
                    adjacency_matrix[num2 - 1, num1 - 1] += 1

    # ì •ê·œí™”
    if np.max(adjacency_matrix) > 0:
        adjacency_matrix = adjacency_matrix / np.max(adjacency_matrix)

    return adjacency_matrix


def create_default_gnn_model() -> nn.Module:
    """
    ê¸°ë³¸ GNN ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Returns:
        nn.Module: ê¸°ë³¸ GNN ëª¨ë¸
    """
    logger.info("ê¸°ë³¸ GNN ëª¨ë¸ ìƒì„± ì¤‘...")

    class DefaultGNNModel(nn.Module):
        def __init__(self, input_dim=45, embedding_dim=32, hidden_dim=64):
            super().__init__()
            self.embedding = nn.Embedding(input_dim + 1, embedding_dim)  # +1ì€ íŒ¨ë”©ìš©
            self.conv1 = nn.Conv1d(embedding_dim, hidden_dim, kernel_size=3, padding=1)
            self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)
            self.fc = nn.Linear(hidden_dim, embedding_dim)

        def forward(self, x):
            # x: [batch_size, seq_len]
            x = self.embedding(x)  # [batch_size, seq_len, embedding_dim]
            x = x.permute(0, 2, 1)  # [batch_size, embedding_dim, seq_len]
            x = F.relu(self.conv1(x))
            x = F.relu(self.conv2(x))
            x = x.mean(dim=2)  # global average pooling
            x = self.fc(x)
            return x

    model = DefaultGNNModel()
    logger.info("ê¸°ë³¸ GNN ëª¨ë¸ ìƒì„± ì™„ë£Œ")
    return model


class ClusterAnalyzer(BaseAnalyzer[Dict[str, Any]]):
    """
    ë¡œë˜ ë²ˆí˜¸ì˜ í´ëŸ¬ìŠ¤í„°ë¥¼ ë¶„ì„í•˜ëŠ” í´ë˜ìŠ¤

    ì´ í´ë˜ìŠ¤ëŠ” ë¡œë˜ ë²ˆí˜¸ ê°„ì˜ ê´€ê³„ë¥¼ ë¶„ì„í•˜ê³ ,
    ë²ˆí˜¸ë“¤ì„ ì˜ë¯¸ ìˆëŠ” í´ëŸ¬ìŠ¤í„°ë¡œ ê·¸ë£¹í™”í•©ë‹ˆë‹¤.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        ì´ˆê¸°í™”

        Args:
            config: ì„¤ì • ê°ì²´
        """
        config = config or {}
        super().__init__(config, name="cluster")

        # GPU ì¥ì¹˜ ì„¤ì •
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.gpu_enabled = self.device.type == 'cuda' and CUML_AVAILABLE
        if self.gpu_enabled:
            logger.info("ğŸš€ cuML ë° CuPy ì‚¬ìš© ê°€ëŠ¥. í´ëŸ¬ìŠ¤í„° ë¶„ì„ì— GPU ê°€ì†ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        elif self.device.type == 'cuda' and not CUML_AVAILABLE:
            logger.warning("cuML ë˜ëŠ” CuPyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. GPU ê°€ì†ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # DBSCAN ì‚¬ìš© ì—¬ë¶€
        self.use_dbscan = False
        if "clustering" in config and "use_dbscan" in config["clustering"]:
            self.use_dbscan = config["clustering"]["use_dbscan"]

        # DBSCAN íŒŒë¼ë¯¸í„°
        self.eps = 0.5
        self.min_samples = 3
        if "clustering" in config and "dbscan" in config["clustering"]:
            dbscan_config = config["clustering"]["dbscan"]
            if "eps" in dbscan_config:
                self.eps = dbscan_config["eps"]
            if "min_samples" in dbscan_config:
                self.min_samples = dbscan_config["min_samples"]

        # KMeans íŒŒë¼ë¯¸í„°
        self.n_clusters = 4
        if "clustering" in config and "n_clusters" in config["clustering"]:
            self.n_clusters = config["clustering"]["n_clusters"]

        # í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ ìµœì†Œ ê¸°ì¤€
        self.min_silhouette_score = 0.1
        if "clustering" in config and "min_silhouette_score" in config["clustering"]:
            self.min_silhouette_score = config["clustering"]["min_silhouette_score"]

        # ìë™ í´ëŸ¬ìŠ¤í„° ìˆ˜ íƒìƒ‰ ì„¤ì •
        self.auto_adjust_clusters = True
        self.min_clusters = 2
        self.max_clusters = 8
        if "clustering" in config and "auto_adjust_clusters" in config["clustering"]:
            self.auto_adjust_clusters = config["clustering"]["auto_adjust_clusters"]
            if "min_clusters" in config["clustering"]:
                self.min_clusters = config["clustering"]["min_clusters"]
            if "max_clusters" in config["clustering"]:
                self.max_clusters = config["clustering"]["max_clusters"]

        # ê²°ê³¼ ì €ì¥ ê²½ë¡œ
        self.result_path = "data/result/analysis"
        if "paths" in config and "analysis_result_dir" in config["paths"]:
            self.result_path = config["paths"]["analysis_result_dir"]

    def _analyze_impl(self, historical_data, *args, **kwargs) -> Dict[str, Any]:
        """
        í´ëŸ¬ìŠ¤í„° ë¶„ì„ì„ ìˆ˜í–‰í•˜ëŠ” ë©”ì¸ ë©”ì„œë“œ (BaseAnalyzer ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„)

        Args:
            historical_data: ë¶„ì„í•  ê³¼ê±° ë‹¹ì²¨ ë²ˆí˜¸ ëª©ë¡

        Returns:
            Dict[str, Any]: í´ëŸ¬ìŠ¤í„° ë¶„ì„ ê²°ê³¼
        """
        try:
            # LotteryNumber ê°ì²´ì—ì„œ numbers ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
            if hasattr(historical_data[0], "numbers"):
                draw_history = [draw.numbers for draw in historical_data]
            else:
                draw_history = historical_data

            # ê¸°ì¡´ analyze_clusters ë©”ì„œë“œ í˜¸ì¶œ
            return self.analyze_clusters(draw_history)

        except Exception as e:
            self.logger.error(f"í´ëŸ¬ìŠ¤í„° ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                "clusters": [],
                "adjacency_matrix": None,
                "embedding": None,
                "cluster_embedding_quality": {},
                "cluster_groups": {},
            }

    def _gpu_clustering(self, embeddings: np.ndarray, n_clusters: int) -> np.ndarray:
        """
        GPU(cuML)ë¥¼ ì‚¬ìš©í•˜ì—¬ K-Means í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

        Args:
            embeddings: ì„ë² ë”© ë²¡í„° (NumPy)
            n_clusters: í´ëŸ¬ìŠ¤í„° ìˆ˜

        Returns:
            í´ëŸ¬ìŠ¤í„° ë ˆì´ë¸” (NumPy)
        """
        if not self.gpu_enabled:
            raise RuntimeError("cuMLì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ê±°ë‚˜ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        try:
            # NumPy ë°°ì—´ì„ CuPy ë°°ì—´ë¡œ ë³€í™˜
            gpu_embeddings = cp.asarray(embeddings, dtype=cp.float32)

            # cuML K-Means ëª¨ë¸ ì´ˆê¸°í™” ë° í•™ìŠµ
            gpu_kmeans = cuKMeans(n_clusters=n_clusters, random_state=42)
            labels_gpu = gpu_kmeans.fit_predict(gpu_embeddings)

            # CuPy ë°°ì—´ì„ NumPy ë°°ì—´ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜
            return cp.asnumpy(labels_gpu)

        except Exception as e:
            logger.error(f"GPU í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {e}. CPUë¡œ í´ë°±í•©ë‹ˆë‹¤.")
            # ì‹¤íŒ¨ ì‹œ CPU KMeans ì‚¬ìš©
            kmeans = KMeans(n_clusters=n_clusters, n_init="auto", random_state=42)
            labels = kmeans.fit_predict(embeddings)
            return labels

    def _compute_cluster_quality(
        self, embeddings: np.ndarray, labels: np.ndarray
    ) -> Dict[str, Any]:
        """
        í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ ì§€í‘œ ê³„ì‚°

        Args:
            embeddings: ì„ë² ë”© ë²¡í„°
            labels: í´ëŸ¬ìŠ¤í„° ë ˆì´ë¸”

        Returns:
            í’ˆì§ˆ ì§€í‘œ ë”•ì…”ë„ˆë¦¬
        """
        quality_metrics = {}

        # í´ëŸ¬ìŠ¤í„° ê°œìˆ˜
        unique_labels = np.unique(labels)
        if -1 in unique_labels:  # DBSCANì—ì„œëŠ” -1ì´ ë…¸ì´ì¦ˆ í¬ì¸íŠ¸
            unique_labels = unique_labels[unique_labels != -1]
        cluster_count = len(unique_labels)
        quality_metrics["cluster_count"] = int(cluster_count)

        # ì‹¤ë£¨ì—£ ì ìˆ˜ ê³„ì‚°
        silhouette = 0.0
        if cluster_count > 1 and len(embeddings) > cluster_count:
            try:
                # -1 ë ˆì´ë¸”(ë…¸ì´ì¦ˆ)ì„ ì œì™¸í•˜ê³  ê³„ì‚°
                valid_indices = labels != -1
                if np.sum(valid_indices) > cluster_count:
                    silhouette = float(
                        silhouette_score(
                            embeddings[valid_indices],
                            labels[valid_indices],
                            sample_size=min(1000, np.sum(valid_indices)),
                        )
                    )
            except Exception as e:
                self.logger.warning(f"ì‹¤ë£¨ì—£ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")

        quality_metrics["silhouette_score"] = silhouette

        # í´ëŸ¬ìŠ¤í„° ìŠ¤ì½”ì–´ ê³„ì‚° (silhouette score ê¸°ë°˜)
        cluster_score = 0.0
        if cluster_count > 1:
            # ì‹¤ë£¨ì—£ ì ìˆ˜ë¥¼ 0~1 ë²”ìœ„ë¡œ ë³€í™˜ (-1~1 ë²”ìœ„ì—ì„œ)
            cluster_score = (silhouette + 1) / 2
        else:
            # í´ëŸ¬ìŠ¤í„°ê°€ 1ê°œì¸ ê²½ìš° ë˜ëŠ” ê±°ë¦¬ê°€ 0ì¸ ê²½ìš° ê²½ê³  ì¶œë ¥
            self.logger.warning(
                "í´ëŸ¬ìŠ¤í„° ìˆ˜ê°€ 1ê°œì´ê±°ë‚˜ ê±°ë¦¬ê°€ 0ì…ë‹ˆë‹¤. cluster_score=0.0ìœ¼ë¡œ ì„¤ì •"
            )
            cluster_score = 0.0

        quality_metrics["cluster_score"] = float(cluster_score)

        # í´ëŸ¬ìŠ¤í„° ê°„ ê±°ë¦¬
        avg_distance = 0.0
        if cluster_count > 1:
            # í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì  ê³„ì‚°
            centroids = []
            for label in unique_labels:
                if label == -1:  # ë…¸ì´ì¦ˆ ë¬´ì‹œ
                    continue
                mask = labels == label
                if np.sum(mask) > 0:
                    centroid = np.mean(embeddings[mask], axis=0)
                    centroids.append(centroid)

            # ì¤‘ì‹¬ì  ê°„ ê±°ë¦¬ ê³„ì‚°
            if len(centroids) > 1:
                distances = []
                for i in range(len(centroids)):
                    for j in range(i + 1, len(centroids)):
                        dist = np.linalg.norm(centroids[i] - centroids[j])
                        distances.append(dist)
                if distances:
                    avg_distance = float(np.mean(distances))

        quality_metrics["avg_distance_between_clusters"] = avg_distance

        # í´ëŸ¬ìŠ¤í„° í¬ê¸° ê· í˜•ì„±
        cluster_sizes = []
        for label in unique_labels:
            if label == -1:  # ë…¸ì´ì¦ˆ ë¬´ì‹œ
                continue
            cluster_sizes.append(np.sum(labels == label))

        std_size = 0.0
        balance_score = 1.0
        if cluster_sizes:
            std_size = float(np.std(cluster_sizes))
            mean_size = float(np.mean(cluster_sizes))
            if mean_size > 0:
                # ê· í˜• ì ìˆ˜: 1ì´ë©´ ì™„ë²½í•˜ê²Œ ê· í˜•, 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë¶ˆê· í˜•
                balance_score = 1.0 - min(1.0, std_size / mean_size)

        quality_metrics["std_of_cluster_size"] = std_size
        quality_metrics["balance_score"] = float(balance_score)

        # ìµœëŒ€ í´ëŸ¬ìŠ¤í„° í¬ê¸°
        largest_cluster_size = max(cluster_sizes) if cluster_sizes else 0
        quality_metrics["largest_cluster_size"] = float(largest_cluster_size)

        # ê°€ì¥ ì‘ì€ í´ëŸ¬ìŠ¤í„° í¬ê¸°
        smallest_cluster_size = min(cluster_sizes) if cluster_sizes else 0
        quality_metrics["smallest_cluster_size"] = float(smallest_cluster_size)

        # í´ëŸ¬ìŠ¤í„° í¬ê¸° ë¹„ìœ¨ (ê°€ì¥ í° / ê°€ì¥ ì‘ì€)
        size_ratio = 0.0
        if smallest_cluster_size > 0:
            size_ratio = largest_cluster_size / smallest_cluster_size
        quality_metrics["cluster_size_ratio"] = float(size_ratio)

        # í´ëŸ¬ìŠ¤í„° ì—”íŠ¸ë¡œí”¼ ì ìˆ˜ (ìƒˆë¡œ ì¶”ê°€)
        entropy_score = 0.0
        if cluster_sizes:
            # í´ëŸ¬ìŠ¤í„° í¬ê¸° ë¹„ìœ¨ ê³„ì‚°
            total_size = sum(cluster_sizes)
            size_ratios = [size / total_size for size in cluster_sizes]

            # ì—”íŠ¸ë¡œí”¼ ê³„ì‚° (ê· ë“±í•œ ë¶„í¬ì¼ìˆ˜ë¡ ë†’ìŒ)
            entropy = -sum(p * np.log2(p) if p > 0 else 0 for p in size_ratios)
            max_entropy = np.log2(len(cluster_sizes)) if len(cluster_sizes) > 0 else 1

            # ì •ê·œí™”ëœ ì—”íŠ¸ë¡œí”¼ (0-1 ë²”ìœ„)
            entropy_score = entropy / max_entropy if max_entropy > 0 else 0

        quality_metrics["cluster_entropy_score"] = float(entropy_score)

        # ì‘ì§‘ë„ ì ìˆ˜ (ìƒˆë¡œ ì¶”ê°€)
        cohesiveness_score = 0.0
        if cluster_count > 1:
            # í´ëŸ¬ìŠ¤í„° ë‚´ ê±°ë¦¬ í‰ê·  ê³„ì‚°
            intra_cluster_distances = []
            for label in unique_labels:
                if label == -1:  # ë…¸ì´ì¦ˆ ë¬´ì‹œ
                    continue
                cluster_points = embeddings[labels == label]
                if len(cluster_points) > 1:
                    # í´ëŸ¬ìŠ¤í„° ë‚´ ëª¨ë“  ì  ê°„ì˜ ê±°ë¦¬ ê³„ì‚°
                    for i in range(len(cluster_points)):
                        for j in range(i + 1, len(cluster_points)):
                            dist = np.linalg.norm(cluster_points[i] - cluster_points[j])
                            intra_cluster_distances.append(dist)

            # ë‚´ë¶€ ê±°ë¦¬ì™€ ì™¸ë¶€ ê±°ë¦¬ì˜ ë¹„ìœ¨ë¡œ ì‘ì§‘ë„ ê³„ì‚°
            if intra_cluster_distances and avg_distance > 0:
                avg_intra_distance = np.mean(intra_cluster_distances)
                cohesiveness_score = 1.0 - (avg_intra_distance / (avg_distance + 0.001))
                # 0-1 ë²”ìœ„ë¡œ ì œí•œ
                cohesiveness_score = max(0.0, min(1.0, cohesiveness_score))

        quality_metrics["cohesiveness_score"] = float(cohesiveness_score)

        # ì •ê·œí™”ëœ í´ëŸ¬ìŠ¤í„° ìˆ˜
        norm_cluster_count = 0.0
        if self.max_clusters > self.min_clusters:
            norm_cluster_count = (cluster_count - self.min_clusters) / (
                self.max_clusters - self.min_clusters
            )
            norm_cluster_count = max(0.0, min(1.0, norm_cluster_count))
        quality_metrics["cluster_count_norm"] = float(norm_cluster_count)

        # ì •ê·œí™”ëœ ìµœëŒ€ í´ëŸ¬ìŠ¤í„° í¬ê¸°
        if len(embeddings) > 0:
            norm_largest_size = largest_cluster_size / len(embeddings)
        else:
            norm_largest_size = 0.0
        quality_metrics["largest_cluster_size_norm"] = float(norm_largest_size)

        # ì •ê·œí™”ëœ í´ëŸ¬ìŠ¤í„° í¬ê¸° ë¹„ìœ¨
        norm_size_ratio = 0.0
        if size_ratio > 0:
            # ë¹„ìœ¨ì„ 0-1 ë²”ìœ„ë¡œ ë³€í™˜ (ë¹„ìœ¨ì´ 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ê· í˜•)
            norm_size_ratio = 1.0 / (1.0 + np.log(1 + size_ratio))
        quality_metrics["cluster_size_ratio_norm"] = float(norm_size_ratio)

        return quality_metrics

    def analyze_clusters(self, draw_history: List[List[int]]) -> Dict[str, Any]:
        """
        ë¡œë˜ ë²ˆí˜¸ì˜ í´ëŸ¬ìŠ¤í„°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.

        Args:
            draw_history: ë¡œë˜ ë²ˆí˜¸ ì´ë ¥

        Returns:
            Dict[str, Any]: í´ëŸ¬ìŠ¤í„° ë¶„ì„ ê²°ê³¼
        """
        self.logger.info(f"{len(draw_history)}ê°œì˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ í´ëŸ¬ìŠ¤í„° ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

        # ë²ˆí˜¸ ì„ë² ë”© ìƒì„±
        embeddings = self._create_number_embeddings(draw_history)
        if embeddings is None or len(embeddings) < 2:
            self.logger.warning("ìœ íš¨í•œ ì„ë² ë”©ì„ ìƒì„±í•  ìˆ˜ ì—†ì–´ ë¶„ì„ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            return {
                "clusters": [],
                "error": "ì„ë² ë”© ìƒì„± ì‹¤íŒ¨",
            }

        labels = None
        cluster_count = self.n_clusters

        # ìë™ í´ëŸ¬ìŠ¤í„° ìˆ˜ ì¡°ì •
        if self.auto_adjust_clusters:
            self.logger.info("ìµœì ì˜ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ ìë™ìœ¼ë¡œ íƒìƒ‰í•©ë‹ˆë‹¤...")
            best_score = -1
            best_k = self.n_clusters

            for k in range(self.min_clusters, self.max_clusters + 1):
                if self.gpu_enabled:
                    # GPU í´ëŸ¬ìŠ¤í„°ë§
                    current_labels = self._gpu_clustering(embeddings, k)
                else:
                    # CPU í´ëŸ¬ìŠ¤í„°ë§
                    kmeans = KMeans(n_clusters=k, n_init='auto', random_state=42)
                    current_labels = kmeans.fit_predict(embeddings)

                # ì‹¤ë£¨ì—£ ì ìˆ˜ ê³„ì‚° (ë°ì´í„° ì¼ë¶€ ìƒ˜í”Œë§)
                sample_size = min(len(embeddings), 1000)
                score = silhouette_score(embeddings, current_labels, sample_size=sample_size)
                self.logger.debug(f"k={k}ì¼ ë•Œ ì‹¤ë£¨ì—£ ì ìˆ˜: {score:.4f}")

                if score > best_score:
                    best_score = score
                    best_k = k

            cluster_count = best_k
            self.logger.info(f"ìµœì ì˜ í´ëŸ¬ìŠ¤í„° ìˆ˜: {cluster_count} (ì‹¤ë£¨ì—£ ì ìˆ˜: {best_score:.4f})")

        # ìµœì¢… í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
        if self.use_dbscan:
            self.logger.info("DBSCAN í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤...")
            dbscan = DBSCAN(eps=self.eps, min_samples=self.min_samples)
            labels = dbscan.fit_predict(embeddings)
        else:
            self.logger.info(f"K-Means í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤ (k={cluster_count})...")
            if self.gpu_enabled:
                labels = self._gpu_clustering(embeddings, cluster_count)
            else:
                kmeans = KMeans(n_clusters=cluster_count, n_init='auto', random_state=42)
                labels = kmeans.fit_predict(embeddings)

        if labels is None:
            self.logger.error("í´ëŸ¬ìŠ¤í„°ë§ ë ˆì´ë¸”ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return {"clusters": [], "error": "ë ˆì´ë¸” ìƒì„± ì‹¤íŒ¨"}

        # í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ í‰ê°€
        self.logger.info("í´ëŸ¬ìŠ¤í„° í’ˆì§ˆì„ í‰ê°€í•©ë‹ˆë‹¤...")
        quality_metrics = self._compute_cluster_quality(embeddings, labels)
        self.logger.info(f"í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ: {quality_metrics}")

        # ê²°ê³¼ ì •ë¦¬ ë° ë°˜í™˜
        cluster_groups = {}
        unique_labels = np.unique(labels)
        for label in unique_labels:
            if label == -1:  # DBSCANì˜ ë…¸ì´ì¦ˆ
                continue
            cluster_groups[int(label)] = (
                np.where(labels == label)[0] + 1
            ).tolist()

        return {
            "clusters": labels.tolist(),
            "adjacency_matrix": None,  # í•„ìš” ì‹œ ìƒì„± ë¡œì§ ì¶”ê°€
            "embedding": embeddings.tolist(),
            "cluster_embedding_quality": quality_metrics,
            "cluster_groups": cluster_groups,
        }

    def create_cluster_feature_vector(
        self, quality_metrics: Dict[str, Any]
    ) -> np.ndarray:
        """
        í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ ì§€í‘œë¥¼ íŠ¹ì„± ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

        Args:
            quality_metrics: í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ ì§€í‘œ ë”•ì…”ë„ˆë¦¬

        Returns:
            np.ndarray: í´ëŸ¬ìŠ¤í„° íŠ¹ì„± ë²¡í„° (12ì°¨ì›)
        """
        # 12ê°œ íŠ¹ì„±ìœ¼ë¡œ êµ¬ì„±ëœ ë²¡í„° ìƒì„± (ê¸°ì¡´ 8ê°œì—ì„œ í™•ì¥)
        feature_vector = np.zeros(12, dtype=np.float32)

        # 1. í´ëŸ¬ìŠ¤í„° ìŠ¤ì½”ì–´
        feature_vector[0] = quality_metrics.get("cluster_score", 0.0)

        # 2. ì‹¤ë£¨ì—£ ì ìˆ˜ (0-1ë¡œ ì •ê·œí™”)
        silhouette = quality_metrics.get("silhouette_score", 0.0)
        feature_vector[1] = (silhouette + 1) / 2  # -1~1 ë²”ìœ„ë¥¼ 0~1ë¡œ ë³€í™˜

        # 3. í´ëŸ¬ìŠ¤í„° ê°„ í‰ê·  ê±°ë¦¬ (ì •ê·œí™”)
        avg_distance = quality_metrics.get("avg_distance_between_clusters", 0.0)
        feature_vector[2] = min(avg_distance / 2.0, 1.0)  # ìµœëŒ€ 2.0ìœ¼ë¡œ ê°€ì •í•˜ê³  ì •ê·œí™”

        # 4. í´ëŸ¬ìŠ¤í„° í¬ê¸° í‘œì¤€í¸ì°¨ (ì •ê·œí™”)
        std_size = quality_metrics.get("std_of_cluster_size", 0.0)
        feature_vector[3] = min(std_size / 10.0, 1.0)  # ìµœëŒ€ 10ìœ¼ë¡œ ê°€ì •í•˜ê³  ì •ê·œí™”

        # 5. ê· í˜• ì ìˆ˜ (ì´ë¯¸ 0-1 ë²”ìœ„)
        feature_vector[4] = quality_metrics.get("balance_score", 1.0)

        # 6. í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ (ì •ê·œí™”)
        cluster_count = quality_metrics.get("cluster_count", 0)
        feature_vector[5] = quality_metrics.get(
            "cluster_count_norm", min(cluster_count / 8.0, 1.0)
        )

        # 7. ìµœëŒ€ í´ëŸ¬ìŠ¤í„° í¬ê¸° (ì •ê·œí™”)
        feature_vector[6] = quality_metrics.get("largest_cluster_size_norm", 0.0)

        # 8. í´ëŸ¬ìŠ¤í„° í¬ê¸° ë¹„ìœ¨ (ì •ê·œí™”)
        feature_vector[7] = quality_metrics.get("cluster_size_ratio_norm", 0.0)

        # 9. í´ëŸ¬ìŠ¤í„° ì—”íŠ¸ë¡œí”¼ ì ìˆ˜ (ìƒˆë¡œ ì¶”ê°€)
        feature_vector[8] = quality_metrics.get("cluster_entropy_score", 0.5)

        # 10. ì‘ì§‘ë„ ì ìˆ˜ (ìƒˆë¡œ ì¶”ê°€)
        feature_vector[9] = quality_metrics.get("cohesiveness_score", 0.5)

        # 11. ìµœì†Œ í´ëŸ¬ìŠ¤í„° í¬ê¸° (ì •ê·œí™”)
        smallest_size = quality_metrics.get("smallest_cluster_size", 0.0)
        if smallest_size > 0:
            feature_vector[10] = min(smallest_size / 10.0, 1.0)  # ìµœëŒ€ 10ìœ¼ë¡œ ê°€ì •
        else:
            feature_vector[10] = 0.0

        # 12. í´ëŸ¬ìŠ¤í„° ë¶„í¬ ê· ì¼ì„± (ì‹¤ë£¨ì—£ê³¼ ì—”íŠ¸ë¡œí”¼ì˜ ì¡°í•©)
        feature_vector[11] = (feature_vector[1] + feature_vector[8]) / 2.0

        return feature_vector

    def extract_cluster_features(
        self, data: List[List[int]]
    ) -> Tuple[np.ndarray, List[str]]:
        """
        í´ëŸ¬ìŠ¤í„° ë¶„ì„ ê²°ê³¼ë¥¼ 10ì°¨ì› íŠ¹ì„± ë²¡í„°ë¡œ ì¶”ì¶œ

        Args:
            data: ë¡œë˜ ë²ˆí˜¸ ì´ë ¥ ë°ì´í„°

        Returns:
            Tuple[np.ndarray, List[str]]: 10ì°¨ì› í´ëŸ¬ìŠ¤í„° íŠ¹ì„± ë²¡í„°ì™€ íŠ¹ì„± ì´ë¦„
        """
        self.logger.info("í´ëŸ¬ìŠ¤í„° íŠ¹ì„± ì¶”ì¶œ ì‹œì‘...")

        try:
            # ë°ì´í„° ë¶€ì¡± ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
            if not data or len(data) < 10:
                self.logger.warning("ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ê¸°ë³¸ í´ëŸ¬ìŠ¤í„° íŠ¹ì„± ë°˜í™˜")
                return self._get_default_cluster_features()

            # 1. ë²ˆí˜¸ë³„ ì„ë² ë”© ìƒì„±
            embeddings = self._create_number_embeddings(data)

            # 2. ì—¬ëŸ¬ K ê°’ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰ ë° í’ˆì§ˆ ì§€í‘œ ê³„ì‚°
            cluster_qualities = {}

            # K-means í´ëŸ¬ìŠ¤í„°ë§ (k=3,4,5)
            for k in [3, 4, 5]:
                try:
                    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
                    labels = kmeans.fit_predict(embeddings)

                    # í’ˆì§ˆ ì§€í‘œ ê³„ì‚°
                    quality = self._compute_cluster_quality(embeddings, labels)
                    cluster_qualities[f"k{k}"] = quality

                except Exception as e:
                    self.logger.warning(f"K={k} í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {e}")
                    cluster_qualities[f"k{k}"] = {
                        "silhouette_score": 0.0,
                        "cluster_score": 0.0,
                    }

            # 3. 10ì°¨ì› íŠ¹ì„± ë²¡í„° êµ¬ì„±
            features = []
            feature_names = []

            # ì‹¤ë£¨ì—£ ì ìˆ˜ (3ê°œ)
            for k in [3, 4, 5]:
                silhouette = cluster_qualities.get(f"k{k}", {}).get(
                    "silhouette_score", 0.0
                )
                # -1~1 ë²”ìœ„ë¥¼ 0~1ë¡œ ì •ê·œí™”
                normalized_silhouette = max(0.0, (silhouette + 1) / 2)
                features.append(normalized_silhouette)
                feature_names.append(f"cluster_silhouette_k{k}")

            # ì¹¼ë¦°ìŠ¤í‚¤-í•˜ë¼ë°”ì¦ˆ ì§€ìˆ˜ (3ê°œ)
            for k in [3, 4, 5]:
                try:
                    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
                    labels = kmeans.fit_predict(embeddings)

                    from sklearn.metrics import calinski_harabasz_score

                    ch_score = calinski_harabasz_score(embeddings, labels)
                    # ì •ê·œí™” (ì¼ë°˜ì ìœ¼ë¡œ 0~1000 ë²”ìœ„)
                    normalized_ch = min(1.0, ch_score / 1000.0)
                    features.append(normalized_ch)
                    feature_names.append(f"cluster_ch_score_k{k}")

                except Exception as e:
                    self.logger.warning(
                        f"ì¹¼ë¦°ìŠ¤í‚¤-í•˜ë¼ë°”ì¦ˆ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨ (k={k}): {e}"
                    )
                    features.append(0.0)
                    feature_names.append(f"cluster_ch_score_k{k}")

            # í´ëŸ¬ìŠ¤í„° ê· í˜•ë„ (1ê°œ)
            try:
                # ìµœì  K ì„ íƒ (ì‹¤ë£¨ì—£ ì ìˆ˜ ê¸°ì¤€)
                best_k = 3
                best_silhouette = -1
                for k in [3, 4, 5]:
                    sil = cluster_qualities.get(f"k{k}", {}).get("silhouette_score", -1)
                    if sil > best_silhouette:
                        best_silhouette = sil
                        best_k = k

                kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)
                labels = kmeans.fit_predict(embeddings)

                # í´ëŸ¬ìŠ¤í„°ë³„ í¬ê¸° ê· í˜•ë„ ê³„ì‚°
                unique, counts = np.unique(labels, return_counts=True)
                balance_score = (
                    1.0 - np.std(counts) / np.mean(counts) if len(counts) > 1 else 0.0
                )
                balance_score = max(0.0, min(1.0, balance_score))

                features.append(balance_score)
                feature_names.append("cluster_balance_score")

            except Exception as e:
                self.logger.warning(f"í´ëŸ¬ìŠ¤í„° ê· í˜•ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
                features.append(0.0)
                feature_names.append("cluster_balance_score")

            # í´ëŸ¬ìŠ¤í„° ë¶„ë¦¬ë„ (1ê°œ)
            try:
                # í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ ê°„ í‰ê·  ê±°ë¦¬
                centers = kmeans.cluster_centers_
                if len(centers) > 1:
                    distances = []
                    for i in range(len(centers)):
                        for j in range(i + 1, len(centers)):
                            dist = np.linalg.norm(centers[i] - centers[j])
                            distances.append(dist)

                    separation_score = np.mean(distances)
                    # ì •ê·œí™” (ì„ë² ë”© ì°¨ì› ê³ ë ¤)
                    separation_score = min(1.0, separation_score / 10.0)
                else:
                    separation_score = 0.0

                features.append(separation_score)
                feature_names.append("cluster_separation_score")

            except Exception as e:
                self.logger.warning(f"í´ëŸ¬ìŠ¤í„° ë¶„ë¦¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
                features.append(0.0)
                feature_names.append("cluster_separation_score")

            # í´ëŸ¬ìŠ¤í„° ë‚´ë¶€ ë¶„ì‚° (1ê°œ)
            try:
                intra_variance = 0.0
                for cluster_id in unique:
                    cluster_points = embeddings[labels == cluster_id]
                    if len(cluster_points) > 1:
                        cluster_center = np.mean(cluster_points, axis=0)
                        variance = np.mean(
                            np.sum((cluster_points - cluster_center) ** 2, axis=1)
                        )
                        intra_variance += variance

                intra_variance /= len(unique)
                # ì •ê·œí™”
                intra_variance = min(1.0, intra_variance / 100.0)

                features.append(intra_variance)
                feature_names.append("cluster_intra_variance")

            except Exception as e:
                self.logger.warning(f"í´ëŸ¬ìŠ¤í„° ë‚´ë¶€ ë¶„ì‚° ê³„ì‚° ì‹¤íŒ¨: {e}")
                features.append(0.0)
                feature_names.append("cluster_intra_variance")

            # í´ëŸ¬ìŠ¤í„° ê°„ ê±°ë¦¬ (1ê°œ)
            try:
                inter_distance = 0.0
                if len(centers) > 1:
                    min_distances = []
                    for i, center in enumerate(centers):
                        other_centers = np.delete(centers, i, axis=0)
                        min_dist = np.min(
                            [np.linalg.norm(center - other) for other in other_centers]
                        )
                        min_distances.append(min_dist)

                    inter_distance = np.mean(min_distances)
                    # ì •ê·œí™”
                    inter_distance = min(1.0, inter_distance / 10.0)

                features.append(inter_distance)
                feature_names.append("cluster_inter_distance")

            except Exception as e:
                self.logger.warning(f"í´ëŸ¬ìŠ¤í„° ê°„ ê±°ë¦¬ ê³„ì‚° ì‹¤íŒ¨: {e}")
                features.append(0.0)
                feature_names.append("cluster_inter_distance")

            # ìµœì¢… ê²€ì¦ (ì •í™•íˆ 10ì°¨ì›ì¸ì§€ í™•ì¸)
            if len(features) != 10:
                self.logger.warning(f"í´ëŸ¬ìŠ¤í„° íŠ¹ì„± ì°¨ì› ë¶ˆì¼ì¹˜: {len(features)} != 10")
                # 10ì°¨ì›ìœ¼ë¡œ ì¡°ì •
                if len(features) < 10:
                    features.extend([0.0] * (10 - len(features)))
                    feature_names.extend(
                        [f"cluster_padding_{i}" for i in range(10 - len(feature_names))]
                    )
                else:
                    features = features[:10]
                    feature_names = feature_names[:10]

            result_vector = np.array(features, dtype=np.float32)

            # NaN/Inf ì²˜ë¦¬
            result_vector = np.nan_to_num(
                result_vector, nan=0.0, posinf=1.0, neginf=0.0
            )
            result_vector = np.clip(result_vector, 0.0, 1.0)

            self.logger.info(f"í´ëŸ¬ìŠ¤í„° íŠ¹ì„± ì¶”ì¶œ ì™„ë£Œ: {len(result_vector)}ì°¨ì›")
            return result_vector, feature_names

        except Exception as e:
            self.logger.error(f"í´ëŸ¬ìŠ¤í„° íŠ¹ì„± ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return self._get_default_cluster_features()

    def _create_number_embeddings(self, data: List[List[int]]) -> np.ndarray:
        """
        ë²ˆí˜¸ë³„ ì„ë² ë”© ìƒì„±

        Args:
            data: ë¡œë˜ ë²ˆí˜¸ ì´ë ¥

        Returns:
            np.ndarray: 45x4 ì„ë² ë”© í–‰ë ¬ (ê° ë²ˆí˜¸ë‹¹ 4ì°¨ì› íŠ¹ì„±)
        """
        embeddings = np.zeros((45, 4), dtype=np.float32)

        try:
            # ê° ë²ˆí˜¸(1~45)ì— ëŒ€í•œ íŠ¹ì„± ê³„ì‚°
            for num in range(1, 46):
                # 1. ì¶œí˜„ ë¹ˆë„
                frequency = sum(1 for draw in data if num in draw)
                embeddings[num - 1, 0] = frequency / len(data) if data else 0.0

                # 2. í‰ê·  ìœ„ì¹˜ (ë²ˆí˜¸ê°€ ë‚˜íƒ€ë‚˜ëŠ” í‰ê·  ìœ„ì¹˜)
                positions = []
                for draw in data:
                    if num in draw:
                        try:
                            pos = draw.index(num)
                            positions.append(pos)
                        except ValueError:
                            continue

                avg_position = (
                    np.mean(positions) / 5.0 if positions else 0.5
                )  # 0~1 ì •ê·œí™”
                embeddings[num - 1, 1] = avg_position

                # 3. ìœ„ì¹˜ ë¶„ì‚°
                pos_variance = (
                    np.var(positions) / 25.0 if len(positions) > 1 else 0.0
                )  # 0~1 ì •ê·œí™”
                embeddings[num - 1, 2] = min(1.0, pos_variance)

                # 4. ìµœê·¼ ì¶œí˜„ìœ¨ (ìµœê·¼ 20íšŒ ì¤‘ ì¶œí˜„ ë¹„ìœ¨)
                recent_data = data[-20:] if len(data) >= 20 else data
                recent_frequency = sum(1 for draw in recent_data if num in draw)
                recent_rate = (
                    recent_frequency / len(recent_data) if recent_data else 0.0
                )
                embeddings[num - 1, 3] = recent_rate

        except Exception as e:
            self.logger.warning(f"ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")

        return embeddings

    def _get_default_cluster_features(self) -> Tuple[np.ndarray, List[str]]:
        """
        ê¸°ë³¸ í´ëŸ¬ìŠ¤í„° íŠ¹ì„± ë°˜í™˜ (ë°ì´í„° ë¶€ì¡± ì‹œ)

        Returns:
            Tuple[np.ndarray, List[str]]: ê¸°ë³¸ 10ì°¨ì› ë²¡í„°ì™€ íŠ¹ì„± ì´ë¦„
        """
        features = [0.0] * 10
        feature_names = [
            "cluster_silhouette_k3",
            "cluster_silhouette_k4",
            "cluster_silhouette_k5",
            "cluster_ch_score_k3",
            "cluster_ch_score_k4",
            "cluster_ch_score_k5",
            "cluster_balance_score",
            "cluster_separation_score",
            "cluster_intra_variance",
            "cluster_inter_distance",
        ]

        return np.array(features, dtype=np.float32), feature_names


__all__ = ["ClusterAnalyzer", "build_pair_graph", "create_default_gnn_model"]
