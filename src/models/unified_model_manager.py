"""
í†µí•© ëª¨ë¸ ê´€ë¦¬ì

TCN, AutoEncoder, LightGBM ëª¨ë¸ë“¤ì„ í†µí•©í•˜ì—¬ GPU ìµœì í™”ëœ ì˜ˆì¸¡ ì‹œìŠ¤í…œì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import time
import torch
import numpy as np
from typing import Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor

from ..utils.unified_logging import get_logger
from ..utils.cache_manager import UnifiedCachePathManager, CacheManager
from ..utils.model_saver import ModelSaver
from ..utils.unified_memory_manager import UnifiedMemoryManager
from ..utils.enhanced_process_pool import EnhancedProcessPool, DynamicBatchSizeController
from ..utils.unified_feature_vector_validator import UnifiedFeatureVectorValidator

# ëª¨ë¸ ì„í¬íŠ¸
from .dl.tcn_model import TCNModel
from .dl.autoencoder_model import AutoencoderModel
from .ml.lightgbm_model import LightGBMModel


logger = get_logger(__name__)


class UnifiedModelManager:
    """í†µí•© ëª¨ë¸ ê´€ë¦¬ì - GPU ìµœì í™”ëœ ëª¨ë¸ í†µí•© ì‹œìŠ¤í…œ"""

    def __init__(
        self,
        config: Optional[Dict[str, Any]] = None,
        cache_path_manager: UnifiedCachePathManager = None,
        batch_controller: DynamicBatchSizeController = None,
        process_pool: EnhancedProcessPool = None,
        feature_validator: UnifiedFeatureVectorValidator = None,
        memory_manager: UnifiedMemoryManager = None,
        model_saver: ModelSaver = None,
    ):
        self.config = config or {}
        self.logger = get_logger(__name__)

        # GPU ê°€ìš©ì„± í™•ì¸
        self.gpu_available = torch.cuda.is_available()
        self.device = torch.device("cuda" if self.gpu_available else "cpu")

        # ìœ í‹¸ë¦¬í‹° ì´ˆê¸°í™” (ì£¼ì…)
        self.cache_path_manager = cache_path_manager
        self.batch_controller = batch_controller
        self.process_pool = process_pool
        self.feature_validator = feature_validator
        self.memory_manager = memory_manager
        self.model_saver = model_saver

        # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë“¤
        self.models = {}
        self.model_configs = {
            "tcn": {
                "input_dim": 168,
                "num_channels": [128, 64, 32],
                "kernel_size": 3,
                "dropout": 0.2,
                "use_gpu": self.gpu_available,
            },
            "autoencoder": {
                "input_dim": 168,
                "hidden_dims": [128, 64, 32],
                "latent_dim": 16,
                "dropout_rate": 0.2,
                "use_gpu": self.gpu_available,
            },
            "lightgbm": {
                "boosting_type": "gbdt",
                "objective": "regression",
                "num_leaves": 31,
                "learning_rate": 0.05,
                "use_gpu": self.gpu_available,
            },
        }

        # ì„¤ì • ì—…ë°ì´íŠ¸
        if config:
            for model_name in self.model_configs:
                if model_name in config:
                    self.model_configs[model_name].update(config[model_name])

        # ëª¨ë¸ ì„±ëŠ¥ í†µê³„
        self.model_stats = {
            "tcn": {"predictions": 0, "avg_time": 0.0, "accuracy": 0.0},
            "autoencoder": {"predictions": 0, "avg_time": 0.0, "anomaly_rate": 0.0},
            "lightgbm": {"predictions": 0, "avg_time": 0.0, "feature_importance": {}},
        }

        # ì•™ìƒë¸” ê°€ì¤‘ì¹˜
        self.ensemble_weights = {
            "tcn": 0.4,
            "autoencoder": 0.3,
            "lightgbm": 0.3,
        }

        logger.info(f"âœ… í†µí•© ëª¨ë¸ ê´€ë¦¬ì ì´ˆê¸°í™” ì™„ë£Œ (GPU: {self.gpu_available})")

    def initialize_models(self, force_reload: bool = False) -> Dict[str, bool]:
        """ëª¨ë“  ëª¨ë¸ ì´ˆê¸°í™”"""
        results = {}

        if force_reload:
            self.models.clear()

        # TCN ëª¨ë¸ ì´ˆê¸°í™”
        if "tcn" not in self.models or force_reload:
            try:
                tcn_cache_manager = None
                if self.cache_path_manager:
                    tcn_cache_manager = CacheManager(
                        path_manager=self.cache_path_manager,
                        cache_type="tcn_model",
                        config=self.config.get("caching", {})
                    )
                self.models["tcn"] = TCNModel(
                    config=self.model_configs["tcn"],
                    cuda_optimizer=self.cuda_optimizer,
                    memory_manager=self.memory_manager,
                    cache_manager=tcn_cache_manager,
                )
                results["tcn"] = True
                logger.info("âœ… TCN ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                logger.error(f"âŒ TCN ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                results["tcn"] = False

        # AutoEncoder ëª¨ë¸ ì´ˆê¸°í™”
        if "autoencoder" not in self.models or force_reload:
            try:
                ae_cache_manager = None
                if self.cache_path_manager:
                    ae_cache_manager = CacheManager(
                        path_manager=self.cache_path_manager,
                        cache_type="autoencoder_model",
                        config=self.config.get("caching", {})
                    )
                self.models["autoencoder"] = AutoencoderModel(
                    config=self.model_configs["autoencoder"],
                    cuda_optimizer=self.cuda_optimizer,
                    memory_manager=self.memory_manager,
                    cache_manager=ae_cache_manager,
                )
                results["autoencoder"] = True
                logger.info("âœ… AutoEncoder ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                logger.error(f"âŒ AutoEncoder ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                results["autoencoder"] = False

        # LightGBM ëª¨ë¸ ì´ˆê¸°í™”
        if "lightgbm" not in self.models or force_reload:
            try:
                lgbm_cache_manager = None
                if self.cache_path_manager:
                    lgbm_cache_manager = CacheManager(
                        path_manager=self.cache_path_manager,
                        cache_type="lightgbm_model",
                        config=self.config.get("caching", {})
                    )
                self.models["lightgbm"] = LightGBMModel(
                    config=self.model_configs["lightgbm"], cache_manager=lgbm_cache_manager
                )
                results["lightgbm"] = True
                logger.info("âœ… LightGBM ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                logger.error(f"âŒ LightGBM ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                results["lightgbm"] = False

        return results

    def fit_all_models(
        self, X: np.ndarray, y: np.ndarray, validation_split: float = 0.2, **kwargs
    ) -> Dict[str, Any]:
        """ëª¨ë“  ëª¨ë¸ í•™ìŠµ"""
        if not self.models:
            self.initialize_models()

        # ì…ë ¥ ê²€ì¦
        if self.feature_validator and not self.feature_validator.validate_all(
            X, [f"feature_{i}" for i in range(X.shape[1])]
        ):
            raise ValueError("ì…ë ¥ ë°ì´í„°ê°€ ê²€ì¦ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")

        # ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì •
        optimal_batch_size = self.batch_controller.get_current_batch_size() if self.batch_controller else 32

        results = {}
        start_time = time.time()

        # ë³‘ë ¬ í•™ìŠµ (CPU ì§‘ì•½ì  ëª¨ë¸ë“¤)
        executor = self.process_pool.get_executor() if self.process_pool else ThreadPoolExecutor(max_workers=3)
        with executor:
            futures = {}

            # TCN ëª¨ë¸ í•™ìŠµ
            if "tcn" in self.models:
                futures["tcn"] = executor.submit(
                    self._fit_model_with_monitoring,
                    "tcn",
                    X,
                    y,
                    validation_split,
                    **kwargs,
                )

            # AutoEncoder ëª¨ë¸ í•™ìŠµ
            if "autoencoder" in self.models:
                futures["autoencoder"] = executor.submit(
                    self._fit_model_with_monitoring,
                    "autoencoder",
                    X,
                    y,
                    validation_split,
                    **kwargs,
                )

            # LightGBM ëª¨ë¸ í•™ìŠµ (ë³„ë„ ìŠ¤ë ˆë“œ)
            if "lightgbm" in self.models:
                futures["lightgbm"] = executor.submit(
                    self._fit_model_with_monitoring,
                    "lightgbm",
                    X,
                    y,
                    validation_split,
                    **kwargs,
                )

            # ê²°ê³¼ ìˆ˜ì§‘
            for model_name, future in futures.items():
                try:
                    results[model_name] = future.result(timeout=3600)  # 1ì‹œê°„ íƒ€ì„ì•„ì›ƒ
                    logger.info(f"âœ… {model_name} ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
                except Exception as e:
                    logger.error(f"âŒ {model_name} ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {e}")
                    results[model_name] = {"success": False, "error": str(e)}

        total_time = time.time() - start_time
        results["total_training_time"] = total_time
        results["batch_size_used"] = optimal_batch_size

        logger.info(f"ğŸ¯ ì „ì²´ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ: {total_time:.2f}ì´ˆ")
        return results

    def _fit_model_with_monitoring(
        self,
        model_name: str,
        X: np.ndarray,
        y: np.ndarray,
        validation_split: float,
        **kwargs,
    ) -> Dict[str, Any]:
        """ëª¨ë‹ˆí„°ë§ê³¼ í•¨ê»˜ ëª¨ë¸ í•™ìŠµ"""
        try:
            start_time = time.time()

            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬
            if self.memory_manager and not self.memory_manager.check_available_memory(X.nbytes * 2):
                raise MemoryError(f"{model_name} ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤")

            # ëª¨ë¸ í•™ìŠµ
            model = self.models[model_name]
            fit_result = model.fit(X, y, validation_split=validation_split, **kwargs)

            # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
            training_time = time.time() - start_time
            self.model_stats[model_name]["avg_time"] = training_time
            if self.batch_controller:
                self.batch_controller.report_success(training_time)
            
            return {"success": True, "result": fit_result, "training_time": training_time}

        except Exception as e:
            if self.batch_controller:
                self.batch_controller.report_failure()
            logger.error(f"âŒ {model_name} ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {e}", exc_info=True)
            return {"success": False, "error": str(e)}

    def predict_ensemble(
        self, X: np.ndarray, use_weights: bool = True, **kwargs
    ) -> Dict[str, Any]:
        """ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì•™ìƒë¸” ì˜ˆì¸¡"""
        if not self.models:
            self.initialize_models()
        
        predictions = {}
        for model_name, model in self.models.items():
            try:
                start_time = time.time()
                predictions[model_name] = model.predict(X, **kwargs)
                elapsed_time = time.time() - start_time
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                stats = self.model_stats[model_name]
                stats["avg_time"] = (stats["avg_time"] * stats["predictions"] + elapsed_time) / (stats["predictions"] + 1)
                stats["predictions"] += 1
                
            except Exception as e:
                logger.error(f"{model_name} ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
                predictions[model_name] = None
        
        # None ê°’ ì œê±°
        valid_predictions = {k: v for k, v in predictions.items() if v is not None}
        
        if not valid_predictions:
            raise RuntimeError("ëª¨ë“  ëª¨ë¸ì—ì„œ ì˜ˆì¸¡ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            
        return self._combine_predictions(valid_predictions, use_weights)

    def _combine_predictions(self, predictions: Dict[str, Any], use_weights: bool) -> Optional[np.ndarray]:
        """ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡ ê²°í•©"""
        if not predictions:
            return None

        final_prediction = None
        total_weight = 0.0

        for model_name, pred in predictions.items():
            weight = self.ensemble_weights[model_name] if use_weights else 1.0
            
            if final_prediction is None:
                final_prediction = pred * weight
            else:
                # ì˜ˆì¸¡ ê²°ê³¼ì˜ shapeì´ ë‹¤ë¥¼ ê²½ìš° ë¸Œë¡œë“œìºìŠ¤íŒ… ë˜ëŠ” ë¦¬ìƒ˜í”Œë§ í•„ìš”
                if final_prediction.shape != pred.shape:
                    logger.warning(f"ì˜ˆì¸¡ ê²°ê³¼ shape ë¶ˆì¼ì¹˜: {final_prediction.shape} vs {pred.shape}")
                    # ê°„ë‹¨í•œ í•´ê²°ì±…: ì‘ì€ ìª½ì— ë§ì¶° ìë¥´ê¸° (ë” ë‚˜ì€ ë°©ë²• í•„ìš”)
                    min_len = min(final_prediction.shape[0], pred.shape[0])
                    final_prediction = final_prediction[:min_len]
                    pred = pred[:min_len]
                
                final_prediction += pred * weight
            
            total_weight += weight
            
        if total_weight > 0 and final_prediction is not None:
            return final_prediction / total_weight
        return final_prediction


    def save_all_models(self, save_dir: Optional[str] = None) -> Dict[str, bool]:
        """ëª¨ë“  ëª¨ë¸ ì €ì¥"""
        if not self.model_saver:
            logger.warning("ModelSaverê°€ ì—†ì–´ ëª¨ë¸ì„ ì €ì¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {}

        results = {}
        for model_name, model in self.models.items():
            try:
                self.model_saver.save_model(model, model_name)
                results[model_name] = True
            except Exception as e:
                logger.error(f"{model_name} ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
                results[model_name] = False
        return results

    def load_all_models(self, load_dir: Optional[str] = None) -> Dict[str, bool]:
        """ëª¨ë“  ëª¨ë¸ ë¡œë“œ"""
        if not self.model_saver:
            logger.warning("ModelSaverê°€ ì—†ì–´ ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {}
            
        results = {}
        for model_name in self.model_configs.keys():
            try:
                self.models[model_name] = self.model_saver.load_model(model_name)
                results[model_name] = True
            except Exception as e:
                logger.error(f"{model_name} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                results[model_name] = False
        return results

    def get_model_stats(self) -> Dict[str, Any]:
        """ëª¨ë¸ë³„ ì„±ëŠ¥ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        # ì¶”ê°€: ì‹¤ì‹œê°„ìœ¼ë¡œ í‰ê·  ì‹œê°„ ë‹¤ì‹œ ê³„ì‚°
        for model_name, stats in self.model_stats.items():
            if "total_time" in stats and "predictions" in stats and stats["predictions"] > 0:
                stats["avg_time"] = stats["total_time"] / stats["predictions"]
        return self.model_stats

    def update_ensemble_weights(self, new_weights: Dict[str, float]):
        """ì•™ìƒë¸” ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•˜ê³  ì •ê·œí™”í•©ë‹ˆë‹¤."""
        total_weight = sum(new_weights.values())
        if abs(total_weight - 1.0) > 1e-6:
            # ì •ê·œí™”
            logger.info(f"ê°€ì¤‘ì¹˜ ì´í•©ì´ 1ì´ ì•„ë‹ˆë¯€ë¡œ ì •ê·œí™”í•©ë‹ˆë‹¤. í•©ê³„: {total_weight}")
            new_weights = {k: v / total_weight for k, v in new_weights.items()}

        for model_name, weight in new_weights.items():
            if model_name in self.ensemble_weights:
                self.ensemble_weights[model_name] = weight
        logger.info(f"ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {self.ensemble_weights}")
