"""
ê¸°ë³¸ ëª¨ë¸ í´ë˜ìŠ¤ (Base Model Class)

ì´ ëª¨ë“ˆì€ ëª¨ë“  ëª¨ë¸ì´ ìƒì†ë°›ëŠ” ê¸°ë³¸ ëª¨ë¸ í´ë˜ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
í‘œì¤€ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•˜ì—¬ ëª¨ë¸ ê°„ í˜¸í™˜ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤.
"""

import os
import json
import torch
import numpy as np
from typing import Dict, List, Any, Optional, Union, Tuple
from abc import ABC, abstractmethod
import time
from contextlib import nullcontext

# âœ… src/utils í†µí•© ì‹œìŠ¤í…œ í™œìš©
from ..utils.unified_logging import get_logger
from ..utils import (
    get_unified_memory_manager,
    get_cuda_optimizer
)

logger = get_logger(__name__)


class GPUDeviceManager:
    """
    GPU ì¥ì¹˜ ê´€ë¦¬ì - src/utils í†µí•© ì‹œìŠ¤í…œ ê¸°ë°˜ ê³ ì„±ëŠ¥ ë©”ëª¨ë¦¬ ê´€ë¦¬
    
    ê¸°ì¡´ API í˜¸í™˜ì„±ì„ ìœ ì§€í•˜ë©´ì„œ src/utilsì˜ ê°•ë ¥í•œ ê¸°ëŠ¥ë“¤ì„ í†µí•©:
    - ìŠ¤ë§ˆíŠ¸ ë©”ëª¨ë¦¬ í• ë‹¹
    - GPU ë©”ëª¨ë¦¬ í’€ë§
    - ìë™ ë©”ëª¨ë¦¬ ì •ë¦¬
    - OOM ìë™ ë³µêµ¬
    """
    
    def __init__(self):
        # âœ… ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.gpu_available = torch.cuda.is_available()
        
        # âœ… src/utils í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        try:
            self.memory_mgr = get_unified_memory_manager()
            self.cuda_opt = get_cuda_optimizer()
            self._unified_system_available = True
            logger.info("âœ… í†µí•© ë©”ëª¨ë¦¬ ê´€ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"âš ï¸ í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë“œë¡œ í´ë°±: {e}")
            self.memory_mgr = None
            self.cuda_opt = None
            self._unified_system_available = False
        
        if self.gpu_available:
            logger.info(f"âœ… GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name()}")
            logger.info(f"GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
            
            # âœ… CUDA ìµœì í™” ì„¤ì • (í†µí•© ì‹œìŠ¤í…œì´ ìˆëŠ” ê²½ìš°)
            if self._unified_system_available and self.cuda_opt:
                self.cuda_opt.set_tf32_enabled(True)
                logger.info("ğŸš€ TF32 ìµœì í™” í™œì„±í™”")
        else:
            logger.info("âš ï¸ GPU ì‚¬ìš© ë¶ˆê°€ëŠ¥, CPU ëª¨ë“œë¡œ ì‹¤í–‰")
    
    def to_device(self, tensor_or_data: Union[torch.Tensor, np.ndarray, List]) -> torch.Tensor:
        """
        ë°ì´í„°ë¥¼ ì ì ˆí•œ deviceë¡œ ì´ë™ (ìŠ¤ë§ˆíŠ¸ ë©”ëª¨ë¦¬ í• ë‹¹ ì ìš©)
        
        Args:
            tensor_or_data: ì´ë™í•  ë°ì´í„°
            
        Returns:
            deviceë¡œ ì´ë™ëœ í…ì„œ
        """
        try:
            # âœ… í†µí•© ì‹œìŠ¤í…œì´ ìˆìœ¼ë©´ ìŠ¤ë§ˆíŠ¸ í• ë‹¹ ì‚¬ìš©
            if self._unified_system_available and self.memory_mgr:
                return self._smart_to_device(tensor_or_data)
            else:
                # ê¸°ì¡´ ë°©ì‹ í´ë°±
                return self._legacy_to_device(tensor_or_data)
                
        except Exception as e:
            logger.warning(f"âš ï¸ ìŠ¤ë§ˆíŠ¸ ë³€í™˜ ì‹¤íŒ¨, ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ í´ë°±: {e}")
            return self._legacy_to_device(tensor_or_data)
    
    def _smart_to_device(self, tensor_or_data: Union[torch.Tensor, np.ndarray, List]) -> torch.Tensor:
        """ìŠ¤ë§ˆíŠ¸ ë©”ëª¨ë¦¬ í• ë‹¹ì„ ì‚¬ìš©í•œ device ì´ë™"""
        # âœ… ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì¡´ì¬ í™•ì¸ (íƒ€ì… ì²´ì»¤ ëª…í™•í™”)
        if self.memory_mgr is None:
            raise RuntimeError("í†µí•© ë©”ëª¨ë¦¬ ê´€ë¦¬ìê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        # ë°ì´í„° í¬ê¸° ì¶”ì •
        if isinstance(tensor_or_data, torch.Tensor):
            size = tensor_or_data.numel()
            tensor = tensor_or_data
        elif isinstance(tensor_or_data, np.ndarray):
            size = tensor_or_data.size
            tensor = torch.from_numpy(tensor_or_data)
        elif isinstance(tensor_or_data, (list, tuple)):
            size = len(tensor_or_data)
            tensor = torch.tensor(tensor_or_data)
        else:
            raise TypeError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” íƒ€ì…: {type(tensor_or_data)}")
        
        # âœ… ìŠ¤ë§ˆíŠ¸ ë©”ëª¨ë¦¬ í• ë‹¹ìœ¼ë¡œ device ì´ë™
        device_type = "gpu" if self.device.type == "cuda" else "cpu"
        
        # ì„ì‹œ í• ë‹¹ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©
        with self.memory_mgr.temporary_allocation(
            size=size * 4,  # float32 ê¸°ì¤€
            prefer_device=device_type
        ) as work_tensor:
            return tensor.to(self.device, non_blocking=True)
    
    def _legacy_to_device(self, tensor_or_data: Union[torch.Tensor, np.ndarray, List]) -> torch.Tensor:
        """ê¸°ì¡´ ë°©ì‹ì˜ device ì´ë™ (í´ë°±ìš©)"""
        if isinstance(tensor_or_data, torch.Tensor):
            return tensor_or_data.to(self.device, non_blocking=True)
        elif isinstance(tensor_or_data, np.ndarray):
            return torch.from_numpy(tensor_or_data).to(self.device, non_blocking=True)
        elif isinstance(tensor_or_data, (list, tuple)):
            return torch.tensor(tensor_or_data).to(self.device, non_blocking=True)
        else:
            raise TypeError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” íƒ€ì…: {type(tensor_or_data)}")
    
    def check_memory_usage(self) -> Dict[str, float]:
        """
        GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸ (í†µí•© ì‹œìŠ¤í…œ ì •ë³´ í¬í•¨)
        
        Returns:
            ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì •ë³´
        """
        if not self.gpu_available:
            return {"gpu_available": False}
        
        # âœ… ê¸°ë³¸ GPU ë©”ëª¨ë¦¬ ì •ë³´
        allocated = torch.cuda.memory_allocated() / 1024**3
        reserved = torch.cuda.memory_reserved() / 1024**3
        total = torch.cuda.get_device_properties(0).total_memory / 1024**3
        
        result = {
            "gpu_available": True,
            "allocated_gb": allocated,
            "reserved_gb": reserved,
            "total_gb": total,
            "usage_percent": (allocated / total) * 100
        }
        
        # âœ… í†µí•© ì‹œìŠ¤í…œ ì •ë³´ ì¶”ê°€
        if self._unified_system_available and self.memory_mgr:
            try:
                unified_stats = self.memory_mgr.get_memory_status()
                result["unified_memory"] = unified_stats
                result["memory_efficiency"] = unified_stats.get("efficiency", 0.0)
                result["pool_utilization"] = unified_stats.get("pool_utilization", 0.0)
            except Exception as e:
                logger.debug(f"í†µí•© ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        return result
    
    def clear_cache(self):
        """GPU ìºì‹œ ì •ë¦¬ (í†µí•© ì‹œìŠ¤í…œ ì—°ë™)"""
        if self.gpu_available:
            # âœ… í†µí•© ì‹œìŠ¤í…œì´ ìˆìœ¼ë©´ ìŠ¤ë§ˆíŠ¸ ì •ë¦¬
            if self._unified_system_available and self.memory_mgr:
                try:
                    # í†µí•© ë©”ëª¨ë¦¬ ê´€ë¦¬ìì˜ ì •ë¦¬ ê¸°ëŠ¥ ì‚¬ìš©
                    self.memory_mgr.cleanup_unused_memory()
                    logger.info("ğŸ§¹ ìŠ¤ë§ˆíŠ¸ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
                except Exception as e:
                    logger.warning(f"âš ï¸ ìŠ¤ë§ˆíŠ¸ ì •ë¦¬ ì‹¤íŒ¨, ê¸°ë³¸ ì •ë¦¬ë¡œ í´ë°±: {e}")
                    torch.cuda.empty_cache()
                    logger.info("GPU ìºì‹œ ì •ë¦¬ ì™„ë£Œ (ê¸°ë³¸ ëª¨ë“œ)")
            else:
                # ê¸°ì¡´ ë°©ì‹
                torch.cuda.empty_cache()
                logger.info("GPU ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
    
    def get_optimal_batch_size(self, data_size: int, model_complexity: float = 1.0) -> int:
        """
        ğŸš€ ìƒˆë¡œìš´ ê¸°ëŠ¥: í˜„ì¬ ë©”ëª¨ë¦¬ ìƒíƒœì— ë”°ë¥¸ ìµœì  ë°°ì¹˜ í¬ê¸° ê³„ì‚°
        
        Args:
            data_size: ì²˜ë¦¬í•  ë°ì´í„° í¬ê¸°
            model_complexity: ëª¨ë¸ ë³µì¡ë„ (1.0 = ê¸°ë³¸)
            
        Returns:
            ìµœì  ë°°ì¹˜ í¬ê¸°
        """
        if not self.gpu_available:
            return min(32, data_size)  # CPU ê¸°ë³¸ê°’
        
        if self._unified_system_available and self.memory_mgr:
            try:
                # í†µí•© ì‹œìŠ¤í…œì˜ ì§€ëŠ¥ì  ë°°ì¹˜ í¬ê¸° ê³„ì‚°
                memory_stats = self.memory_mgr.get_memory_status()
                gpu_util = memory_stats.get("gpu_utilization", 0.5)
                
                # GPU ì‚¬ìš©ë¥ ì— ë”°ë¥¸ ë™ì  ì¡°ì •
                if gpu_util < 0.3:
                    base_batch = 128
                elif gpu_util < 0.7:
                    base_batch = 64
                else:
                    base_batch = 32
                
                # ëª¨ë¸ ë³µì¡ë„ ë°˜ì˜
                optimal_batch = int(base_batch / model_complexity)
                
                return min(max(optimal_batch, 1), data_size)
                
            except Exception as e:
                logger.debug(f"ì§€ëŠ¥ì  ë°°ì¹˜ í¬ê¸° ê³„ì‚° ì‹¤íŒ¨: {e}")
        
        # í´ë°±: ê¸°ë³¸ ê³„ì‚°
        memory_info = self.check_memory_usage()
        usage_percent = memory_info.get("usage_percent", 50)
        
        if usage_percent < 30:
            return min(64, data_size)
        elif usage_percent < 70:
            return min(32, data_size)
        else:
            return min(16, data_size)
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """
        ğŸš€ ìƒˆë¡œìš´ ê¸°ëŠ¥: ì„±ëŠ¥ í†µê³„ ë°˜í™˜
        
        Returns:
            ìƒì„¸í•œ ì„±ëŠ¥ í†µê³„
        """
        stats = {
            "device": str(self.device),
            "gpu_available": self.gpu_available,
            "unified_system": self._unified_system_available,
            "memory_info": self.check_memory_usage()
        }
        
        if self._unified_system_available:
            if self.memory_mgr:
                try:
                    stats["memory_performance"] = self.memory_mgr.get_performance_metrics()
                except Exception as e:
                    logger.debug(f"ë©”ëª¨ë¦¬ ì„±ëŠ¥ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            
            if self.cuda_opt:
                try:
                    stats["cuda_optimization"] = self.cuda_opt.get_optimization_stats()
                except Exception as e:
                    logger.debug(f"CUDA ìµœì í™” í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        return stats


class BaseModel(ABC):
    """
    ëª¨ë“  DAEBAK_AI ëª¨ë¸ì˜ ê¸°ë³¸ í´ë˜ìŠ¤

    ì´ í´ë˜ìŠ¤ëŠ” ë¡œë˜ ë²ˆí˜¸ ì˜ˆì¸¡ ëª¨ë¸ì´ êµ¬í˜„í•´ì•¼ í•˜ëŠ” ê¸°ë³¸ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        ê¸°ë³¸ ëª¨ë¸ ì´ˆê¸°í™” (src/utils í†µí•© ì‹œìŠ¤í…œ ì ìš©)

        Args:
            config: ëª¨ë¸ ì„¤ì •
        """
        # ê¸°ë³¸ ì„¤ì •
        self.config = config or {}

        # âœ… GPU ì¥ì¹˜ ê´€ë¦¬ì ì´ˆê¸°í™” (í†µí•© ì‹œìŠ¤í…œ í¬í•¨)
        self.device_manager = GPUDeviceManager()
        self.device = self.device_manager.device

        # ëª¨ë¸ ìƒíƒœ
        self.is_trained = False
        self.training_history = []
        self.model_name = self.__class__.__name__

        # âœ… ì„±ëŠ¥ ìµœì í™” ì„¤ì •
        self.enable_smart_batching = self.config.get("enable_smart_batching", True)
        self.auto_memory_optimization = self.config.get("auto_memory_optimization", True)

        # ëª¨ë¸ ë©”íƒ€ë°ì´í„° (í†µí•© ì‹œìŠ¤í…œ ì •ë³´ í¬í•¨)
        self.metadata = {
            "model_type": self.model_name,
            "created_at": time.strftime("%Y-%m-%d %H:%M:%S"),
            "updated_at": time.strftime("%Y-%m-%d %H:%M:%S"),
            "version": "2.0.0",  # src/utils í†µí•©ìœ¼ë¡œ ë²„ì „ ì—…
            "device": str(self.device),
            "gpu_available": self.device_manager.gpu_available,
            "unified_system": self.device_manager._unified_system_available,
            "smart_features": {
                "smart_batching": self.enable_smart_batching,
                "auto_memory_optimization": self.auto_memory_optimization,
                "tf32_enabled": (self.device_manager._unified_system_available and 
                               self.device_manager.cuda_opt is not None)
            }
        }

        logger.info(f"âœ… {self.model_name} ì´ˆê¸°í™” ì™„ë£Œ: ì¥ì¹˜ {self.device}")
        
        # í†µí•© ì‹œìŠ¤í…œ ìƒíƒœ ë¡œê·¸
        if self.device_manager._unified_system_available:
            logger.info("ğŸš€ í†µí•© ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ í™œì„±í™”")
        else:
            logger.info("âš ï¸ ê¸°ë³¸ ëª¨ë“œë¡œ ì‹¤í–‰ (í†µí•© ì‹œìŠ¤í…œ ë¹„í™œì„±í™”)")

    @abstractmethod
    def fit(self, X: np.ndarray, y: np.ndarray, **kwargs) -> Dict[str, Any]:
        """
        ëª¨ë¸ í›ˆë ¨ (í‘œì¤€ ì¸í„°í˜ì´ìŠ¤)

        Args:
            X: íŠ¹ì„± ë²¡í„°
            y: ë ˆì´ë¸”/íƒ€ê²Ÿ
            **kwargs: ì¶”ê°€ ë§¤ê°œë³€ìˆ˜

        Returns:
            í›ˆë ¨ ê²°ê³¼ ë° ë©”íƒ€ë°ì´í„°
        """
        raise NotImplementedError("fit ë©”ì„œë“œë¥¼ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤.")

    @abstractmethod
    def predict(self, X: np.ndarray, **kwargs) -> np.ndarray:
        """
        ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜í–‰ (í‘œì¤€ ì¸í„°í˜ì´ìŠ¤)

        Args:
            X: íŠ¹ì„± ë²¡í„°
            **kwargs: ì¶”ê°€ ë§¤ê°œë³€ìˆ˜

        Returns:
            ì˜ˆì¸¡ê°’
        """
        raise NotImplementedError("predict ë©”ì„œë“œë¥¼ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤.")

    @abstractmethod
    def save(self, path: str) -> bool:
        """
        ëª¨ë¸ ì €ì¥ (í‘œì¤€ ì¸í„°í˜ì´ìŠ¤)

        Args:
            path: ì €ì¥ ê²½ë¡œ

        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        raise NotImplementedError("save ë©”ì„œë“œë¥¼ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤.")

    @abstractmethod
    def load(self, path: str) -> bool:
        """
        ëª¨ë¸ ë¡œë“œ (í‘œì¤€ ì¸í„°í˜ì´ìŠ¤)

        Args:
            path: ë¡œë“œí•  ëª¨ë¸ ê²½ë¡œ

        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        raise NotImplementedError("load ë©”ì„œë“œë¥¼ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤.")

    def evaluate(self, X: np.ndarray, y: np.ndarray, **kwargs) -> Dict[str, Any]:
        """
        ëª¨ë¸ í‰ê°€ (ì„ íƒì  êµ¬í˜„)

        Args:
            X: íŠ¹ì„± ë²¡í„°
            y: ë ˆì´ë¸”/íƒ€ê²Ÿ
            **kwargs: ì¶”ê°€ ë§¤ê°œë³€ìˆ˜

        Returns:
            í‰ê°€ ê²°ê³¼
        """
        if not self.is_trained:
            return {"error": "í›ˆë ¨ë˜ì§€ ì•Šì€ ëª¨ë¸ì€ í‰ê°€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

        return {
            "message": "evaluate ë©”ì„œë“œê°€ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
            "status": "not_implemented",
        }

    def _ensure_directory(self, path: str) -> None:
        """
        ì €ì¥ ê²½ë¡œì˜ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³ , ì—†ìœ¼ë©´ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            path: íŒŒì¼ ê²½ë¡œ
        """
        directory = os.path.dirname(path)
        if directory and not os.path.exists(directory):
            os.makedirs(directory, exist_ok=True)
            logger.info(f"ë””ë ‰í† ë¦¬ ìƒì„±: {directory}")

    def get_feature_vector(
        self,
        feature_path: str = "data/cache/feature_vector_full.npy",
        names_path: str = "data/cache/feature_vector_full.names.json",
    ) -> Tuple[np.ndarray, List[str]]:
        """
        íŠ¹ì„± ë²¡í„°ì™€ íŠ¹ì„± ì´ë¦„ì„ ë¡œë“œí•©ë‹ˆë‹¤.

        Args:
            feature_path: íŠ¹ì„± ë²¡í„° íŒŒì¼ ê²½ë¡œ
            names_path: íŠ¹ì„± ì´ë¦„ íŒŒì¼ ê²½ë¡œ

        Returns:
            íŠ¹ì„± ë²¡í„°ì™€ íŠ¹ì„± ì´ë¦„ì˜ íŠœí”Œ
        """
        try:
            # ë²¡í„° ë°ì´í„° ë¡œë“œ
            if not os.path.exists(feature_path):
                raise FileNotFoundError(
                    f"íŠ¹ì„± ë²¡í„° íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {feature_path}"
                )

            vector = np.load(feature_path)

            # íŠ¹ì„± ì´ë¦„ ë¡œë“œ
            if not os.path.exists(names_path):
                raise FileNotFoundError(
                    f"íŠ¹ì„± ì´ë¦„ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {names_path}"
                )

            with open(names_path, "r", encoding="utf-8") as f:
                feature_names = json.load(f)

            logger.info(
                f"íŠ¹ì„± ë²¡í„° ë¡œë“œ ì™„ë£Œ: {feature_path}, í˜•íƒœ={vector.shape}, íŠ¹ì„± ìˆ˜={len(feature_names)}"
            )
            return vector, feature_names

        except Exception as e:
            logger.error(f"íŠ¹ì„± ë²¡í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            raise

    def get_model_info(self) -> Dict[str, Any]:
        """
        ëª¨ë¸ ì •ë³´ ë°˜í™˜ (í†µí•© ì‹œìŠ¤í…œ ì •ë³´ í¬í•¨)

        Returns:
            ëª¨ë¸ ë©”íƒ€ë°ì´í„°ì™€ ìƒíƒœ ì •ë³´
        """
        info = self.metadata.copy()
        info.update({
            "is_trained": self.is_trained,
            "training_history_length": len(self.training_history),
            "device": str(self.device),
            "gpu_memory_info": self.device_manager.check_memory_usage(),
            "performance_stats": self.device_manager.get_performance_stats(),
        })
        return info
    
    def get_optimal_batch_size(self, data_size: int, model_complexity: float = 1.0) -> int:
        """
        ğŸš€ ìŠ¤ë§ˆíŠ¸ ë°°ì¹˜ í¬ê¸° ê³„ì‚°
        
        Args:
            data_size: ì²˜ë¦¬í•  ë°ì´í„° í¬ê¸°
            model_complexity: ëª¨ë¸ ë³µì¡ë„ (1.0 = ê¸°ë³¸)
            
        Returns:
            ìµœì  ë°°ì¹˜ í¬ê¸°
        """
        if not self.enable_smart_batching:
            return min(32, data_size)  # ìŠ¤ë§ˆíŠ¸ ë°°ì¹˜ ë¹„í™œì„±í™”ì‹œ ê¸°ë³¸ê°’
        
        return self.device_manager.get_optimal_batch_size(data_size, model_complexity)
    
    def smart_data_transfer(self, data: Union[torch.Tensor, np.ndarray, List]) -> torch.Tensor:
        """
        ğŸš€ ìŠ¤ë§ˆíŠ¸ ë°ì´í„° ì „ì†¡ (í†µí•© ë©”ëª¨ë¦¬ ê´€ë¦¬ í™œìš©)
        
        Args:
            data: ì „ì†¡í•  ë°ì´í„°
            
        Returns:
            deviceë¡œ ì´ë™ëœ í…ì„œ
        """
        if not self.auto_memory_optimization:
            # ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
            return self.device_manager._legacy_to_device(data)
        
        return self.device_manager.to_device(data)
    
    def optimize_memory_usage(self):
        """
        ğŸš€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”
        """
        if self.auto_memory_optimization:
            self.device_manager.clear_cache()
            logger.info(f"{self.model_name}: ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ")
        else:
            logger.debug(f"{self.model_name}: ë©”ëª¨ë¦¬ ìµœì í™” ë¹„í™œì„±í™”")
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """
        ğŸš€ ì„±ëŠ¥ ìš”ì•½ ì •ë³´ ë°˜í™˜
        
        Returns:
            ì„±ëŠ¥ ìš”ì•½ í†µê³„
        """
        self.device_manager.get_performance_stats()
        memory_info = self.device_manager.check_memory_usage()
        
        summary = {
            "model_name": self.model_name,
            "device": str(self.device),
            "is_trained": self.is_trained,
            "unified_system_active": self.device_manager._unified_system_available,
            "memory_efficiency": memory_info.get("memory_efficiency", 0.0),
            "gpu_utilization": memory_info.get("usage_percent", 0.0),
            "smart_features_enabled": {
                "smart_batching": self.enable_smart_batching,
                "auto_memory_optimization": self.auto_memory_optimization
            }
        }
        
        return summary


class ThreeDigitMixin:
    """3ìë¦¬ ì˜ˆì¸¡ ëª¨ë“œë¥¼ ìœ„í•œ Mixin í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.supports_3digit_mode = True
        self.is_3digit_mode = False
        self.three_digit_model = None

    def enable_3digit_mode(self) -> bool:
        """3ìë¦¬ ì˜ˆì¸¡ ëª¨ë“œ í™œì„±í™”"""
        if not self.supports_3digit_mode:
            logger.warning(f"{self.model_name}ì€ 3ìë¦¬ ì˜ˆì¸¡ ëª¨ë“œë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return False
        
        self.is_3digit_mode = True
        logger.info(f"{self.model_name}: 3ìë¦¬ ì˜ˆì¸¡ ëª¨ë“œ í™œì„±í™”")
        return True

    def disable_3digit_mode(self) -> bool:
        """3ìë¦¬ ì˜ˆì¸¡ ëª¨ë“œ ë¹„í™œì„±í™”"""
        self.is_3digit_mode = False
        logger.info(f"{self.model_name}: 3ìë¦¬ ì˜ˆì¸¡ ëª¨ë“œ ë¹„í™œì„±í™”")
        return True

    def predict_3digit_combinations(
        self, X: np.ndarray, top_k: int = 100, **kwargs
    ) -> List[Tuple[Tuple[int, int, int], float]]:
        """
        3ìë¦¬ ì¡°í•© ì˜ˆì¸¡ (êµ¬í˜„ í•„ìš”)

        Args:
            X: íŠ¹ì„± ë²¡í„°
            top_k: ìƒìœ„ kê°œ ì¡°í•© ë°˜í™˜
            **kwargs: ì¶”ê°€ ë§¤ê°œë³€ìˆ˜

        Returns:
            3ìë¦¬ ì¡°í•©ê³¼ í™•ë¥ ì˜ ë¦¬ìŠ¤íŠ¸
        """
        if not self.is_3digit_mode:
            raise ValueError("3ìë¦¬ ì˜ˆì¸¡ ëª¨ë“œê°€ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        if self.three_digit_model is None:
            raise ValueError("3ìë¦¬ ì˜ˆì¸¡ ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„
        raise NotImplementedError("3ìë¦¬ ì˜ˆì¸¡ ë©”ì„œë“œë¥¼ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤.")

    def fit_3digit_mode(
        self, X: np.ndarray, y_3digit: np.ndarray, **kwargs
    ) -> Dict[str, Any]:
        """
        3ìë¦¬ ì˜ˆì¸¡ ëª¨ë“œ í›ˆë ¨ (êµ¬í˜„ í•„ìš”)

        Args:
            X: íŠ¹ì„± ë²¡í„°
            y_3digit: 3ìë¦¬ ì¡°í•© ë ˆì´ë¸”
            **kwargs: ì¶”ê°€ ë§¤ê°œë³€ìˆ˜

        Returns:
            í›ˆë ¨ ê²°ê³¼
        """
        if not self.supports_3digit_mode:
            return {"error": "3ìë¦¬ ì˜ˆì¸¡ ëª¨ë“œë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."}
        
        # í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„
        raise NotImplementedError("3ìë¦¬ ëª¨ë“œ í›ˆë ¨ ë©”ì„œë“œë¥¼ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤.")


class ModelWithAMP(BaseModel):
    """
    Automatic Mixed Precision (AMP)ë¥¼ ì§€ì›í•˜ëŠ” ëª¨ë¸ ê¸°ë³¸ í´ë˜ìŠ¤

    PyTorch ëª¨ë¸ì— AMPë¥¼ ì ìš©í•˜ê¸° ìœ„í•œ ê³µí†µ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        AMP ì§€ì› ëª¨ë¸ ì´ˆê¸°í™”

        Args:
            config: ëª¨ë¸ ì„¤ì •
        """
        super().__init__(config)

        # AMP ê´€ë ¨ ì„¤ì •
        self.use_amp = (
            self.config.get("use_amp", True) if torch.cuda.is_available() else False
        )

        # Scaler ì´ˆê¸°í™”
        if self.use_amp:
            self.scaler = torch.cuda.amp.GradScaler()
            logger.info(f"{self.model_name}: AMP í™œì„±í™”ë¨")
        else:
            self.scaler = None
            logger.info(f"{self.model_name}: AMP ë¹„í™œì„±í™”ë¨")

    def get_amp_context(self):
        """AMP ì»¨í…ìŠ¤íŠ¸ ë°˜í™˜ - nullcontext ì‚¬ìš©ìœ¼ë¡œ ê°œì„ """
        return torch.cuda.amp.autocast() if self.use_amp else nullcontext()

    def train_step_with_amp(self, model, inputs, targets, optimizer, loss_fn, **kwargs):
        """
        AMPë¥¼ ì ìš©í•œ í›ˆë ¨ ë‹¨ê³„

        Args:
            model: í›ˆë ¨í•  ëª¨ë¸
            inputs: ì…ë ¥ ë°ì´í„°
            targets: íƒ€ê²Ÿ ë°ì´í„°
            optimizer: ì˜µí‹°ë§ˆì´ì €
            loss_fn: ì†ì‹¤ í•¨ìˆ˜
            **kwargs: ì¶”ê°€ ë§¤ê°œë³€ìˆ˜

        Returns:
            ì†ì‹¤ê°’
        """
        # ëª¨ë¸ì„ í›ˆë ¨ ëª¨ë“œë¡œ ì„¤ì •
        model.train()

        # ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”
        optimizer.zero_grad()

        # ë°ì´í„°ë¥¼ ì ì ˆí•œ deviceë¡œ ì´ë™
        inputs = self.device_manager.to_device(inputs)
        targets = self.device_manager.to_device(targets)

        try:
            with self.get_amp_context():
                outputs = model(inputs)
                loss = loss_fn(outputs, targets)

            if self.use_amp and self.scaler:
                # ìŠ¤ì¼€ì¼ëœ ê·¸ë˜ë””ì–¸íŠ¸ë¡œ ì—­ì „íŒŒ
                self.scaler.scale(loss).backward()
                # ìŠ¤ì¼€ì¼ëœ ê·¸ë˜ë””ì–¸íŠ¸ë¡œ ì˜µí‹°ë§ˆì´ì € ìŠ¤í…
                self.scaler.step(optimizer)
                # ìŠ¤ì¼€ì¼ëŸ¬ ì—…ë°ì´íŠ¸
                self.scaler.update()
            else:
                # ì¼ë°˜ ì—­ì „íŒŒ
                loss.backward()
                optimizer.step()

            return loss.item()
            
        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                logger.warning("GPU ë©”ëª¨ë¦¬ ë¶€ì¡±, ìºì‹œ ì •ë¦¬ í›„ ì¬ì‹œë„")
                self.device_manager.clear_cache()
                raise MemoryError(f"GPU ë©”ëª¨ë¦¬ ë¶€ì¡±: {e}")
            else:
                raise


class EnsembleBaseModel(BaseModel):
    """
    ì•™ìƒë¸” ëª¨ë¸ ê¸°ë³¸ í´ë˜ìŠ¤

    ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ê²°í•©í•˜ëŠ” ì•™ìƒë¸” ëª¨ë¸ì„ ìœ„í•œ ê¸°ë³¸ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        ì•™ìƒë¸” ëª¨ë¸ ì´ˆê¸°í™”

        Args:
            config: ëª¨ë¸ ì„¤ì •
        """
        super().__init__(config)

        # ëª¨ë¸ ë° ê°€ì¤‘ì¹˜ ëª©ë¡
        self.models = []
        self.weights = []

    def add_model(self, model: BaseModel, weight: float = 1.0):
        """
        ì•™ìƒë¸”ì— ëª¨ë¸ ì¶”ê°€

        Args:
            model: ì¶”ê°€í•  ëª¨ë¸
            weight: ëª¨ë¸ ê°€ì¤‘ì¹˜ (ê¸°ë³¸ê°’: 1.0)
        """
        self.models.append(model)
        self.weights.append(weight)
        logger.info(f"ì•™ìƒë¸”ì— {model.model_name} ì¶”ê°€ (ê°€ì¤‘ì¹˜: {weight})")

    def fit(self, X: np.ndarray, y: np.ndarray, **kwargs) -> Dict[str, Any]:
        """
        ëª¨ë“  êµ¬ì„± ëª¨ë¸ í›ˆë ¨

        Args:
            X: íŠ¹ì„± ë²¡í„°
            y: ë ˆì´ë¸”/íƒ€ê²Ÿ
            **kwargs: ì¶”ê°€ ë§¤ê°œë³€ìˆ˜

        Returns:
            í›ˆë ¨ ê²°ê³¼
        """
        results = {}

        for i, model in enumerate(self.models):
            logger.info(
                f"ì•™ìƒë¸” êµ¬ì„± ëª¨ë¸ {i+1}/{len(self.models)} ({model.model_name}) í›ˆë ¨ ì¤‘..."
            )
            try:
                result = model.fit(X, y, **kwargs)
                results[model.model_name] = result
            except Exception as e:
                logger.error(f"ëª¨ë¸ {model.model_name} í›ˆë ¨ ì‹¤íŒ¨: {e}")
                results[model.model_name] = {"error": str(e)}

        self.is_trained = all(model.is_trained for model in self.models)

        return {
            "ensemble_results": results,
            "is_trained": self.is_trained,
            "model_count": len(self.models),
            "gpu_memory_info": self.device_manager.check_memory_usage(),
        }

    def predict(self, X: np.ndarray, **kwargs) -> np.ndarray:
        """
        ì•™ìƒë¸” ì˜ˆì¸¡ ìˆ˜í–‰

        Args:
            X: íŠ¹ì„± ë²¡í„°
            **kwargs: ì¶”ê°€ ë§¤ê°œë³€ìˆ˜

        Returns:
            ì•™ìƒë¸” ì˜ˆì¸¡ê°’
        """
        if not self.models:
            raise ValueError("ì•™ìƒë¸”ì— ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")

        if not self.is_trained:
            raise ValueError("í›ˆë ¨ë˜ì§€ ì•Šì€ ì•™ìƒë¸” ëª¨ë¸ì…ë‹ˆë‹¤.")

        predictions = []

        for model, weight in zip(self.models, self.weights):
            try:
                pred = model.predict(X, **kwargs)
                predictions.append((pred, weight))
            except Exception as e:
                logger.warning(f"ëª¨ë¸ {model.model_name} ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
                continue

        if not predictions:
            raise RuntimeError("ëª¨ë“  ëª¨ë¸ ì˜ˆì¸¡ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

        return self._combine_predictions(predictions)

    def _combine_predictions(self, predictions) -> np.ndarray:
        """
        ëª¨ë¸ ì˜ˆì¸¡ê°’ ê²°í•©

        Args:
            predictions: (ì˜ˆì¸¡ê°’, ê°€ì¤‘ì¹˜) íŠœí”Œ ë¦¬ìŠ¤íŠ¸

        Returns:
            ê°€ì¤‘ í‰ê·  ì˜ˆì¸¡ê°’
        """
        if not predictions:
            raise ValueError("ê²°í•©í•  ì˜ˆì¸¡ê°’ì´ ì—†ìŠµë‹ˆë‹¤.")

        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        weighted_sum = None
        total_weight = 0

        for pred, weight in predictions:
            if weighted_sum is None:
                weighted_sum = pred * weight
            else:
                weighted_sum += pred * weight
            total_weight += weight

        return weighted_sum / total_weight if total_weight > 0 and weighted_sum is not None else (weighted_sum or predictions[0][0])

    def save(self, path: str) -> bool:
        """
        ì•™ìƒë¸” ëª¨ë¸ ì €ì¥

        Args:
            path: ì €ì¥ ê²½ë¡œ

        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            self._ensure_directory(path)

            # ì•™ìƒë¸” ë©”íƒ€ë°ì´í„° ì €ì¥
            ensemble_data = {
                "model_count": len(self.models),
                "weights": self.weights,
                "model_names": [model.model_name for model in self.models],
                "metadata": self.metadata,
            }

            # ê° ëª¨ë¸ ê°œë³„ ì €ì¥
            model_paths = []
            for i, model in enumerate(self.models):
                model_path = f"{path}_model_{i}_{model.model_name}"
                success = model.save(model_path)
                if success:
                    model_paths.append(model_path)
                else:
                    logger.warning(f"ëª¨ë¸ {model.model_name} ì €ì¥ ì‹¤íŒ¨")

            ensemble_data["model_paths"] = model_paths

            # ì•™ìƒë¸” ì •ë³´ ì €ì¥
            with open(f"{path}_ensemble.json", "w", encoding="utf-8") as f:
                json.dump(ensemble_data, f, ensure_ascii=False, indent=2)

            logger.info(f"ì•™ìƒë¸” ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {path}")
            return True

        except Exception as e:
            logger.error(f"ì•™ìƒë¸” ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    def load(self, path: str) -> bool:
        """
        ì•™ìƒë¸” ëª¨ë¸ ë¡œë“œ

        Args:
            path: ë¡œë“œí•  ëª¨ë¸ ê²½ë¡œ

        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            # ì•™ìƒë¸” ì •ë³´ ë¡œë“œ
            with open(f"{path}_ensemble.json", "r", encoding="utf-8") as f:
                ensemble_data = json.load(f)

            # ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
            self.models = []
            self.weights = ensemble_data["weights"]

            # ê° ëª¨ë¸ ë¡œë“œ
            for i, model_path in enumerate(ensemble_data["model_paths"]):
                model_name = ensemble_data["model_names"][i]
                model = self._create_model_instance(model_name)
                
                if model and model.load(model_path):
                    self.models.append(model)
                else:
                    logger.warning(f"ëª¨ë¸ {model_name} ë¡œë“œ ì‹¤íŒ¨")

            self.is_trained = len(self.models) > 0
            self.metadata = ensemble_data.get("metadata", self.metadata)

            logger.info(f"ì•™ìƒë¸” ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {len(self.models)}ê°œ ëª¨ë¸")
            return True

        except Exception as e:
            logger.error(f"ì•™ìƒë¸” ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def _create_model_instance(self, model_type: str) -> Optional[BaseModel]:
        """ëª¨ë¸ íƒ€ì… ë¬¸ìì—´ë¡œë¶€í„° ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        if model_type == "TCNModel":
            from .dl.tcn_model import TCNModel
            # TODO: ë¡œë“œëœ ëª¨ë¸ì— ëŒ€í•œ ìºì‹œ ê´€ë¦¬ì ì£¼ì… ë°©ë²•ì„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.
            # í˜„ì¬ëŠ” ìºì‹œë¥¼ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.
            return TCNModel(config=self.config, cache_manager=None)
        elif model_type == "AutoencoderModel":
            from .dl.autoencoder_model import AutoencoderModel
            return AutoencoderModel(self.config)
        elif model_type == "LightGBMModel":
            from .ml.lightgbm_model import LightGBMModel
            return LightGBMModel(self.config)
        else:
            logger.error(f"ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ íƒ€ì…: {model_type}")
            return None


__all__ = ["BaseModel", "ModelWithAMP", "EnsembleBaseModel"]
