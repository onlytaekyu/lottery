"""
src/models Î™®Îìà ÏóÖÍ∑∏Î†àÏù¥Îìú ÌÖúÌîåÎ¶ø
src/utils ÏãúÏä§ÌÖúÏùÑ ÏôÑÏ†ÑÌûà ÌôúÏö©Ìïú Í≥†ÏÑ±Îä• Î™®Îç∏ Íµ¨ÌòÑ Í∞ÄÏù¥Îìú
"""

import asyncio
import torch
import torch.nn as nn
import numpy as np
from typing import Dict, Any, List, Optional, Union
from dataclasses import dataclass
from abc import ABC, abstractmethod

# ‚úÖ src/utils ÌïµÏã¨ API ÏôÑÏ†Ñ ÌôúÏö©
from src.utils import (
    get_unified_memory_manager,
    get_unified_async_manager,
    get_enhanced_process_pool,
    get_cuda_optimizer,
    get_feature_validator,
    get_config_validator,
    initialize_all_systems
)
from src.utils.unified_logging import get_logger
from src.shared.types import ModelPrediction


@dataclass
class ModelConfig:
    """Î™®Îç∏ ÏÑ§Ï†ï"""
    use_gpu: bool = True
    use_tensorrt: bool = True
    use_amp: bool = True
    batch_size: int = 64
    max_batch_size: int = 1024
    tensorrt_precision: str = "fp16"  # fp32, fp16, int8
    enable_caching: bool = True
    optimize_memory: bool = True
    async_inference: bool = True


class OptimizedModelTemplate(nn.Module, ABC):
    """
    üöÄ src/utils ÏôÑÏ†Ñ ÌôúÏö© Î™®Îç∏ ÌÖúÌîåÎ¶ø
    
    Î™®Îì† models Î™®ÎìàÏù¥ Îî∞ÎùºÏïº Ìï† ÌëúÏ§Ä Íµ¨Ï°∞:
    - TensorRT ÏûêÎèô ÏµúÏ†ÅÌôî
    - GPU Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî
    - ÎπÑÎèôÍ∏∞ Î∞∞Ïπò Ï∂îÎ°†
    - ÏûêÎèô Îç∞Ïù¥ÌÑ∞ Í≤ÄÏ¶ù
    - Ïä§ÎßàÌä∏ Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨
    """
    
    def __init__(self, config: Optional[ModelConfig] = None):
        super().__init__()
        
        self.config = config or ModelConfig()
        
        # ‚úÖ 1. src/utils ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî
        self.memory_mgr = get_unified_memory_manager()
        self.async_mgr = get_unified_async_manager()
        self.process_pool = get_enhanced_process_pool()
        self.cuda_opt = get_cuda_optimizer()
        self.validator = get_feature_validator()
        self.config_validator = get_config_validator()
        self.logger = get_logger(__name__)
        
        # ‚úÖ 2. ÎîîÎ∞îÏù¥Ïä§ ÏÑ§Ï†ï
        if self.config.use_gpu and torch.cuda.is_available():
            self.device = torch.device("cuda")
            self.cuda_opt.set_tf32_enabled(True)
        else:
            self.device = torch.device("cpu")
        
        # ‚úÖ 3. Î™®Îç∏ ÏÉÅÌÉú Í¥ÄÎ¶¨
        self.is_optimized = False
        self.optimized_model = None
        self.model_cache = {}
        
        # ‚úÖ 4. ÏÑ±Îä• ÌÜµÍ≥Ñ
        self.inference_count = 0
        self.total_inference_time = 0.0
        self.batch_sizes = []
        
        self.logger.info(f"‚úÖ {self.__class__.__name__} Ï¥àÍ∏∞Ìôî ÏôÑÎ£å (GPU: {self.config.use_gpu})")
    
    @abstractmethod
    def _build_model(self) -> nn.Module:
        """Î™®Îç∏ Íµ¨Ï°∞ Ï†ïÏùò (ÌïòÏúÑ ÌÅ¥ÎûòÏä§ÏóêÏÑú Íµ¨ÌòÑ)"""
    
    @abstractmethod
    def _forward_impl(self, x: torch.Tensor) -> torch.Tensor:
        """Î™®Îç∏ forward Î°úÏßÅ (ÌïòÏúÑ ÌÅ¥ÎûòÏä§ÏóêÏÑú Íµ¨ÌòÑ)"""
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """ÏµúÏ†ÅÌôîÎêú forward Ìå®Ïä§"""
        # ‚úÖ AMP Ïª®ÌÖçÏä§Ìä∏ÏóêÏÑú Ïã§Ìñâ
        if self.config.use_amp and self.device.type == "cuda":
            with self.cuda_opt.amp_context():
                return self._forward_impl(x)
        else:
            return self._forward_impl(x)
    
    async def optimize_for_inference(self, sample_inputs: List[torch.Tensor]) -> 'OptimizedModelTemplate':
        """
        üöÄ Ï∂îÎ°†Ïö© TensorRT ÏµúÏ†ÅÌôî
        
        Args:
            sample_inputs: ÎåÄÌëú ÏûÖÎ†• ÏÉòÌîåÎì§
            
        Returns:
            ÏµúÏ†ÅÌôîÎêú Î™®Îç∏
        """
        if self.is_optimized:
            self.logger.info("üîÑ Ïù¥ÎØ∏ ÏµúÏ†ÅÌôîÎêú Î™®Îç∏ ÏÇ¨Ïö©")
            return self
        
        try:
            self.logger.info("üöÄ TensorRT ÏµúÏ†ÅÌôî ÏãúÏûë...")
            
            # ‚úÖ ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ Í≤ÄÏ¶ù
            validated_inputs = []
            for inp in sample_inputs:
                if isinstance(inp, torch.Tensor):
                    validated_inputs.append(inp.to(self.device))
                else:
                    # numpy Î∞∞Ïó¥ Îì±ÏùÑ ÌÖêÏÑúÎ°ú Î≥ÄÌôò
                    tensor_inp = torch.tensor(inp, device=self.device, dtype=torch.float32)
                    validated_inputs.append(tensor_inp)
            
            # ‚úÖ TensorRT ÏµúÏ†ÅÌôî Ïã§Ìñâ
            self.optimized_model = self.cuda_opt.tensorrt_optimize_advanced(
                model=self,
                input_examples=validated_inputs,
                precision=self.config.tensorrt_precision,
                dynamic_shapes=True,
                max_batch_size=self.config.max_batch_size,
                model_name=f"{self.__class__.__name__}_optimized"
            )
            
            self.is_optimized = True
            self.logger.info("‚úÖ TensorRT ÏµúÏ†ÅÌôî ÏôÑÎ£å")
            
            return self
            
        except Exception as e:
            self.logger.error(f"‚ùå TensorRT ÏµúÏ†ÅÌôî Ïã§Ìå®: {e}")
            self.logger.info("üîÑ ÏõêÎ≥∏ Î™®Îç∏Î°ú Ìè¥Î∞±")
            return self
    
    async def predict_async(self, inputs: Union[torch.Tensor, np.ndarray, List]) -> ModelPrediction:
        """
        üöÄ ÎπÑÎèôÍ∏∞ Îã®Ïùº ÏòàÏ∏°
        
        Args:
            inputs: ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞
            
        Returns:
            ÏòàÏ∏° Í≤∞Í≥º
        """
        # ‚úÖ ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨
        processed_input = await self._preprocess_input_async(inputs)
        
        # ‚úÖ Ïä§ÎßàÌä∏ Î©îÎ™®Î¶¨ Ìï†ÎãπÏúºÎ°ú Ï∂îÎ°†
        with self.memory_mgr.temporary_allocation(
            size=processed_input.numel(),
            prefer_device=self.device.type
        ) as work_tensor:
            
            # Ï∂îÎ°† Ïã§Ìñâ
            start_time = asyncio.get_event_loop().time()
            
            if self.is_optimized and self.optimized_model:
                prediction = self.optimized_model(processed_input)
            else:
                prediction = self.forward(processed_input)
            
            end_time = asyncio.get_event_loop().time()
            
            # ÏÑ±Îä• ÌÜµÍ≥Ñ ÏóÖÎç∞Ïù¥Ìä∏
            self.inference_count += 1
            self.total_inference_time += (end_time - start_time)
            self.batch_sizes.append(processed_input.shape[0] if len(processed_input.shape) > 0 else 1)
            
            # Í≤∞Í≥º ÌõÑÏ≤òÎ¶¨
            result = await self._postprocess_prediction_async(prediction)
            
            return result
    
    async def predict_batch_async(self, inputs: List[Union[torch.Tensor, np.ndarray]]) -> List[ModelPrediction]:
        """
        üöÄ ÎπÑÎèôÍ∏∞ Î∞∞Ïπò ÏòàÏ∏°
        
        Args:
            inputs: ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ Î¶¨Ïä§Ìä∏
            
        Returns:
            ÏòàÏ∏° Í≤∞Í≥º Î¶¨Ïä§Ìä∏
        """
        if len(inputs) == 0:
            return []
        
        # ‚úÖ ÎèôÏ†Å Î∞∞Ïπò ÌÅ¨Í∏∞ Í≤∞Ï†ï
        optimal_batch_size = self._calculate_optimal_batch_size(inputs)
        
        # ‚úÖ ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ Î∞∞Ïπò Î∂ÑÌï†
        batches = [inputs[i:i+optimal_batch_size] for i in range(0, len(inputs), optimal_batch_size)]
        
        # ‚úÖ Ìñ•ÏÉÅÎêú ÌîÑÎ°úÏÑ∏Ïä§ ÌíÄÏùÑ ÏÇ¨Ïö©Ìïú Î≥ëÎ†¨ Ï≤òÎ¶¨
        batch_results = await self.process_pool.async_process_batch(
            batches,
            process_func=self._predict_single_batch,
            use_gpu=self.config.use_gpu,
            max_workers=4,
            gpu_memory_limit=0.8
        )
        
        # Í≤∞Í≥º Î≥ëÌï©
        all_predictions = []
        for batch_result in batch_results:
            all_predictions.extend(batch_result)
        
        return all_predictions
    
    async def _preprocess_input_async(self, inputs: Union[torch.Tensor, np.ndarray, List]) -> torch.Tensor:
        """ÎπÑÎèôÍ∏∞ ÏûÖÎ†• Ï†ÑÏ≤òÎ¶¨"""
        # ‚úÖ ÏûÖÎ†• ÌÉÄÏûÖÎ≥Ñ Ï≤òÎ¶¨
        if isinstance(inputs, torch.Tensor):
            tensor_input = inputs.to(self.device)
        elif isinstance(inputs, np.ndarray):
            tensor_input = torch.tensor(inputs, device=self.device, dtype=torch.float32)
        elif isinstance(inputs, (list, tuple)):
            tensor_input = torch.tensor(inputs, device=self.device, dtype=torch.float32)
        else:
            raise ValueError(f"ÏßÄÏõêÎêòÏßÄ ÏïäÎäî ÏûÖÎ†• ÌÉÄÏûÖ: {type(inputs)}")
        
        # ‚úÖ ÏûÖÎ†• Ï∞®Ïõê Ï°∞Ï†ï
        if len(tensor_input.shape) == 1:
            tensor_input = tensor_input.unsqueeze(0)  # Î∞∞Ïπò Ï∞®Ïõê Ï∂îÍ∞Ä
        
        # ‚úÖ ÏûÖÎ†• Í≤ÄÏ¶ù
        if self.config.optimize_memory:
            validation_result = self.validator.validate_vector(
                tensor_input.cpu().numpy(),
                check_range=True,
                normalize=False
            )
            
            if not validation_result.is_valid:
                self.logger.warning(f"‚ö†Ô∏è ÏûÖÎ†• Í≤ÄÏ¶ù Ïã§Ìå®: {validation_result.errors}")
                if validation_result.corrected_vector is not None:
                    tensor_input = torch.tensor(
                        validation_result.corrected_vector,
                        device=self.device,
                        dtype=torch.float32
                    )
        
        return tensor_input
    
    async def _postprocess_prediction_async(self, prediction: torch.Tensor) -> ModelPrediction:
        """ÎπÑÎèôÍ∏∞ ÏòàÏ∏° Í≤∞Í≥º ÌõÑÏ≤òÎ¶¨"""
        # CPUÎ°ú Ïù¥Îèô
        prediction_cpu = prediction.cpu()
        
        # ÌôïÎ•† Í≥ÑÏÇ∞ (ÏÜåÌîÑÌä∏Îß•Ïä§ Ï†ÅÏö©)
        if prediction_cpu.dim() > 1 and prediction_cpu.shape[-1] > 1:
            probabilities = torch.softmax(prediction_cpu, dim=-1)
        else:
            probabilities = torch.sigmoid(prediction_cpu)
        
        # ÏòàÏ∏° Í≤∞Í≥º ÏÉùÏÑ±
        result = ModelPrediction(
            predictions=prediction_cpu.numpy(),
            probabilities=probabilities.numpy(),
            model_name=self.__class__.__name__,
            is_optimized=self.is_optimized,
            device=str(self.device)
        )
        
        return result
    
    def _predict_single_batch(self, batch: List[Union[torch.Tensor, np.ndarray]]) -> List[ModelPrediction]:
        """Îã®Ïùº Î∞∞Ïπò ÏòàÏ∏° (Î≥ëÎ†¨ Ï≤òÎ¶¨Ïö©)"""
        predictions = []
        
        for inp in batch:
            # ÎèôÍ∏∞ Ï≤òÎ¶¨ (Î≥ëÎ†¨ ÏõåÏª§ ÎÇ¥Î∂Ä)
            with self.memory_mgr.temporary_allocation(
                size=self._estimate_input_size(inp),
                prefer_device=self.device.type
            ) as work_tensor:
                
                # ÏûÖÎ†• Ï†ÑÏ≤òÎ¶¨
                if isinstance(inp, torch.Tensor):
                    processed_inp = inp.to(self.device)
                else:
                    processed_inp = torch.tensor(inp, device=self.device, dtype=torch.float32)
                
                # Ï∂îÎ°† Ïã§Ìñâ
                if self.is_optimized and self.optimized_model:
                    prediction = self.optimized_model(processed_inp)
                else:
                    prediction = self.forward(processed_inp)
                
                # ÌõÑÏ≤òÎ¶¨
                result = self._postprocess_prediction_sync(prediction)
                predictions.append(result)
        
        return predictions
    
    def _postprocess_prediction_sync(self, prediction: torch.Tensor) -> ModelPrediction:
        """ÎèôÍ∏∞ ÏòàÏ∏° Í≤∞Í≥º ÌõÑÏ≤òÎ¶¨"""
        prediction_cpu = prediction.cpu()
        
        if prediction_cpu.dim() > 1 and prediction_cpu.shape[-1] > 1:
            probabilities = torch.softmax(prediction_cpu, dim=-1)
        else:
            probabilities = torch.sigmoid(prediction_cpu)
        
        return ModelPrediction(
            predictions=prediction_cpu.numpy(),
            probabilities=probabilities.numpy(),
            model_name=self.__class__.__name__,
            is_optimized=self.is_optimized,
            device=str(self.device)
        )
    
    def _calculate_optimal_batch_size(self, inputs: List) -> int:
        """ÏµúÏ†Å Î∞∞Ïπò ÌÅ¨Í∏∞ Í≥ÑÏÇ∞"""
        if not inputs:
            return self.config.batch_size
        
        # ‚úÖ GPU Î©îÎ™®Î¶¨ ÏÉÅÌÉú ÌôïÏù∏
        if self.device.type == "cuda":
            gpu_memory_free = self.memory_mgr.get_gpu_memory_available()
            
            # Î©îÎ™®Î¶¨ ÏÉÅÌÉúÏóê Îî∞Î•∏ ÎèôÏ†Å Î∞∞Ïπò ÌÅ¨Í∏∞ Ï°∞Ï†ï
            if gpu_memory_free > 0.8:
                return min(self.config.max_batch_size, len(inputs))
            elif gpu_memory_free > 0.5:
                return min(self.config.batch_size * 2, len(inputs))
            else:
                return min(self.config.batch_size // 2, len(inputs))
        
        return min(self.config.batch_size, len(inputs))
    
    def _estimate_input_size(self, inp: Union[torch.Tensor, np.ndarray, List]) -> int:
        """ÏûÖÎ†• ÌÅ¨Í∏∞ Ï∂îÏ†ï"""
        if isinstance(inp, torch.Tensor):
            return inp.numel() * inp.element_size()
        elif isinstance(inp, np.ndarray):
            return inp.nbytes
        elif isinstance(inp, (list, tuple)):
            return len(inp) * 4  # float32 Í∏∞Ï§Ä
        else:
            return 1024  # Í∏∞Î≥∏Í∞í
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """ÏÑ±Îä• ÌÜµÍ≥Ñ Î∞òÌôò"""
        avg_inference_time = self.total_inference_time / max(self.inference_count, 1)
        avg_batch_size = sum(self.batch_sizes) / max(len(self.batch_sizes), 1)
        
        return {
            "inference_count": self.inference_count,
            "total_inference_time": self.total_inference_time,
            "avg_inference_time": avg_inference_time,
            "avg_batch_size": avg_batch_size,
            "is_optimized": self.is_optimized,
            "device": str(self.device),
            "memory_stats": self.memory_mgr.get_memory_status(),
            "cuda_stats": self.cuda_opt.get_cache_stats() if self.device.type == "cuda" else {}
        }
    
    def save_model(self, path: str, save_optimized: bool = True):
        """Î™®Îç∏ Ï†ÄÏû•"""
        try:
            # ‚úÖ Í∏∞Î≥∏ Î™®Îç∏ Ï†ÄÏû•
            model_state = {
                "model_state_dict": self.state_dict(),
                "config": self.config.__dict__,
                "model_class": self.__class__.__name__,
                "is_optimized": self.is_optimized,
                "performance_stats": self.get_performance_stats()
            }
            
            torch.save(model_state, path)
            
            # ‚úÖ ÏµúÏ†ÅÌôîÎêú Î™®Îç∏ Ï†ÄÏû• (ÏÑ†ÌÉùÏ†Å)
            if save_optimized and self.is_optimized and self.optimized_model:
                optimized_path = path.replace('.pt', '_optimized.pt')
                torch.save(self.optimized_model, optimized_path)
                self.logger.info(f"‚úÖ ÏµúÏ†ÅÌôîÎêú Î™®Îç∏ Ï†ÄÏû•: {optimized_path}")
            
            self.logger.info(f"‚úÖ Î™®Îç∏ Ï†ÄÏû• ÏôÑÎ£å: {path}")
            
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Ï†ÄÏû• Ïã§Ìå®: {e}")
            raise
    
    def load_model(self, path: str, load_optimized: bool = True):
        """Î™®Îç∏ Î°úÎìú"""
        try:
            # ‚úÖ Í∏∞Î≥∏ Î™®Îç∏ Î°úÎìú
            model_state = torch.load(path, map_location=self.device)
            
            self.load_state_dict(model_state["model_state_dict"])
            self.is_optimized = model_state.get("is_optimized", False)
            
            # ‚úÖ ÏµúÏ†ÅÌôîÎêú Î™®Îç∏ Î°úÎìú (ÏÑ†ÌÉùÏ†Å)
            if load_optimized and self.is_optimized:
                optimized_path = path.replace('.pt', '_optimized.pt')
                try:
                    self.optimized_model = torch.load(optimized_path, map_location=self.device)
                    self.logger.info(f"‚úÖ ÏµúÏ†ÅÌôîÎêú Î™®Îç∏ Î°úÎìú: {optimized_path}")
                except FileNotFoundError:
                    self.logger.warning(f"‚ö†Ô∏è ÏµúÏ†ÅÌôîÎêú Î™®Îç∏ ÌååÏùº ÏóÜÏùå: {optimized_path}")
                    self.is_optimized = False
            
            self.logger.info(f"‚úÖ Î™®Îç∏ Î°úÎìú ÏôÑÎ£å: {path}")
            
        except Exception as e:
            self.logger.error(f"‚ùå Î™®Îç∏ Î°úÎìú Ïã§Ìå®: {e}")
            raise
    
    def cleanup(self):
        """Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨"""
        if self.optimized_model:
            del self.optimized_model
        
        self.model_cache.clear()
        
        # GPU Î©îÎ™®Î¶¨ Ï†ïÎ¶¨
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        self.logger.info(f"‚úÖ {self.__class__.__name__} Ï†ïÎ¶¨ ÏôÑÎ£å")


# =====================================
# Ïã§Ï†ú Î™®Îç∏ Íµ¨ÌòÑ ÏòàÏãú
# =====================================

class EnhancedLotteryPredictor(OptimizedModelTemplate):
    """
    Ìñ•ÏÉÅÎêú Î°úÎòê ÏòàÏ∏° Î™®Îç∏ - ÌÖúÌîåÎ¶ø ÌôúÏö© ÏòàÏãú
    Í∏∞Ï°¥ Î™®Îç∏Îì§ÏùÑ src/utils Í∏∞Î∞òÏúºÎ°ú ÏôÑÏ†Ñ Ïû¨Íµ¨ÏÑ±
    """
    
    def __init__(self, input_dim: int = 100, hidden_dims: List[int] = [256, 128, 64], 
                 output_dim: int = 45, config: Optional[ModelConfig] = None):
        super().__init__(config)
        
        self.input_dim = input_dim
        self.hidden_dims = hidden_dims
        self.output_dim = output_dim
        
        # Î™®Îç∏ Íµ¨Ï°∞ Íµ¨Ï∂ï
        self.model = self._build_model()
        self.to(self.device)
        
        self.logger.info(f"‚úÖ LotteryPredictor Íµ¨Ï∂ï ÏôÑÎ£å: {input_dim}->{hidden_dims}->{output_dim}")
    
    def _build_model(self) -> nn.Module:
        """Î°úÎòê ÏòàÏ∏° Î™®Îç∏ Íµ¨Ï°∞ Ï†ïÏùò"""
        layers = []
        
        # ÏûÖÎ†• Î†àÏù¥Ïñ¥
        layers.append(nn.Linear(self.input_dim, self.hidden_dims[0]))
        layers.append(nn.ReLU())
        layers.append(nn.Dropout(0.1))
        
        # ÌûàÎì† Î†àÏù¥Ïñ¥Îì§
        for i in range(len(self.hidden_dims) - 1):
            layers.append(nn.Linear(self.hidden_dims[i], self.hidden_dims[i+1]))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(0.1))
        
        # Ï∂úÎ†• Î†àÏù¥Ïñ¥
        layers.append(nn.Linear(self.hidden_dims[-1], self.output_dim))
        
        return nn.Sequential(*layers)
    
    def _forward_impl(self, x: torch.Tensor) -> torch.Tensor:
        """Î™®Îç∏ forward Ìå®Ïä§"""
        return self.model(x)
    
    async def predict_lottery_numbers(self, features: Union[torch.Tensor, np.ndarray]) -> Dict[str, Any]:
        """Î°úÎòê Î≤àÌò∏ ÏòàÏ∏°"""
        # ‚úÖ ÎπÑÎèôÍ∏∞ ÏòàÏ∏° Ïã§Ìñâ
        prediction = await self.predict_async(features)
        
        # ‚úÖ Î°úÎòê Î≤àÌò∏ ÌòïÌÉúÎ°ú Î≥ÄÌôò
        probabilities = prediction.probabilities
        if len(probabilities.shape) > 1:
            probabilities = probabilities[0]  # Ï≤´ Î≤àÏß∏ Î∞∞Ïπò
        
        # ÏÉÅÏúÑ 6Í∞ú Î≤àÌò∏ ÏÑ†ÌÉù
        top_6_indices = np.argsort(probabilities)[-6:]
        lottery_numbers = [int(idx + 1) for idx in top_6_indices]  # 1-45 Î≤îÏúÑÎ°ú Î≥ÄÌôò
        lottery_numbers.sort()
        
        # Ïã†Î¢∞ÎèÑ Í≥ÑÏÇ∞
        confidence = np.mean(probabilities[top_6_indices])
        
        return {
            "numbers": lottery_numbers,
            "confidence": float(confidence),
            "all_probabilities": probabilities.tolist(),
            "model_performance": self.get_performance_stats()
        }


# =====================================
# Î™®Îç∏ Ìå©ÌÜ†Î¶¨
# =====================================

class OptimizedModelFactory:
    """ÏµúÏ†ÅÌôîÎêú Î™®Îç∏ ÏÉùÏÑ± Ìå©ÌÜ†Î¶¨"""
    
    def __init__(self):
        self.logger = get_logger(__name__)
        self.config_validator = get_config_validator()
    
    async def create_model(self, model_type: str, config_path: str, **kwargs) -> OptimizedModelTemplate:
        """Î™®Îç∏ ÏÉùÏÑ±"""
        # ‚úÖ ÏÑ§Ï†ï Í≤ÄÏ¶ù
        if not self.config_validator.validate_config_file(config_path):
            raise ValueError(f"Î™®Îç∏ ÏÑ§Ï†ï ÌååÏùº Í≤ÄÏ¶ù Ïã§Ìå®: {config_path}")
        
        # Î™®Îç∏ ÌÉÄÏûÖÎ≥Ñ ÏÉùÏÑ±
        if model_type == "lottery_predictor":
            return EnhancedLotteryPredictor(**kwargs)
        elif model_type == "pattern_analyzer":
            # Îã§Î•∏ Î™®Îç∏ ÌÉÄÏûÖÎì§...
            pass
        else:
            raise ValueError(f"ÏßÄÏõêÎêòÏßÄ ÏïäÎäî Î™®Îç∏ ÌÉÄÏûÖ: {model_type}")
    
    async def optimize_model(self, model: OptimizedModelTemplate, 
                            sample_inputs: List[torch.Tensor]) -> OptimizedModelTemplate:
        """Î™®Îç∏ ÏµúÏ†ÅÌôî"""
        return await model.optimize_for_inference(sample_inputs)


# =====================================
# ÏÇ¨Ïö© ÏòàÏãú
# =====================================

async def main():
    """ÌÖúÌîåÎ¶ø ÏÇ¨Ïö© ÏòàÏãú"""
    
    # ‚úÖ 1. ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî
    initialize_all_systems()
    
    # ‚úÖ 2. Î™®Îç∏ ÏÉùÏÑ±
    config = ModelConfig(
        use_gpu=True,
        use_tensorrt=True,
        batch_size=64,
        tensorrt_precision="fp16"
    )
    
    model = EnhancedLotteryPredictor(
        input_dim=100,
        hidden_dims=[256, 128, 64],
        output_dim=45,
        config=config
    )
    
    # ‚úÖ 3. Î™®Îç∏ ÏµúÏ†ÅÌôî
    sample_input = torch.randn(1, 100)
    optimized_model = await model.optimize_for_inference([sample_input])
    
    # ‚úÖ 4. ÏòàÏ∏° Ïã§Ìñâ
    try:
        test_features = np.random.randn(100)
        result = await optimized_model.predict_lottery_numbers(test_features)
        
        print("‚úÖ ÏòàÏ∏° ÏôÑÎ£å!")
        print(f"ÏòàÏ∏° Î≤àÌò∏: {result['numbers']}")
        print(f"Ïã†Î¢∞ÎèÑ: {result['confidence']:.3f}")
        
        # ÏÑ±Îä• ÌÜµÍ≥Ñ
        stats = optimized_model.get_performance_stats()
        print(f"ÌèâÍ∑† Ï∂îÎ°† ÏãúÍ∞Ñ: {stats['avg_inference_time']:.4f}Ï¥à")
        print(f"TensorRT ÏµúÏ†ÅÌôî: {'‚úÖ' if stats['is_optimized'] else '‚ùå'}")
        
    finally:
        # ‚úÖ 5. Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨
        optimized_model.cleanup()


if __name__ == "__main__":
    asyncio.run(main()) 