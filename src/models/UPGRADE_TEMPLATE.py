"""
src/models ëª¨ë“ˆ ì—…ê·¸ë ˆì´ë“œ í…œí”Œë¦¿
src/utils ì‹œìŠ¤í…œì„ ì™„ì „íˆ í™œìš©í•œ ê³ ì„±ëŠ¥ ëª¨ë¸ êµ¬í˜„ ê°€ì´ë“œ
"""

import asyncio
import torch
import torch.nn as nn
import numpy as np
from typing import Dict, Any, List, Optional, Union
from dataclasses import dataclass
from abc import ABC, abstractmethod

# âœ… src/utils í•µì‹¬ API ì™„ì „ í™œìš©
from src.utils import (
    get_unified_memory_manager,
    get_unified_async_manager,
    get_enhanced_process_pool,
    get_cuda_optimizer,
    get_feature_validator,
    get_config_validator,
    initialize_all_systems
)
from src.utils.unified_logging import get_logger
from src.shared.types import ModelPrediction


@dataclass
class ModelConfig:
    """ëª¨ë¸ ì„¤ì •"""
    use_gpu: bool = True
    use_tensorrt: bool = True
    use_amp: bool = True
    batch_size: int = 64
    max_batch_size: int = 1024
    tensorrt_precision: str = "fp16"  # fp32, fp16, int8
    enable_caching: bool = True
    optimize_memory: bool = True
    async_inference: bool = True


class OptimizedModelTemplate(nn.Module, ABC):
    """
    ğŸš€ src/utils ì™„ì „ í™œìš© ëª¨ë¸ í…œí”Œë¦¿
    
    ëª¨ë“  models ëª¨ë“ˆì´ ë”°ë¼ì•¼ í•  í‘œì¤€ êµ¬ì¡°:
    - TensorRT ìë™ ìµœì í™”
    - GPU ë©”ëª¨ë¦¬ ìµœì í™”
    - ë¹„ë™ê¸° ë°°ì¹˜ ì¶”ë¡ 
    - ìë™ ë°ì´í„° ê²€ì¦
    - ìŠ¤ë§ˆíŠ¸ ë©”ëª¨ë¦¬ ê´€ë¦¬
    """
    
    def __init__(self, config: Optional[ModelConfig] = None):
        super().__init__()
        
        self.config = config or ModelConfig()
        
        # âœ… 1. src/utils ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self.memory_mgr = get_unified_memory_manager()
        self.async_mgr = get_unified_async_manager()
        self.process_pool = get_enhanced_process_pool()
        self.cuda_opt = get_cuda_optimizer()
        self.validator = get_feature_validator()
        self.config_validator = get_config_validator()
        self.logger = get_logger(__name__)
        
        # âœ… 2. ë””ë°”ì´ìŠ¤ ì„¤ì •
        if self.config.use_gpu and torch.cuda.is_available():
            self.device = torch.device("cuda")
            self.cuda_opt.set_tf32_enabled(True)
        else:
            self.device = torch.device("cpu")
        
        # âœ… 3. ëª¨ë¸ ìƒíƒœ ê´€ë¦¬
        self.is_optimized = False
        self.optimized_model = None
        self.model_cache = {}
        
        # âœ… 4. ì„±ëŠ¥ í†µê³„
        self.inference_count = 0
        self.total_inference_time = 0.0
        self.batch_sizes = []
        
        self.logger.info(f"âœ… {self.__class__.__name__} ì´ˆê¸°í™” ì™„ë£Œ (GPU: {self.config.use_gpu})")
    
    @abstractmethod
    def _build_model(self) -> nn.Module:
        """ëª¨ë¸ êµ¬ì¡° ì •ì˜ (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
    
    @abstractmethod
    def _forward_impl(self, x: torch.Tensor) -> torch.Tensor:
        """ëª¨ë¸ forward ë¡œì§ (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """ìµœì í™”ëœ forward íŒ¨ìŠ¤"""
        # âœ… AMP ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì‹¤í–‰
        if self.config.use_amp and self.device.type == "cuda":
            with self.cuda_opt.amp_context():
                return self._forward_impl(x)
        else:
            return self._forward_impl(x)
    
    async def optimize_for_inference(self, sample_inputs: List[torch.Tensor]) -> 'OptimizedModelTemplate':
        """
        ğŸš€ ì¶”ë¡ ìš© TensorRT ìµœì í™”
        
        Args:
            sample_inputs: ëŒ€í‘œ ì…ë ¥ ìƒ˜í”Œë“¤
            
        Returns:
            ìµœì í™”ëœ ëª¨ë¸
        """
        if self.is_optimized:
            self.logger.info("ğŸ”„ ì´ë¯¸ ìµœì í™”ëœ ëª¨ë¸ ì‚¬ìš©")
            return self
        
        try:
            self.logger.info("ğŸš€ TensorRT ìµœì í™” ì‹œì‘...")
            
            # âœ… ì…ë ¥ ë°ì´í„° ê²€ì¦
            validated_inputs = []
            for inp in sample_inputs:
                if isinstance(inp, torch.Tensor):
                    validated_inputs.append(inp.to(self.device))
                else:
                    # numpy ë°°ì—´ ë“±ì„ í…ì„œë¡œ ë³€í™˜
                    tensor_inp = torch.tensor(inp, device=self.device, dtype=torch.float32)
                    validated_inputs.append(tensor_inp)
            
            # âœ… TensorRT ìµœì í™” ì‹¤í–‰
            self.optimized_model = self.cuda_opt.tensorrt_optimize_advanced(
                model=self,
                input_examples=validated_inputs,
                precision=self.config.tensorrt_precision,
                dynamic_shapes=True,
                max_batch_size=self.config.max_batch_size,
                model_name=f"{self.__class__.__name__}_optimized"
            )
            
            self.is_optimized = True
            self.logger.info("âœ… TensorRT ìµœì í™” ì™„ë£Œ")
            
            return self
            
        except Exception as e:
            self.logger.error(f"âŒ TensorRT ìµœì í™” ì‹¤íŒ¨: {e}")
            self.logger.info("ğŸ”„ ì›ë³¸ ëª¨ë¸ë¡œ í´ë°±")
            return self
    
    async def predict_async(self, inputs: Union[torch.Tensor, np.ndarray, List]) -> ModelPrediction:
        """
        ğŸš€ ë¹„ë™ê¸° ë‹¨ì¼ ì˜ˆì¸¡
        
        Args:
            inputs: ì…ë ¥ ë°ì´í„°
            
        Returns:
            ì˜ˆì¸¡ ê²°ê³¼
        """
        # âœ… ì…ë ¥ ë°ì´í„° ì „ì²˜ë¦¬
        processed_input = await self._preprocess_input_async(inputs)
        
        # âœ… ìŠ¤ë§ˆíŠ¸ ë©”ëª¨ë¦¬ í• ë‹¹ìœ¼ë¡œ ì¶”ë¡ 
        with self.memory_mgr.temporary_allocation(
            size=processed_input.numel(),
            prefer_device=self.device.type
        ) as work_tensor:
            
            # ì¶”ë¡  ì‹¤í–‰
            start_time = asyncio.get_event_loop().time()
            
            if self.is_optimized and self.optimized_model:
                prediction = self.optimized_model(processed_input)
            else:
                prediction = self.forward(processed_input)
            
            end_time = asyncio.get_event_loop().time()
            
            # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
            self.inference_count += 1
            self.total_inference_time += (end_time - start_time)
            self.batch_sizes.append(processed_input.shape[0] if len(processed_input.shape) > 0 else 1)
            
            # ê²°ê³¼ í›„ì²˜ë¦¬
            result = await self._postprocess_prediction_async(prediction)
            
            return result
    
    async def predict_batch_async(self, inputs: List[Union[torch.Tensor, np.ndarray]]) -> List[ModelPrediction]:
        """
        ğŸš€ ë¹„ë™ê¸° ë°°ì¹˜ ì˜ˆì¸¡
        
        Args:
            inputs: ì…ë ¥ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ì˜ˆì¸¡ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if len(inputs) == 0:
            return []
        
        # âœ… ë™ì  ë°°ì¹˜ í¬ê¸° ê²°ì •
        optimal_batch_size = self._calculate_optimal_batch_size(inputs)
        
        # âœ… ì…ë ¥ ë°ì´í„° ë°°ì¹˜ ë¶„í• 
        batches = [inputs[i:i+optimal_batch_size] for i in range(0, len(inputs), optimal_batch_size)]
        
        # âœ… í–¥ìƒëœ í”„ë¡œì„¸ìŠ¤ í’€ì„ ì‚¬ìš©í•œ ë³‘ë ¬ ì²˜ë¦¬
        batch_results = await self.process_pool.async_process_batch(
            batches,
            process_func=self._predict_single_batch,
            use_gpu=self.config.use_gpu,
            max_workers=4,
            gpu_memory_limit=0.8
        )
        
        # ê²°ê³¼ ë³‘í•©
        all_predictions = []
        for batch_result in batch_results:
            all_predictions.extend(batch_result)
        
        return all_predictions
    
    async def _preprocess_input_async(self, inputs: Union[torch.Tensor, np.ndarray, List]) -> torch.Tensor:
        """ë¹„ë™ê¸° ì…ë ¥ ì „ì²˜ë¦¬"""
        # âœ… ì…ë ¥ íƒ€ì…ë³„ ì²˜ë¦¬
        if isinstance(inputs, torch.Tensor):
            tensor_input = inputs.to(self.device)
        elif isinstance(inputs, np.ndarray):
            tensor_input = torch.tensor(inputs, device=self.device, dtype=torch.float32)
        elif isinstance(inputs, (list, tuple)):
            tensor_input = torch.tensor(inputs, device=self.device, dtype=torch.float32)
        else:
            raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ì…ë ¥ íƒ€ì…: {type(inputs)}")
        
        # âœ… ì…ë ¥ ì°¨ì› ì¡°ì •
        if len(tensor_input.shape) == 1:
            tensor_input = tensor_input.unsqueeze(0)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        
        # âœ… ì…ë ¥ ê²€ì¦
        if self.config.optimize_memory:
            validation_result = self.validator.validate_vector(
                tensor_input.cpu().numpy(),
                check_range=True,
                normalize=False
            )
            
            if not validation_result.is_valid:
                self.logger.warning(f"âš ï¸ ì…ë ¥ ê²€ì¦ ì‹¤íŒ¨: {validation_result.errors}")
                if validation_result.corrected_vector is not None:
                    tensor_input = torch.tensor(
                        validation_result.corrected_vector,
                        device=self.device,
                        dtype=torch.float32
                    )
        
        return tensor_input
    
    async def _postprocess_prediction_async(self, prediction: torch.Tensor) -> ModelPrediction:
        """ë¹„ë™ê¸° ì˜ˆì¸¡ ê²°ê³¼ í›„ì²˜ë¦¬"""
        # CPUë¡œ ì´ë™
        prediction_cpu = prediction.cpu()
        
        # í™•ë¥  ê³„ì‚° (ì†Œí”„íŠ¸ë§¥ìŠ¤ ì ìš©)
        if prediction_cpu.dim() > 1 and prediction_cpu.shape[-1] > 1:
            probabilities = torch.softmax(prediction_cpu, dim=-1)
        else:
            probabilities = torch.sigmoid(prediction_cpu)
        
        # ì˜ˆì¸¡ ê²°ê³¼ ìƒì„±
        result = ModelPrediction(
            predictions=prediction_cpu.numpy(),
            probabilities=probabilities.numpy(),
            model_name=self.__class__.__name__,
            is_optimized=self.is_optimized,
            device=str(self.device)
        )
        
        return result
    
    def _predict_single_batch(self, batch: List[Union[torch.Tensor, np.ndarray]]) -> List[ModelPrediction]:
        """ë‹¨ì¼ ë°°ì¹˜ ì˜ˆì¸¡ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
        predictions = []
        
        for inp in batch:
            # ë™ê¸° ì²˜ë¦¬ (ë³‘ë ¬ ì›Œì»¤ ë‚´ë¶€)
            with self.memory_mgr.temporary_allocation(
                size=self._estimate_input_size(inp),
                prefer_device=self.device.type
            ) as work_tensor:
                
                # ì…ë ¥ ì „ì²˜ë¦¬
                if isinstance(inp, torch.Tensor):
                    processed_inp = inp.to(self.device)
                else:
                    processed_inp = torch.tensor(inp, device=self.device, dtype=torch.float32)
                
                # ì¶”ë¡  ì‹¤í–‰
                if self.is_optimized and self.optimized_model:
                    prediction = self.optimized_model(processed_inp)
                else:
                    prediction = self.forward(processed_inp)
                
                # í›„ì²˜ë¦¬
                result = self._postprocess_prediction_sync(prediction)
                predictions.append(result)
        
        return predictions
    
    def _postprocess_prediction_sync(self, prediction: torch.Tensor) -> ModelPrediction:
        """ë™ê¸° ì˜ˆì¸¡ ê²°ê³¼ í›„ì²˜ë¦¬"""
        prediction_cpu = prediction.cpu()
        
        if prediction_cpu.dim() > 1 and prediction_cpu.shape[-1] > 1:
            probabilities = torch.softmax(prediction_cpu, dim=-1)
        else:
            probabilities = torch.sigmoid(prediction_cpu)
        
        return ModelPrediction(
            predictions=prediction_cpu.numpy(),
            probabilities=probabilities.numpy(),
            model_name=self.__class__.__name__,
            is_optimized=self.is_optimized,
            device=str(self.device)
        )
    
    def _calculate_optimal_batch_size(self, inputs: List) -> int:
        """ìµœì  ë°°ì¹˜ í¬ê¸° ê³„ì‚°"""
        if not inputs:
            return self.config.batch_size
        
        # âœ… GPU ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
        if self.device.type == "cuda":
            gpu_memory_free = self.memory_mgr.get_gpu_memory_available()
            
            # ë©”ëª¨ë¦¬ ìƒíƒœì— ë”°ë¥¸ ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì •
            if gpu_memory_free > 0.8:
                return min(self.config.max_batch_size, len(inputs))
            elif gpu_memory_free > 0.5:
                return min(self.config.batch_size * 2, len(inputs))
            else:
                return min(self.config.batch_size // 2, len(inputs))
        
        return min(self.config.batch_size, len(inputs))
    
    def _estimate_input_size(self, inp: Union[torch.Tensor, np.ndarray, List]) -> int:
        """ì…ë ¥ í¬ê¸° ì¶”ì •"""
        if isinstance(inp, torch.Tensor):
            return inp.numel() * inp.element_size()
        elif isinstance(inp, np.ndarray):
            return inp.nbytes
        elif isinstance(inp, (list, tuple)):
            return len(inp) * 4  # float32 ê¸°ì¤€
        else:
            return 1024  # ê¸°ë³¸ê°’
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        avg_inference_time = self.total_inference_time / max(self.inference_count, 1)
        avg_batch_size = sum(self.batch_sizes) / max(len(self.batch_sizes), 1)
        
        return {
            "inference_count": self.inference_count,
            "total_inference_time": self.total_inference_time,
            "avg_inference_time": avg_inference_time,
            "avg_batch_size": avg_batch_size,
            "is_optimized": self.is_optimized,
            "device": str(self.device),
            "memory_stats": self.memory_mgr.get_memory_status(),
            "cuda_stats": self.cuda_opt.get_cache_stats() if self.device.type == "cuda" else {}
        }
    
    def save_model(self, path: str, save_optimized: bool = True):
        """ëª¨ë¸ ì €ì¥"""
        try:
            # âœ… ê¸°ë³¸ ëª¨ë¸ ì €ì¥
            model_state = {
                "model_state_dict": self.state_dict(),
                "config": self.config.__dict__,
                "model_class": self.__class__.__name__,
                "is_optimized": self.is_optimized,
                "performance_stats": self.get_performance_stats()
            }
            
            torch.save(model_state, path)
            
            # âœ… ìµœì í™”ëœ ëª¨ë¸ ì €ì¥ (ì„ íƒì )
            if save_optimized and self.is_optimized and self.optimized_model:
                optimized_path = path.replace('.pt', '_optimized.pt')
                torch.save(self.optimized_model, optimized_path)
                self.logger.info(f"âœ… ìµœì í™”ëœ ëª¨ë¸ ì €ì¥: {optimized_path}")
            
            self.logger.info(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {path}")
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise
    
    def load_model(self, path: str, load_optimized: bool = True):
        """ëª¨ë¸ ë¡œë“œ"""
        try:
            # âœ… ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ
            model_state = torch.load(path, map_location=self.device)
            
            self.load_state_dict(model_state["model_state_dict"])
            self.is_optimized = model_state.get("is_optimized", False)
            
            # âœ… ìµœì í™”ëœ ëª¨ë¸ ë¡œë“œ (ì„ íƒì )
            if load_optimized and self.is_optimized:
                optimized_path = path.replace('.pt', '_optimized.pt')
                try:
                    self.optimized_model = torch.load(optimized_path, map_location=self.device)
                    self.logger.info(f"âœ… ìµœì í™”ëœ ëª¨ë¸ ë¡œë“œ: {optimized_path}")
                except FileNotFoundError:
                    self.logger.warning(f"âš ï¸ ìµœì í™”ëœ ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {optimized_path}")
                    self.is_optimized = False
            
            self.logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {path}")
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.optimized_model:
            del self.optimized_model
        
        self.model_cache.clear()
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        self.logger.info(f"âœ… {self.__class__.__name__} ì •ë¦¬ ì™„ë£Œ")


# =====================================
# ì‹¤ì œ ëª¨ë¸ êµ¬í˜„ ì˜ˆì‹œ
# =====================================

class EnhancedLotteryPredictor(OptimizedModelTemplate):
    """
    í–¥ìƒëœ ë¡œë˜ ì˜ˆì¸¡ ëª¨ë¸ - í…œí”Œë¦¿ í™œìš© ì˜ˆì‹œ
    ê¸°ì¡´ ëª¨ë¸ë“¤ì„ src/utils ê¸°ë°˜ìœ¼ë¡œ ì™„ì „ ì¬êµ¬ì„±
    """
    
    def __init__(self, input_dim: int = 100, hidden_dims: List[int] = [256, 128, 64], 
                 output_dim: int = 45, config: Optional[ModelConfig] = None):
        super().__init__(config)
        
        self.input_dim = input_dim
        self.hidden_dims = hidden_dims
        self.output_dim = output_dim
        
        # ëª¨ë¸ êµ¬ì¡° êµ¬ì¶•
        self.model = self._build_model()
        self.to(self.device)
        
        self.logger.info(f"âœ… LotteryPredictor êµ¬ì¶• ì™„ë£Œ: {input_dim}->{hidden_dims}->{output_dim}")
    
    def _build_model(self) -> nn.Module:
        """ë¡œë˜ ì˜ˆì¸¡ ëª¨ë¸ êµ¬ì¡° ì •ì˜"""
        layers = []
        
        # ì…ë ¥ ë ˆì´ì–´
        layers.append(nn.Linear(self.input_dim, self.hidden_dims[0]))
        layers.append(nn.ReLU())
        layers.append(nn.Dropout(0.1))
        
        # íˆë“  ë ˆì´ì–´ë“¤
        for i in range(len(self.hidden_dims) - 1):
            layers.append(nn.Linear(self.hidden_dims[i], self.hidden_dims[i+1]))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(0.1))
        
        # ì¶œë ¥ ë ˆì´ì–´
        layers.append(nn.Linear(self.hidden_dims[-1], self.output_dim))
        
        return nn.Sequential(*layers)
    
    def _forward_impl(self, x: torch.Tensor) -> torch.Tensor:
        """ëª¨ë¸ forward íŒ¨ìŠ¤"""
        return self.model(x)
    
    async def predict_lottery_numbers(self, features: Union[torch.Tensor, np.ndarray]) -> Dict[str, Any]:
        """ë¡œë˜ ë²ˆí˜¸ ì˜ˆì¸¡"""
        # âœ… ë¹„ë™ê¸° ì˜ˆì¸¡ ì‹¤í–‰
        prediction = await self.predict_async(features)
        
        # âœ… ë¡œë˜ ë²ˆí˜¸ í˜•íƒœë¡œ ë³€í™˜
        probabilities = prediction.probabilities
        if len(probabilities.shape) > 1:
            probabilities = probabilities[0]  # ì²« ë²ˆì§¸ ë°°ì¹˜
        
        # ìƒìœ„ 6ê°œ ë²ˆí˜¸ ì„ íƒ
        top_6_indices = np.argsort(probabilities)[-6:]
        lottery_numbers = [int(idx + 1) for idx in top_6_indices]  # 1-45 ë²”ìœ„ë¡œ ë³€í™˜
        lottery_numbers.sort()
        
        # ì‹ ë¢°ë„ ê³„ì‚°
        confidence = np.mean(probabilities[top_6_indices])
        
        return {
            "numbers": lottery_numbers,
            "confidence": float(confidence),
            "all_probabilities": probabilities.tolist(),
            "model_performance": self.get_performance_stats()
        }


# =====================================
# ëª¨ë¸ íŒ©í† ë¦¬
# =====================================

class OptimizedModelFactory:
    """ìµœì í™”ëœ ëª¨ë¸ ìƒì„± íŒ©í† ë¦¬"""
    
    def __init__(self):
        self.logger = get_logger(__name__)
        self.config_validator = get_config_validator()
    
    async def create_model(self, model_type: str, config_path: str, **kwargs) -> OptimizedModelTemplate:
        """ëª¨ë¸ ìƒì„±"""
        # âœ… ì„¤ì • ê²€ì¦
        if not self.config_validator.validate_config_file(config_path):
            raise ValueError(f"ëª¨ë¸ ì„¤ì • íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨: {config_path}")
        
        # ëª¨ë¸ íƒ€ì…ë³„ ìƒì„±
        if model_type == "lottery_predictor":
            return EnhancedLotteryPredictor(**kwargs)
        elif model_type == "pattern_analyzer":
            # ë‹¤ë¥¸ ëª¨ë¸ íƒ€ì…ë“¤...
            pass
        else:
            raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {model_type}")
    
    async def optimize_model(self, model: OptimizedModelTemplate, 
                            sample_inputs: List[torch.Tensor]) -> OptimizedModelTemplate:
        """ëª¨ë¸ ìµœì í™”"""
        return await model.optimize_for_inference(sample_inputs)


# =====================================
# ì‚¬ìš© ì˜ˆì‹œ
# =====================================

async def main():
    """í…œí”Œë¦¿ ì‚¬ìš© ì˜ˆì‹œ"""
    
    # âœ… 1. ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    initialize_all_systems()
    
    # âœ… 2. ëª¨ë¸ ìƒì„±
    config = ModelConfig(
        use_gpu=True,
        use_tensorrt=True,
        batch_size=64,
        tensorrt_precision="fp16"
    )
    
    model = EnhancedLotteryPredictor(
        input_dim=100,
        hidden_dims=[256, 128, 64],
        output_dim=45,
        config=config
    )
    
    # âœ… 3. ëª¨ë¸ ìµœì í™”
    sample_input = torch.randn(1, 100)
    optimized_model = await model.optimize_for_inference([sample_input])
    
    # âœ… 4. ì˜ˆì¸¡ ì‹¤í–‰
    try:
        test_features = np.random.randn(100)
        result = await optimized_model.predict_lottery_numbers(test_features)
        
        print("âœ… ì˜ˆì¸¡ ì™„ë£Œ!")
        print(f"ì˜ˆì¸¡ ë²ˆí˜¸: {result['numbers']}")
        print(f"ì‹ ë¢°ë„: {result['confidence']:.3f}")
        
        # ì„±ëŠ¥ í†µê³„
        stats = optimized_model.get_performance_stats()
        print(f"í‰ê·  ì¶”ë¡  ì‹œê°„: {stats['avg_inference_time']:.4f}ì´ˆ")
        print(f"TensorRT ìµœì í™”: {'âœ…' if stats['is_optimized'] else 'âŒ'}")
        
    finally:
        # âœ… 5. ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        optimized_model.cleanup()


if __name__ == "__main__":
    asyncio.run(main()) 