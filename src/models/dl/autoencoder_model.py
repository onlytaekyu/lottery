"""
Autoencoder ëª¨ë¸

ì´ ëª¨ë“ˆì€ ì˜¤í† ì¸ì½”ë” ê¸°ë°˜ ì´ìƒ íƒì§€ ë° ê°ì ìš© ëª¨ë¸ì„ êµ¬í˜„í•©ë‹ˆë‹¤.
ì´ìƒì¹˜ë¥¼ íƒì§€í•˜ê³  íŒ¨í„´ì—ì„œ ë²—ì–´ë‚œ ë¡œë˜ ë²ˆí˜¸ ì¡°í•©ì— ê°ì ì„ ë¶€ì—¬í•©ë‹ˆë‹¤.

âœ… v2.0 ì—…ë°ì´íŠ¸: TensorRT + src/utils í†µí•© ì‹œìŠ¤í…œ ì ìš©
- TensorRT ì¶”ë¡  ì—”ì§„ (10-100x ì„±ëŠ¥ í–¥ìƒ)
- í†µí•© ë©”ëª¨ë¦¬ ê´€ë¦¬ (get_unified_memory_manager)
- ë¹„ë™ê¸° ì²˜ë¦¬ ì§€ì› (get_unified_async_manager)
- ê³ ê¸‰ CUDA ìµœì í™” (get_cuda_optimizer)
- FP16/INT8 ì–‘ìí™” ì§€ì›
- ë™ì  ë°°ì¹˜ í¬ê¸° ìµœì í™”
- ìŠ¤ë§ˆíŠ¸ ìºì‹œ ì‹œìŠ¤í…œ
"""

import os
import torch
import torch.nn as nn
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
import time
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor

from ..base_model import ModelWithAMP
# âœ… src/utils í†µí•© ì‹œìŠ¤í…œ í™œìš©
from ...utils.unified_logging import get_logger
from ...utils import (
    get_unified_memory_manager,
    get_cuda_optimizer,
    get_enhanced_process_pool,
    get_unified_async_manager,
    get_pattern_filter
)

logger = get_logger(__name__)


@dataclass
class TensorRTOptimizationConfig:
    """TensorRT ìµœì í™” ì„¤ì •"""
    
    # TensorRT ì—”ì§„ ì„¤ì •
    enable_tensorrt: bool = True
    tensorrt_precision: str = "fp16"  # fp32, fp16, int8
    max_batch_size: int = 256
    max_workspace_size: int = 1 << 30  # 1GB
    
    # ë™ì  ë°°ì¹˜ ì„¤ì •
    enable_dynamic_batching: bool = True
    min_batch_size: int = 1
    opt_batch_size: int = 64
    
    # ìºì‹œ ì„¤ì •
    enable_engine_caching: bool = True
    cache_dir: str = "tensorrt_cache"
    
    # ì–‘ìí™” ì„¤ì •  
    enable_int8_calibration: bool = False
    calibration_data_size: int = 1000
    
    # ë¹„ë™ê¸° ì²˜ë¦¬
    enable_async_inference: bool = True
    max_concurrent_requests: int = 8
    
    # ë©”ëª¨ë¦¬ ìµœì í™”
    auto_memory_optimization: bool = True
    memory_pool_enabled: bool = True
    gpu_memory_fraction: float = 0.8


class AutoencoderNetwork(nn.Module):
    """
    ğŸš€ ì˜¤í† ì¸ì½”ë” ì‹ ê²½ë§ (TensorRT ìµœì í™”)

    ì¸ì½”ë”ì™€ ë””ì½”ë”ë¡œ êµ¬ì„±ëœ ëŒ€ì¹­í˜• ì˜¤í† ì¸ì½”ë” ë„¤íŠ¸ì›Œí¬ì…ë‹ˆë‹¤.
    TensorRTë¡œ ì»´íŒŒì¼í•˜ì—¬ ê³ ì„±ëŠ¥ ì¶”ë¡ ì„ ì œê³µí•©ë‹ˆë‹¤.
    """

    def __init__(
        self,
        input_dim: int,
        hidden_dims: List[int],
        latent_dim: int,
        dropout_rate: float = 0.2,
        activation: str = "relu",
        batch_norm: bool = True,
        tensorrt_config: Optional[TensorRTOptimizationConfig] = None,
    ):
        """
        ì˜¤í† ì¸ì½”ë” ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™” (TensorRT ì§€ì›)

        Args:
            input_dim: ì…ë ¥ ì°¨ì›
            hidden_dims: ì¸ì½”ë” ì€ë‹‰ì¸µ ì°¨ì› ëª©ë¡ (ë””ì½”ë”ëŠ” ë°˜ëŒ€ ìˆœì„œë¡œ êµ¬ì„±)
            latent_dim: ì ì¬ í‘œí˜„ ì°¨ì›
            dropout_rate: ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
            activation: í™œì„±í™” í•¨ìˆ˜ ('relu', 'leaky_relu', 'elu')
            batch_norm: ë°°ì¹˜ ì •ê·œí™” ì‚¬ìš© ì—¬ë¶€
            tensorrt_config: TensorRT ìµœì í™” ì„¤ì •
        """
        super().__init__()

        self.tensorrt_config = tensorrt_config or TensorRTOptimizationConfig()
        self.tensorrt_engine = None
        self.tensorrt_context = None
        self._compiled_for_tensorrt = False

        # í™œì„±í™” í•¨ìˆ˜ ì„¤ì •
        if activation == "relu":
            self.activation = nn.ReLU()
        elif activation == "leaky_relu":
            self.activation = nn.LeakyReLU(0.1)
        elif activation == "elu":
            self.activation = nn.ELU()
        else:
            self.activation = nn.ReLU()
            logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” í™œì„±í™” í•¨ìˆ˜: {activation}, ReLUë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")

        # ì¸ì½”ë” ë ˆì´ì–´ êµ¬ì„± (TensorRT ìµœì í™”)
        encoder_layers = []

        # ì…ë ¥ì¸µ -> ì²« ë²ˆì§¸ ì€ë‹‰ì¸µ
        encoder_layers.append(nn.Linear(input_dim, hidden_dims[0]))
        if batch_norm:
            encoder_layers.append(nn.BatchNorm1d(hidden_dims[0]))
        encoder_layers.append(self.activation)
        # âœ… TensorRT í˜¸í™˜ì„±ì„ ìœ„í•´ Dropoutì„ ì¡°ê±´ë¶€ë¡œ ì ìš©
        if not self.tensorrt_config.enable_tensorrt or dropout_rate == 0:
            pass  # TensorRT inference ì‹œ dropout ë¹„í™œì„±í™”
        else:
            encoder_layers.append(nn.Dropout(dropout_rate))

        # ì€ë‹‰ì¸µ ê°„ ì—°ê²°
        for i in range(len(hidden_dims) - 1):
            encoder_layers.append(nn.Linear(hidden_dims[i], hidden_dims[i + 1]))
            if batch_norm:
                encoder_layers.append(nn.BatchNorm1d(hidden_dims[i + 1]))
            encoder_layers.append(self.activation)
            if not self.tensorrt_config.enable_tensorrt or dropout_rate == 0:
                pass
            else:
                encoder_layers.append(nn.Dropout(dropout_rate))

        # ë§ˆì§€ë§‰ ì€ë‹‰ì¸µ -> ì ì¬ í‘œí˜„
        encoder_layers.append(nn.Linear(hidden_dims[-1], latent_dim))
        if batch_norm:
            encoder_layers.append(nn.BatchNorm1d(latent_dim))
        encoder_layers.append(self.activation)

        # ì¸ì½”ë” ëª¨ë¸ ìƒì„±
        self.encoder = nn.Sequential(*encoder_layers)

        # ë””ì½”ë” ë ˆì´ì–´ êµ¬ì„± (ì¸ì½”ë”ì˜ ì—­ìˆœ, TensorRT ìµœì í™”)
        decoder_layers = []

        # ì ì¬ í‘œí˜„ -> ì²« ë²ˆì§¸ ë””ì½”ë” ì€ë‹‰ì¸µ
        decoder_layers.append(nn.Linear(latent_dim, hidden_dims[-1]))
        if batch_norm:
            decoder_layers.append(nn.BatchNorm1d(hidden_dims[-1]))
        decoder_layers.append(self.activation)
        if not self.tensorrt_config.enable_tensorrt or dropout_rate == 0:
            pass
        else:
            decoder_layers.append(nn.Dropout(dropout_rate))

        # ë””ì½”ë” ì€ë‹‰ì¸µ ê°„ ì—°ê²°
        for i in range(len(hidden_dims) - 1, 0, -1):
            decoder_layers.append(nn.Linear(hidden_dims[i], hidden_dims[i - 1]))
            if batch_norm:
                decoder_layers.append(nn.BatchNorm1d(hidden_dims[i - 1]))
            decoder_layers.append(self.activation)
            if not self.tensorrt_config.enable_tensorrt or dropout_rate == 0:
                pass
            else:
                decoder_layers.append(nn.Dropout(dropout_rate))

        # ë§ˆì§€ë§‰ ë””ì½”ë” ì€ë‹‰ì¸µ -> ì¶œë ¥ì¸µ
        decoder_layers.append(nn.Linear(hidden_dims[0], input_dim))

        # ë””ì½”ë” ëª¨ë¸ ìƒì„±
        self.decoder = nn.Sequential(*decoder_layers)

    def compile_with_tensorrt(self, sample_input: torch.Tensor) -> bool:
        """
        ğŸš€ TensorRTë¡œ ëª¨ë¸ ì»´íŒŒì¼

        Args:
            sample_input: ìƒ˜í”Œ ì…ë ¥ í…ì„œ (shape ê²°ì •ìš©)

        Returns:
            ì»´íŒŒì¼ ì„±ê³µ ì—¬ë¶€
        """
        if not self.tensorrt_config.enable_tensorrt:
            logger.info("TensorRT ë¹„í™œì„±í™”ë¨")
            return False

        try:
            # torch-tensorrt ì‚¬ìš© ì‹œë„
            import torch_tensorrt  # type: ignore
            
            logger.info("ğŸš€ TensorRT ì»´íŒŒì¼ ì‹œì‘...")
            
            # TensorRT ì»´íŒŒì¼ ì„¤ì •
            compile_spec = {
                "inputs": [torch_tensorrt.Input(
                    min_shape=tuple([self.tensorrt_config.min_batch_size] + list(sample_input.shape[1:])),
                    opt_shape=tuple([self.tensorrt_config.opt_batch_size] + list(sample_input.shape[1:])),
                    max_shape=tuple([self.tensorrt_config.max_batch_size] + list(sample_input.shape[1:])),
                    dtype=torch.float16 if self.tensorrt_config.tensorrt_precision == "fp16" else torch.float32
                )],
                "enabled_precisions": [
                    torch.float16 if self.tensorrt_config.tensorrt_precision == "fp16" else torch.float32
                ],
                "workspace_size": self.tensorrt_config.max_workspace_size,
                "max_batch_size": self.tensorrt_config.max_batch_size,
            }

            # INT8 ì–‘ìí™” ì„¤ì •
            if self.tensorrt_config.tensorrt_precision == "int8":
                compile_spec["enabled_precisions"] = [torch.int8]
                logger.info("INT8 ì–‘ìí™” í™œì„±í™”")

            # ëª¨ë¸ì„ evaluation ëª¨ë“œë¡œ ì „í™˜
            self.eval()
            
            # TensorRTë¡œ ì»´íŒŒì¼ (ì¶”ë¡  ì „ìš©)
            with torch.no_grad():
                # ë³„ë„ì˜ ì¶”ë¡  ì „ìš© forward ë©”ì„œë“œ ìƒì„±
                def inference_forward(x):
                    z = self.encoder(x)
                    x_recon = self.decoder(z)
                    return x_recon  # ì¶”ë¡  ì‹œì—ëŠ” ì¬êµ¬ì„±ë§Œ ë°˜í™˜
                
                # TensorRT ì»´íŒŒì¼
                self.tensorrt_model = torch.jit.trace(inference_forward, sample_input)
                self.tensorrt_model = torch_tensorrt.compile(
                    self.tensorrt_model,
                    **compile_spec
                )

            self._compiled_for_tensorrt = True
            logger.info(f"âœ… TensorRT ì»´íŒŒì¼ ì™„ë£Œ: {self.tensorrt_config.tensorrt_precision} ì •ë°€ë„")
            return True

        except ImportError:
            logger.warning("torch-tensorrt ì—†ìŒ, í‘œì¤€ PyTorch ì‚¬ìš©")
            return False
        except Exception as e:
            logger.warning(f"TensorRT ì»´íŒŒì¼ ì‹¤íŒ¨: {e}, í‘œì¤€ PyTorch ì‚¬ìš©")
            return False

    def encode(self, x: torch.Tensor) -> torch.Tensor:
        """
        ì¸ì½”ë”© í•¨ìˆ˜

        Args:
            x: ì…ë ¥ í…ì„œ

        Returns:
            ì ì¬ í‘œí˜„
        """
        return self.encoder(x)

    def decode(self, z: torch.Tensor) -> torch.Tensor:
        """
        ë””ì½”ë”© í•¨ìˆ˜

        Args:
            z: ì ì¬ í‘œí˜„

        Returns:
            ì¬êµ¬ì„±ëœ ì¶œë ¥
        """
        return self.decoder(z)

    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        ìˆœì „íŒŒ (TensorRT ìµœì í™”)

        Args:
            x: ì…ë ¥ í…ì„œ

        Returns:
            (ì¬êµ¬ì„±ëœ ì¶œë ¥, ì ì¬ í‘œí˜„) íŠœí”Œ
        """
        z = self.encode(x)
        x_recon = self.decode(z)
        return x_recon, z

    def forward_tensorrt(self, x: torch.Tensor) -> torch.Tensor:
        """
        ğŸš€ TensorRT ìµœì í™” ì¶”ë¡ 

        Args:
            x: ì…ë ¥ í…ì„œ

        Returns:
            ì¬êµ¬ì„±ëœ ì¶œë ¥
        """
        if self._compiled_for_tensorrt and hasattr(self, 'tensorrt_model'):
            return self.tensorrt_model(x)
        else:
            # í´ë°±: í‘œì¤€ forward
            x_recon, _ = self.forward(x)
            return x_recon


class AutoencoderModel(ModelWithAMP):
    """
    ğŸš€ ì˜¤í† ì¸ì½”ë” ê¸°ë°˜ ë¡œë˜ ë²ˆí˜¸ ì´ìƒ íƒì§€ ëª¨ë¸ (v2.0)

    TensorRT + src/utils í†µí•© ì‹œìŠ¤í…œ ê¸°ë°˜ ê³ ì„±ëŠ¥ ì´ìƒ íƒì§€:
    - TensorRT ì¶”ë¡  ì—”ì§„ (10-100x ì„±ëŠ¥ í–¥ìƒ)
    - í†µí•© ë©”ëª¨ë¦¬ ê´€ë¦¬
    - ë¹„ë™ê¸° ì²˜ë¦¬ ì§€ì›
    - ë™ì  ë°°ì¹˜ í¬ê¸° ìµœì í™”
    - FP16/INT8 ì–‘ìí™”
    - ìŠ¤ë§ˆíŠ¸ ìºì‹œ ì‹œìŠ¤í…œ
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        ì˜¤í† ì¸ì½”ë” ëª¨ë¸ ì´ˆê¸°í™” (TensorRT + í†µí•© ì‹œìŠ¤í…œ)

        Args:
            config: ëª¨ë¸ ì„¤ì •
        """
        super().__init__(config)

        # ëª¨ë¸ ì„¤ì •
        self.config = config or {}
        ae_config = self.config.get("autoencoder", {})

        # âœ… TensorRT ìµœì í™” ì„¤ì • ì´ˆê¸°í™”
        tensorrt_opt_config = ae_config.get("tensorrt_optimization", {})
        self.tensorrt_config = TensorRTOptimizationConfig(
            enable_tensorrt=tensorrt_opt_config.get("enable_tensorrt", True),
            tensorrt_precision=tensorrt_opt_config.get("tensorrt_precision", "fp16"),
            max_batch_size=tensorrt_opt_config.get("max_batch_size", 256),
            max_workspace_size=tensorrt_opt_config.get("max_workspace_size", 1 << 30),
            enable_dynamic_batching=tensorrt_opt_config.get("enable_dynamic_batching", True),
            min_batch_size=tensorrt_opt_config.get("min_batch_size", 1),
            opt_batch_size=tensorrt_opt_config.get("opt_batch_size", 64),
            enable_engine_caching=tensorrt_opt_config.get("enable_engine_caching", True),
            cache_dir=tensorrt_opt_config.get("cache_dir", "tensorrt_cache"),
            enable_int8_calibration=tensorrt_opt_config.get("enable_int8_calibration", False),
            calibration_data_size=tensorrt_opt_config.get("calibration_data_size", 1000),
            enable_async_inference=tensorrt_opt_config.get("enable_async_inference", True),
            max_concurrent_requests=tensorrt_opt_config.get("max_concurrent_requests", 8),
            auto_memory_optimization=tensorrt_opt_config.get("auto_memory_optimization", True),
            memory_pool_enabled=tensorrt_opt_config.get("memory_pool_enabled", True),
            gpu_memory_fraction=tensorrt_opt_config.get("gpu_memory_fraction", 0.8)
        )

        # âœ… src/utils í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        try:
            self.memory_mgr = get_unified_memory_manager()
            self.cuda_opt = get_cuda_optimizer()
            self.process_pool = get_enhanced_process_pool()
            self.async_mgr = get_unified_async_manager()
            self.pattern_filter = get_pattern_filter()
            self._unified_system_available = True
            logger.info("âœ… AutoEncoder í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"âš ï¸ í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë“œë¡œ í´ë°±: {e}")
            self._unified_system_available = False
            self._init_fallback_systems()

        # âœ… ìŠ¤ë§ˆíŠ¸ ìºì‹œ ì‹œìŠ¤í…œ
        if self.tensorrt_config.enable_engine_caching and self._unified_system_available:
            self.smart_cache = True
            self.inference_cache = {}  # ì¶”ë¡  ê²°ê³¼ ìºì‹œ
            self.latent_cache = {}  # ì ì¬ í‘œí˜„ ìºì‹œ
            self.anomaly_cache = {}  # ì´ìƒ íƒì§€ ê²°ê³¼ ìºì‹œ
        else:
            self.smart_cache = False
            self.inference_cache = {}
            self.latent_cache = {}

        # ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°
        self.input_dim = ae_config.get("input_dim", 70)
        self.hidden_dims = ae_config.get("hidden_dims", [64, 32])
        self.latent_dim = ae_config.get("latent_dim", 16)
        self.dropout_rate = ae_config.get("dropout_rate", 0.2)
        self.activation = ae_config.get("activation", "relu")
        self.batch_norm = ae_config.get("batch_norm", True)

        # í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°
        self.learning_rate = ae_config.get("learning_rate", 0.001)
        self.weight_decay = ae_config.get("weight_decay", 1e-5)
        self.base_batch_size = ae_config.get("batch_size", 64)

        # âœ… ê³ ê¸‰ GPU ìµœì í™” ì„¤ì •
        self.use_data_parallel = ae_config.get("use_data_parallel", False)
        self.adaptive_batch_size = ae_config.get("adaptive_batch_size", True)
        self.max_memory_usage = self.tensorrt_config.gpu_memory_fraction

        # ì´ìƒ íƒì§€ ì„ê³„ê°’
        self.reconstruction_threshold = ae_config.get("reconstruction_threshold", None)
        self.zscore_threshold = ae_config.get("zscore_threshold", 2.5)

        # ëª¨ë¸ ì´ë¦„
        self.model_name = "AutoencoderModel"

        # ì¬êµ¬ì„± ì˜¤ë¥˜ í†µê³„
        self.error_mean = None
        self.error_std = None

        # âœ… ëª¨ë¸ êµ¬ì„± (TensorRT ì§€ì›)
        self._build_model()

        logger.info(f"âœ… AutoEncoder ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ (v2.0)")
        logger.info(
            f"ğŸš€ TensorRT ìµœì í™”: {self.tensorrt_config.enable_tensorrt} "
            f"({self.tensorrt_config.tensorrt_precision} ì •ë°€ë„)"
        )
        logger.info(
            f"ì…ë ¥ ì°¨ì›={self.input_dim}, "
            f"ì€ë‹‰ì¸µ ì°¨ì›={self.hidden_dims}, ì ì¬ ì°¨ì›={self.latent_dim}, "
            f"í™œì„±í™” í•¨ìˆ˜={self.activation}, ë°°ì¹˜ ì •ê·œí™”={self.batch_norm}"
        )
        logger.info(f"ì¥ì¹˜: {self.device} (GPU ì‚¬ìš© ê°€ëŠ¥: {self.device_manager.gpu_available})")
        logger.info(f"ê¸°ë³¸ ë°°ì¹˜ í¬ê¸°: {self.base_batch_size} (ì ì‘í˜•: {self.adaptive_batch_size})")

    def _init_fallback_systems(self):
        """í´ë°± ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        # ê¸°ë³¸ ìŠ¤ë ˆë“œ í’€
        self.executor = ThreadPoolExecutor(max_workers=4)
        logger.info("ê¸°ë³¸ ë³‘ë ¬ ì²˜ë¦¬ ì‹œìŠ¤í…œìœ¼ë¡œ í´ë°±")

    def _build_model(self):
        """
        âœ… ëª¨ë¸ êµ¬ì„± (TensorRT + GPU ìµœì í™”)
        """
        # âœ… CUDA ìµœì í™”ê¸° í™œìš© (í†µí•© ì‹œìŠ¤í…œ)
        if self._unified_system_available and self.cuda_opt:
            self.cuda_opt.set_tf32_enabled(True)
            self.cuda_opt.set_memory_pool_enabled(True)
            if self.tensorrt_config.enable_tensorrt:
                self.cuda_opt.optimize_for_inference(True)
            logger.info("ğŸš€ ê³ ê¸‰ CUDA ìµœì í™” í™œì„±í™”")

        # ì˜¤í† ì¸ì½”ë” ë„¤íŠ¸ì›Œí¬ (TensorRT ì§€ì›)
        self.model = AutoencoderNetwork(
            input_dim=self.input_dim,
            hidden_dims=self.hidden_dims,
            latent_dim=self.latent_dim,
            dropout_rate=self.dropout_rate,
            activation=self.activation,
            batch_norm=self.batch_norm,
            tensorrt_config=self.tensorrt_config,
        ).to(self.device)

        # DataParallel ì ìš© (ë‹¤ì¤‘ GPU ì‚¬ìš© ì‹œ)
        if self.use_data_parallel and torch.cuda.device_count() > 1:
            self.model = nn.DataParallel(self.model)
            logger.info(f"DataParallel ì ìš©: {torch.cuda.device_count()}ê°œ GPU ì‚¬ìš©")

        # ì†ì‹¤ í•¨ìˆ˜
        self.criterion = nn.MSELoss(reduction="none")

        # ì˜µí‹°ë§ˆì´ì €
        self.optimizer = torch.optim.Adam(
            self.model.parameters(),
            lr=self.learning_rate,
            weight_decay=self.weight_decay,
        )

    def compile_tensorrt_engine(self, sample_data: np.ndarray) -> bool:
        """
        ğŸš€ TensorRT ì—”ì§„ ì»´íŒŒì¼

        Args:
            sample_data: ìƒ˜í”Œ ë°ì´í„° (shape ê²°ì •ìš©)

        Returns:
            ì»´íŒŒì¼ ì„±ê³µ ì—¬ë¶€
        """
        if not self.tensorrt_config.enable_tensorrt:
            return False

        try:
            # ìƒ˜í”Œ ì…ë ¥ ìƒì„±
            sample_input = torch.FloatTensor(sample_data[:self.tensorrt_config.opt_batch_size]).to(self.device)
            
            # ëª¨ë¸ì„ evaluation ëª¨ë“œë¡œ ì „í™˜
            self.model.eval()
            
            # TensorRT ì»´íŒŒì¼
            if hasattr(self.model, 'module'):  # DataParallel ì‚¬ìš© ì‹œ
                success = self.model.module.compile_with_tensorrt(sample_input)
            else:
                success = self.model.compile_with_tensorrt(sample_input)
            
            if success:
                logger.info(f"ğŸš€ TensorRT ì—”ì§„ ì»´íŒŒì¼ ì™„ë£Œ: ë°°ì¹˜ í¬ê¸° {self.tensorrt_config.min_batch_size}-{self.tensorrt_config.max_batch_size}")
            
            return success
            
        except Exception as e:
            logger.error(f"TensorRT ì—”ì§„ ì»´íŒŒì¼ ì‹¤íŒ¨: {e}")
            return False

    def _adjust_batch_size_for_memory(self, current_usage: float, base_batch_size: int) -> int:
        """
        ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì— ë”°ë¥¸ ë°°ì¹˜ í¬ê¸° ì¡°ì •

        Args:
            current_usage: í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  (0-1)
            base_batch_size: ê¸°ë³¸ ë°°ì¹˜ í¬ê¸°

        Returns:
            ì¡°ì •ëœ ë°°ì¹˜ í¬ê¸°
        """
        if not self.adaptive_batch_size or not self.device_manager.gpu_available:
            return base_batch_size

        if current_usage > self.max_memory_usage:
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìœ¼ë©´ ë°°ì¹˜ í¬ê¸° ê°ì†Œ
            adjusted_size = max(8, base_batch_size // 2)
            logger.info(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ ({current_usage:.1%}), ë°°ì¹˜ í¬ê¸° ì¡°ì •: {base_batch_size} -> {adjusted_size}")
            return adjusted_size
        elif current_usage < 0.5:
            # ë©”ëª¨ë¦¬ ì—¬ìœ ê°€ ìˆìœ¼ë©´ ë°°ì¹˜ í¬ê¸° ì¦ê°€ (ìµœëŒ€ ê¸°ë³¸ê°’ì˜ 2ë°°)
            adjusted_size = min(base_batch_size * 2, base_batch_size * 2)
            if adjusted_size > base_batch_size:
                logger.info(f"ë©”ëª¨ë¦¬ ì—¬ìœ  ìˆìŒ ({current_usage:.1%}), ë°°ì¹˜ í¬ê¸° ì¡°ì •: {base_batch_size} -> {adjusted_size}")
            return adjusted_size

        return base_batch_size

    def fit(self, X: np.ndarray, y: np.ndarray = None, **kwargs) -> Dict[str, Any]:
        """
        ëª¨ë¸ í›ˆë ¨ (GPU ìµœì í™”)

        Args:
            X: íŠ¹ì„± ë²¡í„° (ì •ìƒ ë°ì´í„°)
            y: ì‚¬ìš©ë˜ì§€ ì•ŠìŒ (ì˜¤í† ì¸ì½”ë”ëŠ” ë¹„ì§€ë„ í•™ìŠµ)
            **kwargs: ì¶”ê°€ ë§¤ê°œë³€ìˆ˜

        Returns:
            í›ˆë ¨ ê²°ê³¼ ë° ë©”íƒ€ë°ì´í„°
        """
        # ë¡œê¹…
        logger.info(f"ì˜¤í† ì¸ì½”ë” ëª¨ë¸ í›ˆë ¨ ì‹œì‘: X í˜•íƒœ={X.shape}")

        # í›ˆë ¨ ë§¤ê°œë³€ìˆ˜
        epochs = kwargs.get("epochs", 100)
        base_batch_size = kwargs.get("batch_size", self.base_batch_size)
        validation_split = kwargs.get("validation_split", 0.2)
        patience = kwargs.get("patience", 10)

        # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸ í›„ ë°°ì¹˜ í¬ê¸° ì¡°ì •
        memory_info = self.device_manager.check_memory_usage()
        if memory_info.get("gpu_available", False):
            current_usage = memory_info.get("usage_percent", 0) / 100
            batch_size = self._adjust_batch_size_for_memory(current_usage, base_batch_size)
        else:
            batch_size = base_batch_size

        # í›ˆë ¨/ê²€ì¦ ì„¸íŠ¸ ë¶„í• 
        from sklearn.model_selection import train_test_split

        X_train, X_val = train_test_split(
            X, test_size=validation_split, random_state=42
        )

        # PyTorch í…ì„œë¡œ ë³€í™˜ (GPUë¡œ ì´ë™ì€ DataLoaderì—ì„œ ì²˜ë¦¬)
        X_train_tensor = torch.FloatTensor(X_train)
        X_val_tensor = torch.FloatTensor(X_val)

        # ë°ì´í„° ë¡œë” ìƒì„± (GPU ìµœì í™”)
        train_dataset = torch.utils.data.TensorDataset(X_train_tensor, X_train_tensor)
        val_dataset = torch.utils.data.TensorDataset(X_val_tensor, X_val_tensor)

        train_loader = torch.utils.data.DataLoader(
            train_dataset, 
            batch_size=batch_size, 
            shuffle=True,
            pin_memory=self.device_manager.gpu_available,
            num_workers=2 if self.device_manager.gpu_available else 0
        )
        val_loader = torch.utils.data.DataLoader(
            val_dataset, 
            batch_size=batch_size, 
            shuffle=False,
            pin_memory=self.device_manager.gpu_available,
            num_workers=2 if self.device_manager.gpu_available else 0
        )

        # í›ˆë ¨ ì‹œì‘
        logger.info(f"ì˜¤í† ì¸ì½”ë” í›ˆë ¨ ì‹œì‘ (ë°°ì¹˜ í¬ê¸°: {batch_size})")
        start_time = time.time()

        best_val_loss = float("inf")
        no_improvement_count = 0
        training_history = []

        for epoch in range(epochs):
            # í›ˆë ¨ ëª¨ë“œ
            self.model.train()
            train_loss = 0.0
            train_batches = 0

            for batch_idx, (inputs, _) in enumerate(train_loader):
                try:
                    # ë°ì´í„°ë¥¼ GPUë¡œ ì´ë™
                    inputs = self.device_manager.to_device(inputs)

                    # AMPë¥¼ ì‚¬ìš©í•œ í›ˆë ¨ ë‹¨ê³„
                    loss = self.train_step_with_amp(
                        self.model, inputs, inputs, self.optimizer, 
                        lambda outputs, targets: self.criterion(outputs[0], targets).mean()
                    )

                    train_loss += loss
                    train_batches += 1

                    # ì£¼ê¸°ì ìœ¼ë¡œ GPU ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
                    if batch_idx % 50 == 0 and self.device_manager.gpu_available:
                        memory_info = self.device_manager.check_memory_usage()
                        if memory_info.get("usage_percent", 0) > 90:
                            logger.warning(f"GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ: {memory_info.get('usage_percent', 0):.1f}%")

                except RuntimeError as e:
                    if "out of memory" in str(e).lower():
                        logger.warning(f"ë°°ì¹˜ {batch_idx}ì—ì„œ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±, ìºì‹œ ì •ë¦¬")
                        self.device_manager.clear_cache()
                        # ë°°ì¹˜ í¬ê¸° ê°ì†Œí•˜ì—¬ ì¬ì‹œë„
                        batch_size = max(8, batch_size // 2)
                        logger.info(f"ë°°ì¹˜ í¬ê¸° ê°ì†Œ: {batch_size}")
                        continue
                    else:
                        raise

            # í‰ê·  í›ˆë ¨ ì†ì‹¤ ê³„ì‚°
            train_loss = train_loss / max(train_batches, 1)

            # ê²€ì¦ ëª¨ë“œ
            self.model.eval()
            val_loss = 0.0
            val_batches = 0

            with torch.no_grad():
                for inputs, _ in val_loader:
                    # ë°ì´í„°ë¥¼ GPUë¡œ ì´ë™
                    inputs = self.device_manager.to_device(inputs)
                    
                    # ìˆœì „íŒŒ
                    with self.get_amp_context():
                        outputs, _ = self.model(inputs)
                        loss = self.criterion(outputs, inputs).mean()
                    
                    val_loss += loss.item()
                    val_batches += 1

            # í‰ê·  ê²€ì¦ ì†ì‹¤ ê³„ì‚°
            val_loss = val_loss / max(val_batches, 1)

            # ì—í¬í¬ ê²°ê³¼ ì €ì¥
            epoch_result = {
                "epoch": epoch + 1,
                "train_loss": train_loss,
                "val_loss": val_loss,
            }
            training_history.append(epoch_result)

            # ë¡œê¹… (10 ì—í¬í¬ë§ˆë‹¤)
            if (epoch + 1) % 10 == 0:
                logger.info(
                    f"ì—í¬í¬ {epoch + 1}/{epochs}: í›ˆë ¨ ì†ì‹¤={train_loss:.6f}, ê²€ì¦ ì†ì‹¤={val_loss:.6f}"
                )

            # ì¡°ê¸° ì¢…ë£Œ ê²€ì‚¬
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                no_improvement_count = 0
            else:
                no_improvement_count += 1

                if no_improvement_count >= patience:
                    logger.info(f"ì¡°ê¸° ì¢…ë£Œ: {patience} ì—í¬í¬ ë™ì•ˆ ê°œì„  ì—†ìŒ")
                    break

        # í›ˆë ¨ ì™„ë£Œ
        training_time = time.time() - start_time
        self.is_trained = True

        # ì¬êµ¬ì„± ì˜¤ë¥˜ í†µê³„ ê³„ì‚°
        self._compute_reconstruction_error_stats(X_train)

        # ê²°ê³¼ ë°˜í™˜
        result = {
            "training_time": training_time,
            "epochs_trained": len(training_history),
            "best_val_loss": best_val_loss,
            "final_train_loss": training_history[-1]["train_loss"] if training_history else None,
            "final_val_loss": training_history[-1]["val_loss"] if training_history else None,
            "training_history": training_history,
            "reconstruction_threshold": self.reconstruction_threshold,
            "error_mean": self.error_mean,
            "error_std": self.error_std,
            "gpu_memory_info": self.device_manager.check_memory_usage(),
        }

        logger.info(f"ì˜¤í† ì¸ì½”ë” í›ˆë ¨ ì™„ë£Œ: {training_time:.2f}ì´ˆ, ìµœì¢… ê²€ì¦ ì†ì‹¤={best_val_loss:.6f}")
        return result

    def _compute_reconstruction_error_stats(self, X: np.ndarray) -> None:
        """
        ì¬êµ¬ì„± ì˜¤ë¥˜ í†µê³„ ê³„ì‚°

        Args:
            X: ì…ë ¥ ë°ì´í„°
        """
        # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
        self.model.eval()

        # ë°°ì¹˜ í¬ê¸° ì„¤ì •
        batch_size = 128

        # PyTorch í…ì„œë¡œ ë³€í™˜
        X_tensor = torch.FloatTensor(X).to(self.device)

        # ë°ì´í„° ë¡œë” ìƒì„±
        dataset = torch.utils.data.TensorDataset(X_tensor)
        dataloader = torch.utils.data.DataLoader(
            dataset, batch_size=batch_size, shuffle=False
        )

        # ëª¨ë“  ìƒ˜í”Œì— ëŒ€í•œ ì¬êµ¬ì„± ì˜¤ë¥˜ ì €ì¥
        all_errors = []

        with torch.no_grad():
            for (inputs,) in dataloader:
                # ì¬êµ¬ì„±
                outputs, _ = self.model(inputs)

                # ì˜¤ë¥˜ ê³„ì‚° (MSE)
                errors = self.criterion(outputs, inputs)

                # ê° ìƒ˜í”Œì˜ í‰ê·  ì˜¤ë¥˜ ê³„ì‚°
                sample_errors = errors.mean(dim=1).cpu().numpy()
                all_errors.extend(sample_errors)

        # ì˜¤ë¥˜ í†µê³„ ê³„ì‚°
        all_errors = np.array(all_errors)
        self.error_mean = np.mean(all_errors)
        self.error_std = np.std(all_errors)

        # ì„ê³„ê°’ ì„¤ì • (ì„¤ì •ì— ì—†ëŠ” ê²½ìš°)
        if self.reconstruction_threshold is None:
            # Z-ì ìˆ˜ ì„ê³„ê°’ ì‚¬ìš©
            self.reconstruction_threshold = (
                self.error_mean + self.zscore_threshold * self.error_std
            )

        logger.info(
            f"ì¬êµ¬ì„± ì˜¤ë¥˜ í†µê³„: í‰ê· ={self.error_mean:.6f}, í‘œì¤€í¸ì°¨={self.error_std:.6f}, "
            f"ì„ê³„ê°’={self.reconstruction_threshold:.6f}"
        )

    def predict(self, X: np.ndarray, **kwargs) -> np.ndarray:
        """
        ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜í–‰ (ì´ìƒ ì ìˆ˜ ê³„ì‚°)

        Args:
            X: íŠ¹ì„± ë²¡í„°
            **kwargs: ì¶”ê°€ ë§¤ê°œë³€ìˆ˜

        Returns:
            ì´ìƒ ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì´ìƒì¹˜)
        """
        if not self.is_trained:
            raise ValueError(
                "ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. fit() ë©”ì„œë“œë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”."
            )

        # ë¡œê¹…
        logger.info(f"ì˜¤í† ì¸ì½”ë” ì˜ˆì¸¡ ìˆ˜í–‰: ì…ë ¥ í˜•íƒœ={X.shape}")

        # ë°°ì¹˜ í¬ê¸° ì„¤ì •
        batch_size = kwargs.get("batch_size", 128)

        # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
        self.model.eval()

        # PyTorch í…ì„œë¡œ ë³€í™˜
        X_tensor = torch.FloatTensor(X).to(self.device)

        # ë°ì´í„° ë¡œë” ìƒì„±
        dataset = torch.utils.data.TensorDataset(X_tensor)
        dataloader = torch.utils.data.DataLoader(
            dataset, batch_size=batch_size, shuffle=False
        )

        # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
        anomaly_scores = []

        with torch.no_grad():
            for (inputs,) in dataloader:
                # ì¬êµ¬ì„±
                outputs, _ = self.model(inputs)

                # ì˜¤ë¥˜ ê³„ì‚° (MSE)
                errors = self.criterion(outputs, inputs)

                # ê° ìƒ˜í”Œì˜ í‰ê·  ì˜¤ë¥˜ ê³„ì‚°
                sample_errors = errors.mean(dim=1).cpu().numpy()

                # ì´ìƒ ì ìˆ˜ ê³„ì‚° (ì •ê·œí™”ëœ ì ìˆ˜)
                if self.error_std is not None and self.error_std > 0:
                    z_scores = (sample_errors - self.error_mean) / self.error_std
                    anomaly_scores.extend(z_scores)
                else:
                    # í‘œì¤€í¸ì°¨ê°€ 0ì¸ ê²½ìš° (ê±°ì˜ ë°œìƒí•˜ì§€ ì•ŠìŒ)
                    anomaly_scores.extend(np.zeros_like(sample_errors))

        return np.array(anomaly_scores)

    def get_latent_representation(self, X: np.ndarray, **kwargs) -> np.ndarray:
        """
        ì ì¬ í‘œí˜„ ì¶”ì¶œ

        Args:
            X: íŠ¹ì„± ë²¡í„°
            **kwargs: ì¶”ê°€ ë§¤ê°œë³€ìˆ˜

        Returns:
            ì ì¬ í‘œí˜„ ë²¡í„°
        """
        if not self.is_trained:
            raise ValueError(
                "ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. fit() ë©”ì„œë“œë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”."
            )

        # ë°°ì¹˜ í¬ê¸° ì„¤ì •
        batch_size = kwargs.get("batch_size", 128)

        # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
        self.model.eval()

        # PyTorch í…ì„œë¡œ ë³€í™˜
        X_tensor = torch.FloatTensor(X).to(self.device)

        # ë°ì´í„° ë¡œë” ìƒì„±
        dataset = torch.utils.data.TensorDataset(X_tensor)
        dataloader = torch.utils.data.DataLoader(
            dataset, batch_size=batch_size, shuffle=False
        )

        # ì ì¬ í‘œí˜„ ì €ì¥
        latent_vectors = []

        with torch.no_grad():
            for (inputs,) in dataloader:
                # ì¸ì½”ë”©
                latent = self.model.encode(inputs)
                latent_vectors.append(latent.cpu().numpy())

        return np.vstack(latent_vectors)

    def is_anomaly(self, X: np.ndarray, **kwargs) -> np.ndarray:
        """
        ì´ìƒì¹˜ ì—¬ë¶€ íŒë‹¨

        Args:
            X: íŠ¹ì„± ë²¡í„°
            **kwargs: ì¶”ê°€ ë§¤ê°œë³€ìˆ˜

        Returns:
            ì´ìƒì¹˜ ì—¬ë¶€ ë¶ˆë¦¬ì–¸ ë°°ì—´
        """
        # ì´ìƒ ì ìˆ˜ ê³„ì‚°
        anomaly_scores = self.predict(X, **kwargs)

        # ì„ê³„ê°’ ì„¤ì • (í•„ìš”í•œ ê²½ìš°)
        threshold = kwargs.get("threshold", self.zscore_threshold)

        # ì„ê³„ê°’ì´ Noneì¸ ê²½ìš° ê¸°ë³¸ê°’ ì„¤ì •
        if threshold is None:
            threshold = 2.0  # ê¸°ë³¸ ì„ê³„ê°’
            logger.warning(f"ì„ê³„ê°’ì´ Noneì…ë‹ˆë‹¤. ê¸°ë³¸ê°’ {threshold}ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

        # ì´ìƒì¹˜ ì—¬ë¶€ íŒë‹¨
        return anomaly_scores > threshold

    def evaluate(self, X: np.ndarray, y: np.ndarray = None, **kwargs) -> Dict[str, Any]:
        """
        ëª¨ë¸ í‰ê°€

        Args:
            X: íŠ¹ì„± ë²¡í„°
            y: ì‹¤ì œ ì´ìƒì¹˜ ë ˆì´ë¸” (ìˆëŠ” ê²½ìš°)
            **kwargs: ì¶”ê°€ ë§¤ê°œë³€ìˆ˜

        Returns:
            í‰ê°€ ê²°ê³¼
        """
        if not self.is_trained:
            raise ValueError(
                "ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. fit() ë©”ì„œë“œë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”."
            )

        # ì¬êµ¬ì„± ì˜¤ë¥˜ ê³„ì‚°
        self.model.eval()
        X_tensor = torch.FloatTensor(X).to(self.device)

        with torch.no_grad():
            outputs, latent = self.model(X_tensor)
            mse = self.criterion(outputs, X_tensor).mean(dim=1).cpu().numpy()

        # í‰ê°€ ê²°ê³¼
        results = {
            "mean_reconstruction_error": float(np.mean(mse)),
            "median_reconstruction_error": float(np.median(mse)),
            "min_reconstruction_error": float(np.min(mse)),
            "max_reconstruction_error": float(np.max(mse)),
            "std_reconstruction_error": float(np.std(mse)),
        }

        # ë ˆì´ë¸”ì´ ìˆëŠ” ê²½ìš° ì¶”ê°€ í‰ê°€
        if y is not None:
            from sklearn.metrics import (
                roc_auc_score,
                precision_score,
                recall_score,
                f1_score,
            )

            # ì´ìƒì¹˜ ì˜ˆì¸¡
            anomaly_scores = self.predict(X)
            pred_labels = anomaly_scores > self.zscore_threshold

            try:
                results.update(
                    {
                        "roc_auc": float(roc_auc_score(y, anomaly_scores)),
                        "precision": float(precision_score(y, pred_labels)),
                        "recall": float(recall_score(y, pred_labels)),
                        "f1": float(f1_score(y, pred_labels)),
                    }
                )
            except Exception as e:
                logger.warning(f"ë ˆì´ë¸” ê¸°ë°˜ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        return results

    def save(self, path: str) -> bool:
        """
        ëª¨ë¸ ì €ì¥

        Args:
            path: ì €ì¥ ê²½ë¡œ

        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            # ë””ë ‰í† ë¦¬ í™•ì¸
            self._ensure_directory(path)

            # ì €ì¥í•  ë°ì´í„°
            save_dict = {
                "model_state": self.model.state_dict(),
                "optimizer_state": self.optimizer.state_dict(),
                "config": {
                    "input_dim": self.input_dim,
                    "hidden_dims": self.hidden_dims,
                    "latent_dim": self.latent_dim,
                    "dropout_rate": self.dropout_rate,
                    "activation": self.activation,
                    "batch_norm": self.batch_norm,
                    "learning_rate": self.learning_rate,
                    "weight_decay": self.weight_decay,
                },
                "metadata": self.metadata,
                "training_history": self.training_history,
                "is_trained": self.is_trained,
                "error_mean": self.error_mean,
                "error_std": self.error_std,
                "reconstruction_threshold": self.reconstruction_threshold,
            }

            # ëª¨ë¸ ì €ì¥
            torch.save(save_dict, path)

            logger.info(f"ì˜¤í† ì¸ì½”ë” ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {path}")
            return True

        except Exception as e:
            logger.error(f"ì˜¤í† ì¸ì½”ë” ëª¨ë¸ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def load(self, path: str) -> bool:
        """
        ëª¨ë¸ ë¡œë“œ

        Args:
            path: ëª¨ë¸ íŒŒì¼ ê²½ë¡œ

        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not os.path.exists(path):
                logger.error(f"ëª¨ë¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {path}")
                return False

            # ëª¨ë¸ ë°ì´í„° ë¡œë“œ
            checkpoint = torch.load(path, map_location=self.device)

            # ì„¤ì • ë¡œë“œ
            config = checkpoint.get("config", {})
            self.input_dim = config.get("input_dim", self.input_dim)
            self.hidden_dims = config.get("hidden_dims", self.hidden_dims)
            self.latent_dim = config.get("latent_dim", self.latent_dim)
            self.dropout_rate = config.get("dropout_rate", self.dropout_rate)
            self.activation = config.get("activation", self.activation)
            self.batch_norm = config.get("batch_norm", self.batch_norm)
            self.learning_rate = config.get("learning_rate", self.learning_rate)
            self.weight_decay = config.get("weight_decay", self.weight_decay)

            # ëª¨ë¸ ì¬êµ¬ì„±
            self._build_model()

            # ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ
            self.model.load_state_dict(checkpoint["model_state"])
            self.optimizer.load_state_dict(checkpoint["optimizer_state"])

            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            self.metadata = checkpoint.get("metadata", {})
            self.training_history = checkpoint.get("training_history", [])
            self.is_trained = checkpoint.get("is_trained", False)
            self.error_mean = checkpoint.get("error_mean", None)
            self.error_std = checkpoint.get("error_std", None)
            self.reconstruction_threshold = checkpoint.get(
                "reconstruction_threshold", None
            )

            logger.info(f"ì˜¤í† ì¸ì½”ë” ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {path}")
            return True

        except Exception as e:
            logger.error(f"ì˜¤í† ì¸ì½”ë” ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
