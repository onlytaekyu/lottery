"""
ì ì‘í˜• ê°€ì¤‘ì¹˜ ì‹œìŠ¤í…œ (Adaptive Weight System)

ì„±ê³¼ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì ì‘ê³¼ ë©”íƒ€ ëŸ¬ë‹ì„ í†µí•œ ë™ì  ì „ëµ ê°€ì¤‘ì¹˜ ì¡°ì • ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- ì‹¤ì‹œê°„ ì„±ê³¼ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì¡°ì •
- ë©”íƒ€ ëŸ¬ë‹ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ìµœì í™” (GPU ê°€ì†)
- ë‹¤ì¤‘ ëª©í‘œ ìµœì í™” (ROI, ì ì¤‘ë¥ , ë¦¬ìŠ¤í¬)
- ì‹œê°„ ê°€ì¤‘ ì„±ê³¼ í‰ê°€
- ì ì‘í˜• í•™ìŠµë¥  ì¡°ì •
- ê°€ì¤‘ì¹˜ ì•ˆì •ì„± ë³´ì¥
- GPU ê¸°ë°˜ íŠ¹ì„± ì¶”ì¶œ ë° ëª¨ë¸ í•™ìŠµ

âœ… v2.0 ì—…ë°ì´íŠ¸: src/utils í†µí•© ì‹œìŠ¤í…œ ì ìš©
- í†µí•© ë©”ëª¨ë¦¬ ê´€ë¦¬ì 
- ë¹„ë™ê¸° ì²˜ë¦¬ ì§€ì›
- ìŠ¤ë§ˆíŠ¸ ìºì‹œ ì‹œìŠ¤í…œ
- ë³‘ë ¬ íŠ¹ì„± ì¶”ì¶œ
- GPU ìµœì í™” ê°•í™”
"""

import json
import numpy as np
from typing import Dict, List, Any, Optional
from datetime import datetime
from dataclasses import dataclass, asdict
from collections import deque, defaultdict
import math
from scipy.optimize import minimize
import warnings
import asyncio
from concurrent.futures import ThreadPoolExecutor

# âœ… src/utils í†µí•© ì‹œìŠ¤í…œ í™œìš©
from ..utils.unified_logging import get_logger
from ..utils.unified_config import get_config
from ..utils.cache_manager import UnifiedCachePathManager
from ..utils.unified_memory_manager import get_unified_memory_manager, MemoryConfig
from ..utils import (
    get_cuda_optimizer,
    get_enhanced_process_pool,
    get_unified_async_manager,
    get_pattern_filter
)
from ..monitoring.performance_tracker import PerformanceMetrics

logger = get_logger(__name__)


@dataclass
class WeightUpdateConfig:
    """ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ì„¤ì •"""

    learning_rate: float = 0.01  # í•™ìŠµë¥ 
    momentum: float = 0.9  # ëª¨ë©˜í…€
    decay_rate: float = 0.95  # ì‹œê°„ ê°ì‡ ìœ¨
    min_weight: float = 0.05  # ìµœì†Œ ê°€ì¤‘ì¹˜
    max_weight: float = 0.6  # ìµœëŒ€ ê°€ì¤‘ì¹˜
    stability_threshold: float = 0.1  # ì•ˆì •ì„± ì„ê³„ê°’
    adaptation_window: int = 20  # ì ì‘ ìœˆë„ìš° í¬ê¸°
    performance_memory: int = 100  # ì„±ëŠ¥ ë©”ëª¨ë¦¬ í¬ê¸°
    multi_objective_weights: Dict[str, float] = None  # ë‹¤ì¤‘ ëª©í‘œ ê°€ì¤‘ì¹˜
    use_gpu: bool = True  # GPU ì‚¬ìš© ì—¬ë¶€
    batch_size: int = 64  # ë°°ì¹˜ í¬ê¸°
    memory_limit: float = 0.8  # ë©”ëª¨ë¦¬ ì‚¬ìš© ì œí•œ
    # âœ… ìƒˆë¡œìš´ ìµœì í™” ì„¤ì •
    enable_async_processing: bool = True  # ë¹„ë™ê¸° ì²˜ë¦¬ í™œì„±í™”
    use_smart_caching: bool = True  # ìŠ¤ë§ˆíŠ¸ ìºì‹œ í™œì„±í™”
    parallel_workers: int = 4  # ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜
    cache_ttl: int = 3600  # ìºì‹œ TTL (ì´ˆ)


@dataclass
class StrategyWeight:
    """ì „ëµ ê°€ì¤‘ì¹˜ ì •ë³´"""

    strategy: str
    current_weight: float
    target_weight: float
    momentum: float
    performance_score: float
    stability_score: float
    confidence: float
    last_update: datetime
    update_count: int


@dataclass
class WeightOptimizationResult:
    """ê°€ì¤‘ì¹˜ ìµœì í™” ê²°ê³¼"""

    optimized_weights: Dict[str, float]
    expected_performance: float
    optimization_score: float
    convergence_iterations: int
    stability_metrics: Dict[str, float]
    recommendations: List[str]


class GPUAcceleratedMetaLearner:
    """
    ğŸš€ GPU ê°€ì† ë©”íƒ€ ëŸ¬ë‹ ì‹œìŠ¤í…œ (v2.0)
    
    src/utils í†µí•© ì‹œìŠ¤í…œ ê¸°ë°˜ ê³ ì„±ëŠ¥ ë©”íƒ€ ëŸ¬ë‹:
    - í†µí•© ë©”ëª¨ë¦¬ ê´€ë¦¬
    - ë¹„ë™ê¸° ì²˜ë¦¬ ì§€ì›
    - ìŠ¤ë§ˆíŠ¸ ìºì‹œ ì‹œìŠ¤í…œ
    - ë³‘ë ¬ íŠ¹ì„± ì¶”ì¶œ
    - GPU ìµœì í™” ê°•í™”
    """

    def __init__(self, config: WeightUpdateConfig):
        self.config = config
        self.logger = get_logger(__name__)
        
        # âœ… src/utils í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        try:
            self.memory_mgr = get_unified_memory_manager()
            self.cuda_opt = get_cuda_optimizer()
            self.process_pool = get_enhanced_process_pool()
            self.async_mgr = get_unified_async_manager()
            self.pattern_filter = get_pattern_filter()
            self._unified_system_available = True
            self.logger.info("âœ… í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë“œë¡œ í´ë°±: {e}")
            self._unified_system_available = False
            self._init_fallback_systems()

        # GPU ì‚¬ìš© ì„¤ì •
        self.use_gpu = config.use_gpu
        self.meta_model = None
        self.using_gpu = False
        self._initialize_model()

        # âœ… ìŠ¤ë§ˆíŠ¸ ìºì‹œ ì‹œìŠ¤í…œ 
        if config.use_smart_caching and self._unified_system_available:
            self.smart_cache = True
            self.cache_ttl = config.cache_ttl
            self.feature_cache = {}  # ìŠ¤ë§ˆíŠ¸ ìºì‹œë¡œ ê´€ë¦¬
        else:
            # ê¸°ì¡´ ìºì‹œ ì‹œìŠ¤í…œ í´ë°±
            self.smart_cache = False
            self.feature_cache = {}
            self.cache_max_size = 1000

        # âœ… ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
        self.parallel_workers = config.parallel_workers
        self.enable_async = config.enable_async_processing

        # í•™ìŠµ ë°ì´í„° ì €ì¥ì†Œ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
        self.training_data = deque(maxlen=config.performance_memory)
        self.model_trained = False

        # âœ… ë²¡í„°í™”ëœ íŠ¹ì„± ì¶”ì¶œê¸° (ì„±ëŠ¥ ìµœì í™”)
        self.feature_extractors = {
            "performance_trend": self._extract_performance_trend_vectorized,
            "volatility_features": self._extract_volatility_features_vectorized,
            "correlation_features": self._extract_correlation_features_vectorized,
            "temporal_features": self._extract_temporal_features_vectorized,
        }
        
        # íŠ¹ì„± í¬ê¸° ë¯¸ë¦¬ ê³„ì‚°
        self.feature_sizes = {
            "performance_trend": 10,
            "volatility_features": 10,
            "correlation_features": 10,
            "temporal_features": 10,
        }
        self.total_feature_size = sum(self.feature_sizes.values())

    def _init_fallback_systems(self):
        """í´ë°± ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        # ê¸°ë³¸ ë©”ëª¨ë¦¬ ê´€ë¦¬ì
        memory_config = MemoryConfig(
            max_memory_usage=self.config.memory_limit,
            use_memory_pooling=True,
            pool_size=32,
            auto_cleanup_interval=30.0,
        )
        self.memory_manager = get_unified_memory_manager()
        
        # ê¸°ë³¸ ìŠ¤ë ˆë“œ í’€
        self.executor = ThreadPoolExecutor(max_workers=self.config.parallel_workers)

    def _initialize_model(self):
        """
        ëª¨ë¸ ì´ˆê¸°í™” (í†µí•© ì‹œìŠ¤í…œ GPU ìµœì í™” ì ìš©)
        """
        try:
            if self.use_gpu:
                # âœ… CUDA ìµœì í™”ê¸° í™œìš©
                if self._unified_system_available and self.cuda_opt:
                    self.cuda_opt.set_tf32_enabled(True)
                    self.cuda_opt.set_memory_pool_enabled(True)
                    self.logger.info("ğŸš€ CUDA ìµœì í™” í™œì„±í™”")
                
                # cuML ì‹œë„
                try:
                    from cuml.ensemble import RandomForestRegressor as CuMLRandomForestRegressor  # type: ignore
                    self.meta_model = CuMLRandomForestRegressor(
                        n_estimators=100,
                        max_depth=10,
                        random_state=42,
                        n_streams=4,  # GPU ìŠ¤íŠ¸ë¦¼ ìˆ˜
                    )
                    self.logger.info("âœ… GPU ê°€ì† cuML RandomForest ì´ˆê¸°í™” ì™„ë£Œ")
                    self.using_gpu = True
                except ImportError:
                    self.logger.warning("cuML ì—†ìŒ, scikit-learnìœ¼ë¡œ fallback")
                    self._initialize_cpu_model()
                    self.using_gpu = False
            else:
                self._initialize_cpu_model()
                self.using_gpu = False
                
        except Exception as e:
            self.logger.error(f"ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self._initialize_cpu_model()
            self.using_gpu = False

    def _initialize_cpu_model(self):
        """CPU ëª¨ë¸ ì´ˆê¸°í™”"""
        from sklearn.ensemble import RandomForestRegressor
        self.meta_model = RandomForestRegressor(
            n_estimators=100, 
            max_depth=10, 
            random_state=42, 
            n_jobs=-1
        )

    async def extract_features_batch_async(
        self, 
        performance_histories: List[Dict[str, List[PerformanceMetrics]]]
    ) -> np.ndarray:
        """
        ğŸš€ ë¹„ë™ê¸° ë°°ì¹˜ íŠ¹ì„± ì¶”ì¶œ (í†µí•© ì‹œìŠ¤í…œ í™œìš©)
        """
        if not self.enable_async or not self._unified_system_available:
            return self.extract_features_batch(performance_histories)
        
        try:
            batch_size = len(performance_histories)
            features_batch = np.zeros((batch_size, self.total_feature_size))
            
            # âœ… ë¹„ë™ê¸° ì²˜ë¦¬ë¡œ ë³‘ë ¬ íŠ¹ì„± ì¶”ì¶œ
            async with self.async_mgr.semaphore(self.parallel_workers):
                tasks = []
                for batch_idx, performance_history in enumerate(performance_histories):
                    task = self._extract_features_async_worker(
                        performance_history, batch_idx
                    )
                    tasks.append(task)
                
                # ëª¨ë“  íƒœìŠ¤í¬ ì™„ë£Œ ëŒ€ê¸°
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # ê²°ê³¼ ìˆ˜ì§‘
                for batch_idx, result in enumerate(results):
                    if isinstance(result, Exception):
                        self.logger.warning(f"ë°°ì¹˜ {batch_idx} íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨: {result}")
                        features_batch[batch_idx] = np.zeros(self.total_feature_size)
                    else:
                        features_batch[batch_idx] = result

            return features_batch

        except Exception as e:
            self.logger.error(f"ë¹„ë™ê¸° ë°°ì¹˜ íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            # ë™ê¸° ë°©ì‹ í´ë°±
            return self.extract_features_batch(performance_histories)

    async def _extract_features_async_worker(
        self, 
        performance_history: Dict[str, List[PerformanceMetrics]], 
        batch_idx: int
    ) -> np.ndarray:
        """ë¹„ë™ê¸° íŠ¹ì„± ì¶”ì¶œ ì›Œì»¤"""
        try:
            # âœ… ìŠ¤ë§ˆíŠ¸ ìºì‹œ í™•ì¸
            if self.smart_cache:
                cache_key = await self._get_cache_key_async(performance_history)
                if cache_key in self.feature_cache:
                    # ìºì‹œ ë§Œë£Œ í™•ì¸
                    cached_data = self.feature_cache[cache_key]
                    if self._is_cache_valid(cached_data):
                        return cached_data['features']
            
            # íŠ¹ì„± ì¶”ì¶œ
            feature_vector = await self._extract_features_async(performance_history)
            
            # âœ… ìŠ¤ë§ˆíŠ¸ ìºì‹œ ì €ì¥
            if self.smart_cache:
                cache_data = {
                    'features': feature_vector,
                    'timestamp': datetime.now().timestamp(),
                    'ttl': self.cache_ttl
                }
                self.feature_cache[cache_key] = cache_data
                
                # ìºì‹œ í¬ê¸° ê´€ë¦¬
                await self._manage_cache_size()
            
            return feature_vector
            
        except Exception as e:
            self.logger.error(f"ë¹„ë™ê¸° íŠ¹ì„± ì¶”ì¶œ ì›Œì»¤ ì‹¤íŒ¨: {e}")
            return np.zeros(self.total_feature_size)

    def extract_features_batch(
        self, 
        performance_histories: List[Dict[str, List[PerformanceMetrics]]]
    ) -> np.ndarray:
        """
        ë°°ì¹˜ ë‹¨ìœ„ íŠ¹ì„± ì¶”ì¶œ (ê°œì„ ëœ ë™ê¸° ë²„ì „)
        """
        try:
            batch_size = len(performance_histories)
            features_batch = np.zeros((batch_size, self.total_feature_size))
            
            # âœ… í†µí•© ë©”ëª¨ë¦¬ ê´€ë¦¬ìë¡œ ë©”ëª¨ë¦¬ í• ë‹¹
            if self._unified_system_available and self.memory_mgr:
                with self.memory_mgr.temporary_allocation(
                    size=batch_size * self.total_feature_size * 8,  # float64 ê¸°ì¤€
                    prefer_device="cpu"
                ) as work_mem:
                    # ë°°ì¹˜ ë‹¨ìœ„ë¡œ íŠ¹ì„± ì¶”ì¶œ
                    for batch_idx, performance_history in enumerate(performance_histories):
                        features_batch[batch_idx] = self._extract_features_with_cache(
                            performance_history
                        )
            else:
                # í´ë°±: ê¸°ë³¸ ë°©ì‹
                for batch_idx, performance_history in enumerate(performance_histories):
                    features_batch[batch_idx] = self.extract_features(performance_history)

            return features_batch

        except Exception as e:
            self.logger.error(f"ë°°ì¹˜ íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return np.zeros((len(performance_histories), self.total_feature_size))

    def _extract_features_with_cache(
        self, 
        performance_history: Dict[str, List[PerformanceMetrics]]
    ) -> np.ndarray:
        """ìºì‹œë¥¼ í™œìš©í•œ íŠ¹ì„± ì¶”ì¶œ"""
        try:
            # ìºì‹œ í‚¤ ìƒì„±
            cache_key = self._get_cache_key(performance_history)
            
            # ìºì‹œ í™•ì¸
            if cache_key in self.feature_cache:
                if self.smart_cache:
                    cached_data = self.feature_cache[cache_key]
                    if self._is_cache_valid(cached_data):
                        return cached_data['features']
                else:
                    return self.feature_cache[cache_key]
            
            # íŠ¹ì„± ì¶”ì¶œ
            feature_vector = self.extract_features(performance_history)
            
            # ìºì‹œ ì €ì¥
            if self.smart_cache:
                cache_data = {
                    'features': feature_vector,
                    'timestamp': datetime.now().timestamp(),
                    'ttl': self.cache_ttl
                }
                self.feature_cache[cache_key] = cache_data
            else:
                # ê¸°ë³¸ ìºì‹œ (í¬ê¸° ì œí•œ)
                if len(self.feature_cache) < self.cache_max_size:
                    self.feature_cache[cache_key] = feature_vector
            
            return feature_vector
            
        except Exception as e:
            self.logger.error(f"ìºì‹œ íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return np.zeros(self.total_feature_size)

    def _is_cache_valid(self, cached_data: Dict[str, Any]) -> bool:
        """ìºì‹œ ìœ íš¨ì„± ê²€ì‚¬"""
        try:
            current_time = datetime.now().timestamp()
            cache_time = cached_data.get('timestamp', 0)
            ttl = cached_data.get('ttl', self.cache_ttl)
            
            return (current_time - cache_time) < ttl
            
        except Exception:
            return False

    async def _manage_cache_size(self):
        """ìºì‹œ í¬ê¸° ê´€ë¦¬ (ë¹„ë™ê¸°)"""
        try:
            if len(self.feature_cache) > self.cache_max_size * 1.5:
                # ì˜¤ë˜ëœ ìºì‹œ í•­ëª© ì œê±°
                datetime.now().timestamp()
                expired_keys = []
                
                for key, cached_data in self.feature_cache.items():
                    if not self._is_cache_valid(cached_data):
                        expired_keys.append(key)
                
                for key in expired_keys:
                    del self.feature_cache[key]
                
                self.logger.info(f"ë§Œë£Œëœ ìºì‹œ {len(expired_keys)}ê°œ ì œê±°")
                
        except Exception as e:
            self.logger.error(f"ìºì‹œ ê´€ë¦¬ ì‹¤íŒ¨: {e}")

    async def _get_cache_key_async(self, performance_history: Dict[str, List[PerformanceMetrics]]) -> str:
        """ë¹„ë™ê¸° ìºì‹œ í‚¤ ìƒì„±"""
        return self._get_cache_key(performance_history)

    async def _extract_features_async(
        self, 
        performance_history: Dict[str, List[PerformanceMetrics]]
    ) -> np.ndarray:
        """ë¹„ë™ê¸° íŠ¹ì„± ì¶”ì¶œ"""
        # CPU ì§‘ì•½ì  ì‘ì—…ì„ ìŠ¤ë ˆë“œ í’€ì—ì„œ ì‹¤í–‰
        if self._unified_system_available and self.process_pool:
            return await self.process_pool.run_in_thread(
                self.extract_features, performance_history
            )
        else:
            return self.extract_features(performance_history)

    def get_performance_stats(self) -> Dict[str, Any]:
        """
        ğŸš€ ì„±ëŠ¥ í†µê³„ ë°˜í™˜ (í†µí•© ì‹œìŠ¤í…œ ì •ë³´ í¬í•¨)
        """
        stats = {
            "model_type": "GPUAcceleratedMetaLearner",
            "using_gpu": self.using_gpu,
            "unified_system_available": self._unified_system_available,
            "smart_cache_enabled": self.smart_cache,
            "async_processing_enabled": self.enable_async,
            "parallel_workers": self.parallel_workers,
            "cache_size": len(self.feature_cache),
            "model_trained": self.model_trained,
            "feature_size": self.total_feature_size
        }
        
        # í†µí•© ì‹œìŠ¤í…œ í†µê³„
        if self._unified_system_available:
            if self.memory_mgr:
                try:
                    stats["memory_performance"] = self.memory_mgr.get_performance_metrics()
                except Exception as e:
                    self.logger.debug(f"ë©”ëª¨ë¦¬ ì„±ëŠ¥ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            
            if self.cuda_opt:
                try:
                    stats["cuda_optimization"] = self.cuda_opt.get_optimization_stats()
                except Exception as e:
                    self.logger.debug(f"CUDA ìµœì í™” í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        return stats

    def optimize_memory_usage(self):
        """
        ğŸš€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”
        """
        if self._unified_system_available and self.memory_mgr:
            self.memory_mgr.cleanup_unused_memory()
            self.logger.info("ğŸ§¹ í†µí•© ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ")
        
        # ìºì‹œ ì •ë¦¬
        if self.smart_cache:
            asyncio.create_task(self._manage_cache_size())
        else:
            # ê¸°ë³¸ ìºì‹œ ì •ë¦¬
            if len(self.feature_cache) > self.cache_max_size:
                self.feature_cache.clear()
                self.logger.info("ìºì‹œ ì •ë¦¬ ì™„ë£Œ")

    def get_optimal_batch_size(self, data_size: int) -> int:
        """
        ğŸš€ ìµœì  ë°°ì¹˜ í¬ê¸° ê³„ì‚°
        """
        if self._unified_system_available and self.memory_mgr:
            try:
                memory_stats = self.memory_mgr.get_memory_status()
                cpu_util = memory_stats.get("cpu_utilization", 0.5)
                
                # CPU ì‚¬ìš©ë¥ ì— ë”°ë¥¸ ë™ì  ì¡°ì •
                if cpu_util < 0.3:
                    base_batch = 128
                elif cpu_util < 0.7:
                    base_batch = 64
                else:
                    base_batch = 32
                
                return min(max(base_batch, 1), data_size)
                
            except Exception as e:
                self.logger.debug(f"ìµœì  ë°°ì¹˜ í¬ê¸° ê³„ì‚° ì‹¤íŒ¨: {e}")
        
        # í´ë°±: ê¸°ë³¸ ë°°ì¹˜ í¬ê¸°
        return min(self.config.batch_size, data_size)

    def _get_cache_key(self, performance_history: Dict[str, List[PerformanceMetrics]]) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        try:
            # ì„±ëŠ¥ ë°ì´í„°ì˜ í•´ì‹œ ìƒì„±
            key_data = []
            for strategy, metrics_list in performance_history.items():
                if metrics_list:
                    latest_metric = metrics_list[-1]
                    key_data.append(f"{strategy}_{latest_metric.roi:.4f}_{latest_metric.win_rate:.4f}")
            return "_".join(sorted(key_data))
        except:
            return f"cache_{hash(str(performance_history)) % 10000}"

    def extract_features(
        self, performance_history: Dict[str, List[PerformanceMetrics]]
    ) -> np.ndarray:
        """ì„±ëŠ¥ ì´ë ¥ì—ì„œ íŠ¹ì„± ì¶”ì¶œ (ë²¡í„°í™” ìµœì í™”)"""
        try:
            features = np.zeros(self.total_feature_size)
            feature_idx = 0

            for extractor_name, extractor_func in self.feature_extractors.items():
                try:
                    feature_size = self.feature_sizes[extractor_name]
                    feature_vector = extractor_func(performance_history)
                    
                    # í¬ê¸° ë§ì¶¤
                    if len(feature_vector) > feature_size:
                        feature_vector = feature_vector[:feature_size]
                    elif len(feature_vector) < feature_size:
                        feature_vector = np.pad(feature_vector, (0, feature_size - len(feature_vector)))
                    
                    features[feature_idx:feature_idx + feature_size] = feature_vector
                    feature_idx += feature_size
                    
                except Exception as e:
                    self.logger.warning(f"íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨ ({extractor_name}): {e}")
                    # ê¸°ë³¸ê°’ìœ¼ë¡œ ì±„ì›€
                    feature_idx += self.feature_sizes[extractor_name]

            return features

        except Exception as e:
            self.logger.error(f"íŠ¹ì„± ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return np.zeros(self.total_feature_size)

    def _extract_performance_trend_vectorized(
        self, performance_history: Dict[str, List[PerformanceMetrics]]
    ) -> np.ndarray:
        """ì„±ëŠ¥ íŠ¸ë Œë“œ íŠ¹ì„± ì¶”ì¶œ (ë²¡í„°í™”)"""
        try:
            features = np.zeros(10)

            # ëª¨ë“  ì „ëµì˜ ë°ì´í„°ë¥¼ ë°°ì—´ë¡œ ë³€í™˜
            roi_data = []
            win_rate_data = []
            
            for strategy, metrics_list in performance_history.items():
                if len(metrics_list) >= 2:
                    recent_metrics = metrics_list[-10:]
                    roi_values = np.array([m.roi for m in recent_metrics])
                    win_rate_values = np.array([m.win_rate for m in recent_metrics])
                    
                    roi_data.append(roi_values)
                    win_rate_data.append(win_rate_values)

            if roi_data and win_rate_data:
                # ë²¡í„°í™”ëœ íŠ¸ë Œë“œ ê³„ì‚°
                roi_array = np.array(roi_data)
                win_rate_array = np.array(win_rate_data)
                
                # ì „ì²´ íŠ¸ë Œë“œ (í‰ê· )
                if roi_array.size > 0:
                    roi_trend = np.mean([np.polyfit(range(len(vals)), vals, 1)[0] 
                                       for vals in roi_array if len(vals) > 1])
                    features[0] = roi_trend if not np.isnan(roi_trend) else 0.0
                
                if win_rate_array.size > 0:
                    win_rate_trend = np.mean([np.polyfit(range(len(vals)), vals, 1)[0] 
                                            for vals in win_rate_array if len(vals) > 1])
                    features[1] = win_rate_trend if not np.isnan(win_rate_trend) else 0.0
                
                # ë³€ë™ì„± ë©”íŠ¸ë¦­
                features[2] = np.mean([np.std(vals) for vals in roi_array if len(vals) > 1])
                features[3] = np.mean([np.std(vals) for vals in win_rate_array if len(vals) > 1])
                
                # ìµœê·¼ ì„±ê³¼
                features[4] = np.mean([vals[-1] for vals in roi_array if len(vals) > 0])
                features[5] = np.mean([vals[-1] for vals in win_rate_array if len(vals) > 0])

            return features

        except Exception as e:
            self.logger.error(f"ì„±ëŠ¥ íŠ¸ë Œë“œ íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return np.zeros(10)

    def _extract_volatility_features_vectorized(
        self, performance_history: Dict[str, List[PerformanceMetrics]]
    ) -> np.ndarray:
        """ë³€ë™ì„± íŠ¹ì„± ì¶”ì¶œ (ë²¡í„°í™”)"""
        try:
            features = np.zeros(10)
            
            all_roi_values = []
            all_sharpe_values = []
            all_volatility_values = []

            for strategy, metrics_list in performance_history.items():
                if len(metrics_list) >= 2:
                    recent_metrics = metrics_list[-10:]
                    roi_values = [m.roi for m in recent_metrics]
                    sharpe_values = [m.sharpe_ratio for m in recent_metrics]
                    
                    all_roi_values.extend(roi_values)
                    all_sharpe_values.extend(sharpe_values)
                    
                    # ê°œë³„ ì „ëµ ë³€ë™ì„±
                    if len(roi_values) > 1:
                        all_volatility_values.append(np.std(roi_values))

            if all_roi_values:
                # ì „ì²´ ë³€ë™ì„± ë©”íŠ¸ë¦­
                features[0] = np.std(all_roi_values)
                features[1] = np.mean(all_sharpe_values) if all_sharpe_values else 0.0
                features[2] = np.var(all_roi_values)
                features[3] = np.mean(all_volatility_values) if all_volatility_values else 0.0
                
                # ë¶„ìœ„ìˆ˜ ê¸°ë°˜ íŠ¹ì„±
                features[4] = np.percentile(all_roi_values, 25)
                features[5] = np.percentile(all_roi_values, 75)
                features[6] = np.percentile(all_roi_values, 90) - np.percentile(all_roi_values, 10)

            return features

        except Exception as e:
            self.logger.error(f"ë³€ë™ì„± íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return np.zeros(10)

    def _extract_correlation_features_vectorized(
        self, performance_history: Dict[str, List[PerformanceMetrics]]
    ) -> np.ndarray:
        """ìƒê´€ê´€ê³„ íŠ¹ì„± ì¶”ì¶œ (ë²¡í„°í™”)"""
        try:
            features = np.zeros(10)
            
            strategies = list(performance_history.keys())
            if len(strategies) < 2:
                return features

            # ROI ë°ì´í„° í–‰ë ¬ êµ¬ì„±
            roi_matrix = []
            win_rate_matrix = []
            
            min_length = min(len(metrics_list) for metrics_list in performance_history.values())
            if min_length < 2:
                return features

            for strategy in strategies:
                metrics_list = performance_history[strategy][-min_length:]
                roi_values = [m.roi for m in metrics_list]
                win_rate_values = [m.win_rate for m in metrics_list]
                
                roi_matrix.append(roi_values)
                win_rate_matrix.append(win_rate_values)

            # ìƒê´€ê´€ê³„ í–‰ë ¬ ê³„ì‚°
            roi_matrix = np.array(roi_matrix)
            win_rate_matrix = np.array(win_rate_matrix)
            
            if roi_matrix.shape[1] > 1:
                roi_corr = np.corrcoef(roi_matrix)
                win_rate_corr = np.corrcoef(win_rate_matrix)
                
                # ìƒê´€ê´€ê³„ íŠ¹ì„±
                features[0] = np.mean(roi_corr[np.triu_indices_from(roi_corr, k=1)])
                features[1] = np.std(roi_corr[np.triu_indices_from(roi_corr, k=1)])
                features[2] = np.mean(win_rate_corr[np.triu_indices_from(win_rate_corr, k=1)])
                features[3] = np.std(win_rate_corr[np.triu_indices_from(win_rate_corr, k=1)])
                
                # ë‹¤ì´ë²„ì‹œí‹° ë©”íŠ¸ë¦­
                features[4] = 1.0 - np.mean(np.abs(roi_corr[np.triu_indices_from(roi_corr, k=1)]))

            return features

        except Exception as e:
            self.logger.error(f"ìƒê´€ê´€ê³„ íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return np.zeros(10)

    def _extract_temporal_features_vectorized(
        self, performance_history: Dict[str, List[PerformanceMetrics]]
    ) -> np.ndarray:
        """ì‹œê°„ì  íŠ¹ì„± ì¶”ì¶œ (ë²¡í„°í™”)"""
        try:
            features = np.zeros(10)
            
            current_time = datetime.now()
            all_timestamps = []
            all_performance_values = []

            for strategy, metrics_list in performance_history.items():
                for metric in metrics_list:
                    all_timestamps.append(metric.timestamp)
                    all_performance_values.append(metric.roi)

            if all_timestamps:
                # ì‹œê°„ ê¸°ë°˜ íŠ¹ì„±
                time_diffs = [(current_time - ts).total_seconds() / 3600 for ts in all_timestamps]  # ì‹œê°„ ë‹¨ìœ„
                
                features[0] = np.mean(time_diffs)
                features[1] = np.std(time_diffs)
                features[2] = min(time_diffs) if time_diffs else 0
                features[3] = max(time_diffs) if time_diffs else 0
                
                # ì‹œê°„ ê°€ì¤‘ ì„±ê³¼
                weights = np.exp(-np.array(time_diffs) / 24)  # 24ì‹œê°„ ë°˜ê°ê¸°
                if len(all_performance_values) == len(weights):
                    features[4] = np.average(all_performance_values, weights=weights)

            return features

        except Exception as e:
            self.logger.error(f"ì‹œê°„ì  íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return np.zeros(10)

    def train_meta_model_gpu(self, training_data: List[Dict[str, Any]]) -> bool:
        """GPU ê°€ì† ë©”íƒ€ ëª¨ë¸ í›ˆë ¨"""
        try:
            if not training_data:
                self.logger.warning("í›ˆë ¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False

            self.logger.info(f"GPU ë©”íƒ€ ëª¨ë¸ í›ˆë ¨ ì‹œì‘: {len(training_data)}ê°œ ìƒ˜í”Œ")

            # íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ì¤€ë¹„
            features_list = []
            targets = []

            for data_point in training_data:
                performance_history = data_point["performance_history"]
                target_performance = data_point["target_performance"]
                
                # ë°°ì¹˜ ë‹¨ìœ„ íŠ¹ì„± ì¶”ì¶œ
                features = self.extract_features(performance_history)
                features_list.append(features)
                targets.append(target_performance)

            X = np.array(features_list)
            y = np.array(targets)

            if X.shape[0] == 0:
                self.logger.warning("íŠ¹ì„± í–‰ë ¬ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return False

            # GPU ë©”ëª¨ë¦¬ ì²´í¬
            if self.using_gpu:
                try:
                    import cupy as cp  # type: ignore
                    memory_info = cp.cuda.runtime.memGetInfo()
                    available_memory = memory_info[0] / (1024**3)  # GB
                    required_memory = X.nbytes * 3 / (1024**3)  # ì¶”ì •ì¹˜
                    
                    if required_memory > available_memory * 0.8:
                        self.logger.warning(f"GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜ˆìƒ, CPUë¡œ fallback")
                        self._initialize_cpu_model()
                        self.using_gpu = False
                except:
                    pass

            # âœ… ë©”íƒ€ ëª¨ë¸ ì¡´ì¬ í™•ì¸
            if self.meta_model is None:
                raise RuntimeError("ë©”íƒ€ ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

            # ëª¨ë¸ í›ˆë ¨
            self.meta_model.fit(X, y)
            self.model_trained = True

            # í›ˆë ¨ ì„±ê³¼ í‰ê°€
            if hasattr(self.meta_model, 'score'):
                score = self.meta_model.score(X, y)
                self.logger.info(f"ë©”íƒ€ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ: RÂ² score = {score:.4f}")
            else:
                self.logger.info("ë©”íƒ€ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ")

            return True

        except Exception as e:
            self.logger.error(f"GPU ë©”íƒ€ ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨: {e}")
            return False

    def predict_performance_batch(self, features_batch: np.ndarray) -> np.ndarray:
        """ë°°ì¹˜ ë‹¨ìœ„ ì„±ëŠ¥ ì˜ˆì¸¡ (GPU ê°€ì†)"""
        try:
            if not self.model_trained or self.meta_model is None:
                self.logger.warning("ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return np.zeros(len(features_batch))

            predictions = self.meta_model.predict(features_batch)
            
            # GPU í…ì„œë¥¼ numpyë¡œ ë³€í™˜ (cuML ì‚¬ìš© ì‹œ)
            if self.using_gpu and hasattr(predictions, 'get'):
                predictions = predictions.get()

            return predictions

        except Exception as e:
            self.logger.error(f"ë°°ì¹˜ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return np.zeros(len(features_batch))

    def get_feature_importance(self) -> Dict[str, float]:
        """íŠ¹ì„± ì¤‘ìš”ë„ ë°˜í™˜"""
        try:
            if not self.model_trained or self.meta_model is None:
                return {}

            importance = self.meta_model.feature_importances_
            
            # GPU í…ì„œë¥¼ numpyë¡œ ë³€í™˜ (cuML ì‚¬ìš© ì‹œ)
            if self.using_gpu and hasattr(importance, 'get'):
                importance = importance.get()

            # íŠ¹ì„± ì´ë¦„ê³¼ ë§¤í•‘
            feature_names = []
            for extractor_name, size in self.feature_sizes.items():
                for i in range(size):
                    feature_names.append(f"{extractor_name}_{i}")

            return dict(zip(feature_names, importance))

        except Exception as e:
            self.logger.error(f"íŠ¹ì„± ì¤‘ìš”ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {}

    def clear_cache(self):
        """ìºì‹œ ì •ë¦¬"""
        self.feature_cache.clear()
        
    def get_memory_usage(self) -> Dict[str, float]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜"""
        try:
            memory_info = {"cache_size": len(self.feature_cache)}
            
            if self.using_gpu:
                try:
                    import cupy as cp  # type: ignore
                    gpu_memory = cp.cuda.runtime.memGetInfo()
                    memory_info["gpu_total_gb"] = gpu_memory[1] / (1024**3)
                    memory_info["gpu_available_gb"] = gpu_memory[0] / (1024**3)
                    memory_info["gpu_used_gb"] = (gpu_memory[1] - gpu_memory[0]) / (1024**3)
                except:
                    pass
                    
            return memory_info
        except Exception as e:
            self.logger.error(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}


class AdaptiveWeightSystem:
    """ì ì‘í˜• ê°€ì¤‘ì¹˜ ì‹œìŠ¤í…œ v2.0"""

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        AdaptiveWeightSystem ì´ˆê¸°í™”

        Args:
            config: ì„¤ì • ê°ì²´
        """
        self.config = config or {}
        self.logger = get_logger(__name__)

        # í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self.opt_config = self._setup_optimization_config()
        self.memory_manager = get_unified_memory_manager()
        self.cuda_optimizer = get_cuda_optimizer()
        self.process_pool = get_enhanced_process_pool()

        # ë°ì´í„° ì €ì¥ ì„¤ì •
        paths = get_config()
        cache_path_manager = UnifiedCachePathManager(paths)
        self.cache_dir = cache_path_manager.get_path("adaptive_weights")
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # ì „ëµë³„ ê°€ì¤‘ì¹˜ ë° ì„±ëŠ¥ ë°ì´í„°
        self.strategy_weights: Dict[str, StrategyWeight] = {}
        self.performance_history: Dict[str, deque[PerformanceMetrics]] = defaultdict(
            lambda: deque(maxlen=self.opt_config.performance_memory)
        )

        # ìµœì í™” ì´ë ¥
        self.optimization_history = []

        # ì´ˆê¸° ê°€ì¤‘ì¹˜ ì„¤ì •
        self.initialize_weights()

        self.logger.info("ì ì‘í˜• ê°€ì¤‘ì¹˜ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")

    def _setup_optimization_config(self) -> WeightUpdateConfig:
        """ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ì„¤ì • ì´ˆê¸°í™”"""
        weight_config = self.config.get("weight_update", {})
        multi_obj_weights = weight_config.get(
            "multi_objective_weights",
            {"roi": 0.4, "win_rate": 0.3, "stability": 0.2, "risk": 0.1},
        )

        return WeightUpdateConfig(
            learning_rate=weight_config.get("learning_rate", 0.01),
            momentum=weight_config.get("momentum", 0.9),
            decay_rate=weight_config.get("decay_rate", 0.95),
            min_weight=weight_config.get("min_weight", 0.05),
            max_weight=weight_config.get("max_weight", 0.6),
            stability_threshold=weight_config.get("stability_threshold", 0.1),
            adaptation_window=weight_config.get("adaptation_window", 20),
            performance_memory=weight_config.get("performance_memory", 100),
            multi_objective_weights=multi_obj_weights,
            use_gpu=weight_config.get("use_gpu", True),
            batch_size=weight_config.get("batch_size", 64),
            memory_limit=weight_config.get("memory_limit", 0.8),
            enable_async_processing=weight_config.get("enable_async_processing", True),
            use_smart_caching=weight_config.get("use_smart_caching", True),
            parallel_workers=weight_config.get("parallel_workers", 4),
            cache_ttl=weight_config.get("cache_ttl", 3600),
        )

    def initialize_weights(self, strategies: Optional[List[str]] = None) -> None:
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        try:
            if strategies is None:
                strategies = [
                    "frequency_based",
                    "cluster_analysis",
                    "trend_following",
                    "ai_ensemble",
                    "contrarian",
                ]

            # ê· ë“± ê°€ì¤‘ì¹˜ë¡œ ì´ˆê¸°í™”
            equal_weight = 1.0 / len(strategies)

            for strategy in strategies:
                self.strategy_weights[strategy] = StrategyWeight(
                    strategy=strategy,
                    current_weight=equal_weight,
                    target_weight=equal_weight,
                    momentum=0.0,
                    performance_score=0.0,
                    stability_score=1.0,
                    confidence=0.5,
                    last_update=datetime.now(),
                    update_count=0,
                )

            self.logger.info(f"ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” ì™„ë£Œ: {len(strategies)}ê°œ ì „ëµ")

        except Exception as e:
            self.logger.error(f"ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {str(e)}")

    def update_weights(
        self, performance_metrics: Dict[str, PerformanceMetrics]
    ) -> Dict[str, float]:
        """ì„±ê³¼ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸"""
        try:
            self.logger.info("ì„±ê³¼ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ì‹œì‘")

            # ì„±ê³¼ ì ìˆ˜ ê³„ì‚°
            performance_scores = self._calculate_performance_scores(performance_metrics)

            # íƒ€ê²Ÿ ê°€ì¤‘ì¹˜ ê³„ì‚°
            target_weights = self._calculate_target_weights(performance_scores)

            # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ (ëª¨ë©˜í…€ ì ìš©)
            updated_weights = self._apply_momentum_update(target_weights)

            # ì•ˆì •ì„± ê²€ì‚¬
            stable_weights = self._ensure_weight_stability(updated_weights)

            # ê°€ì¤‘ì¹˜ ì •ê·œí™”
            normalized_weights = self._normalize_weights(stable_weights)

            # ê°€ì¤‘ì¹˜ ê°ì²´ ì—…ë°ì´íŠ¸
            self._update_weight_objects(normalized_weights, performance_scores)

            # ì´ë ¥ ì €ì¥
            self._save_weight_history(normalized_weights)

            self.logger.info("ì„±ê³¼ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            return normalized_weights

        except Exception as e:
            self.logger.error(f"ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return self.get_current_weights()

    def _calculate_performance_scores(
        self, performance_metrics: Dict[str, PerformanceMetrics]
    ) -> Dict[str, float]:
        """ì„±ê³¼ ì ìˆ˜ ê³„ì‚°"""
        try:
            scores = {}

            for strategy, metrics in performance_metrics.items():
                # ë‹¤ì¤‘ ëª©í‘œ ì ìˆ˜ ê³„ì‚°
                roi_score = self._normalize_score(metrics.roi, -1.0, 1.0)
                win_rate_score = metrics.win_rate
                stability_score = (
                    1.0 / (1.0 + metrics.volatility) if metrics.volatility > 0 else 1.0
                )
                risk_score = 1.0 - min(1.0, max(0.0, metrics.max_drawdown))

                # ê°€ì¤‘ í‰ê·  ê³„ì‚°
                weighted_score = (
                    self.opt_config.multi_objective_weights["roi"] * roi_score
                    + self.opt_config.multi_objective_weights["win_rate"]
                    * win_rate_score
                    + self.opt_config.multi_objective_weights["stability"]
                    * stability_score
                    + self.opt_config.multi_objective_weights["risk"] * risk_score
                )

                # ì‹œê°„ ê°€ì¤‘ ì ìš©
                time_weight = self._calculate_time_weight(metrics.timestamp)
                final_score = weighted_score * time_weight

                scores[strategy] = max(0.0, min(1.0, final_score))

            return scores

        except Exception as e:
            self.logger.error(f"ì„±ê³¼ ì ìˆ˜ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {}

    def _normalize_score(self, value: float, min_val: float, max_val: float) -> float:
        """ì ìˆ˜ ì •ê·œí™”"""
        try:
            if max_val == min_val:
                return 0.5

            normalized = (value - min_val) / (max_val - min_val)
            return max(0.0, min(1.0, normalized))

        except Exception as e:
            return 0.5

    def _calculate_time_weight(self, timestamp: datetime) -> float:
        """ì‹œê°„ ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        try:
            now = datetime.now()
            time_diff = (now - timestamp).total_seconds() / 3600  # ì‹œê°„ ë‹¨ìœ„

            # ì§€ìˆ˜ ê°ì‡ 
            time_weight = math.exp(-time_diff * 0.1)  # 10ì‹œê°„ ë°˜ê°ê¸°

            return max(0.1, min(1.0, time_weight))

        except Exception as e:
            return 1.0

    def _calculate_target_weights(
        self, performance_scores: Dict[str, float]
    ) -> Dict[str, float]:
        """íƒ€ê²Ÿ ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        try:
            if not performance_scores:
                return self.get_current_weights()

            # ì†Œí”„íŠ¸ë§¥ìŠ¤ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚°
            scores = np.array(list(performance_scores.values()))

            # ì˜¨ë„ ë§¤ê°œë³€ìˆ˜ ì ìš© (ê³¼ë„í•œ ì§‘ì¤‘ ë°©ì§€)
            temperature = 2.0
            exp_scores = np.exp(scores / temperature)
            softmax_weights = exp_scores / np.sum(exp_scores)

            # ìµœì†Œ/ìµœëŒ€ ê°€ì¤‘ì¹˜ ì œí•œ ì ìš©
            clipped_weights = np.clip(
                softmax_weights,
                self.opt_config.min_weight,
                self.opt_config.max_weight,
            )

            # ì •ê·œí™”
            normalized_weights = clipped_weights / np.sum(clipped_weights)

            # ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            target_weights = {}
            for i, strategy in enumerate(performance_scores.keys()):
                target_weights[strategy] = float(normalized_weights[i])

            return target_weights

        except Exception as e:
            self.logger.error(f"íƒ€ê²Ÿ ê°€ì¤‘ì¹˜ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return self.get_current_weights()

    def _apply_momentum_update(
        self, target_weights: Dict[str, float]
    ) -> Dict[str, float]:
        """ëª¨ë©˜í…€ ì ìš© ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸"""
        try:
            updated_weights = {}

            for strategy, target_weight in target_weights.items():
                if strategy in self.strategy_weights:
                    current_weight = self.strategy_weights[strategy].current_weight
                    current_momentum = self.strategy_weights[strategy].momentum

                    # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
                    gradient = target_weight - current_weight

                    # ëª¨ë©˜í…€ ì—…ë°ì´íŠ¸
                    new_momentum = (
                        self.opt_config.momentum * current_momentum
                        + self.opt_config.learning_rate * gradient
                    )

                    # ìƒˆë¡œìš´ ê°€ì¤‘ì¹˜ ê³„ì‚°
                    new_weight = current_weight + new_momentum

                    updated_weights[strategy] = new_weight

                    # ëª¨ë©˜í…€ ì €ì¥
                    self.strategy_weights[strategy].momentum = new_momentum
                else:
                    updated_weights[strategy] = target_weight

            return updated_weights

        except Exception as e:
            self.logger.error(f"ëª¨ë©˜í…€ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return target_weights

    def _ensure_weight_stability(self, weights: Dict[str, float]) -> Dict[str, float]:
        """ê°€ì¤‘ì¹˜ ì•ˆì •ì„± ë³´ì¥"""
        try:
            stable_weights = {}

            for strategy, weight in weights.items():
                if strategy in self.strategy_weights:
                    current_weight = self.strategy_weights[strategy].current_weight

                    # ë³€í™”ëŸ‰ ì œí•œ
                    max_change = self.opt_config.stability_threshold
                    weight_change = weight - current_weight

                    if abs(weight_change) > max_change:
                        # ë³€í™”ëŸ‰ ì œí•œ ì ìš©
                        limited_change = np.sign(weight_change) * max_change
                        stable_weight = current_weight + limited_change
                    else:
                        stable_weight = weight

                    stable_weights[strategy] = stable_weight
                else:
                    stable_weights[strategy] = weight

            return stable_weights

        except Exception as e:
            self.logger.error(f"ê°€ì¤‘ì¹˜ ì•ˆì •ì„± ë³´ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return weights

    def _normalize_weights(self, weights: Dict[str, float]) -> Dict[str, float]:
        """ê°€ì¤‘ì¹˜ ì •ê·œí™”"""
        try:
            total_weight = sum(weights.values())

            if total_weight == 0:
                # ê· ë“± ë¶„ë°°
                equal_weight = 1.0 / len(weights)
                return {strategy: equal_weight for strategy in weights.keys()}

            # ì •ê·œí™”
            normalized = {
                strategy: weight / total_weight for strategy, weight in weights.items()
            }

            # ìµœì†Œ/ìµœëŒ€ ê°€ì¤‘ì¹˜ ì œí•œ ì¬ì ìš©
            clipped = {}
            for strategy, weight in normalized.items():
                clipped[strategy] = max(
                    self.opt_config.min_weight,
                    min(self.opt_config.max_weight, weight),
                )

            # ì¬ì •ê·œí™”
            total_clipped = sum(clipped.values())
            if total_clipped > 0:
                final_weights = {
                    strategy: weight / total_clipped
                    for strategy, weight in clipped.items()
                }
            else:
                equal_weight = 1.0 / len(clipped)
                final_weights = {strategy: equal_weight for strategy in clipped.keys()}

            return final_weights

        except Exception as e:
            self.logger.error(f"ê°€ì¤‘ì¹˜ ì •ê·œí™” ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return weights

    def _update_weight_objects(
        self, weights: Dict[str, float], performance_scores: Dict[str, float]
    ) -> None:
        """ê°€ì¤‘ì¹˜ ê°ì²´ ì—…ë°ì´íŠ¸"""
        try:
            current_time = datetime.now()

            for strategy, weight in weights.items():
                if strategy in self.strategy_weights:
                    weight_obj = self.strategy_weights[strategy]

                    # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
                    weight_obj.target_weight = weight
                    weight_obj.current_weight = weight
                    weight_obj.performance_score = performance_scores.get(strategy, 0.0)
                    weight_obj.last_update = current_time
                    weight_obj.update_count += 1

                    # ì•ˆì •ì„± ì ìˆ˜ ê³„ì‚°
                    if len(self.weight_history[strategy]) > 1:
                        recent_weights = list(self.weight_history[strategy])[-5:]
                        weight_variance = (
                            np.var(recent_weights) if len(recent_weights) > 1 else 0
                        )
                        weight_obj.stability_score = 1.0 / (1.0 + weight_variance)

                    # ì‹ ë¢°ë„ ê³„ì‚°
                    weight_obj.confidence = min(1.0, weight_obj.update_count / 10.0)

        except Exception as e:
            self.logger.error(f"ê°€ì¤‘ì¹˜ ê°ì²´ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")

    def _save_weight_history(self, weights: Dict[str, float]) -> None:
        """ê°€ì¤‘ì¹˜ ì´ë ¥ ì €ì¥"""
        try:
            for strategy, weight in weights.items():
                self.weight_history[strategy].append(weight)

                # ì´ë ¥ í¬ê¸° ì œí•œ
                if (
                    len(self.weight_history[strategy])
                    > self.opt_config.performance_memory
                ):
                    self.weight_history[strategy].popleft()

        except Exception as e:
            self.logger.error(f"ê°€ì¤‘ì¹˜ ì´ë ¥ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")

    def optimize_weights_with_meta_learning(
        self,
        performance_history: Dict[str, List[PerformanceMetrics]],
        target_performance: float = 0.1,
    ) -> WeightOptimizationResult:
        """ë©”íƒ€ ëŸ¬ë‹ì„ í†µí•œ ê°€ì¤‘ì¹˜ ìµœì í™”"""
        try:
            self.logger.info("ë©”íƒ€ ëŸ¬ë‹ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ìµœì í™” ì‹œì‘")

            # íŠ¹ì„± ì¶”ì¶œ
            features = self.meta_learner.extract_features(performance_history)

            # ë©”íƒ€ ëª¨ë¸ ì˜ˆì¸¡
            predicted_performance = self.meta_learner.predict_performance_batch([features])[0]

            # ìµœì í™” ëª©ì í•¨ìˆ˜ ì •ì˜
            def objective(weights):
                """ê°€ì¤‘ì¹˜ ìµœì í™” ëª©ì í•¨ìˆ˜"""
                try:
                    # ì œì•½ ì¡°ê±´ í™•ì¸
                    if np.sum(weights) != 1.0 or np.any(
                        weights < self.opt_config.min_weight
                    ):
                        return 1e10

                    # ì˜ˆìƒ ì„±ëŠ¥ ê³„ì‚°
                    expected_perf = 0
                    for i, strategy in enumerate(self.strategy_weights.keys()):
                        if (
                            strategy in performance_history
                            and performance_history[strategy]
                        ):
                            recent_metrics = performance_history[strategy][-1]
                            strategy_score = self._calculate_strategy_score(
                                recent_metrics
                            )
                            expected_perf += weights[i] * strategy_score

                    # ëª©í‘œ ì„±ëŠ¥ê³¼ì˜ ì°¨ì´ + ë‹¤ì–‘ì„± íŒ¨ë„í‹°
                    performance_loss = abs(expected_perf - target_performance)
                    diversity_penalty = -np.sum(
                        weights * np.log(weights + 1e-10)
                    )  # ì—”íŠ¸ë¡œí”¼

                    return performance_loss - 0.1 * diversity_penalty

                except Exception as e:
                    return 1e10

            # ì œì•½ ì¡°ê±´ ì„¤ì •
            constraints = [{"type": "eq", "fun": lambda x: np.sum(x) - 1.0}]

            bounds = [
                (self.opt_config.min_weight, self.opt_config.max_weight)
                for _ in range(len(self.strategy_weights))
            ]

            # ì´ˆê¸°ê°’ ì„¤ì •
            initial_weights = np.array(
                [w.current_weight for w in self.strategy_weights.values()]
            )

            # ìµœì í™” ì‹¤í–‰
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                result = minimize(
                    objective,
                    initial_weights,
                    method="SLSQP",
                    bounds=bounds,
                    constraints=constraints,
                    options={"maxiter": 1000, "ftol": 1e-6},
                )

            # ê²°ê³¼ ì²˜ë¦¬
            if result.success:
                optimized_weights = result.x
                optimization_score = -result.fun
                convergence_iterations = result.nit
            else:
                self.logger.warning("ê°€ì¤‘ì¹˜ ìµœì í™” ì‹¤íŒ¨, í˜„ì¬ ê°€ì¤‘ì¹˜ ìœ ì§€")
                optimized_weights = initial_weights
                optimization_score = 0.0
                convergence_iterations = 0

            # ê°€ì¤‘ì¹˜ ë”•ì…”ë„ˆë¦¬ ìƒì„±
            weight_dict = {}
            for i, strategy in enumerate(self.strategy_weights.keys()):
                weight_dict[strategy] = float(optimized_weights[i])

            # ì•ˆì •ì„± ë©”íŠ¸ë¦­ ê³„ì‚°
            stability_metrics = self._calculate_stability_metrics(weight_dict)

            # ì¶”ì²œì‚¬í•­ ìƒì„±
            recommendations = self._generate_optimization_recommendations(
                weight_dict, predicted_performance, optimization_score
            )

            # ê²°ê³¼ ê°ì²´ ìƒì„±
            optimization_result = WeightOptimizationResult(
                optimized_weights=weight_dict,
                expected_performance=predicted_performance,
                optimization_score=optimization_score,
                convergence_iterations=convergence_iterations,
                stability_metrics=stability_metrics,
                recommendations=recommendations,
            )

            # ìµœì í™” ì´ë ¥ ì €ì¥
            self.optimization_history.append(
                {
                    "timestamp": datetime.now(),
                    "result": optimization_result,
                    "target_performance": target_performance,
                }
            )

            self.logger.info("ë©”íƒ€ ëŸ¬ë‹ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ìµœì í™” ì™„ë£Œ")
            return optimization_result

        except Exception as e:
            self.logger.error(f"ê°€ì¤‘ì¹˜ ìµœì í™” ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return WeightOptimizationResult({}, 0, 0, 0, {}, ["ìµœì í™” ì‹¤íŒ¨"])

    def _calculate_strategy_score(self, metrics: PerformanceMetrics) -> float:
        """ì „ëµ ì ìˆ˜ ê³„ì‚°"""
        try:
            # ë‹¤ì¤‘ ëª©í‘œ ì ìˆ˜
            roi_score = self._normalize_score(metrics.roi, -1.0, 1.0)
            win_rate_score = metrics.win_rate
            stability_score = (
                1.0 / (1.0 + metrics.volatility) if metrics.volatility > 0 else 1.0
            )
            risk_score = 1.0 - min(1.0, max(0.0, metrics.max_drawdown))

            # ê°€ì¤‘ í‰ê· 
            weighted_score = (
                self.opt_config.multi_objective_weights["roi"] * roi_score
                + self.opt_config.multi_objective_weights["win_rate"]
                * win_rate_score
                + self.opt_config.multi_objective_weights["stability"]
                * stability_score
                + self.opt_config.multi_objective_weights["risk"] * risk_score
            )

            return max(0.0, min(1.0, weighted_score))

        except Exception as e:
            return 0.0

    def _calculate_stability_metrics(
        self, weights: Dict[str, float]
    ) -> Dict[str, float]:
        """ì•ˆì •ì„± ë©”íŠ¸ë¦­ ê³„ì‚°"""
        try:
            metrics = {}

            # ê°€ì¤‘ì¹˜ ë¶„ì‚°
            weight_values = list(weights.values())
            metrics["weight_variance"] = float(np.var(weight_values))

            # ê°€ì¤‘ì¹˜ ì—”íŠ¸ë¡œí”¼ (ë‹¤ì–‘ì„±)
            entropy = -np.sum([w * np.log(w + 1e-10) for w in weight_values])
            metrics["weight_entropy"] = float(entropy)

            # ìµœëŒ€ ê°€ì¤‘ì¹˜ ë¹„ìœ¨
            max_weight = max(weight_values)
            metrics["max_weight_ratio"] = float(max_weight)

            # ìœ íš¨ ì „ëµ ìˆ˜
            effective_strategies = 1 / np.sum([w**2 for w in weight_values])
            metrics["effective_strategies"] = float(effective_strategies)

            return metrics

        except Exception as e:
            self.logger.error(f"ì•ˆì •ì„± ë©”íŠ¸ë¦­ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {}

    def _generate_optimization_recommendations(
        self,
        weights: Dict[str, float],
        predicted_performance: float,
        optimization_score: float,
    ) -> List[str]:
        """ìµœì í™” ì¶”ì²œì‚¬í•­ ìƒì„±"""
        recommendations = []

        try:
            # ì„±ëŠ¥ ê¸°ë°˜ ì¶”ì²œ
            if predicted_performance < 0:
                recommendations.append(
                    "ì˜ˆìƒ ì„±ëŠ¥ì´ ìŒìˆ˜ì…ë‹ˆë‹¤. ì „ëµ ì¬ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤."
                )
            elif predicted_performance < 0.05:
                recommendations.append(
                    "ì˜ˆìƒ ì„±ëŠ¥ì´ ë‚®ìŠµë‹ˆë‹¤. ë³´ë‹¤ ë³´ìˆ˜ì ì¸ ì ‘ê·¼ì„ ê¶Œì¥í•©ë‹ˆë‹¤."
                )

            # ê°€ì¤‘ì¹˜ ë¶„í¬ ê¸°ë°˜ ì¶”ì²œ
            max_weight = max(weights.values())
            if max_weight > 0.5:
                recommendations.append(
                    "íŠ¹ì • ì „ëµì— ê³¼ë„í•˜ê²Œ ì§‘ì¤‘ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë‹¤ì–‘ì„±ì„ ê³ ë ¤í•˜ì„¸ìš”."
                )

            min_weight = min(weights.values())
            if min_weight < 0.1:
                recommendations.append(
                    "ì¼ë¶€ ì „ëµì˜ ê°€ì¤‘ì¹˜ê°€ ë„ˆë¬´ ë‚®ìŠµë‹ˆë‹¤. í¬íŠ¸í´ë¦¬ì˜¤ ê· í˜•ì„ ê²€í† í•˜ì„¸ìš”."
                )

            # ìµœì í™” ì ìˆ˜ ê¸°ë°˜ ì¶”ì²œ
            if optimization_score < 0.1:
                recommendations.append(
                    "ìµœì í™” íš¨ê³¼ê°€ ì œí•œì ì…ë‹ˆë‹¤. ëª©í‘œ ì„±ëŠ¥ì„ ì¬ì¡°ì •í•˜ê±°ë‚˜ ì „ëµì„ ë³€ê²½í•˜ì„¸ìš”."
                )

            # ê¸ì •ì  ì¶”ì²œ
            if predicted_performance > 0.1 and optimization_score > 0.2:
                recommendations.append(
                    "ìµœì í™” ê²°ê³¼ê°€ ì–‘í˜¸í•©ë‹ˆë‹¤. í˜„ì¬ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•´ë³´ì„¸ìš”."
                )

        except Exception as e:
            recommendations.append("ì¶”ì²œì‚¬í•­ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

        return recommendations

    def get_current_weights(self) -> Dict[str, float]:
        """í˜„ì¬ ê°€ì¤‘ì¹˜ ë°˜í™˜"""
        try:
            return {
                strategy: weight_obj.current_weight
                for strategy, weight_obj in self.strategy_weights.items()
            }
        except Exception as e:
            self.logger.error(f"í˜„ì¬ ê°€ì¤‘ì¹˜ ë°˜í™˜ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {}

    def get_weight_summary(self) -> Dict[str, Any]:
        """ê°€ì¤‘ì¹˜ ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        try:
            summary = {
                "current_weights": self.get_current_weights(),
                "weight_objects": {},
                "stability_metrics": {},
                "optimization_history_count": len(self.optimization_history),
                "last_update": None,
            }

            # ê°€ì¤‘ì¹˜ ê°ì²´ ì •ë³´
            for strategy, weight_obj in self.strategy_weights.items():
                summary["weight_objects"][strategy] = {
                    "current_weight": weight_obj.current_weight,
                    "target_weight": weight_obj.target_weight,
                    "performance_score": weight_obj.performance_score,
                    "stability_score": weight_obj.stability_score,
                    "confidence": weight_obj.confidence,
                    "update_count": weight_obj.update_count,
                    "last_update": weight_obj.last_update.isoformat(),
                }

                if summary[
                    "last_update"
                ] is None or weight_obj.last_update > datetime.fromisoformat(
                    summary["last_update"]
                ):
                    summary["last_update"] = weight_obj.last_update.isoformat()

            # ì•ˆì •ì„± ë©”íŠ¸ë¦­
            current_weights = self.get_current_weights()
            if current_weights:
                summary["stability_metrics"] = self._calculate_stability_metrics(
                    current_weights
                )

            return summary

        except Exception as e:
            self.logger.error(f"ê°€ì¤‘ì¹˜ ìš”ì•½ ì •ë³´ ë°˜í™˜ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {}

    def save_weights(self) -> None:
        """ê°€ì¤‘ì¹˜ ì €ì¥"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

            # ê°€ì¤‘ì¹˜ ë°ì´í„° ì¤€ë¹„
            weight_data = {
                "timestamp": timestamp,
                "strategy_weights": {},
                "weight_history": {},
                "optimization_history": self.optimization_history,
                "config": asdict(self.opt_config),
            }

            # ì „ëµ ê°€ì¤‘ì¹˜
            for strategy, weight_obj in self.strategy_weights.items():
                weight_data["strategy_weights"][strategy] = {
                    "strategy": weight_obj.strategy,
                    "current_weight": weight_obj.current_weight,
                    "target_weight": weight_obj.target_weight,
                    "momentum": weight_obj.momentum,
                    "performance_score": weight_obj.performance_score,
                    "stability_score": weight_obj.stability_score,
                    "confidence": weight_obj.confidence,
                    "last_update": weight_obj.last_update.isoformat(),
                    "update_count": weight_obj.update_count,
                }

            # ê°€ì¤‘ì¹˜ ì´ë ¥
            for strategy, history in self.weight_history.items():
                weight_data["weight_history"][strategy] = list(history)

            # íŒŒì¼ ì €ì¥
            weights_file = self.cache_dir / f"adaptive_weights_{timestamp}.json"
            with open(weights_file, "w", encoding="utf-8") as f:
                json.dump(weight_data, f, ensure_ascii=False, indent=2, default=str)

            self.logger.info(f"ê°€ì¤‘ì¹˜ ì €ì¥ ì™„ë£Œ: {weights_file}")

        except Exception as e:
            self.logger.error(f"ê°€ì¤‘ì¹˜ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")

    def load_weights(self, date: Optional[str] = None) -> bool:
        """ê°€ì¤‘ì¹˜ ë¡œë“œ"""
        try:
            if date:
                pattern = f"adaptive_weights_{date}*.json"
            else:
                pattern = "adaptive_weights_*.json"

            weight_files = list(self.cache_dir.glob(pattern))
            if not weight_files:
                self.logger.warning(f"ê°€ì¤‘ì¹˜ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pattern}")
                return False

            # ìµœì‹  íŒŒì¼ ì„ íƒ
            latest_file = max(weight_files, key=lambda x: x.name)

            with open(latest_file, "r", encoding="utf-8") as f:
                weight_data = json.load(f)

            # ì „ëµ ê°€ì¤‘ì¹˜ ë³µì›
            for strategy, data in weight_data.get("strategy_weights", {}).items():
                self.strategy_weights[strategy] = StrategyWeight(
                    strategy=data["strategy"],
                    current_weight=data["current_weight"],
                    target_weight=data["target_weight"],
                    momentum=data["momentum"],
                    performance_score=data["performance_score"],
                    stability_score=data["stability_score"],
                    confidence=data["confidence"],
                    last_update=datetime.fromisoformat(data["last_update"]),
                    update_count=data["update_count"],
                )

            # ê°€ì¤‘ì¹˜ ì´ë ¥ ë³µì›
            for strategy, history in weight_data.get("weight_history", {}).items():
                self.weight_history[strategy] = deque(
                    history, maxlen=self.opt_config.performance_memory
                )

            # ìµœì í™” ì´ë ¥ ë³µì›
            self.optimization_history = weight_data.get("optimization_history", [])

            self.logger.info(f"ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ: {latest_file}")
            return True

        except Exception as e:
            self.logger.error(f"ê°€ì¤‘ì¹˜ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return False
