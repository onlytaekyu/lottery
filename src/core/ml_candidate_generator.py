"""
ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ë¡œë˜ ë²ˆí˜¸ í›„ë³´ ìƒì„±ê¸°

ì´ ëª¨ë“ˆì€ í†µê³„ ëª¨ë¸ê³¼ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ ê²°í•©í•˜ì—¬ ë¡œë˜ ë²ˆí˜¸ í›„ë³´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
StatisticalModelë¡œ í›„ë³´ë¥¼ ìƒì„±í•˜ê³ , LightGBMê³¼ XGBoost ëª¨ë¸ë¡œ ì ìˆ˜ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
"""

import os
import time
import logging
import numpy as np
from typing import Dict, List, Any, Optional, Union, Tuple, Set, cast
from pathlib import Path
import concurrent.futures
from concurrent.futures import ThreadPoolExecutor
import joblib
import gc
import random
import json
from collections import defaultdict
from sklearn.cluster import KMeans
from datetime import datetime

from ..utils.unified_logging import get_logger
from ..utils.unified_config import ConfigProxy, load_config
from ..utils.performance_utils import MemoryTracker
from ..utils.batch_controller import CPUBatchProcessor
from ..utils.unified_performance import get_profiler, Profiler
from ..utils.pattern_filter import PatternFilter, get_pattern_filter
from ..utils.normalizer import Normalizer
from ..models.statistical_model import StatisticalModel
from ..models.lightgbm_model import LightGBMModel
from ..models.xgboost_model import XGBoostModel
from ..analysis.enhanced_pattern_vectorizer import EnhancedPatternVectorizer
from ..shared.types import LotteryNumber
from ..shared.graph_utils import calculate_pair_frequency
from ..utils.unified_report import save_performance_report

# ë¡œê±° ì„¤ì •
logger = get_logger(__name__)


class MLCandidateGenerator:
    """ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ë¡œë˜ ë²ˆí˜¸ í›„ë³´ ìƒì„±ê¸°"""

    def __init__(self, config: Optional[Union[Dict[str, Any], ConfigProxy]] = None):
        """
        ì´ˆê¸°í™”

        Args:
            config: ì„¤ì • ê°ì²´
        """
        # ì„¤ì • ë¡œë“œ
        if config is None:
            config = load_config()
        elif isinstance(config, dict):
            config = ConfigProxy(config)

        self.config = config
        self.logger = get_logger(__name__)
        self.profiler = get_profiler()  # ì „ì—­ í”„ë¡œíŒŒì¼ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš©
        self.memory_tracker = MemoryTracker()

        # ConfigProxyë¥¼ Dictë¡œ ë³€í™˜
        config_dict = None
        if hasattr(config, "to_dict"):
            config_dict = config.to_dict()
        else:
            config_dict = config if isinstance(config, dict) else {}

        # íŒ¨í„´ í•„í„° ì´ˆê¸°í™”
        self.pattern_filter = get_pattern_filter(config_dict)

        # í†µê³„ ëª¨ë¸ ì´ˆê¸°í™”
        self.statistical_model = StatisticalModel(config_dict)

        # ë²¡í„°ë¼ì´ì € ì´ˆê¸°í™”
        self.pattern_vectorizer = EnhancedPatternVectorizer(config_dict)

        # ML ëª¨ë¸ ì´ˆê¸°í™”
        self.lightgbm_model = None
        self.xgboost_model = None

        # ë°°ì¹˜ ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        cpu_count = os.cpu_count() or 4
        self.batch_processor = CPUBatchProcessor(
            n_jobs=max(1, cpu_count - 1),  # í•˜ë‚˜ì˜ ì½”ì–´ëŠ” ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì— ë‚¨ê²¨ë‘ 
            batch_size=self.config.safe_get("batch_size", 100),
            max_batch_size=self.config.safe_get("max_batch_size", 500),
            backend=self.config.safe_get("parallel_backend", "threading"),
        )

        # ëª¨ë¸ ê°€ì¤‘ì¹˜ ì„¤ì •
        self.lgbm_weight = self.config.safe_get(
            "recommendation.model_weights.lgbm", 0.5
        )
        self.xgb_weight = self.config.safe_get("recommendation.model_weights.xgb", 0.5)

        # ìƒì„± ì„¤ì • (ìƒˆë¡œ ì¶”ê°€)
        self.generation_config = self.config.safe_get("generation", {})
        self.structured_ratio = self.generation_config.get("structured_ratio", 0.3)
        self.roi_ratio = self.generation_config.get("roi_ratio", 0.3)
        self.model_ratio = self.generation_config.get("model_ratio", 0.4)

        # ì •ê·œí™” ìœ í‹¸ë¦¬í‹° ì´ˆê¸°í™”
        self.normalizer = Normalizer(self.config)

        # ğŸš€ ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        from ..utils.memory_manager import MemoryManager, MemoryConfig
        from ..utils.cuda_optimizers import get_cuda_optimizer, CudaConfig
        from ..utils.process_pool_manager import get_process_pool_manager
        from ..utils.hybrid_optimizer import get_hybrid_optimizer

        # ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™”
        try:
            memory_config = MemoryConfig(
                max_memory_usage=0.85,  # ML í›„ë³´ ìƒì„±ì€ ë” ë§ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©
                use_memory_pooling=True,
                pool_size=256,  # ëŒ€ìš©ëŸ‰ í›„ë³´ ìƒì„±ì„ ìœ„í•œ í° í’€
            )
            self.memory_manager = MemoryManager(memory_config)
            self.logger.info("âœ… ML í›„ë³´ ìƒì„±ê¸° ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.memory_manager = None

        # CUDA ìµœì í™” ì´ˆê¸°í™” (ì‹±ê¸€í†¤ ì‚¬ìš©)
        try:
            from ..utils.cuda_singleton_manager import (
                get_singleton_cuda_optimizer,
                CudaSingletonConfig,
            )

            cuda_config = CudaSingletonConfig(
                use_amp=True,
                batch_size=256,  # ML í›„ë³´ ìƒì„±ì€ í° ë°°ì¹˜ ì‚¬ìš©
                use_cudnn=True,
            )
            self.cuda_optimizer = get_singleton_cuda_optimizer(
                config=cuda_config, requester_name="ml_candidate_generator"
            )

            if self.cuda_optimizer:
                self.logger.debug("CUDA ìµœì í™”ê¸° ì—°ê²° ì™„ë£Œ (ml_candidate_generator)")
            else:
                self.logger.debug("CUDA ì‚¬ìš© ë¶ˆê°€ëŠ¥ (ml_candidate_generator)")
        except Exception as e:
            self.logger.debug(f"CUDA ìµœì í™”ê¸° ì—°ê²° ì‹¤íŒ¨ (ml_candidate_generator): {e}")
            self.cuda_optimizer = None

        # í”„ë¡œì„¸ìŠ¤ í’€ ê´€ë¦¬ì ì´ˆê¸°í™”
        try:
            pool_config = {
                "max_workers": min(
                    8, os.cpu_count() or 1
                ),  # ML í›„ë³´ ìƒì„±ì€ ìµœëŒ€ ì›Œì»¤ ì‚¬ìš©
                "chunk_size": 200,
                "timeout": 900,  # ë” ê¸´ íƒ€ì„ì•„ì›ƒ
            }
            self.process_pool_manager = get_process_pool_manager(pool_config)
            self.logger.info("âœ… ML í›„ë³´ ìƒì„±ê¸° í”„ë¡œì„¸ìŠ¤ í’€ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"í”„ë¡œì„¸ìŠ¤ í’€ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.process_pool_manager = None

        # í•˜ì´ë¸Œë¦¬ë“œ ìµœì í™” ì´ˆê¸°í™”
        try:
            hybrid_config = {
                "auto_optimization": True,
                "memory_threshold": 0.8,
                "cpu_threshold": 70.0,
                "gpu_threshold": 0.8,
            }
            self.hybrid_optimizer = get_hybrid_optimizer(hybrid_config)
            self.logger.info("âœ… ML í›„ë³´ ìƒì„±ê¸° í•˜ì´ë¸Œë¦¬ë“œ ìµœì í™” ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            self.logger.warning(f"í•˜ì´ë¸Œë¦¬ë“œ ìµœì í™” ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.hybrid_optimizer = None

        # ë²¡í„° ìºì‹œ ì´ˆê¸°í™”
        try:
            from ..utils.state_vector_cache import get_cache

            self.vector_cache = get_cache(self.config)
        except ImportError:
            self.logger.warning(
                "ë²¡í„° ìºì‹œ ì´ˆê¸°í™” ì‹¤íŒ¨: state_vector_cache ëª¨ë“ˆì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
            self.vector_cache = None

        self.logger.info("ğŸ‰ ML í›„ë³´ ìƒì„±ê¸° ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")

    def _safe_float(self, value: Any) -> float:
        """
        ì–´ë–¤ ê°’ì´ë“  ì•ˆì „í•˜ê²Œ floatë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

        Args:
            value: ë³€í™˜í•  ê°’

        Returns:
            ë³€í™˜ëœ float ê°’ (ë³€í™˜ ì‹¤íŒ¨ ì‹œ 0.5 ë°˜í™˜)
        """
        try:
            # numpy ë°°ì—´ ë˜ëŠ” ìŠ¤ì¹¼ë¼ íŠ¹ë³„ ì²˜ë¦¬
            if hasattr(value, "item") and callable(getattr(value, "item")):
                return float(value.item())
            # ì¼ë°˜ ê°’ì€ ì§ì ‘ float ë³€í™˜
            return float(value)
        except (TypeError, ValueError, AttributeError):
            # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
            return 0.5

    def load_ml_models(self) -> Tuple[bool, bool]:
        """
        ML ëª¨ë¸ ë¡œë“œ

        Returns:
            LightGBM ë¡œë“œ ì„±ê³µ ì—¬ë¶€, XGBoost ë¡œë“œ ì„±ê³µ ì—¬ë¶€
        """
        lgbm_success = False
        xgb_success = False

        try:
            # ConfigProxyë¥¼ Dictë¡œ ë³€í™˜
            config_dict = None
            if hasattr(self.config, "to_dict"):
                config_dict = self.config.to_dict()
            else:
                config_dict = self.config if isinstance(self.config, dict) else {}

            # LightGBM ëª¨ë¸ ë¡œë“œ
            with self.profiler.profile("load_lightgbm_model"):
                self.lightgbm_model = LightGBMModel(config_dict)
                model_path = self.config.safe_get("paths.model_save_dir", "savedModels")
                lgbm_path = Path(model_path) / "lightgbm_model.pkl"

                if lgbm_path.exists():
                    lgbm_success = self.lightgbm_model.load(str(lgbm_path))
                    if lgbm_success:
                        self.logger.info(f"LightGBM ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {lgbm_path}")
                    else:
                        self.logger.warning(f"LightGBM ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {lgbm_path}")
                else:
                    self.logger.warning(
                        f"LightGBM ëª¨ë¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {lgbm_path}"
                    )

            # XGBoost ëª¨ë¸ ë¡œë“œ
            with self.profiler.profile("load_xgboost_model"):
                self.xgboost_model = XGBoostModel(config_dict)
                xgb_path = Path(model_path) / "xgboost_model.pkl"

                if xgb_path.exists():
                    xgb_success = self.xgboost_model.load(str(xgb_path))
                    if xgb_success:
                        self.logger.info(f"XGBoost ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {xgb_path}")
                    else:
                        self.logger.warning(f"XGBoost ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {xgb_path}")
                else:
                    self.logger.warning(
                        f"XGBoost ëª¨ë¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {xgb_path}"
                    )

            return lgbm_success, xgb_success

        except Exception as e:
            self.logger.error(f"ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return False, False

    def generate_candidates(
        self, historical_data: List[LotteryNumber]
    ) -> List[Dict[str, Any]]:
        """
        í›„ë³´ ë²ˆí˜¸ ì¡°í•© ìƒì„±

        Args:
            historical_data: ê³¼ê±° ë‹¹ì²¨ ë²ˆí˜¸ ë°ì´í„°

        Returns:
            ìŠ¤ì½”ì–´ë§ëœ í›„ë³´ ë²ˆí˜¸ ì¡°í•© ëª©ë¡
        """
        with self.profiler.profile("generate_candidates"):
            self.memory_tracker.start()

            # ì„¤ì •ì—ì„œ ì›í•˜ëŠ” í›„ë³´ ê°œìˆ˜ ê°€ì ¸ì˜¤ê¸°
            target_count = self.config.safe_get(
                "statistical_model.candidate_count", 200
            )

            # ìƒˆë¡œìš´ ìµœì í™” í›„ë³´ ìƒì„± ì „ëµ ì‚¬ìš©
            if self.generation_config.get("enable_structured_generation", False):
                n_structured = int(target_count * self.structured_ratio)
            else:
                n_structured = 0

            if self.generation_config.get("enable_roi_guided_generation", False):
                n_roi = int(target_count * self.roi_ratio)
            else:
                n_roi = 0

            if self.generation_config.get("enable_model_guided_generation", False):
                n_model = int(target_count * self.model_ratio)
            else:
                n_model = 0

            n_statistical = target_count - (n_structured + n_roi + n_model)

            raw_candidates = []
            candidate_sources = {}

            # 1. êµ¬ì¡°í™”ëœ í›„ë³´ ìƒì„± (ìŒ ë¹ˆë„ + ì¤‘ì‹¬ì„± + í´ëŸ¬ìŠ¤í„° ê¸°ë°˜)
            if n_structured > 0:
                with self.profiler.profile("structured_generation"):
                    self.logger.info(
                        f"êµ¬ì¡°í™”ëœ í›„ë³´ ìƒì„± ì¤‘... (ëª©í‘œ: {n_structured}ê°œ)"
                    )
                    structured_candidates = self._generate_structured_candidates(
                        historical_data, n_structured
                    )
                    raw_candidates.extend(structured_candidates)
                    for cand in structured_candidates:
                        candidate_sources[tuple(cand)] = "structured"
                    self.logger.info(
                        f"êµ¬ì¡°í™”ëœ í›„ë³´ ìƒì„± ì™„ë£Œ: {len(structured_candidates)}ê°œ"
                    )

            # 2. ROI ê¸°ë°˜ í›„ë³´ ìƒì„±
            if n_roi > 0:
                with self.profiler.profile("roi_generation"):
                    self.logger.info(f"ROI ê¸°ë°˜ í›„ë³´ ìƒì„± ì¤‘... (ëª©í‘œ: {n_roi}ê°œ)")
                    roi_candidates = self._generate_roi_guided_candidates(
                        historical_data, n_roi
                    )
                    raw_candidates.extend(roi_candidates)
                    for cand in roi_candidates:
                        candidate_sources[tuple(cand)] = "roi"
                    self.logger.info(
                        f"ROI ê¸°ë°˜ í›„ë³´ ìƒì„± ì™„ë£Œ: {len(roi_candidates)}ê°œ"
                    )

            # 3. ëª¨ë¸ ê¸°ë°˜ í›„ë³´ ìƒì„±
            if n_model > 0:
                with self.profiler.profile("model_generation"):
                    self.logger.info(f"ëª¨ë¸ ê¸°ë°˜ í›„ë³´ ìƒì„± ì¤‘... (ëª©í‘œ: {n_model}ê°œ)")
                    # ì—­ìƒ˜í”Œë§ ê¸°ë°˜ í›„ë³´ ìƒì„± ì‚¬ìš© ì—¬ë¶€ í™•ì¸
                    if self.generation_config.get("enable_inverse_sampling", False):
                        model_candidates = (
                            self._generate_model_guided_candidates_advanced(
                                historical_data, n_model
                            )
                        )
                        generation_method = "ì—­ìƒ˜í”Œë§"
                    else:
                        model_candidates = self._generate_model_guided_candidates(
                            historical_data, n_model
                        )
                        generation_method = "ê¸°ë³¸"

                    raw_candidates.extend(model_candidates)
                    for cand in model_candidates:
                        candidate_sources[tuple(cand)] = f"model_{generation_method}"
                    self.logger.info(
                        f"ëª¨ë¸ ê¸°ë°˜ í›„ë³´ ìƒì„± ì™„ë£Œ({generation_method}): {len(model_candidates)}ê°œ"
                    )

            # 4. í†µê³„ ê¸°ë°˜ í›„ë³´ ìƒì„± (ê¸°ì¡´ ë°©ì‹)
            if n_statistical > 0:
                with self.profiler.profile("statistical_model_generation"):
                    self.logger.info(
                        f"í†µê³„ ê¸°ë°˜ í›„ë³´ ìƒì„± ì¤‘... (ëª©í‘œ: {n_statistical}ê°œ)"
                    )
                    statistical_candidates = self._generate_statistical_candidates(
                        historical_data, n_statistical
                    )
                    raw_candidates.extend(statistical_candidates)
                    for cand in statistical_candidates:
                        candidate_sources[tuple(cand)] = "statistical"
                    self.logger.info(
                        f"í†µê³„ ê¸°ë°˜ í›„ë³´ ìƒì„± ì™„ë£Œ: {len(statistical_candidates)}ê°œ"
                    )

            # ì¤‘ë³µ ì œê±°
            unique_candidates = []
            unique_set = set()
            for cand in raw_candidates:
                cand_tuple = tuple(cand)
                if cand_tuple not in unique_set:
                    unique_set.add(cand_tuple)
                    unique_candidates.append(cand)

            self.logger.info(f"ì¤‘ë³µ ì œê±° í›„ ì´ í›„ë³´ ìˆ˜: {len(unique_candidates)}")

            # ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()

            # 5. í•„í„°ë§ ë° ìœ„í—˜ë„ ì ìˆ˜ ê³„ì‚°
            with self.profiler.profile("filtering_risk_scoring"):
                self.logger.info("í›„ë³´ í•„í„°ë§ ë° ìœ„í—˜ë„ ì ìˆ˜ ê³„ì‚° ì¤‘...")
                filtered_candidates = self._filter_and_score_candidates(
                    unique_candidates, historical_data
                )

                # ì†ŒìŠ¤ ì •ë³´ ì¶”ê°€
                for candidate in filtered_candidates:
                    cand_tuple = tuple(candidate["numbers"])
                    if cand_tuple in candidate_sources:
                        candidate["source"] = candidate_sources[cand_tuple]
                    else:
                        candidate["source"] = "unknown"

                self.logger.info(f"í•„í„°ë§ í›„ í›„ë³´ ìˆ˜: {len(filtered_candidates)}")

            # ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()

            # 6. ML ëª¨ë¸ ë¡œë“œ
            lgbm_loaded, xgb_loaded = self.load_ml_models()

            # 7. ML ëª¨ë¸ë¡œ ì ìˆ˜ ì˜ˆì¸¡
            with self.profiler.profile("ml_scoring"):
                self.logger.info("ML ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í›„ë³´ ì ìˆ˜ ê³„ì‚° ì¤‘...")
                scored_candidates = self._score_candidates_with_ml(
                    filtered_candidates, historical_data, lgbm_loaded, xgb_loaded
                )
                self.logger.info(f"ìµœì¢… í›„ë³´ ìˆ˜: {len(scored_candidates)}")

            # 8. ìƒì„±ëœ í›„ë³´ë¥¼ íŒŒì¼ë¡œ ì €ì¥
            self._save_candidates_to_file(scored_candidates)

            # 9. ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±
            self._save_performance_report(scored_candidates, historical_data)

            # ë©”ëª¨ë¦¬ ì •ë¦¬
            self.memory_tracker.stop()
            gc.collect()

            return scored_candidates

    def _generate_structured_candidates(
        self, historical_data: List[LotteryNumber], n_candidates: int
    ) -> List[List[int]]:
        """
        êµ¬ì¡°í™”ëœ í›„ë³´ ìƒì„± (ìŒ ë¹ˆë„ + ì¤‘ì‹¬ì„± + í´ëŸ¬ìŠ¤í„° ê¸°ë°˜)

        Args:
            historical_data: ê³¼ê±° ë‹¹ì²¨ ë²ˆí˜¸ ë°ì´í„°
            n_candidates: ìƒì„±í•  í›„ë³´ ìˆ˜

        Returns:
            í›„ë³´ ë²ˆí˜¸ ì¡°í•© ëª©ë¡
        """
        candidates = []

        try:
            # ìŒ ë¹ˆë„ ë°ì´í„° ë¡œë“œ
            pair_frequency = {}
            pair_freq_path = Path("data/cache/pair_frequency.npy")

            if not pair_freq_path.exists():
                # íŒŒì¼ì´ ì—†ìœ¼ë©´ ìŒ ë¹ˆë„ ë¶„ì„ ì‹¤í–‰
                self.logger.info("ìŒ ë¹ˆë„ ë°ì´í„° íŒŒì¼ì´ ì—†ì–´ ê³„ì‚°í•©ë‹ˆë‹¤.")
                pair_frequency = calculate_pair_frequency(
                    historical_data, logger=self.logger
                )

                # ê³„ì‚°ëœ ê°’ ì €ì¥
                cache_dir = Path("data/cache")
                cache_dir.mkdir(exist_ok=True, parents=True)

                # Dictionaryë¥¼ NumPy ë°°ì—´ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
                # 1. ìŒ ì¸ë±ìŠ¤ ë° ê°’ ì¶”ì¶œ
                pairs = list(pair_frequency.keys())
                values = list(pair_frequency.values())

                # 2. NumPy ë°°ì—´ë¡œ ë³€í™˜
                pair_data = np.array(
                    [(p[0], p[1], v) for p, v in zip(pairs, values)],
                    dtype=[("num1", "i4"), ("num2", "i4"), ("freq", "f4")],
                )

                # 3. ì €ì¥
                np.save(pair_freq_path, pair_data)
                self.logger.info(f"ìŒ ë¹ˆë„ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {pair_freq_path}")
            else:
                try:
                    # íŒŒì¼ì—ì„œ ë¡œë“œ
                    pair_data = np.load(pair_freq_path, allow_pickle=True)

                    # NumPy ë°°ì—´ì„ Dictionaryë¡œ ë³€í™˜
                    for item in pair_data:
                        pair = (int(item["num1"]), int(item["num2"]))
                        pair_frequency[pair] = float(item["freq"])

                    self.logger.info(
                        f"ìŒ ë¹ˆë„ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(pair_frequency)} í•­ëª©"
                    )
                except Exception as e:
                    self.logger.warning(f"ìŒ ë¹ˆë„ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
                    pair_frequency = calculate_pair_frequency(
                        historical_data, logger=self.logger
                    )

            # 2. ì„¸ê·¸ë¨¼íŠ¸ ì¤‘ì‹¬ì„± ë°ì´í„° ë¡œë“œ
            segment_centrality_vector = None
            centrality_path = Path("data/cache/segment_centrality_vector.npy")

            if centrality_path.exists():
                try:
                    segment_centrality_vector = np.load(centrality_path)
                    self.logger.info(
                        f"ì„¸ê·¸ë¨¼íŠ¸ ì¤‘ì‹¬ì„± ë²¡í„° ë¡œë“œ ì™„ë£Œ: {segment_centrality_vector.shape}"
                    )
                except Exception as e:
                    self.logger.warning(f"ì„¸ê·¸ë¨¼íŠ¸ ì¤‘ì‹¬ì„± ë²¡í„° ë¡œë“œ ì‹¤íŒ¨: {e}")

            # 3. í´ëŸ¬ìŠ¤í„° ì„ë² ë”© ë°ì´í„° ë¡œë“œ
            cluster_embeddings = None
            embedding_path = Path("data/cache/cluster_embeddings.npy")

            if embedding_path.exists():
                try:
                    cluster_embeddings = np.load(embedding_path)
                    self.logger.info(
                        f"í´ëŸ¬ìŠ¤í„° ì„ë² ë”© ë¡œë“œ ì™„ë£Œ: {cluster_embeddings.shape}"
                    )
                except Exception as e:
                    self.logger.warning(f"í´ëŸ¬ìŠ¤í„° ì„ë² ë”© ë¡œë“œ ì‹¤íŒ¨: {e}")

            # 4. ìŒ ë¹ˆë„ ë°ì´í„°ë¥¼ ê³ ë¹ˆë„ì™€ ì¤‘ë¹ˆë„ë¡œ ë¶„ë¦¬ (ë‹¤ì–‘ì„± í–¥ìƒ)
            top_pairs = []
            mid_pairs = []

            if pair_frequency:
                # ë¹ˆë„ ê¸°ì¤€ ì •ë ¬
                sorted_pairs = sorted(
                    pair_frequency.items(), key=lambda x: x[1], reverse=True
                )

                # ìƒìœ„ 50ê°œ ê³ ë¹ˆë„ ìŒ
                top_pairs = [pair for pair, _ in sorted_pairs[:50]]

                # ì¤‘ê°„ ë¹ˆë„ ìŒ (100~300 ìˆœìœ„)
                mid_range_start = min(100, len(sorted_pairs) - 1)
                mid_range_end = min(300, len(sorted_pairs))
                if mid_range_end > mid_range_start:
                    # ì¤‘ê°„ ë¹ˆë„ ìŒì—ì„œ ìµœëŒ€ 200ê°œê¹Œì§€ ìƒ˜í”Œë§
                    mid_freq_pairs = [
                        pair for pair, _ in sorted_pairs[mid_range_start:mid_range_end]
                    ]
                    # ëœë¤ ìƒ˜í”Œë§ (ìµœëŒ€ 100ê°œ)
                    sample_size = min(100, len(mid_freq_pairs))
                    if sample_size > 0:
                        mid_pairs = random.sample(mid_freq_pairs, sample_size)

            self.logger.info(
                f"ê³ ë¹ˆë„ ìŒ: {len(top_pairs)}ê°œ, ì¤‘ë¹ˆë„ ìŒ: {len(mid_pairs)}ê°œ"
            )

            # 5. ìƒìœ„ 10ê°œ ì¤‘ì‹¬ì„± ë²ˆí˜¸ ì¶”ì¶œ
            high_centrality_numbers = []
            if segment_centrality_vector is not None:
                # ì„¸ê·¸ë¨¼íŠ¸ ì¤‘ì‹¬ì„± ë²¡í„°ë¥¼ ê°œë³„ ë²ˆí˜¸ ì¤‘ì‹¬ì„±ìœ¼ë¡œ ë³€í™˜
                number_centrality = {}
                for i in range(9):  # 9ê°œ ì„¸ê·¸ë¨¼íŠ¸
                    seg_start = i * 5 + 1
                    seg_end = min((i + 1) * 5, 45)

                    for num in range(seg_start, seg_end + 1):
                        number_centrality[num] = segment_centrality_vector[i]

                # ìƒìœ„ 10ê°œ ì¤‘ì‹¬ì„± ë²ˆí˜¸ ì¶”ì¶œ
                high_centrality_numbers = sorted(
                    number_centrality.keys(),
                    key=lambda x: number_centrality.get(x, 0),
                    reverse=True,
                )[:10]
            else:
                # ì¤‘ì‹¬ì„± ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ë¹ˆë„ ê¸°ì¤€ ìƒìœ„ ë²ˆí˜¸ ì‚¬ìš©
                frequency_map = self.statistical_model.frequency_map
                high_centrality_numbers = sorted(
                    frequency_map.keys(),
                    key=lambda x: frequency_map.get(x, 0),
                    reverse=True,
                )[:10]

            # 6. í´ëŸ¬ìŠ¤í„°ë§ ì ìš© (ìˆëŠ” ê²½ìš°)
            number_clusters = {}
            n_clusters = 5  # ê¸°ë³¸ í´ëŸ¬ìŠ¤í„° ìˆ˜

            if (
                cluster_embeddings is not None
                and isinstance(cluster_embeddings, np.ndarray)
                and cluster_embeddings.size > 0
            ):
                try:
                    # KMeansë¡œ ë²ˆí˜¸ í´ëŸ¬ìŠ¤í„°ë§
                    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
                    kmeans.fit(np.array(cluster_embeddings, dtype=np.float64))

                    # ê° ë²ˆí˜¸ì˜ í´ëŸ¬ìŠ¤í„° í• ë‹¹
                    for i in range(45):
                        number_clusters[i + 1] = int(kmeans.labels_[i])

                    self.logger.info(f"ë²ˆí˜¸ í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ: {n_clusters}ê°œ í´ëŸ¬ìŠ¤í„°")
                except Exception as e:
                    self.logger.warning(f"í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {e}")

            # 7. í›„ë³´ ìƒì„±
            while len(candidates) < n_candidates:
                new_candidate = self._create_structured_candidate(
                    top_pairs,
                    high_centrality_numbers,
                    cluster_embeddings,
                    mid_pairs=mid_pairs,
                    number_clusters=number_clusters,
                )

                if new_candidate not in candidates:
                    candidates.append(new_candidate)

            self.logger.info(f"êµ¬ì¡°í™”ëœ í›„ë³´ ìƒì„± ì™„ë£Œ: {len(candidates)}ê°œ")

        except Exception as e:
            self.logger.error(f"êµ¬ì¡°í™”ëœ í›„ë³´ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            # ìµœì†Œí•œì˜ í›„ë³´ ìƒì„±
            while len(candidates) < n_candidates:
                candidates.append(sorted(random.sample(range(1, 46), 6)))

        return candidates

    def _create_structured_candidate(
        self,
        top_pairs: List[Tuple[int, int]],
        high_centrality_numbers: List[int],
        cluster_embeddings: Optional[np.ndarray] = None,
        mid_pairs: List[Tuple[int, int]] = [],
        number_clusters: Dict[int, int] = {},
    ) -> List[int]:
        """
        êµ¬ì¡°í™”ëœ ë‹¨ì¼ í›„ë³´ ìƒì„± (ìŒ ë¹ˆë„ + ì¤‘ì‹¬ì„± + ì„ íƒì  í´ëŸ¬ìŠ¤í„° ê¸°ë°˜)

        Args:
            top_pairs: ìƒìœ„ ë¹ˆë„ ìŒ ëª©ë¡
            high_centrality_numbers: ë†’ì€ ì¤‘ì‹¬ì„± ë²ˆí˜¸ ëª©ë¡
            cluster_embeddings: í´ëŸ¬ìŠ¤í„° ì„ë² ë”© (ì„ íƒì )
            mid_pairs: ì¤‘ë¹ˆë„ ìŒ ëª©ë¡
            number_clusters: ë²ˆí˜¸ í´ëŸ¬ìŠ¤í„° í• ë‹¹ ë”•ì…”ë„ˆë¦¬

        Returns:
            í›„ë³´ ë²ˆí˜¸ ì¡°í•© (ì •ë ¬ë¨)
        """
        # ì„ íƒëœ ë²ˆí˜¸ ì§‘í•©
        selected_numbers = set()

        # í´ëŸ¬ìŠ¤í„° ë‹¤ì–‘ì„± ì¶”ì ì„ ìœ„í•œ ì„ íƒëœ í´ëŸ¬ìŠ¤í„°
        selected_clusters = set()

        # í´ëŸ¬ìŠ¤í„° ì •ë³´ê°€ ìˆëŠ”ì§€ í™•ì¸
        has_cluster_info = bool(number_clusters)

        # 1. ê³ ë¹ˆë„ ë° ì¤‘ë¹ˆë„ ìŒì„ í˜¼í•©í•˜ì—¬ ì‚¬ìš© (ë‹¤ì–‘ì„± í–¥ìƒ)
        # 80% í™•ë¥ ë¡œ top_pairs, 20% í™•ë¥ ë¡œ mid_pairsì—ì„œ ì‹œì‘
        use_mid_pairs_first = random.random() < 0.2 and mid_pairs

        # ìŒ ì„ íƒ ì†ŒìŠ¤ ê²°ì •
        primary_pairs = mid_pairs if use_mid_pairs_first and mid_pairs else top_pairs
        secondary_pairs = top_pairs if use_mid_pairs_first and mid_pairs else mid_pairs

        # ì²« ë²ˆì§¸ ìŒ ì„ íƒ (primary_pairsì—ì„œ)
        if primary_pairs:
            # ëœë¤í•˜ê²Œ ìŒ ì„ íƒ
            pair_idx = random.randint(0, min(len(primary_pairs) - 1, 49))
            selected_pair = primary_pairs[pair_idx]

            # ë²ˆí˜¸ ì¶”ê°€
            selected_numbers.add(selected_pair[0])
            selected_numbers.add(selected_pair[1])

            # í´ëŸ¬ìŠ¤í„° ì¶”ì 
            if has_cluster_info:
                selected_clusters.add(number_clusters.get(selected_pair[0], -1))
                selected_clusters.add(number_clusters.get(selected_pair[1], -1))

        # ë‘ ë²ˆì§¸ ìŒ ì„ íƒ (secondary_pairsì—ì„œ, ìˆë‹¤ë©´)
        if len(selected_numbers) < 4 and secondary_pairs:
            # 10ë²ˆ ì‹œë„í•˜ì—¬ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ìŒ ì°¾ê¸°
            for _ in range(10):
                pair_idx = random.randint(0, min(len(secondary_pairs) - 1, 49))
                selected_pair = secondary_pairs[pair_idx]

                # ê¸°ì¡´ ë²ˆí˜¸ì™€ ì¤‘ë³µë˜ì§€ ì•ŠëŠ”ì§€ í™•ì¸
                if (
                    selected_pair[0] not in selected_numbers
                    and selected_pair[1] not in selected_numbers
                ):
                    selected_numbers.add(selected_pair[0])
                    selected_numbers.add(selected_pair[1])

                    # í´ëŸ¬ìŠ¤í„° ì¶”ì 
                    if has_cluster_info:
                        selected_clusters.add(number_clusters.get(selected_pair[0], -1))
                        selected_clusters.add(number_clusters.get(selected_pair[1], -1))
                    break

        # 2. ë†’ì€ ì¤‘ì‹¬ì„± ë²ˆí˜¸ì—ì„œ 1ê°œ ì¶”ê°€
        if len(selected_numbers) < 5 and high_centrality_numbers:
            # ì•„ì§ ì„ íƒë˜ì§€ ì•Šì€ ì¤‘ì‹¬ì„± ë²ˆí˜¸ë§Œ ê³ ë ¤
            available_centrality = [
                num for num in high_centrality_numbers if num not in selected_numbers
            ]

            if available_centrality:
                # ëœë¤í•˜ê²Œ í•˜ë‚˜ ì„ íƒ
                selected_num = random.choice(available_centrality)
                selected_numbers.add(selected_num)

                # í´ëŸ¬ìŠ¤í„° ì¶”ì 
                if has_cluster_info:
                    selected_clusters.add(number_clusters.get(selected_num, -1))

        # 3. í´ëŸ¬ìŠ¤í„° ë‹¤ì–‘ì„±ì„ ìœ„í•œ ì¶”ê°€ - ìµœì†Œ 3ê°œ ì´ìƒì˜ í´ëŸ¬ìŠ¤í„°ì—ì„œ ë²ˆí˜¸ ì„ íƒ
        # í´ëŸ¬ìŠ¤í„° ì •ë³´ê°€ ìˆê³  ì•„ì§ ë²ˆí˜¸ë¥¼ ë” ì„ íƒí•´ì•¼ í•˜ëŠ” ê²½ìš°
        if has_cluster_info and len(selected_numbers) < 6:
            # í´ëŸ¬ìŠ¤í„° ìˆ˜ ê²°ì • (ìµœì†Œ 3ê°œ, ìµœëŒ€ 5ê°œ)
            n_clusters = max(3, min(5, len(number_clusters.values())))

            # í˜„ì¬ ì„ íƒëœ ê³ ìœ  í´ëŸ¬ìŠ¤í„° ìˆ˜ í™•ì¸
            if len(selected_clusters) < 3:
                # ì•„ì§ í¬í•¨ë˜ì§€ ì•Šì€ í´ëŸ¬ìŠ¤í„° ì°¾ê¸°
                missing_clusters = [
                    c
                    for c in range(n_clusters)
                    if c not in selected_clusters and c != -1
                ]

                # ê° í´ëŸ¬ìŠ¤í„°ì—ì„œ ë²ˆí˜¸ ì¶”ê°€ (ìµœì†Œ 3ê°œ í´ëŸ¬ìŠ¤í„°ê¹Œì§€)
                for cluster_id in missing_clusters:
                    if len(selected_numbers) >= 6 or len(selected_clusters) >= 3:
                        break

                    # í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ì— ì†í•œ ë²ˆí˜¸ ì¤‘ ì•„ì§ ì„ íƒë˜ì§€ ì•Šì€ ë²ˆí˜¸ ì°¾ê¸°
                    cluster_numbers = [
                        num
                        for num, cluster in number_clusters.items()
                        if cluster == cluster_id and num not in selected_numbers
                    ]

                    if cluster_numbers:
                        selected_num = random.choice(cluster_numbers)
                        selected_numbers.add(selected_num)
                        selected_clusters.add(number_clusters.get(selected_num, -1))

        # 4. ë‚˜ë¨¸ì§€ ë²ˆí˜¸ëŠ” ë¹ˆë„ ê¸°ë°˜ìœ¼ë¡œ ì±„ìš°ê¸°
        while len(selected_numbers) < 6:
            if (
                hasattr(self.statistical_model, "frequency_map")
                and self.statistical_model.frequency_map
            ):
                frequency_map = self.statistical_model.frequency_map

                # í´ëŸ¬ìŠ¤í„° ë‹¤ì–‘ì„± ê³ ë ¤
                if has_cluster_info and len(selected_clusters) < 3:
                    # ì•„ì§ í¬í•¨ë˜ì§€ ì•Šì€ í´ëŸ¬ìŠ¤í„° ì°¾ê¸°
                    missing_clusters = [
                        c
                        for c in range(len(set(number_clusters.values())))
                        if c not in selected_clusters and c != -1
                    ]

                    if missing_clusters:
                        # í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ì— ì†í•œ ë²ˆí˜¸ë“¤ ì¤‘ ì•„ì§ ì„ íƒë˜ì§€ ì•Šì€ ë²ˆí˜¸ ì°¾ê¸°
                        available_numbers = []
                        for cluster_id in missing_clusters:
                            cluster_numbers = [
                                num
                                for num, cluster in number_clusters.items()
                                if cluster == cluster_id and num not in selected_numbers
                            ]
                            available_numbers.extend(cluster_numbers)

                        if available_numbers:
                            # ë¹ˆë„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
                            available_numbers.sort(
                                key=lambda x: frequency_map.get(x, 0), reverse=True
                            )
                            selected_num = available_numbers[0]
                            selected_numbers.add(selected_num)
                            selected_clusters.add(number_clusters.get(selected_num, -1))
                            continue

                # ì¼ë°˜ì ì¸ ë¹ˆë„ ê¸°ë°˜ ì„ íƒ
                top_numbers = sorted(
                    [num for num in range(1, 46) if num not in selected_numbers],
                    key=lambda x: frequency_map.get(x, 0),
                    reverse=True,
                )[:20]

                if top_numbers:
                    selected_numbers.add(random.choice(top_numbers))
                    continue

            # ë¹ˆë„ ë§µì´ ì—†ê±°ë‚˜ ë¬¸ì œê°€ ìˆìœ¼ë©´ ë¬´ì‘ìœ„ ì„ íƒ
            remaining_numbers = [
                num for num in range(1, 46) if num not in selected_numbers
            ]
            selected_num = random.choice(remaining_numbers)
            selected_numbers.add(selected_num)

            # í´ëŸ¬ìŠ¤í„° ì¶”ì 
            if has_cluster_info:
                selected_clusters.add(number_clusters.get(selected_num, -1))

        # ìµœì¢… ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ ë° ì •ë ¬
        return sorted(list(selected_numbers))

    def _generate_statistical_candidates(
        self, historical_data: List[LotteryNumber], n_candidates: int = 200
    ) -> List[List[int]]:
        """
        StatisticalModelì„ ì‚¬ìš©í•˜ì—¬ ì´ˆê¸° í›„ë³´ ìƒì„±

        Args:
            historical_data: ê³¼ê±° ë‹¹ì²¨ ë²ˆí˜¸ ë°ì´í„°
            n_candidates: ìƒì„±í•  í›„ë³´ ìˆ˜

        Returns:
            í›„ë³´ ë²ˆí˜¸ ì¡°í•© ëª©ë¡
        """
        candidates = []

        try:
            # StatisticalModelì˜ predict ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ë³¸ ì˜ˆì¸¡ ìƒì„±
            prediction = self.statistical_model.predict(historical_data)
            if prediction and hasattr(prediction, "numbers"):
                candidates.append(prediction.numbers)

            # ì¶”ê°€ í›„ë³´ ìƒì„±
            frequency_map = {}
            recency_map = {}

            # StatisticalModel í•™ìŠµ
            self.statistical_model.train(historical_data)

            # ë¹ˆë„ ë° ìµœê·¼ì„± ë§µ ê°€ì ¸ì˜¤ê¸°
            if hasattr(self.statistical_model, "frequency_map"):
                frequency_map = self.statistical_model.frequency_map
            if hasattr(self.statistical_model, "recency_map"):
                recency_map = self.statistical_model.recency_map

            # ì „ì²´ ë²ˆí˜¸ ëª©ë¡
            all_numbers = list(range(1, 46))

            # ë²ˆí˜¸ë³„ í†µí•© ì ìˆ˜ ê³„ì‚°
            number_scores = {}
            freq_weight = self.config.safe_get("frequency_weights.long_term", 0.6)
            recent_weight = 1.0 - freq_weight

            for num in all_numbers:
                freq_score = frequency_map.get(num, 0.0)
                recent_score = 1.0 - recency_map.get(
                    num, 1.0
                )  # ìµœê·¼ì„± ì ìˆ˜ ë°˜ì „ (1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ)
                combined_score = freq_weight * freq_score + recent_weight * recent_score
                number_scores[num] = combined_score

            # ì ìˆ˜ ê¸°ì¤€ ìƒìœ„ ë²ˆí˜¸ ì„ íƒ
            sorted_numbers = sorted(
                all_numbers, key=lambda x: number_scores.get(x, 0.0), reverse=True
            )

            # í•„ìš”í•œ í›„ë³´ ìˆ˜
            target_count = n_candidates

            # ìƒìœ„ ë²ˆí˜¸ë“¤ë¡œ ë‹¤ì–‘í•œ ì¡°í•© ìƒì„±
            top_n = min(25, len(sorted_numbers))  # ìƒìœ„ 25ê°œ ë²ˆí˜¸ ì‚¬ìš©
            top_numbers = sorted_numbers[:top_n]

            while len(candidates) < target_count and len(top_numbers) >= 6:
                # ìƒìœ„ ë²ˆí˜¸ë“¤ì—ì„œ 6ê°œ ì„ íƒ
                combination = sorted(random.sample(top_numbers, 6))

                if combination not in candidates:
                    candidates.append(combination)

            # í›„ë³´ ìˆ˜ê°€ ì—¬ì „íˆ ë¶€ì¡±í•˜ë©´ ë¬´ì‘ìœ„ ì¡°í•© ì¶”ê°€
            while len(candidates) < target_count:
                combination = sorted(random.sample(all_numbers, 6))

                if combination not in candidates:
                    candidates.append(combination)

            self.logger.info(f"í†µê³„ ëª¨ë¸ë¡œ {len(candidates)}ê°œ í›„ë³´ ìƒì„± ì™„ë£Œ")

        except Exception as e:
            self.logger.error(f"í›„ë³´ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            # ìµœì†Œí•œì˜ í›„ë³´ ìƒì„±
            for _ in range(10):
                candidates.append(sorted(random.sample(range(1, 46), 6)))

        return candidates

    def _filter_and_score_candidates(
        self, candidates: List[List[int]], historical_data: List[LotteryNumber]
    ) -> List[Dict[str, Any]]:
        """
        í›„ë³´ í•„í„°ë§ ë° ì ìˆ˜ ê³„ì‚°

        Args:
            candidates: í›„ë³´ ë²ˆí˜¸ ì¡°í•© ëª©ë¡
            historical_data: ê³¼ê±° ë‹¹ì²¨ ë²ˆí˜¸ ë°ì´í„°

        Returns:
            í•„í„°ë§ ë° ì ìˆ˜ ê³„ì‚°ëœ í›„ë³´ ëª©ë¡

        Raises:
            ValueError: í•„í„°ë§ í›„ ë‚¨ì€ í›„ë³´ê°€ ì—†ëŠ” ê²½ìš°
            RuntimeError: í•„í„°ë§ ë˜ëŠ” ì ìˆ˜ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ
        """
        # í•„í„°ë§ ì„¤ì •
        filtering_config = self.config.get("filtering", {})
        use_rule_filter = filtering_config.get("use_rule_filter", True)
        use_risk_filter = filtering_config.get("use_risk_filter", True)
        use_roi_filter = filtering_config.get("use_roi_filter", True)

        # ë¦¬ìŠ¤í¬ í•„í„° ì„ê³„ê°’
        risk_threshold = filtering_config.get("risk_threshold", 0.8)

        # ROI í•„í„° ì„ê³„ê°’
        roi_threshold = filtering_config.get("roi_threshold", -0.3)

        # íŒ¨í„´ í•„í„° ê°€ì ¸ì˜¤ê¸°
        pattern_filter = get_pattern_filter()

        # ê²°ê³¼ ëª©ë¡
        filtered_candidates = []

        # ìœ íš¨í•œ í›„ë³´ ìˆ˜ ì¶”ì 
        valid_count = 0

        # í•„í„°ë³„ ê±°ë¶€ ìˆ˜ ì¶”ì 
        rule_rejected = 0
        risk_rejected = 0
        roi_rejected = 0

        # íŒ¨í„´ ë¶„ì„ê¸° ê°€ì ¸ì˜¤ê¸° (ì¬ì‚¬ìš©)
        from ..analysis.pattern_analyzer import PatternAnalyzer

        pattern_analyzer = PatternAnalyzer(self.config)

        # ê° í›„ë³´ì— ëŒ€í•´ ì²˜ë¦¬
        for numbers in candidates:
            # ë²ˆí˜¸ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬ (ì¼ê´€ì„± ìœ ì§€)
            numbers = sorted(numbers)

            # 1. ê·œì¹™ ê¸°ë°˜ í•„í„°ë§ (í™€ì§, ë²”ìœ„ ë“±)
            if use_rule_filter:
                if not pattern_filter.is_valid_combination(numbers):
                    rule_rejected += 1
                    continue

            # 2. íŒ¨í„´ íŠ¹ì„± ì¶”ì¶œ
            pattern_features = pattern_analyzer.extract_pattern_features(
                numbers, historical_data
            )

            # 3. ë¦¬ìŠ¤í¬ ì ìˆ˜ ê³„ì‚° ë° í•„í„°ë§
            risk_score = pattern_features.get("risk_score", 0.5)
            if use_risk_filter and risk_score > risk_threshold:
                risk_rejected += 1
                continue

            # 4. ROI ê°€ì¤‘ì¹˜ ê³„ì‚° ë° í•„í„°ë§
            roi_weight = pattern_features.get("roi_weight", 0.0)
            if use_roi_filter and roi_weight < roi_threshold:
                roi_rejected += 1
                continue

            # ìœ íš¨í•œ í›„ë³´ë¡œ ì¹´ìš´íŠ¸
            valid_count += 1

            # ê²°ê³¼ ì‚¬ì „ì— ì¶”ê°€ (ë²ˆí˜¸, ì ìˆ˜, íŠ¹ì„± ë“±)
            candidate_result = {
                "numbers": numbers,
                "risk_score": risk_score,
                "roi_weight": roi_weight,
                "pattern_features": pattern_features,
            }

            # ì¶”ê°€ ì ìˆ˜ ê³„ì‚° (ë¹ˆë„, íŠ¸ë Œë“œ ë“±)
            if "trend_score_avg" in pattern_features:
                candidate_result["trend_score"] = pattern_features["trend_score_avg"]

            if "frequent_pair_score" in pattern_features:
                candidate_result["pair_score"] = pattern_features["frequent_pair_score"]

            # ê²°ê³¼ ëª©ë¡ì— ì¶”ê°€
            filtered_candidates.append(candidate_result)

        # í•„í„°ë§ ê²°ê³¼ ë¡œê¹…
        self.logger.info(
            f"í•„í„°ë§ ê²°ê³¼: ì´ {len(candidates)}ê°œ ì¤‘ {valid_count}ê°œ í†µê³¼ "
            f"(ê·œì¹™ í•„í„°: {rule_rejected}ê°œ ê±°ë¶€, ë¦¬ìŠ¤í¬ í•„í„°: {risk_rejected}ê°œ ê±°ë¶€, ROI í•„í„°: {roi_rejected}ê°œ ê±°ë¶€)"
        )

        # í•„í„°ë§ í›„ í›„ë³´ê°€ ì—†ì„ ë•Œ ê²½ê³ 
        if not filtered_candidates:
            error_msg = "ëª¨ë“  í›„ë³´ê°€ í•„í„°ë§ë˜ì—ˆìŠµë‹ˆë‹¤. í•„í„° ì„ê³„ê°’ì„ ì¡°ì •í•˜ì„¸ìš”."
            self.logger.error(error_msg)
            raise ValueError(error_msg)

        return filtered_candidates

    def _score_candidates_with_ml(
        self,
        candidates: List[Dict[str, Any]],
        historical_data: List[LotteryNumber],
        lgbm_loaded: bool,
        xgb_loaded: bool,
    ) -> List[Dict[str, Any]]:
        """
        ML ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í›„ë³´ ì ìˆ˜ ê³„ì‚°

        Args:
            candidates: í›„ë³´ ë²ˆí˜¸ ì¡°í•© ëª©ë¡ (ì‚¬ì „ í˜•íƒœ)
            historical_data: ê³¼ê±° ë‹¹ì²¨ ë²ˆí˜¸ ë°ì´í„°
            lgbm_loaded: LightGBM ëª¨ë¸ ë¡œë“œ ì—¬ë¶€
            xgb_loaded: XGBoost ëª¨ë¸ ë¡œë“œ ì—¬ë¶€

        Returns:
            ML ì ìˆ˜ê°€ ì¶”ê°€ëœ í›„ë³´ ëª©ë¡

        Raises:
            ValueError: ëª¨ë“  ML ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í•œ ê²½ìš°
            RuntimeError: ì ìˆ˜ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ
        """
        # ëª¨ë¸ ë¡œë“œ ì—¬ë¶€ í™•ì¸
        if not lgbm_loaded and not xgb_loaded:
            error_msg = "ML ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì ìˆ˜ ê³„ì‚°ì„ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            self.logger.error(error_msg)
            raise ValueError(error_msg)

        # ëª¨ë¸ ê°ì²´ í™•ì¸
        if lgbm_loaded and self.lightgbm_model is None:
            error_msg = "LightGBM ëª¨ë¸ì´ ë¡œë“œë˜ì—ˆì§€ë§Œ ê°ì²´ê°€ ì—†ìŠµë‹ˆë‹¤."
            self.logger.error(error_msg)
            lgbm_loaded = False

        if xgb_loaded and self.xgboost_model is None:
            error_msg = "XGBoost ëª¨ë¸ì´ ë¡œë“œë˜ì—ˆì§€ë§Œ ê°ì²´ê°€ ì—†ìŠµë‹ˆë‹¤."
            self.logger.error(error_msg)
            xgb_loaded = False

        # ë‹¤ì‹œ í™•ì¸ (ë‘˜ ë‹¤ ì—†ìœ¼ë©´ ì˜¤ë¥˜)
        if not lgbm_loaded and not xgb_loaded:
            error_msg = "ìœ íš¨í•œ ML ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ì ìˆ˜ ê³„ì‚°ì„ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            self.logger.error(error_msg)
            raise ValueError(error_msg)

        # íƒ€ì´ë¨¸ ì‹œì‘
        start_time = time.time()
        self.logger.info(f"{len(candidates)}ê°œ í›„ë³´ì— ëŒ€í•´ ML ì ìˆ˜ ê³„ì‚° ì‹œì‘")

        # ê° í›„ë³´ì— ëŒ€í•´ ML ì ìˆ˜ ê³„ì‚°
        for i, candidate in enumerate(candidates):
            numbers = candidate["numbers"]
            lgbm_score = 0.0
            xgb_score = 0.0
            combined_score = 0.0

            # LightGBM ëª¨ë¸ ì‚¬ìš©
            if lgbm_loaded and self.lightgbm_model is not None:
                # íŠ¹ì„± ì¶”ì¶œ
                lgbm_features = self.lightgbm_model.extract_features_for_prediction(
                    numbers, historical_data
                )

                # íŠ¹ì„± í–‰ë ¬ ì¤€ë¹„ (ê° ë²ˆí˜¸ë³„ë¡œ ë™ì¼í•œ íŠ¹ì„± + ë²ˆí˜¸ ìì²´)
                X_lgbm = np.zeros((6, len(lgbm_features) + 1))
                for j, num in enumerate(numbers):
                    X_lgbm[j, :-1] = lgbm_features
                    X_lgbm[j, -1] = num / 45.0  # ì •ê·œí™”ëœ ë²ˆí˜¸

                # í™•ë¥  ì˜ˆì¸¡
                lgbm_probas = self.lightgbm_model.predict_proba(X_lgbm)
                lgbm_score = float(np.mean(lgbm_probas))
                candidate["lgbm_score"] = lgbm_score

            # XGBoost ëª¨ë¸ ì‚¬ìš©
            if xgb_loaded and self.xgboost_model is not None:
                # íŠ¹ì„± ì¶”ì¶œ
                xgb_features = self.xgboost_model.extract_features_for_prediction(
                    numbers, historical_data
                )

                # íŠ¹ì„± í–‰ë ¬ ì¤€ë¹„ (ê° ë²ˆí˜¸ë³„ë¡œ ë™ì¼í•œ íŠ¹ì„± + ë²ˆí˜¸ ìì²´)
                X_xgb = np.zeros((6, len(xgb_features) + 1))
                for j, num in enumerate(numbers):
                    X_xgb[j, :-1] = xgb_features
                    X_xgb[j, -1] = num / 45.0  # ì •ê·œí™”ëœ ë²ˆí˜¸

                # í™•ë¥  ì˜ˆì¸¡
                xgb_probas = self.xgboost_model.predict_proba(X_xgb)
                xgb_score = float(np.mean(xgb_probas))
                candidate["xgb_score"] = xgb_score

            # ì ìˆ˜ ê²°í•© (ê°€ì¤‘ í‰ê· )
            if lgbm_loaded and xgb_loaded:
                lgbm_weight = self.config.get("model_weights", {}).get("lgbm", 0.5)
                xgb_weight = self.config.get("model_weights", {}).get("xgb", 0.5)
                combined_score = (lgbm_score * lgbm_weight) + (xgb_score * xgb_weight)
            elif lgbm_loaded:
                combined_score = lgbm_score
            elif xgb_loaded:
                combined_score = xgb_score

            # ë¦¬ìŠ¤í¬ ì ìˆ˜ë¥¼ ë°˜ì˜í•œ ìµœì¢… ì ìˆ˜ ê³„ì‚°
            risk_score = candidate.get("risk_score", 0.5)
            risk_weight = self.config.get("risk_weight", 0.3)

            # ROI ê°€ì¤‘ì¹˜ë¥¼ ë°˜ì˜í•œ ì ìˆ˜ ê³„ì‚°
            roi_weight = candidate.get("roi_weight", 0.0)
            roi_factor = self.config.get("roi_factor", 0.2)

            # ìµœì¢… ì ìˆ˜ = ML ì ìˆ˜ * (1 - ë¦¬ìŠ¤í¬ ê°€ì¤‘ì¹˜ * ë¦¬ìŠ¤í¬ ì ìˆ˜) + ROI ê°€ì¤‘ì¹˜ * ROI ê³„ìˆ˜
            final_score = (
                combined_score * (1 - risk_weight * risk_score)
                + roi_weight * roi_factor
            )

            # 0-1 ë²”ìœ„ë¡œ í´ë¦¬í•‘
            final_score = max(0.0, min(1.0, final_score))

            # ì ìˆ˜ ì €ì¥
            candidate["ml_score"] = combined_score
            candidate["combined_score"] = final_score

            # ì§„í–‰ ìƒí™© ë¡œê¹… (100ê°œë§ˆë‹¤)
            if (i + 1) % 100 == 0:
                self.logger.debug(f"ML ì ìˆ˜ ê³„ì‚° ì§„í–‰ ì¤‘: {i+1}/{len(candidates)}")

        # íƒ€ì´ë¨¸ ì¢…ë£Œ
        elapsed_time = time.time() - start_time
        self.logger.info(
            f"ML ì ìˆ˜ ê³„ì‚° ì™„ë£Œ: {len(candidates)}ê°œ í›„ë³´, ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ"
        )

        # ì ìˆ˜ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        candidates.sort(key=lambda x: x.get("combined_score", 0.0), reverse=True)

        return candidates
