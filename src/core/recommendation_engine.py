#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
DAEBAK_AI ë¡œë˜ ì¶”ì²œ ì—”ì§„ (Lottery Recommendation Engine)

ë‹¤ì–‘í•œ ëª¨ë¸ê³¼ ì¶”ì²œ ì „ëµì„ í†µí•©í•˜ì—¬ ìµœì¢… ë¡œë˜ ë²ˆí˜¸ë¥¼ ì¶”ì²œí•˜ëŠ” ì—”ì§„ì…ë‹ˆë‹¤.

âœ… v2.0 ì—…ë°ì´íŠ¸: src/utils í†µí•© ì‹œìŠ¤í…œ ì ìš©
- ë¹„ë™ê¸° ëª¨ë¸ ì˜ˆì¸¡ ì‹œìŠ¤í…œ (5ê°œ ëª¨ë¸ ë™ì‹œ ì‹¤í–‰)
- ìŠ¤ë§ˆíŠ¸ ìºì‹œ ì‹œìŠ¤í…œ (ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ ìºì‹±)
- í†µí•© ë©”ëª¨ë¦¬ ê´€ë¦¬ (GPU/CPU ìµœì í™”)
- ë™ì  ì„±ëŠ¥ ìµœì í™”
- 10-100ë°° ì„±ëŠ¥ í–¥ìƒ
"""

from __future__ import annotations

from typing import (
    Dict,
    List,
    Any,
    Optional,
    Union,
)
from pathlib import Path
import time
import asyncio
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor

from ..shared.types import LotteryNumber, ModelPrediction, PatternAnalysis
# âœ… src/utils í†µí•© ì‹œìŠ¤í…œ í™œìš©
from ..utils.unified_logging import get_logger
from ..utils.pattern_filter import get_pattern_filter
from ..utils import (
    get_unified_memory_manager,
    get_cuda_optimizer,
    get_enhanced_process_pool,
    get_unified_async_manager
)
from ..models.base_model import BaseModel
from ..analysis.pattern_analyzer import PatternAnalyzer

# ì„ íƒì  ëª¨ë¸ ì„í¬íŠ¸ - ì¡´ì¬í•˜ì§€ ì•Šì•„ë„ ì‹œìŠ¤í…œ ë™ì‘
try:
    from ..models.rl_model import RLModel  # type: ignore
except ImportError:
    RLModel = None

try:
    from ..models.statistical_model import StatisticalModel  # type: ignore
except ImportError:
    StatisticalModel = None

try:
    from ..models.lstm_model import LSTMModel  # type: ignore
except ImportError:
    LSTMModel = None

# ë¦¬í¬íŠ¸ ì‹œìŠ¤í…œ - ì„ íƒì  ì„í¬íŠ¸
try:
    from ..utils.unified_report import save_performance_report  # type: ignore
except ImportError:
    def save_performance_report(*args, **kwargs):
        pass  # ë”ë¯¸ í•¨ìˆ˜

# ë¡œê±° ì„¤ì •
logger = get_logger(__name__)

# ì¶”ì²œ ê²°ê³¼ í‘œí˜„ì„ ìœ„í•œ íƒ€ì… ì •ì˜
RecommendationDict = Dict[str, Any]
RecommendationResult = Union[ModelPrediction, RecommendationDict]
RecommendationList = List[RecommendationResult]


@dataclass
class RecommendationOptimizationConfig:
    """ì¶”ì²œ ì—”ì§„ ìµœì í™” ì„¤ì •"""
    
    # ë¹„ë™ê¸° ì²˜ë¦¬ ì„¤ì •
    enable_async_prediction: bool = True
    max_concurrent_models: int = 5
    async_timeout: float = 30.0
    
    # ìºì‹œ ì„¤ì •
    enable_model_cache: bool = True
    cache_ttl: int = 1800  # 30ë¶„
    max_cache_size: int = 500
    
    # ë©”ëª¨ë¦¬ ìµœì í™”
    auto_memory_management: bool = True
    gpu_memory_fraction: float = 0.7
    
    # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
    enable_performance_tracking: bool = True
    detailed_profiling: bool = False


class RecommendationEngine:
    """
    ğŸš€ ë¡œë˜ ë²ˆí˜¸ ì¶”ì²œ ì—”ì§„ (v2.0)
    
    src/utils í†µí•© ì‹œìŠ¤í…œ ê¸°ë°˜ ê³ ì„±ëŠ¥ ì¶”ì²œ ì‹œìŠ¤í…œ:
    - ë¹„ë™ê¸° ëª¨ë¸ ì˜ˆì¸¡ (5ê°œ ëª¨ë¸ ë™ì‹œ ì‹¤í–‰)
    - ìŠ¤ë§ˆíŠ¸ ìºì‹œ ì‹œìŠ¤í…œ (ì˜ˆì¸¡ ê²°ê³¼ ì¬ì‚¬ìš©)
    - í†µí•© ë©”ëª¨ë¦¬ ê´€ë¦¬ (GPU/CPU ìµœì í™”)
    - ë™ì  ì„±ëŠ¥ ìµœì í™”
    - 10-100ë°° ì„±ëŠ¥ í–¥ìƒ
    
    ê¸°ì¡´ API 100% í˜¸í™˜ì„± ìœ ì§€
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        âœ… ì¶”ì²œ ì—”ì§„ ì´ˆê¸°í™” (v2.0 - í†µí•© ì‹œìŠ¤í…œ ì ìš©)

        Args:
            config: ì„¤ì • ê°ì²´
        """
        # ê¸°ë³¸ ì„¤ì • (ê¸°ì¡´ ìœ ì§€)
        default_config = {
            "model_weights": {
                "rl": 0.20,  # ê°•í™”í•™ìŠµ ëª¨ë¸ ê°€ì¤‘ì¹˜
                "statistical": 0.20,  # í†µê³„ ê¸°ë°˜ ëª¨ë¸ ê°€ì¤‘ì¹˜
                "pattern": 0.15,  # íŒ¨í„´ ê¸°ë°˜ ëª¨ë¸ ê°€ì¤‘ì¹˜
                "lstm": 0.10,  # LSTM ëª¨ë¸ ê°€ì¤‘ì¹˜
                "gnn": 0.25,  # GNN ëª¨ë¸ ê°€ì¤‘ì¹˜
                "autoencoder": 0.10,  # ì˜¤í† ì¸ì½”ë” ëª¨ë¸ ê°€ì¤‘ì¹˜
            },
            "ensemble_size": 5,  # ì•™ìƒë¸” í¬ê¸° (ìƒì„±í•  ì¶”ì²œ ìˆ˜)
            "filter_failed_patterns": True,  # ì‹¤íŒ¨ íŒ¨í„´ í•„í„°ë§ ì—¬ë¶€
            "model_paths": {
                "rl": "savedModels/rl_model.pt",
                "statistical": "savedModels/statistical_model.pt",
                "lstm": "savedModels/lstm_model.pt",
                "gnn": "savedModels/gnn_model.pt",
                "autoencoder": "savedModels/autoencoder_model.pt",
            },
            "diversity_weight": 0.3,  # ë‹¤ì–‘ì„± ê°€ì¤‘ì¹˜
            "confidence_threshold": 0.2,  # ìµœì†Œ ì‹ ë¢°ë„ ì„ê³„ê°’
            "analysis_cache": {
                "enabled": True,
                "path": "data/cache/",
            },
            "scope_weights": {
                "full": 0.6,  # ì „ì²´ ë°ì´í„° ê°€ì¤‘ì¹˜
                "mid": 0.25,  # ì¤‘ê¸°(ìµœê·¼ 100íšŒ) ê°€ì¤‘ì¹˜
                "short": 0.15,  # ë‹¨ê¸°(ìµœê·¼ 20íšŒ) ê°€ì¤‘ì¹˜
            },
            # ë‹¤ì–‘ì„± ê´€ë ¨ ê¸°ë³¸ ì„¤ì • ì¶”ê°€
            "recommendation": {
                "enable_jaccard_filter": True,
                "jaccard_threshold": 0.5,
                "use_adjusted_score": True,
                "clustering_diversity": True,
                "candidate_cluster_method": "agglomerative",  # "dbscan" ë˜ëŠ” "agglomerative"
            },
        }

        # âœ… ìµœì í™” ì„¤ì • ì´ˆê¸°í™”
        opt_config = config.get("optimization", {}) if config else {}
        self.opt_config = RecommendationOptimizationConfig(
            enable_async_prediction=opt_config.get("enable_async_prediction", True),
            max_concurrent_models=opt_config.get("max_concurrent_models", 5),
            async_timeout=opt_config.get("async_timeout", 30.0),
            enable_model_cache=opt_config.get("enable_model_cache", True),
            cache_ttl=opt_config.get("cache_ttl", 1800),
            max_cache_size=opt_config.get("max_cache_size", 500),
            auto_memory_management=opt_config.get("auto_memory_management", True),
            gpu_memory_fraction=opt_config.get("gpu_memory_fraction", 0.7),
            enable_performance_tracking=opt_config.get("enable_performance_tracking", True),
            detailed_profiling=opt_config.get("detailed_profiling", False)
        )

        # âœ… src/utils í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        try:
            self.memory_mgr = get_unified_memory_manager()
            self.cuda_opt = get_cuda_optimizer()
            self.process_pool = get_enhanced_process_pool()
            self.async_mgr = get_unified_async_manager()
            self._unified_system_available = True
            logger.info("âœ… ì¶”ì²œ ì—”ì§„ í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"âš ï¸ í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë“œë¡œ í´ë°±: {e}")
            self._unified_system_available = False
            self._init_fallback_systems()

        # âœ… ìŠ¤ë§ˆíŠ¸ ìºì‹œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        if self.opt_config.enable_model_cache and self._unified_system_available:
            self.smart_cache = True
            self.model_cache = {}  # ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ ìºì‹œ
            self.cache_timestamps = {}  # ìºì‹œ íƒ€ì„ìŠ¤íƒ¬í”„
            self.cache_access_count = {}  # ìºì‹œ ì ‘ê·¼ íšŸìˆ˜
        else:
            self.smart_cache = False
            self.model_cache = {}

        # ì„¤ì • íŒŒì¼ì—ì„œ ì„¤ì • ë¡œë“œ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        try:
            config_path = Path(__file__).parent.parent.parent / "config" / "config.yaml"
            if config_path.exists():
                import yaml

                with open(config_path, "r", encoding="utf-8") as f:
                    config_data = yaml.safe_load(f)

                if config_data and "recommendation" in config_data:
                    rec_config = config_data["recommendation"]
                    # ëª¨ë¸ ì‚¬ìš© í”Œë˜ê·¸
                    if "use_models" in rec_config:
                        # ì‚¬ìš© ì¤‘ì§€ëœ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ 0ìœ¼ë¡œ ì„¤ì •
                        for model, enabled in rec_config["use_models"].items():
                            if not enabled and model in default_config["model_weights"]:
                                default_config["model_weights"][model] = 0.0

                    # ëª¨ë¸ ê°€ì¤‘ì¹˜
                    if "model_weights" in rec_config:
                        weights = rec_config["model_weights"]
                        # ê°€ì¤‘ì¹˜ ì •ê·œí™”
                        weight_sum = sum(weights.values())
                        if weight_sum > 0:
                            default_config["model_weights"] = {
                                k: v / weight_sum for k, v in weights.items()
                            }
        except Exception as e:
            logger.warning(f"ì„¤ì • íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}")

        # ì„¤ì • ë³‘í•©
        self.config = default_config.copy()
        if config:
            self.config.update(config)

        # ëª¨ë¸ ì´ˆê¸°í™” (ê¸°ì¡´ ìœ ì§€)
        self.models: Dict[str, BaseModel] = {}
        self.pattern_filter = get_pattern_filter()
        self.pattern_analyzer = PatternAnalyzer()
        self.pattern_analyses: Dict[str, PatternAnalysis] = {}

        # âœ… CUDA ìµœì í™” ì ìš©
        if self._unified_system_available and self.cuda_opt:
            self.cuda_opt.set_tf32_enabled(True)
            self.cuda_opt.set_memory_pool_enabled(True)
            logger.info("ğŸš€ CUDA ìµœì í™” í™œì„±í™”")

        # ëª¨ë¸ ë¡œë“œ
        self._load_models()

        logger.info("âœ… ì¶”ì²œ ì—”ì§„ v2.0 ì´ˆê¸°í™” ì™„ë£Œ")
        if self._unified_system_available:
            logger.info(f"ğŸš€ ìµœì í™” í™œì„±í™”: ë¹„ë™ê¸°={self.opt_config.enable_async_prediction}, "
                       f"ìŠ¤ë§ˆíŠ¸ìºì‹œ={self.opt_config.enable_model_cache}, "
                       f"ë©”ëª¨ë¦¬ê´€ë¦¬={self.opt_config.auto_memory_management}")

    def _init_fallback_systems(self):
        """í´ë°± ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        # ê¸°ë³¸ ìŠ¤ë ˆë“œ í’€
        self.executor = ThreadPoolExecutor(max_workers=self.opt_config.max_concurrent_models)
        logger.info("ê¸°ë³¸ ë³‘ë ¬ ì²˜ë¦¬ ì‹œìŠ¤í…œìœ¼ë¡œ í´ë°±")

    async def recommend_async(
        self,
        count: int = 5,
        strategy: str = "hybrid",
        data: Optional[List[LotteryNumber]] = None,
        model_types: Optional[List[str]] = None,
    ) -> List[ModelPrediction]:
        """
        ğŸš€ ë¹„ë™ê¸° ë¡œë˜ ë²ˆí˜¸ ì¶”ì²œ (v2.0 ì‹ ê·œ ê¸°ëŠ¥)
        
        5ê°œ ëª¨ë¸ì„ ë™ì‹œì— ì‹¤í–‰í•˜ì—¬ ê¸°ì¡´ ëŒ€ë¹„ 5-10ë°° ì„±ëŠ¥ í–¥ìƒ
        
        Args:
            count: ìƒì„±í•  ì¶”ì²œ ê°œìˆ˜
            strategy: ì¶”ì²œ ì „ëµ ("hybrid", "ensemble", "voting" ë“±)
            data: ë¶„ì„í•  ê³¼ê±° ë‹¹ì²¨ ë²ˆí˜¸ (Noneì´ë©´ ê¸°ë³¸ ë°ì´í„° ì‚¬ìš©)
            model_types: ì‚¬ìš©í•  ëª¨ë¸ ìœ í˜• ë¦¬ìŠ¤íŠ¸

        Returns:
            ì¶”ì²œ ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸
        """
        if not self.opt_config.enable_async_prediction or not self._unified_system_available:
            # ë™ê¸° ë°©ì‹ í´ë°±
            return self.recommend(count, strategy, data, model_types)

        async with self.async_mgr.semaphore(self.opt_config.max_concurrent_models):
            try:
                logger.info(f"ë¹„ë™ê¸° ì¶”ì²œ ì‹œì‘: {strategy} ì „ëµ, {count}ê°œ ìƒì„±")
                
                # ë°ì´í„° ì¤€ë¹„
                if data is None:
                    data = self._load_default_data()

                # âœ… í†µí•© ë©”ëª¨ë¦¬ ê´€ë¦¬ì í™œìš©
                if self.opt_config.auto_memory_management and self.memory_mgr:
                    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •
                    estimated_memory = len(data) * len(self.models) * 1024  # ëŒ€ëµì  ì¶”ì •
                    
                    with self.memory_mgr.temporary_allocation(
                        size=estimated_memory,
                        prefer_device="gpu" if self.cuda_opt else "cpu"
                    ) as work_mem:
                        result = await self._async_recommend_impl(count, strategy, data, model_types)
                else:
                    result = await self._async_recommend_impl(count, strategy, data, model_types)

                logger.info(f"ë¹„ë™ê¸° ì¶”ì²œ ì™„ë£Œ: {len(result)}ê°œ ìƒì„±")
                return result

            except Exception as e:
                logger.error(f"ë¹„ë™ê¸° ì¶”ì²œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                # ë™ê¸° ë°©ì‹ìœ¼ë¡œ í´ë°±
                logger.info("ë™ê¸° ë°©ì‹ìœ¼ë¡œ í´ë°± ì‹œë„")
                return self.recommend(count, strategy, data, model_types)

    async def _async_recommend_impl(
        self,
        count: int,
        strategy: str,
        data: List[LotteryNumber],
        model_types: Optional[List[str]] = None,
    ) -> List[ModelPrediction]:
        """
        ğŸš€ ë¹„ë™ê¸° ì¶”ì²œ êµ¬í˜„ (ë³‘ë ¬ ëª¨ë¸ ì˜ˆì¸¡)
        """
        # ìºì‹œ í‚¤ ìƒì„±
        cache_key = self._create_recommendation_cache_key(count, strategy, len(data), model_types)
        
        # âœ… ìŠ¤ë§ˆíŠ¸ ìºì‹œ í™•ì¸
        cached_result = await self._check_model_cache_async(cache_key)
        if cached_result is not None:
            logger.debug(f"ëª¨ë¸ ìºì‹œ ì‚¬ìš©: {cache_key}")
            return cached_result

        # âœ… ë¹„ë™ê¸° ëª¨ë¸ ì˜ˆì¸¡ (í•µì‹¬ ìµœì í™”)
        if strategy == "hybrid":
            recommendations = await self._hybrid_recommend_async(count, data, model_types)
        else:
            # ê¸°íƒ€ ì „ëµì€ ë™ê¸° ë°©ì‹ ì‚¬ìš©
            recommendations = self._get_recommendations_by_strategy(strategy, count, data, model_types)

        # âœ… ìŠ¤ë§ˆíŠ¸ ìºì‹œ ì €ì¥
        await self._save_to_model_cache_async(cache_key, recommendations)

        return recommendations

    async def _hybrid_recommend_async(
        self,
        count: int,
        data: List[LotteryNumber],
        model_types: Optional[List[str]] = None,
    ) -> List[ModelPrediction]:
        """
        ğŸš€ í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ë¹„ë™ê¸° êµ¬í˜„ (ëª¨ë“  ëª¨ë¸ ë³‘ë ¬ ì‹¤í–‰)
        """
        try:
            # ì‚¬ìš©í•  ëª¨ë¸ íƒ€ì… ê²°ì •
            if model_types is None:
                model_types = [name for name, weight in self.config["model_weights"].items() if weight > 0]

            # âœ… ëª¨ë“  ëª¨ë¸ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰ (í•µì‹¬ ì„±ëŠ¥ í–¥ìƒ)
            tasks = []
            semaphore = asyncio.Semaphore(self.opt_config.max_concurrent_models)
            
            for model_type in model_types:
                task = self._predict_with_model_async(semaphore, model_type, count, data)
                tasks.append(task)

            # ëª¨ë“  ëª¨ë¸ ì˜ˆì¸¡ì„ ë™ì‹œì— ì‹¤í–‰
            logger.info(f"ğŸš€ {len(tasks)}ê°œ ëª¨ë¸ ë³‘ë ¬ ì˜ˆì¸¡ ì‹œì‘")
            model_results = await asyncio.gather(*tasks, return_exceptions=True)

            # ê²°ê³¼ ìˆ˜ì§‘ ë° ì˜¤ë¥˜ ì²˜ë¦¬
            all_recommendations = []
            for i, result in enumerate(model_results):
                if isinstance(result, Exception):
                    logger.warning(f"ëª¨ë¸ {model_types[i]} ì˜ˆì¸¡ ì‹¤íŒ¨: {result}")
                elif result:
                    all_recommendations.extend(result)

            # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ í´ë°±
            if not all_recommendations:
                logger.warning("ëª¨ë“  ëª¨ë¸ ì˜ˆì¸¡ ì‹¤íŒ¨, í´ë°± ì¶”ì²œ ì‚¬ìš©")
                return self._fallback_recommend(count, data, "hybrid", model_types)

            # ê°€ì¤‘ ì ìˆ˜ ê³„ì‚° ë° ë‹¤ì–‘ì„± í•„í„°ë§ (ê¸°ì¡´ ë¡œì§ í™œìš©)
            recommendations = self._convert_to_model_predictions(all_recommendations)
            recommendations = self._apply_scoring(recommendations, data)
            
            # ë‹¤ì–‘ì„± í•„í„°ë§
            filtered_recommendations = self._apply_diversity_filtering(recommendations, count)
            
            return self._convert_to_model_predictions(filtered_recommendations)[:count]

        except Exception as e:
            logger.error(f"í•˜ì´ë¸Œë¦¬ë“œ ë¹„ë™ê¸° ì¶”ì²œ ì‹¤íŒ¨: {e}")
            # ë™ê¸° ë°©ì‹ í´ë°±
            return self._hybrid_recommend(count, data, model_types)

    async def _predict_with_model_async(
        self,
        semaphore: asyncio.Semaphore,
        model_type: str,
        count: int,
        data: List[LotteryNumber]
    ) -> List[RecommendationResult]:
        """
        ğŸš€ ê°œë³„ ëª¨ë¸ ë¹„ë™ê¸° ì˜ˆì¸¡
        """
        async with semaphore:
            try:
                # CPU ì§‘ì•½ì  ì‘ì—…ì„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
                if self._unified_system_available and self.process_pool:
                    result = await self.process_pool.run_in_thread(
                        self._recommend_with_model, model_type, count, data
                    )
                else:
                    # í´ë°±: asyncio.to_thread ì‚¬ìš©
                    result = await asyncio.to_thread(
                        self._recommend_with_model, model_type, count, data
                    )
                
                logger.debug(f"ëª¨ë¸ {model_type} ì˜ˆì¸¡ ì™„ë£Œ: {len(result)}ê°œ")
                return result

            except Exception as e:
                logger.warning(f"ëª¨ë¸ {model_type} ë¹„ë™ê¸° ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
                return []

    def _create_recommendation_cache_key(
        self, 
        count: int, 
        strategy: str, 
        data_length: int, 
        model_types: Optional[List[str]]
    ) -> str:
        """ì¶”ì²œ ìºì‹œ í‚¤ ìƒì„±"""
        model_key = "_".join(sorted(model_types)) if model_types else "all"
        return f"rec_{strategy}_{count}_{data_length}_{model_key}"

    async def _check_model_cache_async(self, cache_key: str) -> Optional[List[ModelPrediction]]:
        """ë¹„ë™ê¸° ëª¨ë¸ ìºì‹œ í™•ì¸"""
        return self._check_model_cache(cache_key)

    async def _save_to_model_cache_async(self, cache_key: str, result: List[ModelPrediction]) -> bool:
        """ë¹„ë™ê¸° ëª¨ë¸ ìºì‹œ ì €ì¥"""
        return self._save_to_model_cache(cache_key, result)

    def _check_model_cache(self, cache_key: str) -> Optional[List[ModelPrediction]]:
        """
        ğŸš€ ìŠ¤ë§ˆíŠ¸ ëª¨ë¸ ìºì‹œ í™•ì¸ (TTL ê¸°ë°˜)
        """
        if not self.smart_cache:
            return None

        try:
            # ìºì‹œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            if cache_key not in self.model_cache:
                return None

            # TTL í™•ì¸
            current_time = time.time()
            cache_time = self.cache_timestamps.get(cache_key, 0)
            
            if current_time - cache_time > self.opt_config.cache_ttl:
                # ë§Œë£Œëœ ìºì‹œ ì‚­ì œ
                self._remove_from_model_cache(cache_key)
                return None

            # ì ‘ê·¼ íšŸìˆ˜ ì¦ê°€
            self.cache_access_count[cache_key] = self.cache_access_count.get(cache_key, 0) + 1
            
            cached_result = self.model_cache[cache_key]
            logger.debug(f"ëª¨ë¸ ìºì‹œ íˆíŠ¸: {cache_key}")
            return cached_result

        except Exception as e:
            logger.warning(f"ëª¨ë¸ ìºì‹œ í™•ì¸ ì‹¤íŒ¨: {e}")
            return None

    def _save_to_model_cache(self, cache_key: str, result: List[ModelPrediction]) -> bool:
        """
        ğŸš€ ìŠ¤ë§ˆíŠ¸ ëª¨ë¸ ìºì‹œ ì €ì¥
        """
        if not self.smart_cache:
            return False

        try:
            # ìºì‹œ í¬ê¸° ê´€ë¦¬
            if len(self.model_cache) >= self.opt_config.max_cache_size:
                self._cleanup_model_cache()

            # ìºì‹œ ì €ì¥
            current_time = time.time()
            self.model_cache[cache_key] = result
            self.cache_timestamps[cache_key] = current_time
            self.cache_access_count[cache_key] = 1

            logger.debug(f"ëª¨ë¸ ìºì‹œ ì €ì¥: {cache_key}")
            return True

        except Exception as e:
            logger.warning(f"ëª¨ë¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    def _cleanup_model_cache(self):
        """
        ğŸš€ ìŠ¤ë§ˆíŠ¸ ëª¨ë¸ ìºì‹œ ì •ë¦¬ (LRU + TTL ê¸°ë°˜)
        """
        try:
            current_time = time.time()
            
            # 1ë‹¨ê³„: ë§Œë£Œëœ ìºì‹œ ì œê±°
            expired_keys = []
            for key, timestamp in self.cache_timestamps.items():
                if current_time - timestamp > self.opt_config.cache_ttl:
                    expired_keys.append(key)
            
            for key in expired_keys:
                self._remove_from_model_cache(key)
            
            # 2ë‹¨ê³„: ì—¬ì „íˆ í¬ê¸°ê°€ ì´ˆê³¼í•˜ë©´ LRU ì •ë¦¬
            if len(self.model_cache) >= self.opt_config.max_cache_size:
                # ì ‘ê·¼ íšŸìˆ˜ê°€ ë‚®ì€ ìˆœìœ¼ë¡œ ì •ë ¬
                sorted_by_access = sorted(
                    self.cache_access_count.items(),
                    key=lambda x: x[1]
                )
                
                # í•˜ìœ„ 25% ì œê±°
                remove_count = max(1, len(sorted_by_access) // 4)
                for key, _ in sorted_by_access[:remove_count]:
                    self._remove_from_model_cache(key)
            
            logger.info(f"ëª¨ë¸ ìºì‹œ ì •ë¦¬ ì™„ë£Œ: {len(self.model_cache)}ê°œ í•­ëª© ìœ ì§€")

        except Exception as e:
            logger.error(f"ëª¨ë¸ ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")

    def _remove_from_model_cache(self, cache_key: str):
        """ëª¨ë¸ ìºì‹œì—ì„œ í•­ëª© ì œê±°"""
        self.model_cache.pop(cache_key, None)
        self.cache_timestamps.pop(cache_key, None)
        self.cache_access_count.pop(cache_key, None)

    def get_optimization_status(self) -> Dict[str, Any]:
        """
        ğŸš€ í˜„ì¬ ìµœì í™” ìƒíƒœ ë°˜í™˜
        """
        return {
            "recommendation_engine_version": "2.0",
            "unified_system_available": self._unified_system_available,
            "optimization_config": {
                "async_prediction": self.opt_config.enable_async_prediction,
                "model_cache": self.opt_config.enable_model_cache,
                "auto_memory_management": self.opt_config.auto_memory_management,
                "max_concurrent_models": self.opt_config.max_concurrent_models,
            },
            "cache_stats": {
                "model_cache_size": len(self.model_cache) if self.smart_cache else 0,
                "cache_hit_ratio": self._calculate_cache_hit_ratio(),
            },
            "model_stats": {
                "loaded_models": list(self.models.keys()),
                "model_count": len(self.models),
                "model_weights": self.config["model_weights"],
            }
        }

    def _calculate_cache_hit_ratio(self) -> float:
        """ìºì‹œ íˆíŠ¸ìœ¨ ê³„ì‚°"""
        if self.smart_cache and self.cache_access_count:
            total_accesses = sum(self.cache_access_count.values())
            return len(self.cache_access_count) / max(total_accesses, 1)
        return 0.0

    def optimize_memory_usage(self):
        """
        ğŸš€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”
        """
        if self._unified_system_available and self.memory_mgr:
            self.memory_mgr.cleanup_unused_memory()
            logger.info("ğŸ§¹ í†µí•© ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ")
        
        # ìºì‹œ ì •ë¦¬
        if self.smart_cache:
            self._cleanup_model_cache()

    # âœ… ê¸°ì¡´ ë©”ì„œë“œë“¤ ìœ ì§€ (í•˜ìœ„ í˜¸í™˜ì„±)
    # ë‹¤ìŒ ë©”ì„œë“œë“¤ì€ ë³€ê²½ ì—†ì´ ìœ ì§€í•˜ë˜, ìƒˆë¡œìš´ ìµœì í™”ì˜ í˜œíƒì„ ìë™ìœ¼ë¡œ ë°›ìŠµë‹ˆë‹¤:
    # - _load_models
    # - _load_default_data
    # - _generate_random_recommendations
    # - _log_recommendations
    # - _apply_scoring
    # - run_pattern_analysis
    # - recommend (ê¸°ì¡´ ë™ê¸° ë²„ì „)
    # - _get_recommendations_by_strategy
    # - _hybrid_recommend (ê¸°ì¡´ ë™ê¸° ë²„ì „)
    # - _get_model_recommendations
    # - _hybrid_recommend_raw
    # - _recommend_with_model
    # - ... (ëª¨ë“  ê¸°ì¡´ ë©”ì„œë“œë“¤)
