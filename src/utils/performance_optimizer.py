# 1. í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import json
import os
import threading
import time
from dataclasses import asdict, dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional

# 2. ì„œë“œíŒŒí‹°
import psutil
import torch

# 3. í”„ë¡œì íŠ¸ ë‚´ë¶€ (ì—†ìŒ)

__all__ = [
    "MaxPerformanceConfig",
    "PerformanceMonitor",
    "MaxPerformanceOptimizer",
]


@dataclass
class MaxPerformanceConfig:
    """ìµœëŒ€ ì„±ëŠ¥ ì„¤ì • (ì ê·¹ì  ì ‘ê·¼)"""

    # CPU ìµœëŒ€ í™œìš© (7800X3D: 8ì½”ì–´ 16ìŠ¤ë ˆë“œ)
    max_workers: int = 12  # ë¬¼ë¦¬ ì½”ì–´ + í•˜ì´í¼ìŠ¤ë ˆë”© í™œìš©
    process_pool_size: int = 8  # í”„ë¡œì„¸ìŠ¤ í’€
    thread_pool_size: int = 16  # ëª¨ë“  ë…¼ë¦¬ ìŠ¤ë ˆë“œ í™œìš©
    chunk_size_factor: float = 0.5  # L3 ìºì‹œ ìµœì í™” (96MB / 2)

    # GPU ìµœëŒ€ í™œìš© (RTX 4090: 24GB VRAM)
    gpu_batch_size: int = 8192  # ëŒ€ìš©ëŸ‰ ë°°ì¹˜
    gpu_memory_fraction: float = 0.9  # 90% GPU ë©”ëª¨ë¦¬ í™œìš© (22GB)
    gpu_streams: int = 8  # ë‹¤ì¤‘ CUDA ìŠ¤íŠ¸ë¦¼
    tensor_cores_enabled: bool = True  # Tensor Core ì™„ì „ í™œìš©

    # RAM ìµœëŒ€ í™œìš© (32GB)
    memory_limit_gb: int = 24  # 24GB í™œìš©
    cache_size_gb: int = 8  # 8GB ëŒ€ìš©ëŸ‰ ìºì‹œ
    buffer_size_gb: int = 4  # 4GB ë²„í¼
    memory_mapping_enabled: bool = True  # ë©”ëª¨ë¦¬ ë§¤í•‘ í™œìš©

    # ëª¨ë‹ˆí„°ë§ & ìë™ ì¡°ì ˆ ì„¤ì •
    monitoring_enabled: bool = True
    auto_adjustment: bool = True
    adjustment_interval: float = 10.0  # 10ì´ˆë§ˆë‹¤ ì²´í¬

    # ì„ê³„ê°’ (ì´ ìˆ˜ì¹˜ ì´ˆê³¼ ì‹œ ìë™ ë‹¤ìš´ê·¸ë ˆì´ë“œ)
    cpu_usage_threshold: float = 90.0  # CPU 90% ì´ˆê³¼ ì‹œ
    memory_usage_threshold: float = 85.0  # ë©”ëª¨ë¦¬ 85% ì´ˆê³¼ ì‹œ
    gpu_memory_threshold: float = 95.0  # GPU ë©”ëª¨ë¦¬ 95% ì´ˆê³¼ ì‹œ
    temperature_threshold: int = 80  # GPU ì˜¨ë„ 80Â°C ì´ˆê³¼ ì‹œ


class PerformanceMonitor:
    """ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ & ìë™ ì¡°ì ˆ"""

    def __init__(self, config: MaxPerformanceConfig):
        self.config = config
        self.monitoring: bool = False
        self.monitor_thread: Optional[threading.Thread] = None
        self.adjustment_history: List[Dict[str, Any]] = []
        self.current_settings: Dict[str, Any] = asdict(config)
        self.lock = threading.RLock()

        # ì¡°ì ˆ ë‹¨ê³„ë³„ ì„¤ì •
        self.adjustment_levels: Dict[str, Dict[str, Any]] = {
            "level_0": {  # ìµœëŒ€ ì„±ëŠ¥ (ê¸°ë³¸)
                "max_workers": 12,
                "gpu_batch_size": 8192,
                "memory_limit_gb": 24,
                "gpu_memory_fraction": 0.9,
            },
            "level_1": {  # 90% ì„±ëŠ¥
                "max_workers": 10,
                "gpu_batch_size": 6144,
                "memory_limit_gb": 20,
                "gpu_memory_fraction": 0.8,
            },
            "level_2": {  # 75% ì„±ëŠ¥
                "max_workers": 8,
                "gpu_batch_size": 4096,
                "memory_limit_gb": 16,
                "gpu_memory_fraction": 0.7,
            },
            "level_3": {  # ì•ˆì „ ëª¨ë“œ (50% ì„±ëŠ¥)
                "max_workers": 6,
                "gpu_batch_size": 2048,
                "memory_limit_gb": 12,
                "gpu_memory_fraction": 0.6,
            },
        }

        self.current_level: str = "level_0"

    # ---------------------------------------------------------------------
    # public helpers
    # ---------------------------------------------------------------------

    def start_monitoring(self) -> None:
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        if self.monitoring:
            return

        self.monitoring = True
        self.monitor_thread = threading.Thread(
            target=self._monitor_loop,
            daemon=True,
            name="MaxPerformanceMonitor",
        )
        self.monitor_thread.start()
        print("ğŸš€ ìµœëŒ€ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘ - ì‹¤ì‹œê°„ ìë™ ì¡°ì ˆ í™œì„±í™”")

    def stop_monitoring(self) -> None:
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=2.0)
        print("ğŸ“Š ëª¨ë‹ˆí„°ë§ ì¤‘ì§€")

    def get_current_settings(self) -> Dict[str, Any]:
        """í˜„ì¬ ì„±ëŠ¥ ì„¤ì • ë°˜í™˜"""
        with self.lock:
            return {
                "level": self.current_level,
                "max_workers": self.current_settings["max_workers"],
                "gpu_batch_size": self.current_settings["gpu_batch_size"],
                "memory_limit_gb": self.current_settings["memory_limit_gb"],
                "gpu_memory_fraction": self.current_settings["gpu_memory_fraction"],
            }

    # ---------------------------------------------------------------------
    # monitor loop
    # ---------------------------------------------------------------------

    def _monitor_loop(self) -> None:
        consecutive_high_usage = 0

        while self.monitoring:
            try:
                metrics = self._collect_metrics()
                issues = self._detect_issues(metrics)

                if issues:
                    consecutive_high_usage += 1
                    print(
                        f"âš ï¸ ì‹œìŠ¤í…œ ë¶€í•˜ ê°ì§€: {', '.join(issues)} (ì—°ì† {consecutive_high_usage}íšŒ)"
                    )

                    # 3íšŒ ì—°ì† ë¬¸ì œ ë°œìƒ ì‹œ ìë™ ì¡°ì ˆ
                    if consecutive_high_usage >= 3 and self.config.auto_adjustment:
                        self._auto_adjust_down()
                        consecutive_high_usage = 0
                else:
                    consecutive_high_usage = 0
                    # ì‹œìŠ¤í…œì´ ì•ˆì •ì ì´ë©´ ì ì§„ì  ì„±ëŠ¥ í–¥ìƒ ì‹œë„
                    if consecutive_high_usage == 0:
                        self._try_performance_upgrade()

                self._log_metrics(metrics, issues)
                time.sleep(self.config.adjustment_interval)

            except Exception as exc:  # pylint: disable=broad-except
                print(f"âŒ ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {exc}")
                time.sleep(30)

    # ---------------------------------------------------------------------
    # metrics helpers
    # ---------------------------------------------------------------------

    def _collect_metrics(self) -> Dict[str, float]:
        metrics: Dict[str, float] = {}

        # CPU ë©”íŠ¸ë¦­
        metrics["cpu_percent"] = psutil.cpu_percent(interval=1)
        metrics["cpu_count"] = psutil.cpu_count()

        # ë©”ëª¨ë¦¬ ë©”íŠ¸ë¦­
        memory = psutil.virtual_memory()
        metrics["memory_percent"] = memory.percent
        metrics["memory_available_gb"] = memory.available / (1024**3)

        # GPU ë©”íŠ¸ë¦­ (ê°€ëŠ¥í•œ ê²½ìš°)
        if torch.cuda.is_available():
            metrics["gpu_memory_allocated"] = torch.cuda.memory_allocated() / (1024**3)
            metrics["gpu_memory_reserved"] = torch.cuda.memory_reserved() / (1024**3)
            total_gpu_memory = torch.cuda.get_device_properties(0).total_memory / (
                1024**3
            )
            metrics["gpu_memory_percent"] = (
                metrics["gpu_memory_allocated"] / total_gpu_memory * 100.0
            )

            # GPU ì˜¨ë„
            try:
                import pynvml  # type: ignore

                pynvml.nvmlInit()
                handle = pynvml.nvmlDeviceGetHandleByIndex(0)
                metrics["gpu_temperature"] = pynvml.nvmlDeviceGetTemperature(
                    handle, pynvml.NVML_TEMPERATURE_GPU
                )
                metrics["gpu_utilization"] = pynvml.nvmlDeviceGetUtilizationRates(
                    handle
                ).gpu
            except Exception:  # pylint: disable=broad-except
                metrics["gpu_temperature"] = 0
                metrics["gpu_utilization"] = 0

        metrics["timestamp"] = time.time()
        return metrics

    def _detect_issues(self, metrics: Dict[str, float]) -> List[str]:
        issues: List[str] = []

        if metrics["cpu_percent"] > self.config.cpu_usage_threshold:
            issues.append(f"CPU ê³¼ë¶€í•˜ {metrics['cpu_percent']:.1f}%")

        if metrics["memory_percent"] > self.config.memory_usage_threshold:
            issues.append(f"ë©”ëª¨ë¦¬ ë¶€ì¡± {metrics['memory_percent']:.1f}%")

        if metrics.get("gpu_memory_percent", 0) > self.config.gpu_memory_threshold:
            issues.append(f"GPU ë©”ëª¨ë¦¬ ë¶€ì¡± {metrics['gpu_memory_percent']:.1f}%")

        if metrics.get("gpu_temperature", 0) > self.config.temperature_threshold:
            issues.append(f"GPU ê³¼ì—´ {metrics['gpu_temperature']}Â°C")

        return issues

    # ---------------------------------------------------------------------
    # adjustment helpers
    # ---------------------------------------------------------------------

    def _auto_adjust_down(self) -> None:
        with self.lock:
            current_idx = list(self.adjustment_levels).index(self.current_level)
            if current_idx < len(self.adjustment_levels) - 1:
                new_level = list(self.adjustment_levels)[current_idx + 1]
                old_level = self.current_level
                self.current_level = new_level

                new_settings = self.adjustment_levels[new_level]
                self.current_settings.update(new_settings)

                adjustment = {
                    "timestamp": time.time(),
                    "action": "downgrade",
                    "from_level": old_level,
                    "to_level": new_level,
                    "settings": new_settings.copy(),
                }
                self.adjustment_history.append(adjustment)

                print(
                    f"ğŸ”½ ì„±ëŠ¥ í•˜í–¥ ì¡°ì ˆ: {old_level} â†’ {new_level}\n"
                    f"   ì›Œì»¤: {new_settings['max_workers']}, "
                    f"ë°°ì¹˜: {new_settings['gpu_batch_size']}"
                )

                self._save_current_settings()
            else:
                print("âš ï¸ ì´ë¯¸ ìµœì € ì„±ëŠ¥ ë‹¨ê³„ì…ë‹ˆë‹¤")

    def _try_performance_upgrade(self) -> None:
        with self.lock:
            recent_adjustments = [
                adj
                for adj in self.adjustment_history
                if time.time() - adj["timestamp"] < 1800
            ]
            if any(adj["action"] == "downgrade" for adj in recent_adjustments):
                return

            current_idx = list(self.adjustment_levels).index(self.current_level)
            if current_idx > 0:
                new_level = list(self.adjustment_levels)[current_idx - 1]
                old_level = self.current_level
                self.current_level = new_level

                new_settings = self.adjustment_levels[new_level]
                self.current_settings.update(new_settings)

                adjustment = {
                    "timestamp": time.time(),
                    "action": "upgrade",
                    "from_level": old_level,
                    "to_level": new_level,
                    "settings": new_settings.copy(),
                }
                self.adjustment_history.append(adjustment)

                print(
                    f"ğŸ”¼ ì„±ëŠ¥ ìƒí–¥ ì¡°ì ˆ: {old_level} â†’ {new_level}\n"
                    f"   ì›Œì»¤: {new_settings['max_workers']}, "
                    f"ë°°ì¹˜: {new_settings['gpu_batch_size']}"
                )

                self._save_current_settings()

    # ---------------------------------------------------------------------
    # misc helpers
    # ---------------------------------------------------------------------

    def _log_metrics(self, metrics: Dict[str, float], issues: List[str]) -> None:
        if int(time.time()) % 600 == 0:  # 10ë¶„ë§ˆë‹¤ ì¶œë ¥
            print(
                f"ğŸ“Š ì‹œìŠ¤í…œ ìƒíƒœ - CPU: {metrics['cpu_percent']:.1f}%, "
                f"RAM: {metrics['memory_percent']:.1f}%, "
                f"GPU: {metrics.get('gpu_memory_percent', 0):.1f}%"
                f"{', ì˜¨ë„: ' + str(int(metrics.get('gpu_temperature', 0))) + 'Â°C' if metrics.get('gpu_temperature', 0) > 0 else ''}"
            )

    def _save_current_settings(self) -> None:
        try:
            settings_file = Path("data/cache/current_performance_settings.json")
            settings_file.parent.mkdir(parents=True, exist_ok=True)

            save_data: Dict[str, Any] = {
                "current_level": self.current_level,
                "settings": self.current_settings,
                "last_updated": time.time(),
                "adjustment_history": self.adjustment_history[-10:],
            }
            settings_file.write_text(
                json.dumps(save_data, indent=2, ensure_ascii=False),
                encoding="utf-8",
            )
        except Exception as exc:  # pylint: disable=broad-except
            print(f"âš ï¸ ì„¤ì • ì €ì¥ ì‹¤íŒ¨: {exc}")


class MaxPerformanceOptimizer:
    """ìµœëŒ€ ì„±ëŠ¥ ìµœì í™” ë§¤ë‹ˆì €"""

    def __init__(self) -> None:
        self.config = MaxPerformanceConfig()
        self.monitor = PerformanceMonitor(self.config)
        self.initialized = False

    # ------------------------------------------------------------------
    # public api
    # ------------------------------------------------------------------

    def initialize(self) -> bool:
        """ìµœëŒ€ ì„±ëŠ¥ ì´ˆê¸°í™”"""
        try:
            print("ğŸš€ ìµœëŒ€ ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™”...")
            print("âš¡ 7800X3D + RTX 4090 + 32GB RAM í’€íŒŒì›Œ ëª¨ë“œ!")

            self._apply_max_performance_settings()

            if self.config.monitoring_enabled:
                self.monitor.start_monitoring()

            self.initialized = True
            self._print_max_performance_summary()

            print("âœ… ìµœëŒ€ ì„±ëŠ¥ ì‹œìŠ¤í…œ ê°€ë™ ì™„ë£Œ!")
            print("ğŸ” ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì¤‘... ë¬¸ì œ ë°œìƒ ì‹œ ìë™ìœ¼ë¡œ ì¡°ì ˆë©ë‹ˆë‹¤.")
            return True
        except Exception as exc:  # pylint: disable=broad-except
            print(f"âŒ ìµœëŒ€ ì„±ëŠ¥ ì´ˆê¸°í™” ì‹¤íŒ¨: {exc}")
            return False

    def get_current_performance_config(self) -> Dict[str, Any]:
        if not self.initialized:
            return {}

        settings = self.monitor.get_current_settings()
        settings.update(
            {
                "gpu_streams": self.config.gpu_streams,
                "memory_mapping": self.config.memory_mapping_enabled,
                "tensor_cores": self.config.tensor_cores_enabled,
                "monitoring": self.config.monitoring_enabled,
            }
        )
        return settings

    def force_performance_level(self, level: str) -> None:
        if level in self.monitor.adjustment_levels:
            self.monitor.current_level = level
            self.monitor.current_settings.update(self.monitor.adjustment_levels[level])
            print(f"ğŸ”§ ì„±ëŠ¥ ë‹¨ê³„ ê°•ì œ ì„¤ì •: {level}")
            self.monitor._save_current_settings()  # pylint: disable=protected-access
        else:
            print(f"âŒ ì˜ëª»ëœ ì„±ëŠ¥ ë‹¨ê³„: {level}")

    def get_performance_report(self) -> Dict[str, Any]:
        return {
            "current_level": self.monitor.current_level,
            "current_settings": self.monitor.get_current_settings(),
            "adjustment_history": self.monitor.adjustment_history[-5:],
            "available_levels": list(self.monitor.adjustment_levels.keys()),
            "monitoring_active": self.config.monitoring_enabled,
        }

    def cleanup(self) -> None:
        if self.config.monitoring_enabled:
            self.monitor.stop_monitoring()
        print("ğŸš€ ìµœëŒ€ ì„±ëŠ¥ ì‹œìŠ¤í…œ ì •ë¦¬ ì™„ë£Œ")

    # ------------------------------------------------------------------
    # internal helpers
    # ------------------------------------------------------------------

    def _apply_max_performance_settings(self) -> None:
        os.environ["OMP_NUM_THREADS"] = str(self.config.thread_pool_size)
        os.environ["MKL_NUM_THREADS"] = str(self.config.thread_pool_size)
        os.environ["NUMEXPR_NUM_THREADS"] = str(self.config.thread_pool_size)

        if torch.cuda.is_available():
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True

            if self.config.tensor_cores_enabled:
                torch.backends.cuda.enable_flash_sdp(True)  # type: ignore[attr-defined]

            torch.cuda.set_per_process_memory_fraction(self.config.gpu_memory_fraction)

            os.environ["PYTORCH_CUDA_ALLOC_CONF"] = (
                "max_split_size_mb:2048,"
                "expandable_segments:True,"
                "roundup_power2_divisions:8"
            )

            self._warmup_gpu()

    def _warmup_gpu(self) -> None:
        try:
            print("ğŸ”¥ GPU ì›Œë°ì—… ì¤‘...")
            dummy = torch.randn(2048, 2048, device="cuda")
            for _ in range(5):
                _ = torch.mm(dummy, dummy)
            torch.cuda.synchronize()
            del dummy  # noqa: F821
            print("âœ… GPU ì›Œë°ì—… ì™„ë£Œ")
        except Exception as exc:  # pylint: disable=broad-except
            print(f"âš ï¸ GPU ì›Œë°ì—… ì‹¤íŒ¨: {exc}")

    def _print_max_performance_summary(self) -> None:
        print("\nğŸš€ ìµœëŒ€ ì„±ëŠ¥ ì„¤ì • ì ìš©:")
        print("â”Œâ”€ CPU ìµœì í™” (7800X3D)")
        print(f"â”œâ”€â”€ ì›Œì»¤ ìˆ˜: {self.config.max_workers}ê°œ (ë¬¼ë¦¬ 8ì½”ì–´ + í•˜ì´í¼ìŠ¤ë ˆë”©)")
        print(f"â”œâ”€â”€ í”„ë¡œì„¸ìŠ¤ í’€: {self.config.process_pool_size}ê°œ")
        print(f"â”œâ”€â”€ ìŠ¤ë ˆë“œ í’€: {self.config.thread_pool_size}ê°œ")
        print("â””â”€â”€ L3 ìºì‹œ: 96MB ìµœì í™”")
        print("â”Œâ”€ GPU ìµœì í™” (RTX 4090)")
        print(f"â”œâ”€â”€ ë°°ì¹˜ í¬ê¸°: {self.config.gpu_batch_size:,}ê°œ (4ë°° ì¦ê°€)")
        print(
            f"â”œâ”€â”€ GPU ë©”ëª¨ë¦¬: {self.config.gpu_memory_fraction*100:.0f}% í™œìš© (~22GB)"
        )
        print(f"â”œâ”€â”€ CUDA ìŠ¤íŠ¸ë¦¼: {self.config.gpu_streams}ê°œ")
        print(
            f"â””â”€â”€ Tensor Core: {'í™œì„±í™”' if self.config.tensor_cores_enabled else 'ë¹„í™œì„±í™”'}"
        )
        print("â”Œâ”€ RAM ìµœì í™” (32GB)")
        print(f"â”œâ”€â”€ ë©”ëª¨ë¦¬ í•œê³„: {self.config.memory_limit_gb}GB")
        print(f"â”œâ”€â”€ ìºì‹œ í¬ê¸°: {self.config.cache_size_gb}GB")
        print(f"â”œâ”€â”€ ë²„í¼ í¬ê¸°: {self.config.buffer_size_gb}GB")
        print(
            f"â””â”€â”€ ë©”ëª¨ë¦¬ ë§¤í•‘: {'í™œì„±í™”' if self.config.memory_mapping_enabled else 'ë¹„í™œì„±í™”'}"
        )
        print("â””â”€ ëª¨ë‹ˆí„°ë§: ì‹¤ì‹œê°„ ìë™ ì¡°ì ˆ í™œì„±í™”")


# =============================================================================
# ì‚¬ìš© ì˜ˆì‹œ (ì§ì ‘ ì‹¤í–‰ ì‹œ)
# =============================================================================


def launch_max_performance() -> Optional[MaxPerformanceOptimizer]:
    optimizer = MaxPerformanceOptimizer()
    if not optimizer.initialize():
        print("âŒ ìµœëŒ€ ì„±ëŠ¥ ëª¨ë“œ ì‹œì‘ ì‹¤íŒ¨")
        return None

    print("\nğŸ¯ ì¶”ì²œ ì‚¬ìš©ë²•:")
    print("1. ì²˜ìŒ 10ë¶„ê°„ ì‹œìŠ¤í…œ ìƒíƒœ ê´€ì°°")
    print("2. ë¬¸ì œ ë°œìƒ ì‹œ ìë™ìœ¼ë¡œ ì„±ëŠ¥ ì¡°ì ˆë¨")
    print("3. ìˆ˜ë™ ì¡°ì ˆ: optimizer.force_performance_level('level_1')")
    print("4. ìƒíƒœ í™•ì¸: optimizer.get_current_performance_config()")
    return optimizer


if __name__ == "__main__":
    OPTIMIZER = launch_max_performance()
    if OPTIMIZER:
        try:
            print("\nâ³ 30ì´ˆê°„ ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§...")
            time.sleep(30)
            current_cfg = OPTIMIZER.get_current_performance_config()
            print("\nğŸ“Š í˜„ì¬ ì„±ëŠ¥ ì„¤ì •:")
            print(f"   ì„±ëŠ¥ ë ˆë²¨: {current_cfg.get('level', 'unknown')}")
            print(f"   ì›Œì»¤ ìˆ˜: {current_cfg.get('max_workers', 0)}ê°œ")
            print(f"   ë°°ì¹˜ í¬ê¸°: {current_cfg.get('gpu_batch_size', 0):,}ê°œ")
            print(f"   ë©”ëª¨ë¦¬ í•œê³„: {current_cfg.get('memory_limit_gb', 0)}GB")
        finally:
            OPTIMIZER.cleanup()
