"""
í†µí•© ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
ëª¨ë“  ìƒˆë¡œìš´ ì‹œìŠ¤í…œë“¤ì˜ í†µí•© í…ŒìŠ¤íŠ¸
"""

import asyncio
import torch
import numpy as np
import time
import threading
from typing import Dict, Any, List

from .unified_logging import get_logger
from .unified_config import get_config
from .compute_strategy import (
    get_compute_executor,
    get_optimal_compute_selector,
    smart_execute,
    TaskType,
    ComputeStrategy,
)
from .unified_memory_manager import (
    get_unified_memory_manager,
    smart_allocate,
    get_memory_status,
    DeviceType,
)
from .unified_async_manager import (
    get_unified_async_manager,
    async_read_file,
    async_write_file,
    async_cache_get,
    async_cache_set,
)
from .auto_recovery_system import (
    get_auto_recovery_system,
    auto_recoverable,
    start_auto_recovery,
    stop_auto_recovery,
    RecoveryStrategy,
)
from .dependency_container import get_container, register, resolve

logger = get_logger(__name__)


class IntegratedSystemTester:
    """í†µí•© ì‹œìŠ¤í…œ í…ŒìŠ¤í„°"""

    def __init__(self):
        self.test_results: Dict[str, Dict[str, Any]] = {}
        self.logger = logger

    def run_all_tests(self) -> Dict[str, Dict[str, Any]]:
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        self.logger.info("ğŸš€ í†µí•© ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘")

        # 1. GPU ì—°ì‚° ìš°ì„ ìˆœìœ„ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
        self.test_compute_strategy_system()

        # 2. í†µí•© ë©”ëª¨ë¦¬ ê´€ë¦¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
        self.test_unified_memory_system()

        # 3. ë¹„ë™ê¸° ì²˜ë¦¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
        asyncio.run(self.test_async_system())

        # 4. ìë™ ë³µêµ¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
        self.test_auto_recovery_system()

        # 5. ì˜ì¡´ì„± ì£¼ì… ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
        self.test_dependency_injection_system()

        # 6. í†µí•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        self.test_integrated_performance()

        self.logger.info("âœ… í†µí•© ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        return self.test_results

    def test_compute_strategy_system(self):
        """GPU ì—°ì‚° ìš°ì„ ìˆœìœ„ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
        self.logger.info("ğŸ”§ GPU ì—°ì‚° ìš°ì„ ìˆœìœ„ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")

        test_result = {
            "gpu_available": torch.cuda.is_available(),
            "strategy_selection": {},
            "smart_execution": {},
            "performance": {},
        }

        try:
            # 1. ì „ëµ ì„ íƒ í…ŒìŠ¤íŠ¸
            selector = get_optimal_compute_selector()

            # ì‘ì€ ë°ì´í„° (CPU ì„ íƒ ì˜ˆìƒ)
            small_data = np.random.rand(100)
            small_size = small_data.nbytes

            # í° ë°ì´í„° (GPU ì„ íƒ ì˜ˆìƒ)
            large_data = np.random.rand(10000, 100)
            large_size = large_data.nbytes

            test_result["strategy_selection"] = {
                "small_data_size": small_size,
                "large_data_size": large_size,
                "gpu_available": torch.cuda.is_available(),
            }

            # 2. ìŠ¤ë§ˆíŠ¸ ì‹¤í–‰ í…ŒìŠ¤íŠ¸
            def test_function(data):
                return np.sum(data)

            start_time = time.time()
            result_small = smart_execute(
                test_function, small_data, TaskType.DATA_PROCESSING
            )
            small_time = time.time() - start_time

            start_time = time.time()
            result_large = smart_execute(
                test_function, large_data, TaskType.DATA_PROCESSING
            )
            large_time = time.time() - start_time

            test_result["smart_execution"] = {
                "small_data_result": float(result_small),
                "large_data_result": float(result_large),
                "small_data_time": small_time,
                "large_data_time": large_time,
            }

            # 3. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
            executor = get_compute_executor()

            # í…ì„œ ì—°ì‚° í…ŒìŠ¤íŠ¸
            if torch.cuda.is_available():
                tensor_data = torch.randn(1000, 1000)

                def tensor_operation(data):
                    return torch.mm(data, data.T)

                start_time = time.time()
                tensor_result = executor.execute(
                    tensor_operation, tensor_data, task_type=TaskType.TENSOR_COMPUTATION
                )
                tensor_time = time.time() - start_time

                test_result["performance"]["tensor_operation_time"] = tensor_time
                test_result["performance"]["tensor_result_shape"] = list(
                    tensor_result.shape
                )

            test_result["status"] = "success"

        except Exception as e:
            test_result["status"] = "error"
            test_result["error"] = str(e)
            self.logger.error(f"GPU ì—°ì‚° ìš°ì„ ìˆœìœ„ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

        self.test_results["compute_strategy"] = test_result
        self.logger.info(
            f"âœ… GPU ì—°ì‚° ìš°ì„ ìˆœìœ„ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {test_result['status']}"
        )

    def test_unified_memory_system(self):
        """í†µí•© ë©”ëª¨ë¦¬ ê´€ë¦¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
        self.logger.info("ğŸ§  í†µí•© ë©”ëª¨ë¦¬ ê´€ë¦¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")

        test_result = {
            "memory_allocation": {},
            "memory_status": {},
            "memory_cleanup": {},
        }

        try:
            memory_manager = get_unified_memory_manager()

            # 1. ë©”ëª¨ë¦¬ í• ë‹¹ í…ŒìŠ¤íŠ¸
            tensor1, alloc_id1 = smart_allocate(1000, torch.float32, DeviceType.CPU)
            tensor2, alloc_id2 = smart_allocate(2000, torch.float32, DeviceType.GPU)

            test_result["memory_allocation"] = {
                "cpu_allocation": {
                    "id": alloc_id1,
                    "size": tensor1.numel(),
                    "device": str(tensor1.device),
                },
                "gpu_allocation": {
                    "id": alloc_id2,
                    "size": tensor2.numel() if tensor2 is not None else 0,
                    "device": str(tensor2.device) if tensor2 is not None else "none",
                },
            }

            # 2. ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
            memory_status = get_memory_status()
            test_result["memory_status"] = memory_status

            # 3. ë©”ëª¨ë¦¬ í•´ì œ í…ŒìŠ¤íŠ¸
            cleanup_success1 = memory_manager.deallocate(alloc_id1)
            cleanup_success2 = memory_manager.deallocate(alloc_id2)

            test_result["memory_cleanup"] = {
                "cpu_cleanup": cleanup_success1,
                "gpu_cleanup": cleanup_success2,
            }

            # 4. ì„ì‹œ ë©”ëª¨ë¦¬ í• ë‹¹ í…ŒìŠ¤íŠ¸
            with memory_manager.temporary_allocation(500, torch.float32) as temp_tensor:
                test_result["temporary_allocation"] = {
                    "size": temp_tensor.numel(),
                    "device": str(temp_tensor.device),
                }

            test_result["status"] = "success"

        except Exception as e:
            test_result["status"] = "error"
            test_result["error"] = str(e)
            self.logger.error(f"í†µí•© ë©”ëª¨ë¦¬ ê´€ë¦¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

        self.test_results["unified_memory"] = test_result
        self.logger.info(
            f"âœ… í†µí•© ë©”ëª¨ë¦¬ ê´€ë¦¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {test_result['status']}"
        )

    async def test_async_system(self):
        """ë¹„ë™ê¸° ì²˜ë¦¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
        self.logger.info("âš¡ ë¹„ë™ê¸° ì²˜ë¦¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")

        test_result = {
            "file_operations": {},
            "cache_operations": {},
            "task_management": {},
        }

        try:
            async_manager = get_unified_async_manager()

            # 1. ë¹„ë™ê¸° íŒŒì¼ ì‘ì—… í…ŒìŠ¤íŠ¸
            test_content = "ë¹„ë™ê¸° íŒŒì¼ í…ŒìŠ¤íŠ¸ ë‚´ìš©"
            test_file = "test_async_file.txt"

            # íŒŒì¼ ì“°ê¸°
            await async_write_file(test_file, test_content)

            # íŒŒì¼ ì½ê¸°
            read_content = await async_read_file(test_file)

            test_result["file_operations"] = {
                "write_success": True,
                "read_success": read_content == test_content,
                "content_match": read_content == test_content,
            }

            # 2. ë¹„ë™ê¸° ìºì‹œ ì‘ì—… í…ŒìŠ¤íŠ¸
            cache_key = "test_cache_key"
            cache_value = {"data": "test_value", "timestamp": time.time()}

            # ìºì‹œ ì €ì¥
            await async_cache_set(cache_key, cache_value)

            # ìºì‹œ ì¡°íšŒ
            cached_data = await async_cache_get(cache_key)

            test_result["cache_operations"] = {
                "cache_set_success": True,
                "cache_get_success": cached_data is not None,
                "cache_data_match": (
                    cached_data == cache_value if cached_data else False
                ),
            }

            # 3. ë¹„ë™ê¸° ì‘ì—… ê´€ë¦¬ í…ŒìŠ¤íŠ¸
            async def test_async_task():
                await asyncio.sleep(0.1)
                return "async_task_result"

            # ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ ë‚´ì—ì„œ ì‘ì—… ì‹¤í–‰
            async with async_manager.session():
                task_id = await async_manager.submit_task(test_async_task())
                # ì‘ì—… ì™„ë£Œ ëŒ€ê¸° (ê°„ë‹¨í•œ êµ¬í˜„)
                await asyncio.sleep(0.2)

            test_result["task_management"] = {
                "task_submission": task_id is not None,
                "task_id": task_id,
            }

            # íŒŒì¼ ì •ë¦¬
            try:
                import os

                os.remove(test_file)
            except:
                pass

            test_result["status"] = "success"

        except Exception as e:
            test_result["status"] = "error"
            test_result["error"] = str(e)
            self.logger.error(f"ë¹„ë™ê¸° ì²˜ë¦¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

        self.test_results["async_system"] = test_result
        self.logger.info(f"âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {test_result['status']}")

    def test_auto_recovery_system(self):
        """ìë™ ë³µêµ¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
        self.logger.info("ğŸ”„ ìë™ ë³µêµ¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")

        test_result = {"system_start": {}, "error_handling": {}, "recovery_stats": {}}

        try:
            recovery_system = get_auto_recovery_system()

            # 1. ì‹œìŠ¤í…œ ì‹œì‘ í…ŒìŠ¤íŠ¸
            start_auto_recovery()
            test_result["system_start"]["started"] = True

            # 2. ì—ëŸ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
            test_errors = [
                RuntimeError("CUDA out of memory"),
                MemoryError("Memory allocation failed"),
                TimeoutError("Operation timed out"),
            ]

            recovery_results = []
            for error in test_errors:
                recovery_context = recovery_system.handle_error(error, {"test": True})
                recovery_results.append(
                    {
                        "error_type": recovery_context.error_type,
                        "strategy": recovery_context.strategy.value,
                        "success": recovery_context.success,
                        "recovery_time": recovery_context.recovery_time,
                        "attempted_actions": [
                            action.value
                            for action in recovery_context.attempted_actions
                        ],
                    }
                )

            test_result["error_handling"] = recovery_results

            # 3. ë³µêµ¬ í†µê³„ í…ŒìŠ¤íŠ¸
            recovery_stats = recovery_system.get_recovery_stats()
            test_result["recovery_stats"] = recovery_stats

            # 4. ìë™ ë³µêµ¬ ë°ì½”ë ˆì´í„° í…ŒìŠ¤íŠ¸
            @auto_recoverable(strategy=RecoveryStrategy.GPU_OOM)
            def test_recoverable_function():
                # ì˜ë„ì ìœ¼ë¡œ ì˜ˆì™¸ ë°œìƒ
                if torch.cuda.is_available():
                    # GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œë®¬ë ˆì´ì…˜
                    large_tensor = torch.randn(10000, 10000, device="cuda")
                    return large_tensor.sum()
                else:
                    return 42

            try:
                result = test_recoverable_function()
                test_result["decorator_test"] = {
                    "success": True,
                    "result": float(result),
                }
            except Exception as e:
                test_result["decorator_test"] = {"success": False, "error": str(e)}

            # ì‹œìŠ¤í…œ ì •ì§€
            stop_auto_recovery()

            test_result["status"] = "success"

        except Exception as e:
            test_result["status"] = "error"
            test_result["error"] = str(e)
            self.logger.error(f"ìë™ ë³µêµ¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

        self.test_results["auto_recovery"] = test_result
        self.logger.info(f"âœ… ìë™ ë³µêµ¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {test_result['status']}")

    def test_dependency_injection_system(self):
        """ì˜ì¡´ì„± ì£¼ì… ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
        self.logger.info("ğŸ”— ì˜ì¡´ì„± ì£¼ì… ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")

        test_result = {
            "container_operations": {},
            "dependency_resolution": {},
            "circular_dependency_detection": {},
        }

        try:
            container = get_container()

            # 1. ì»¨í…Œì´ë„ˆ ê¸°ë³¸ ë™ì‘ í…ŒìŠ¤íŠ¸
            class TestService:
                def __init__(self):
                    self.value = "test_service"

                def get_value(self):
                    return self.value

            # ì„œë¹„ìŠ¤ ë“±ë¡
            register(TestService)

            # ì„œë¹„ìŠ¤ í•´ê²°
            service = resolve(TestService)

            test_result["container_operations"] = {
                "registration": True,
                "resolution": service is not None,
                "service_value": service.get_value() if service else None,
            }

            # 2. ì˜ì¡´ì„± í•´ê²° í…ŒìŠ¤íŠ¸
            class DependentService:
                def __init__(self, test_service: TestService):
                    self.test_service = test_service

                def get_dependent_value(self):
                    return f"dependent_{self.test_service.get_value()}"

            register(DependentService)
            dependent_service = resolve(DependentService)

            test_result["dependency_resolution"] = {
                "dependent_service_created": dependent_service is not None,
                "dependency_injected": (
                    dependent_service.get_dependent_value()
                    if dependent_service
                    else None
                ),
            }

            # 3. ìˆœí™˜ ì°¸ì¡° íƒì§€ í…ŒìŠ¤íŠ¸
            cycles = container.detect_circular_dependencies()
            test_result["circular_dependency_detection"] = {
                "cycles_detected": len(cycles),
                "cycles": cycles,
            }

            test_result["status"] = "success"

        except Exception as e:
            test_result["status"] = "error"
            test_result["error"] = str(e)
            self.logger.error(f"ì˜ì¡´ì„± ì£¼ì… ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

        self.test_results["dependency_injection"] = test_result
        self.logger.info(f"âœ… ì˜ì¡´ì„± ì£¼ì… ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {test_result['status']}")

    def test_integrated_performance(self):
        """í†µí•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        self.logger.info("ğŸï¸ í†µí•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")

        test_result = {
            "system_integration": {},
            "performance_metrics": {},
            "resource_usage": {},
        }

        try:
            # 1. ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸
            memory_manager = get_unified_memory_manager()
            compute_executor = get_compute_executor()

            # ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
            large_data = np.random.rand(5000, 1000)

            def complex_operation(data):
                # ë³µì¡í•œ ì—°ì‚° ì‹œë®¬ë ˆì´ì…˜
                result = np.dot(data, data.T)
                return np.sum(result)

            # ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
            memory_status_before = get_memory_status()

            # í†µí•© ì‹¤í–‰
            start_time = time.time()
            with memory_manager.temporary_allocation(
                large_data.nbytes, torch.float32
            ) as temp_tensor:
                result = smart_execute(
                    complex_operation, large_data, TaskType.DATA_PROCESSING
                )
            execution_time = time.time() - start_time

            # ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
            memory_status_after = get_memory_status()

            test_result["system_integration"] = {
                "execution_success": True,
                "result": float(result),
                "execution_time": execution_time,
            }

            # 2. ì„±ëŠ¥ ë©”íŠ¸ë¦­
            test_result["performance_metrics"] = {
                "data_size_mb": large_data.nbytes / (1024 * 1024),
                "throughput_mb_per_sec": (large_data.nbytes / (1024 * 1024))
                / execution_time,
                "memory_efficiency": (
                    "good" if execution_time < 1.0 else "needs_improvement"
                ),
            }

            # 3. ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰
            test_result["resource_usage"] = {
                "memory_before": memory_status_before,
                "memory_after": memory_status_after,
                "gpu_available": torch.cuda.is_available(),
            }

            test_result["status"] = "success"

        except Exception as e:
            test_result["status"] = "error"
            test_result["error"] = str(e)
            self.logger.error(f"í†µí•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

        self.test_results["integrated_performance"] = test_result
        self.logger.info(f"âœ… í†µí•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {test_result['status']}")

    def print_test_summary(self):
        """í…ŒìŠ¤íŠ¸ ìš”ì•½ ì¶œë ¥"""
        self.logger.info("\n" + "=" * 50)
        self.logger.info("ğŸ“Š í†µí•© ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ìš”ì•½")
        self.logger.info("=" * 50)

        total_tests = len(self.test_results)
        successful_tests = sum(
            1
            for result in self.test_results.values()
            if result.get("status") == "success"
        )

        self.logger.info(f"ì´ í…ŒìŠ¤íŠ¸: {total_tests}")
        self.logger.info(f"ì„±ê³µ: {successful_tests}")
        self.logger.info(f"ì‹¤íŒ¨: {total_tests - successful_tests}")
        self.logger.info(f"ì„±ê³µë¥ : {(successful_tests / total_tests * 100):.1f}%")

        self.logger.info("\nìƒì„¸ ê²°ê³¼:")
        for test_name, result in self.test_results.items():
            status = result.get("status", "unknown")
            status_emoji = "âœ…" if status == "success" else "âŒ"
            self.logger.info(f"{status_emoji} {test_name}: {status}")

            if status == "error":
                self.logger.error(f"   ì˜¤ë¥˜: {result.get('error', 'Unknown error')}")


def run_integrated_system_test():
    """í†µí•© ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    tester = IntegratedSystemTester()
    results = tester.run_all_tests()
    tester.print_test_summary()
    return results


if __name__ == "__main__":
    run_integrated_system_test()
