"""
CUDA ì‹±ê¸€í†¤ ë§¤ë‹ˆì € - ì¤‘ë³µ ì´ˆê¸°í™” ë°©ì§€ ë° ë¦¬ì†ŒìŠ¤ ê´€ë¦¬

ì´ ëª¨ë“ˆì€ CUDA ë¦¬ì†ŒìŠ¤ì˜ ì¤‘ë³µ ì´ˆê¸°í™”ë¥¼ ë°©ì§€í•˜ê³  íš¨ìœ¨ì ì¸ ë©”ëª¨ë¦¬ ê´€ë¦¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
"""

import threading
import time
from typing import Dict, Any, Optional, List, Set
from contextlib import contextmanager
import torch
import torch.cuda
from ..utils.unified_logging import get_logger

logger = get_logger(__name__)


class CudaSingletonConfig:
    """CUDA ì‹±ê¸€í†¤ ì„¤ì • í´ëž˜ìŠ¤"""

    def __init__(
        self,
        use_amp: bool = True,
        batch_size: int = 64,
        use_cudnn: bool = True,
        memory_fraction: float = 0.8,
        device_id: int = 0,
        enable_profiling: bool = False,
    ):
        self.use_amp = use_amp
        self.batch_size = batch_size
        self.use_cudnn = use_cudnn
        self.memory_fraction = memory_fraction
        self.device_id = device_id
        self.enable_profiling = enable_profiling


class CudaSingletonManager:
    """CUDA ë¦¬ì†ŒìŠ¤ ì‹±ê¸€í†¤ ê´€ë¦¬ìž"""

    _instance = None
    _lock = threading.RLock()
    _initialized = False

    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self):
        if CudaSingletonManager._initialized:
            return

        with CudaSingletonManager._lock:
            if CudaSingletonManager._initialized:
                return

            self.logger = get_logger(__name__)
            self.device = None
            self.is_available = False
            self.memory_pools = {}
            self.stream_pools = {}
            self.active_contexts = set()
            self.config = None
            self.requesters = {}  # ìš”ì²­ìžë³„ ì‚¬ìš©ëŸ‰ ì¶”ì 

            # CUDA ì´ˆê¸°í™”
            self._initialize_cuda()

            CudaSingletonManager._initialized = True
            self.logger.info("âœ… CUDA ì‹±ê¸€í†¤ ë§¤ë‹ˆì € ì´ˆê¸°í™” ì™„ë£Œ")

    def _initialize_cuda(self):
        """CUDA í™˜ê²½ ì´ˆê¸°í™”"""
        try:
            if torch.cuda.is_available():
                self.device = torch.device("cuda:0")
                self.is_available = True

                # CUDA ë©”ëª¨ë¦¬ ì„¤ì •
                torch.cuda.empty_cache()

                # cuDNN ìµœì í™”
                torch.backends.cudnn.benchmark = True
                torch.backends.cudnn.enabled = True

                # ë©”ëª¨ë¦¬ í’€ ì´ˆê¸°í™”
                self._initialize_memory_pools()

                self.logger.info(f"ðŸš€ CUDA ì´ˆê¸°í™” ì™„ë£Œ: {torch.cuda.get_device_name()}")
            else:
                self.logger.warning("âš ï¸ CUDA ì‚¬ìš© ë¶ˆê°€ëŠ¥")
                self.is_available = False

        except Exception as e:
            self.logger.error(f"CUDA ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.is_available = False

    def _initialize_memory_pools(self):
        """ë©”ëª¨ë¦¬ í’€ ì´ˆê¸°í™”"""
        try:
            if self.is_available:
                # ê¸°ë³¸ ë©”ëª¨ë¦¬ í’€ë“¤
                pool_sizes = {
                    "small": 256 * 1024 * 1024,  # 256MB
                    "medium": 512 * 1024 * 1024,  # 512MB
                    "large": 1024 * 1024 * 1024,  # 1GB
                }

                for pool_name, size in pool_sizes.items():
                    try:
                        # ë©”ëª¨ë¦¬ í’€ ìƒì„±
                        pool_tensor = torch.empty(
                            size // 4, dtype=torch.float32, device=self.device
                        )
                        self.memory_pools[pool_name] = {
                            "tensor": pool_tensor,
                            "size": size,
                            "allocated": 0,
                            "free_blocks": [],
                        }
                        self.logger.debug(
                            f"ë©”ëª¨ë¦¬ í’€ '{pool_name}' ìƒì„±: {size // (1024*1024)}MB"
                        )
                    except torch.cuda.OutOfMemoryError:
                        self.logger.warning(
                            f"ë©”ëª¨ë¦¬ í’€ '{pool_name}' ìƒì„± ì‹¤íŒ¨: ë©”ëª¨ë¦¬ ë¶€ì¡±"
                        )

        except Exception as e:
            self.logger.error(f"ë©”ëª¨ë¦¬ í’€ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    def register_requester(
        self, requester_name: str, config: Optional[CudaSingletonConfig] = None
    ):
        """ìš”ì²­ìž ë“±ë¡"""
        with self._lock:
            if requester_name not in self.requesters:
                self.requesters[requester_name] = {
                    "config": config,
                    "memory_usage": 0,
                    "active_contexts": 0,
                    "last_access": time.time(),
                }
                self.logger.debug(f"ìš”ì²­ìž ë“±ë¡: {requester_name}")

    def get_cuda_optimizer(self, requester_name: str) -> "CudaOptimizer":
        """CUDA ìµœì í™”ê¸° ë°˜í™˜"""
        self.register_requester(requester_name)
        return CudaOptimizer(self, requester_name)

    @contextmanager
    def device_context(self, requester_name: str):
        """ë””ë°”ì´ìŠ¤ ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬"""
        if not self.is_available:
            yield None
            return

        context_id = f"{requester_name}_{time.time()}"
        self.active_contexts.add(context_id)

        try:
            with torch.cuda.device(self.device):
                yield self.device
        finally:
            self.active_contexts.discard(context_id)

    @contextmanager
    def memory_scope(self, requester_name: str, size_mb: int = 100):
        """ë©”ëª¨ë¦¬ ìŠ¤ì½”í”„ ê´€ë¦¬"""
        if not self.is_available:
            yield None
            return

        allocated_memory = None
        try:
            # ë©”ëª¨ë¦¬ í• ë‹¹
            size_bytes = size_mb * 1024 * 1024
            allocated_memory = torch.empty(
                size_bytes // 4, dtype=torch.float32, device=self.device
            )

            # ì‚¬ìš©ëŸ‰ ì¶”ì 
            if requester_name in self.requesters:
                self.requesters[requester_name]["memory_usage"] += size_bytes
                self.requesters[requester_name]["last_access"] = time.time()

            yield allocated_memory

        except torch.cuda.OutOfMemoryError:
            self.logger.warning(f"ë©”ëª¨ë¦¬ í• ë‹¹ ì‹¤íŒ¨: {size_mb}MB ìš”ì²­")
            yield None
        finally:
            # ë©”ëª¨ë¦¬ í•´ì œ
            if allocated_memory is not None:
                del allocated_memory
                torch.cuda.empty_cache()

                # ì‚¬ìš©ëŸ‰ ì¶”ì  ì—…ë°ì´íŠ¸
                if requester_name in self.requesters:
                    self.requesters[requester_name]["memory_usage"] -= size_bytes

    def get_memory_info(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ì •ë³´ ë°˜í™˜"""
        if not self.is_available:
            return {"available": False}

        try:
            allocated = torch.cuda.memory_allocated()
            reserved = torch.cuda.memory_reserved()
            max_allocated = torch.cuda.max_memory_allocated()

            return {
                "available": True,
                "allocated": allocated,
                "reserved": reserved,
                "max_allocated": max_allocated,
                "free": reserved - allocated,
                "device_name": torch.cuda.get_device_name(),
                "device_count": torch.cuda.device_count(),
            }
        except Exception as e:
            self.logger.error(f"ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"available": False, "error": str(e)}

    def cleanup_inactive_requesters(self, timeout_seconds: int = 300):
        """ë¹„í™œì„± ìš”ì²­ìž ì •ë¦¬"""
        current_time = time.time()
        inactive_requesters = []

        with self._lock:
            for requester_name, info in self.requesters.items():
                if current_time - info["last_access"] > timeout_seconds:
                    inactive_requesters.append(requester_name)

            for requester_name in inactive_requesters:
                del self.requesters[requester_name]
                self.logger.debug(f"ë¹„í™œì„± ìš”ì²­ìž ì •ë¦¬: {requester_name}")

    def get_utilization_stats(self) -> Dict[str, Any]:
        """ì‚¬ìš©ë¥  í†µê³„ ë°˜í™˜"""
        try:
            if not self.is_available:
                return {"available": False}

            # GPU ì‚¬ìš©ë¥  (ê·¼ì‚¬ì¹˜)
            memory_info = self.get_memory_info()
            utilization = (
                (memory_info["allocated"] / memory_info["reserved"]) * 100
                if memory_info["reserved"] > 0
                else 0
            )

            return {
                "available": True,
                "gpu_utilization": utilization,
                "active_contexts": len(self.active_contexts),
                "registered_requesters": len(self.requesters),
                "memory_info": memory_info,
                "requester_stats": {
                    name: {
                        "memory_usage": info["memory_usage"],
                        "active_contexts": info["active_contexts"],
                        "last_access": info["last_access"],
                    }
                    for name, info in self.requesters.items()
                },
            }
        except Exception as e:
            self.logger.error(f"ì‚¬ìš©ë¥  í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"available": False, "error": str(e)}

    def force_cleanup(self):
        """ê°•ì œ ì •ë¦¬"""
        try:
            if self.is_available:
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                self.logger.info("ðŸ§¹ CUDA ë¦¬ì†ŒìŠ¤ ê°•ì œ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"ê°•ì œ ì •ë¦¬ ì‹¤íŒ¨: {e}")


class CudaOptimizer:
    """CUDA ìµœì í™”ê¸° - ì‹±ê¸€í†¤ ë§¤ë‹ˆì € ê¸°ë°˜"""

    def __init__(self, singleton_manager: CudaSingletonManager, requester_name: str):
        self.singleton_manager = singleton_manager
        self.requester_name = requester_name
        self.logger = get_logger(__name__)

    def is_available(self) -> bool:
        """CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ ë°˜í™˜"""
        return self.singleton_manager.is_available

    @contextmanager
    def device_context(self):
        """ë””ë°”ì´ìŠ¤ ì»¨í…ìŠ¤íŠ¸"""
        with self.singleton_manager.device_context(self.requester_name) as device:
            yield device

    @contextmanager
    def memory_scope(self, size_mb: int = 100):
        """ë©”ëª¨ë¦¬ ìŠ¤ì½”í”„"""
        with self.singleton_manager.memory_scope(
            self.requester_name, size_mb
        ) as memory:
            yield memory

    def get_gpu_utilization(self) -> float:
        """GPU ì‚¬ìš©ë¥  ë°˜í™˜"""
        stats = self.singleton_manager.get_utilization_stats()
        return stats.get("gpu_utilization", 0.0)

    def optimize_tensor_operations(self, tensor: torch.Tensor) -> torch.Tensor:
        """í…ì„œ ì—°ì‚° ìµœì í™”"""
        if not self.is_available():
            return tensor

        try:
            with self.device_context():
                # GPUë¡œ ì´ë™
                gpu_tensor = tensor.to(self.singleton_manager.device)

                # ìµœì í™”ëœ ì—°ì‚° ìˆ˜í–‰
                # ì˜ˆ: ë©”ëª¨ë¦¬ ë ˆì´ì•„ì›ƒ ìµœì í™”
                if gpu_tensor.is_contiguous():
                    return gpu_tensor
                else:
                    return gpu_tensor.contiguous()

        except Exception as e:
            self.logger.error(f"í…ì„œ ìµœì í™” ì‹¤íŒ¨: {e}")
            return tensor


# ì „ì—­ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_cuda_singleton = None


def get_singleton_cuda_optimizer(
    config: Optional[CudaSingletonConfig] = None, requester_name: str = "default"
) -> CudaOptimizer:
    """ì‹±ê¸€í†¤ CUDA ìµœì í™”ê¸° ë°˜í™˜"""
    global _cuda_singleton

    if _cuda_singleton is None:
        _cuda_singleton = CudaSingletonManager()

    if config:
        _cuda_singleton.config = config

    return _cuda_singleton.get_cuda_optimizer(requester_name)


def get_cuda_memory_info() -> Dict[str, Any]:
    """CUDA ë©”ëª¨ë¦¬ ì •ë³´ ë°˜í™˜"""
    global _cuda_singleton

    if _cuda_singleton is None:
        _cuda_singleton = CudaSingletonManager()

    return _cuda_singleton.get_memory_info()


def cleanup_cuda_resources():
    """CUDA ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
    global _cuda_singleton

    if _cuda_singleton is not None:
        _cuda_singleton.force_cleanup()
        _cuda_singleton.cleanup_inactive_requesters()
