"""
CUDA ì‹±ê¸€í†¤ ê´€ë¦¬ì

CUDA ìµœì í™”ê¸°ì˜ ì¤‘ë³µ ì´ˆê¸°í™” ë¬¸ì œë¥¼ ì™„ì „íˆ í•´ê²°í•˜ëŠ” ì‹±ê¸€í†¤ ì‹œìŠ¤í…œ
- ì „ì—­ ë‹¨ì¼ CUDA ìµœì í™”ê¸° ì¸ìŠ¤í„´ìŠ¤
- Thread-Safe ë³´ì¥
- ë©”ëª¨ë¦¬ ìµœì í™”ëœ CUDA ë¦¬ì†ŒìŠ¤ ê´€ë¦¬
"""

import threading
import weakref
from typing import Optional, Dict, Any, Set
from dataclasses import dataclass
import torch

from .unified_logging import get_logger

# ì „ì—­ ë³€ìˆ˜ë“¤
_cuda_manager_instance = None
_cuda_manager_lock = threading.RLock()
_cuda_optimizers_cache: Dict[str, Any] = {}

logger = get_logger(__name__)


@dataclass
class CudaSingletonConfig:
    """CUDA ì‹±ê¸€í†¤ ì„¤ì •"""

    use_amp: bool = True
    use_cudnn: bool = True
    use_graphs: bool = False
    batch_size: int = 32
    min_batch_size: int = 1
    max_batch_size: int = 256
    enable_memory_pool: bool = True
    enable_profiling: bool = False


class CudaSingletonManager:
    """
    CUDA ì‹±ê¸€í†¤ ê´€ë¦¬ì

    ëª¨ë“  CUDA ìµœì í™”ê¸°ë¥¼ ì¤‘ì•™ì—ì„œ ê´€ë¦¬í•˜ì—¬ ì¤‘ë³µ ì´ˆê¸°í™”ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
    """

    _instance = None
    _lock = threading.RLock()
    _initialized = False

    def __new__(cls):
        with cls._lock:
            if cls._instance is None:
                cls._instance = super().__new__(cls)
            return cls._instance

    def __init__(self):
        if self._initialized:
            return

        with self._lock:
            if self._initialized:
                return

            self._cuda_available = torch.cuda.is_available()
            self._primary_optimizer = None
            self._config = None
            self._initialization_count = 0
            self._active_references: Set[str] = set()
            self._device_info = {}

            # CUDA ì‚¬ìš© ê°€ëŠ¥ ì‹œ ê¸°ë³¸ ì„¤ì •
            if self._cuda_available:
                self._initialize_cuda_environment()

            self._initialized = True
            logger.info("ğŸš€ CUDA ì‹±ê¸€í†¤ ê´€ë¦¬ì ì´ˆê¸°í™” ì™„ë£Œ")

    def _initialize_cuda_environment(self):
        """CUDA í™˜ê²½ ì´ˆê¸°í™” (í•œ ë²ˆë§Œ)"""
        try:
            # ê¸°ë³¸ CUDA ì„¤ì •
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True

            # ë””ë°”ì´ìŠ¤ ì •ë³´ ìˆ˜ì§‘
            self._device_info = {
                "device_count": torch.cuda.device_count(),
                "current_device": torch.cuda.current_device(),
                "device_name": (
                    torch.cuda.get_device_name(0)
                    if torch.cuda.device_count() > 0
                    else "Unknown"
                ),
                "memory_total": (
                    torch.cuda.get_device_properties(0).total_memory
                    if torch.cuda.device_count() > 0
                    else 0
                ),
            }

            logger.info(
                f"âœ… CUDA í™˜ê²½ ì´ˆê¸°í™” ì™„ë£Œ - {self._device_info['device_name']}"
            )

        except Exception as e:
            logger.warning(f"CUDA í™˜ê²½ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {e}")

    def get_cuda_optimizer(
        self,
        config: Optional[CudaSingletonConfig] = None,
        requester_name: str = "unknown",
    ) -> Optional[Any]:
        """
        CUDA ìµœì í™”ê¸° ë°˜í™˜ (ì‹±ê¸€í†¤)

        Args:
            config: CUDA ì„¤ì • (ì²« ë²ˆì§¸ í˜¸ì¶œ ì‹œì—ë§Œ ì ìš©)
            requester_name: ìš”ì²­í•˜ëŠ” ëª¨ë“ˆëª… (ë¡œê¹…ìš©)

        Returns:
            CUDA ìµœì í™”ê¸° ì¸ìŠ¤í„´ìŠ¤ ë˜ëŠ” None
        """
        with self._lock:
            self._initialization_count += 1
            self._active_references.add(requester_name)

            # CUDA ì‚¬ìš© ë¶ˆê°€ëŠ¥
            if not self._cuda_available:
                if self._initialization_count == 1:  # ì²« ë²ˆì§¸ í˜¸ì¶œì—ì„œë§Œ ë¡œê·¸
                    logger.info("CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
                return None

            # ì²« ë²ˆì§¸ í˜¸ì¶œ ì‹œ ìµœì í™”ê¸° ìƒì„±
            if self._primary_optimizer is None:
                try:
                    # ì„¤ì • ì €ì¥
                    self._config = config or CudaSingletonConfig()

                    # ì‹¤ì œ CUDA ìµœì í™”ê¸° ìƒì„±
                    self._primary_optimizer = self._create_cuda_optimizer()

                    logger.info(
                        f"âœ… CUDA ìµœì í™”ê¸° ìƒì„± ì™„ë£Œ (ìš”ì²­ì: {requester_name})"
                    )

                except Exception as e:
                    logger.error(f"CUDA ìµœì í™”ê¸° ìƒì„± ì‹¤íŒ¨: {e}")
                    return None
            else:
                # ì´ë¯¸ ìƒì„±ëœ ê²½ìš° ì¤‘ë³µ ë°©ì§€ ë¡œê·¸
                if len(self._active_references) <= 3:  # ì²˜ìŒ 3ê°œê¹Œì§€ë§Œ ë¡œê·¸
                    logger.debug(
                        f"ê¸°ì¡´ CUDA ìµœì í™”ê¸° ì¬ì‚¬ìš© (ìš”ì²­ì: {requester_name})"
                    )

            return self._primary_optimizer

    def _create_cuda_optimizer(self) -> Any:
        """ì‹¤ì œ CUDA ìµœì í™”ê¸° ìƒì„±"""
        try:
            # ë™ì  importë¡œ ìˆœí™˜ ì°¸ì¡° ë°©ì§€
            from .cuda_optimizers import BaseCudaOptimizer, CudaConfig

            # CudaConfig ìƒì„±
            config = self._config or CudaSingletonConfig()
            cuda_config = CudaConfig(
                use_amp=config.use_amp,
                use_cudnn=config.use_cudnn,
                use_graphs=config.use_graphs,
                batch_size=config.batch_size,
                min_batch_size=config.min_batch_size,
                max_batch_size=config.max_batch_size,
            )

            # BaseCudaOptimizer ìƒì„± (ë‚´ë¶€ ë¡œê¹… ì–µì œ)
            optimizer = BaseCudaOptimizer(cuda_config)

            return optimizer

        except Exception as e:
            logger.error(f"CUDA ìµœì í™”ê¸° ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            raise

    def is_cuda_available(self) -> bool:
        """CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€"""
        return self._cuda_available

    def get_device_info(self) -> Dict[str, Any]:
        """ë””ë°”ì´ìŠ¤ ì •ë³´ ë°˜í™˜"""
        return self._device_info.copy()

    def get_statistics(self) -> Dict[str, Any]:
        """í†µê³„ ì •ë³´ ë°˜í™˜"""
        with self._lock:
            return {
                "cuda_available": self._cuda_available,
                "initialization_count": self._initialization_count,
                "active_references": list(self._active_references),
                "optimizer_created": self._primary_optimizer is not None,
                "device_info": self._device_info,
            }

    def cleanup(self):
        """ì •ë¦¬ ì‘ì—…"""
        with self._lock:
            if self._primary_optimizer is not None:
                try:
                    if hasattr(self._primary_optimizer, "cleanup"):
                        self._primary_optimizer.cleanup()
                except Exception as e:
                    logger.warning(f"CUDA ìµœì í™”ê¸° ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

                self._primary_optimizer = None

            self._active_references.clear()
            logger.info("CUDA ì‹±ê¸€í†¤ ê´€ë¦¬ì ì •ë¦¬ ì™„ë£Œ")


# ì „ì—­ í•¨ìˆ˜ë“¤
def get_cuda_singleton_manager() -> CudaSingletonManager:
    """ì „ì—­ CUDA ì‹±ê¸€í†¤ ê´€ë¦¬ì ë°˜í™˜"""
    global _cuda_manager_instance

    if _cuda_manager_instance is None:
        with _cuda_manager_lock:
            if _cuda_manager_instance is None:
                _cuda_manager_instance = CudaSingletonManager()

    return _cuda_manager_instance


def get_singleton_cuda_optimizer(
    config: Optional[CudaSingletonConfig] = None, requester_name: str = "unknown"
) -> Optional[Any]:
    """
    ì‹±ê¸€í†¤ CUDA ìµœì í™”ê¸° ë°˜í™˜

    Args:
        config: CUDA ì„¤ì •
        requester_name: ìš”ì²­í•˜ëŠ” ëª¨ë“ˆëª…

    Returns:
        CUDA ìµœì í™”ê¸° ì¸ìŠ¤í„´ìŠ¤ ë˜ëŠ” None
    """
    manager = get_cuda_singleton_manager()
    return manager.get_cuda_optimizer(config, requester_name)


def is_cuda_available() -> bool:
    """CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
    manager = get_cuda_singleton_manager()
    return manager.is_cuda_available()


def get_cuda_device_info() -> Dict[str, Any]:
    """CUDA ë””ë°”ì´ìŠ¤ ì •ë³´ ë°˜í™˜"""
    manager = get_cuda_singleton_manager()
    return manager.get_device_info()


def get_cuda_statistics() -> Dict[str, Any]:
    """CUDA ì‹±ê¸€í†¤ í†µê³„ ë°˜í™˜"""
    manager = get_cuda_singleton_manager()
    return manager.get_statistics()


def cleanup_cuda_singleton():
    """CUDA ì‹±ê¸€í†¤ ì •ë¦¬"""
    global _cuda_manager_instance

    if _cuda_manager_instance is not None:
        _cuda_manager_instance.cleanup()
        _cuda_manager_instance = None


# ëª¨ë“ˆ ì¢…ë£Œ ì‹œ ì •ë¦¬
import atexit

atexit.register(cleanup_cuda_singleton)
