"""
GPU ë©”ëª¨ë¦¬ í’€ë§ ì‹œìŠ¤í…œ - ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€ ë° íš¨ìœ¨ì  ê´€ë¦¬

ì´ ëª¨ë“ˆì€ GPU ë©”ëª¨ë¦¬ì˜ íš¨ìœ¨ì ì¸ í• ë‹¹ê³¼ í•´ì œë¥¼ ê´€ë¦¬í•˜ì—¬ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
"""

import threading
import time
import gc
from typing import Dict, Any, Optional, List, Tuple, Union
from contextlib import contextmanager
from dataclasses import dataclass, field
from enum import Enum
import torch
import torch.cuda
from ..utils.unified_logging import get_logger

logger = get_logger(__name__)


class PoolType(Enum):
    """ë©”ëª¨ë¦¬ í’€ íƒ€ì…"""

    SMALL = "small"  # 64MB ì´í•˜
    MEDIUM = "medium"  # 64MB ~ 256MB
    LARGE = "large"  # 256MB ~ 1GB
    XLARGE = "xlarge"  # 1GB ì´ìƒ


@dataclass
class MemoryBlock:
    """ë©”ëª¨ë¦¬ ë¸”ë¡ ì •ë³´"""

    tensor: torch.Tensor
    size: int
    pool_type: PoolType
    allocated_at: float = field(default_factory=time.time)
    last_used: float = field(default_factory=time.time)
    ref_count: int = 0
    is_free: bool = True


class GPUMemoryPool:
    """GPU ë©”ëª¨ë¦¬ í’€ ê´€ë¦¬ì"""

    def __init__(
        self,
        device_id: int = 0,
        max_pool_size: int = 2 * 1024 * 1024 * 1024,  # 2GB
        cleanup_interval: int = 60,  # 60ì´ˆ
        enable_monitoring: bool = True,
    ):
        self.device_id = device_id
        self.device = torch.device(f"cuda:{device_id}")
        self.max_pool_size = max_pool_size
        self.cleanup_interval = cleanup_interval
        self.enable_monitoring = enable_monitoring

        self.logger = get_logger(__name__)
        self._lock = threading.RLock()

        # ë©”ëª¨ë¦¬ í’€ë“¤
        self.pools: Dict[PoolType, List[MemoryBlock]] = {
            PoolType.SMALL: [],
            PoolType.MEDIUM: [],
            PoolType.LARGE: [],
            PoolType.XLARGE: [],
        }

        # í†µê³„ ì •ë³´
        self.stats = {
            "total_allocated": 0,
            "total_freed": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "peak_usage": 0,
            "allocation_count": 0,
            "deallocation_count": 0,
        }

        # í™œì„± í• ë‹¹ ì¶”ì 
        self.active_allocations: Dict[int, MemoryBlock] = {}

        # ì •ë¦¬ ìŠ¤ë ˆë“œ
        self.cleanup_thread = None
        self.shutdown_event = threading.Event()

        # ì´ˆê¸°í™”
        self._initialize_pools()
        if enable_monitoring:
            self._start_cleanup_thread()

        self.logger.info(f"âœ… GPU ë©”ëª¨ë¦¬ í’€ ì´ˆê¸°í™” ì™„ë£Œ (ë””ë°”ì´ìŠ¤: {device_id})")

    def _initialize_pools(self):
        """ë©”ëª¨ë¦¬ í’€ ì´ˆê¸°í™”"""
        try:
            if not torch.cuda.is_available():
                self.logger.warning("CUDA ì‚¬ìš© ë¶ˆê°€ëŠ¥")
                return

            # ë””ë°”ì´ìŠ¤ ë©”ëª¨ë¦¬ ì •ë³´ í™•ì¸
            total_memory = torch.cuda.get_device_properties(self.device_id).total_memory
            available_memory = total_memory - torch.cuda.memory_allocated(
                self.device_id
            )

            # í’€ í¬ê¸° ê³„ì‚° (ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ì˜ 50% ì´í•˜)
            safe_pool_size = min(self.max_pool_size, int(available_memory * 0.5))

            # í’€ë³„ ì´ˆê¸° í• ë‹¹
            pool_configs = {
                PoolType.SMALL: {"size": 64 * 1024 * 1024, "count": 8},  # 64MB x 8
                PoolType.MEDIUM: {"size": 256 * 1024 * 1024, "count": 4},  # 256MB x 4
                PoolType.LARGE: {"size": 512 * 1024 * 1024, "count": 2},  # 512MB x 2
                PoolType.XLARGE: {"size": 1024 * 1024 * 1024, "count": 1},  # 1GB x 1
            }

            total_required = sum(
                config["size"] * config["count"] for config in pool_configs.values()
            )

            if total_required > safe_pool_size:
                self.logger.warning(
                    f"í’€ í¬ê¸° ì¡°ì •: {total_required} -> {safe_pool_size}"
                )
                # í’€ í¬ê¸°ë¥¼ ë¹„ë¡€ì ìœ¼ë¡œ ì¤„ì„
                scale_factor = safe_pool_size / total_required
                for pool_type, config in pool_configs.items():
                    config["count"] = max(1, int(config["count"] * scale_factor))

            # ì‹¤ì œ í’€ ìƒì„±
            for pool_type, config in pool_configs.items():
                for _ in range(config["count"]):
                    try:
                        tensor = torch.empty(
                            config["size"] // 4,  # float32 ê¸°ì¤€
                            dtype=torch.float32,
                            device=self.device,
                        )

                        block = MemoryBlock(
                            tensor=tensor,
                            size=config["size"],
                            pool_type=pool_type,
                            is_free=True,
                        )

                        self.pools[pool_type].append(block)
                        self.stats["total_allocated"] += config["size"]

                    except torch.cuda.OutOfMemoryError:
                        self.logger.warning(
                            f"í’€ {pool_type.value} ìƒì„± ì‹¤íŒ¨: ë©”ëª¨ë¦¬ ë¶€ì¡±"
                        )
                        break

            self.logger.info(f"ë©”ëª¨ë¦¬ í’€ ì´ˆê¸°í™” ì™„ë£Œ: {self._get_pool_summary()}")

        except Exception as e:
            self.logger.error(f"ë©”ëª¨ë¦¬ í’€ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    def _get_pool_type(self, size: int) -> PoolType:
        """í¬ê¸°ì— ë”°ë¥¸ í’€ íƒ€ì… ê²°ì •"""
        if size <= 64 * 1024 * 1024:
            return PoolType.SMALL
        elif size <= 256 * 1024 * 1024:
            return PoolType.MEDIUM
        elif size <= 1024 * 1024 * 1024:
            return PoolType.LARGE
        else:
            return PoolType.XLARGE

    @contextmanager
    def allocate(self, size: int, dtype: torch.dtype = torch.float32):
        """ë©”ëª¨ë¦¬ í• ë‹¹ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
        allocated_block = None
        try:
            allocated_block = self._allocate_block(size, dtype)
            if allocated_block:
                yield allocated_block.tensor
            else:
                # í’€ì—ì„œ í• ë‹¹ ì‹¤íŒ¨ ì‹œ ì§ì ‘ í• ë‹¹
                tensor = torch.empty(size // 4, dtype=dtype, device=self.device)
                yield tensor
        except Exception as e:
            self.logger.error(f"ë©”ëª¨ë¦¬ í• ë‹¹ ì‹¤íŒ¨: {e}")
            yield None
        finally:
            if allocated_block:
                self._deallocate_block(allocated_block)

    def _allocate_block(self, size: int, dtype: torch.dtype) -> Optional[MemoryBlock]:
        """ë©”ëª¨ë¦¬ ë¸”ë¡ í• ë‹¹"""
        with self._lock:
            pool_type = self._get_pool_type(size)

            # í•´ë‹¹ í’€ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ë¸”ë¡ ì°¾ê¸°
            for block in self.pools[pool_type]:
                if block.is_free and block.size >= size:
                    block.is_free = False
                    block.ref_count += 1
                    block.last_used = time.time()

                    # í™œì„± í• ë‹¹ì— ì¶”ê°€
                    self.active_allocations[id(block)] = block

                    self.stats["cache_hits"] += 1
                    self.stats["allocation_count"] += 1

                    return block

            # í’€ì— ì ì ˆí•œ ë¸”ë¡ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
            try:
                tensor = torch.empty(size // 4, dtype=dtype, device=self.device)
                block = MemoryBlock(
                    tensor=tensor,
                    size=size,
                    pool_type=pool_type,
                    is_free=False,
                    ref_count=1,
                )

                self.pools[pool_type].append(block)
                self.active_allocations[id(block)] = block

                self.stats["cache_misses"] += 1
                self.stats["allocation_count"] += 1
                self.stats["total_allocated"] += size

                return block

            except torch.cuda.OutOfMemoryError:
                self.logger.warning(f"ë©”ëª¨ë¦¬ í• ë‹¹ ì‹¤íŒ¨: {size} bytes")
                # ì •ë¦¬ í›„ ì¬ì‹œë„
                self._emergency_cleanup()
                return None

    def _deallocate_block(self, block: MemoryBlock):
        """ë©”ëª¨ë¦¬ ë¸”ë¡ í•´ì œ"""
        with self._lock:
            if id(block) in self.active_allocations:
                block.ref_count -= 1
                if block.ref_count <= 0:
                    block.is_free = True
                    block.ref_count = 0
                    del self.active_allocations[id(block)]
                    self.stats["deallocation_count"] += 1

    def _emergency_cleanup(self):
        """ê¸´ê¸‰ ì •ë¦¬"""
        try:
            self.logger.warning("ğŸš¨ ê¸´ê¸‰ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œì‘")

            # ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ë¸”ë¡ë“¤ ì •ë¦¬
            cleaned_count = 0
            for pool_type, blocks in self.pools.items():
                to_remove = []
                for i, block in enumerate(blocks):
                    if (
                        block.is_free and time.time() - block.last_used > 30
                    ):  # 30ì´ˆ ì´ìƒ ë¯¸ì‚¬ìš©
                        to_remove.append(i)
                        cleaned_count += 1

                # ì—­ìˆœìœ¼ë¡œ ì œê±°
                for i in reversed(to_remove):
                    del blocks[i]

            # PyTorch ìºì‹œ ì •ë¦¬
            torch.cuda.empty_cache()
            torch.cuda.synchronize()

            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()

            self.logger.info(f"ê¸´ê¸‰ ì •ë¦¬ ì™„ë£Œ: {cleaned_count}ê°œ ë¸”ë¡ ì •ë¦¬")

        except Exception as e:
            self.logger.error(f"ê¸´ê¸‰ ì •ë¦¬ ì‹¤íŒ¨: {e}")

    def _start_cleanup_thread(self):
        """ì •ë¦¬ ìŠ¤ë ˆë“œ ì‹œì‘"""

        def cleanup_worker():
            while not self.shutdown_event.wait(self.cleanup_interval):
                try:
                    self._periodic_cleanup()
                except Exception as e:
                    self.logger.error(f"ì •ê¸° ì •ë¦¬ ì‹¤íŒ¨: {e}")

        self.cleanup_thread = threading.Thread(target=cleanup_worker, daemon=True)
        self.cleanup_thread.start()
        self.logger.debug("ì •ë¦¬ ìŠ¤ë ˆë“œ ì‹œì‘")

    def _periodic_cleanup(self):
        """ì •ê¸° ì •ë¦¬"""
        with self._lock:
            current_time = time.time()
            total_cleaned = 0

            for pool_type, blocks in self.pools.items():
                to_remove = []
                for i, block in enumerate(blocks):
                    # 5ë¶„ ì´ìƒ ë¯¸ì‚¬ìš© ë¸”ë¡ ì •ë¦¬
                    if (
                        block.is_free
                        and current_time - block.last_used > 300
                        and len(blocks) > 1
                    ):  # ìµœì†Œ 1ê°œëŠ” ìœ ì§€
                        to_remove.append(i)
                        total_cleaned += 1

                # ì—­ìˆœìœ¼ë¡œ ì œê±°
                for i in reversed(to_remove):
                    del blocks[i]

            if total_cleaned > 0:
                self.logger.debug(f"ì •ê¸° ì •ë¦¬: {total_cleaned}ê°œ ë¸”ë¡ ì •ë¦¬")
                torch.cuda.empty_cache()

    def get_stats(self) -> Dict[str, Any]:
        """í†µê³„ ì •ë³´ ë°˜í™˜"""
        with self._lock:
            # í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
            current_usage = sum(
                sum(block.size for block in blocks if not block.is_free)
                for blocks in self.pools.values()
            )

            # í’€ë³„ í†µê³„
            pool_stats = {}
            for pool_type, blocks in self.pools.items():
                pool_stats[pool_type.value] = {
                    "total_blocks": len(blocks),
                    "free_blocks": sum(1 for block in blocks if block.is_free),
                    "used_blocks": sum(1 for block in blocks if not block.is_free),
                    "total_size": sum(block.size for block in blocks),
                    "used_size": sum(
                        block.size for block in blocks if not block.is_free
                    ),
                }

            return {
                **self.stats,
                "current_usage": current_usage,
                "active_allocations": len(self.active_allocations),
                "pool_stats": pool_stats,
                "cache_hit_rate": (
                    self.stats["cache_hits"]
                    / (self.stats["cache_hits"] + self.stats["cache_misses"])
                    if (self.stats["cache_hits"] + self.stats["cache_misses"]) > 0
                    else 0
                ),
            }

    def _get_pool_summary(self) -> str:
        """í’€ ìš”ì•½ ì •ë³´"""
        summary = []
        for pool_type, blocks in self.pools.items():
            if blocks:
                total_size = sum(block.size for block in blocks)
                summary.append(
                    f"{pool_type.value}: {len(blocks)}ê°œ ë¸”ë¡ ({total_size // (1024*1024)}MB)"
                )
        return ", ".join(summary)

    def shutdown(self):
        """ë©”ëª¨ë¦¬ í’€ ì¢…ë£Œ"""
        try:
            self.logger.info("GPU ë©”ëª¨ë¦¬ í’€ ì¢…ë£Œ ì‹œì‘")

            # ì •ë¦¬ ìŠ¤ë ˆë“œ ì¢…ë£Œ
            if self.cleanup_thread:
                self.shutdown_event.set()
                self.cleanup_thread.join(timeout=5)

            # ëª¨ë“  ë¸”ë¡ í•´ì œ
            with self._lock:
                for pool_type, blocks in self.pools.items():
                    blocks.clear()

                self.active_allocations.clear()

            # CUDA ìºì‹œ ì •ë¦¬
            torch.cuda.empty_cache()
            torch.cuda.synchronize()

            self.logger.info("âœ… GPU ë©”ëª¨ë¦¬ í’€ ì¢…ë£Œ ì™„ë£Œ")

        except Exception as e:
            self.logger.error(f"ë©”ëª¨ë¦¬ í’€ ì¢…ë£Œ ì‹¤íŒ¨: {e}")


# ì „ì—­ ë©”ëª¨ë¦¬ í’€ ì¸ìŠ¤í„´ìŠ¤
_memory_pools: Dict[int, GPUMemoryPool] = {}
_pool_lock = threading.Lock()


def get_gpu_memory_pool(device_id: int = 0) -> GPUMemoryPool:
    """GPU ë©”ëª¨ë¦¬ í’€ ë°˜í™˜ (ì‹±ê¸€í†¤)"""
    global _memory_pools

    with _pool_lock:
        if device_id not in _memory_pools:
            _memory_pools[device_id] = GPUMemoryPool(device_id=device_id)
        return _memory_pools[device_id]


@contextmanager
def gpu_memory_scope(
    size_mb: int, device_id: int = 0, dtype: torch.dtype = torch.float32
):
    """GPU ë©”ëª¨ë¦¬ ìŠ¤ì½”í”„ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
    pool = get_gpu_memory_pool(device_id)
    size_bytes = size_mb * 1024 * 1024

    with pool.allocate(size_bytes, dtype) as tensor:
        yield tensor


def cleanup_all_memory_pools():
    """ëª¨ë“  ë©”ëª¨ë¦¬ í’€ ì •ë¦¬"""
    global _memory_pools

    with _pool_lock:
        for device_id, pool in _memory_pools.items():
            pool.shutdown()
        _memory_pools.clear()


def get_memory_pool_stats() -> Dict[str, Any]:
    """ëª¨ë“  ë©”ëª¨ë¦¬ í’€ í†µê³„ ë°˜í™˜"""
    global _memory_pools

    with _pool_lock:
        return {
            f"device_{device_id}": pool.get_stats()
            for device_id, pool in _memory_pools.items()
        }
