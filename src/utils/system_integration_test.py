"""
ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸ - ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë° ê²€ì¦

ì´ ëª¨ë“ˆì€ DAEBAK AI ë¡œë˜ ì‹œìŠ¤í…œì˜ ì „ì²´ì ì¸ ì„±ëŠ¥ê³¼ ì•ˆì •ì„±ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""

import time
import gc
import threading
import psutil
import torch
from typing import Dict, List, Any, Optional, Tuple, Callable
from dataclasses import dataclass, field
from contextlib import contextmanager
from ..utils.unified_logging import get_logger
from ..utils.cuda_singleton_manager import (
    get_singleton_cuda_optimizer,
    cleanup_cuda_resources,
)
from ..utils.gpu_memory_pool import get_gpu_memory_pool, cleanup_all_memory_pools
from ..utils.enhanced_process_pool import (
    get_enhanced_process_pool,
    cleanup_process_pool,
)
from ..utils.gpu_accelerated_kernels import get_gpu_pattern_kernels, cleanup_gpu_kernels

logger = get_logger(__name__)


@dataclass
class PerformanceMetrics:
    """ì„±ëŠ¥ ì§€í‘œ"""

    execution_time: float = 0.0
    memory_usage_mb: float = 0.0
    peak_memory_mb: float = 0.0
    gpu_memory_mb: float = 0.0
    cpu_utilization: float = 0.0
    gpu_utilization: float = 0.0
    throughput: float = 0.0
    error_rate: float = 0.0
    success_count: int = 0
    failure_count: int = 0


@dataclass
class TestResult:
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼"""

    test_name: str
    success: bool
    metrics: PerformanceMetrics
    error_message: Optional[str] = None
    details: Dict[str, Any] = field(default_factory=dict)


class SystemIntegrationTester:
    """ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤í„°"""

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.logger = get_logger(__name__)

        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥
        self.test_results: List[TestResult] = []
        self.overall_metrics = PerformanceMetrics()

        # ì„±ëŠ¥ ëª©í‘œê°’
        self.performance_targets = {
            "max_execution_time": 30.0,  # 30ì´ˆ
            "max_memory_usage_mb": 1024,  # 1GB
            "min_gpu_utilization": 15.0,  # 15%
            "min_cpu_utilization": 20.0,  # 20%
            "max_error_rate": 0.01,  # 1%
            "min_throughput": 10.0,  # 10 operations/sec
        }

        self.logger.info("âœ… ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤í„° ì´ˆê¸°í™” ì™„ë£Œ")

    def run_full_integration_test(self) -> Dict[str, Any]:
        """ì „ì²´ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        try:
            self.logger.info("ğŸš€ ì „ì²´ ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
            start_time = time.time()

            # í…ŒìŠ¤íŠ¸ ìˆœì„œ
            test_suite = [
                ("memory_management", self.test_memory_management),
                ("cuda_optimization", self.test_cuda_optimization),
                ("gpu_acceleration", self.test_gpu_acceleration),
                ("multiprocessing", self.test_multiprocessing),
                ("pattern_analysis", self.test_pattern_analysis),
                ("vectorization", self.test_vectorization),
                ("performance_benchmark", self.test_performance_benchmark),
                ("stress_test", self.test_stress_test),
                ("memory_leak_test", self.test_memory_leak),
                ("resource_cleanup", self.test_resource_cleanup),
            ]

            # ê° í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            for test_name, test_func in test_suite:
                try:
                    self.logger.info(f"ğŸ“‹ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘: {test_name}")
                    result = test_func()
                    self.test_results.append(result)

                    if result.success:
                        self.logger.info(f"âœ… {test_name} í…ŒìŠ¤íŠ¸ ì„±ê³µ")
                    else:
                        self.logger.error(
                            f"âŒ {test_name} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {result.error_message}"
                        )

                except Exception as e:
                    self.logger.error(f"âŒ {test_name} í…ŒìŠ¤íŠ¸ ì˜ˆì™¸: {e}")
                    self.test_results.append(
                        TestResult(
                            test_name=test_name,
                            success=False,
                            metrics=PerformanceMetrics(),
                            error_message=str(e),
                        )
                    )

                # í…ŒìŠ¤íŠ¸ ê°„ ì •ë¦¬
                self._cleanup_between_tests()

            # ì „ì²´ ê²°ê³¼ ë¶„ì„
            total_time = time.time() - start_time
            summary = self._generate_test_summary(total_time)

            self.logger.info(f"ğŸ¯ ì „ì²´ í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {total_time:.2f}ì´ˆ")
            return summary

        except Exception as e:
            self.logger.error(f"í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            raise

    def test_memory_management(self) -> TestResult:
        """ë©”ëª¨ë¦¬ ê´€ë¦¬ í…ŒìŠ¤íŠ¸"""
        try:
            with self._performance_monitor("memory_management") as metrics:
                # ë©”ëª¨ë¦¬ í’€ í…ŒìŠ¤íŠ¸
                memory_pool = get_gpu_memory_pool()

                # ë©”ëª¨ë¦¬ í• ë‹¹/í•´ì œ í…ŒìŠ¤íŠ¸
                allocations = []
                for i in range(10):
                    with memory_pool.allocate(64 * 1024 * 1024) as tensor:  # 64MB
                        if tensor is not None:
                            allocations.append(tensor.size())

                # í†µê³„ í™•ì¸
                pool_stats = memory_pool.get_stats()

                # ì„±ê³µ ì¡°ê±´
                success = (
                    len(allocations) > 5  # ìµœì†Œ 5ê°œ í• ë‹¹ ì„±ê³µ
                    and pool_stats["cache_hit_rate"] >= 0.0  # ìºì‹œ íˆíŠ¸ìœ¨ í™•ì¸
                    and metrics.peak_memory_mb
                    < self.performance_targets["max_memory_usage_mb"]
                )

                return TestResult(
                    test_name="memory_management",
                    success=success,
                    metrics=metrics,
                    details={
                        "allocations_count": len(allocations),
                        "pool_stats": pool_stats,
                    },
                )

        except Exception as e:
            return TestResult(
                test_name="memory_management",
                success=False,
                metrics=PerformanceMetrics(),
                error_message=str(e),
            )

    def test_cuda_optimization(self) -> TestResult:
        """CUDA ìµœì í™” í…ŒìŠ¤íŠ¸"""
        try:
            with self._performance_monitor("cuda_optimization") as metrics:
                # CUDA ìµœì í™”ê¸° í…ŒìŠ¤íŠ¸
                cuda_optimizer = get_singleton_cuda_optimizer(
                    requester_name="integration_test"
                )

                if cuda_optimizer.is_available():
                    # GPU ì»¨í…ìŠ¤íŠ¸ í…ŒìŠ¤íŠ¸
                    with cuda_optimizer.device_context():
                        test_tensor = torch.randn(1000, 1000)
                        optimized_tensor = cuda_optimizer.optimize_tensor_operations(
                            test_tensor
                        )

                    # GPU ì‚¬ìš©ë¥  í™•ì¸
                    gpu_utilization = cuda_optimizer.get_gpu_utilization()
                    metrics.gpu_utilization = gpu_utilization

                    success = gpu_utilization >= 0.0  # GPU ì‚¬ìš© ê°€ëŠ¥
                else:
                    success = True  # GPU ì—†ì–´ë„ ì„±ê³µ (CPU í´ë°±)
                    self.logger.warning("GPU ì‚¬ìš© ë¶ˆê°€ëŠ¥ - CPU ëª¨ë“œë¡œ í…ŒìŠ¤íŠ¸")

                return TestResult(
                    test_name="cuda_optimization",
                    success=success,
                    metrics=metrics,
                    details={
                        "gpu_available": cuda_optimizer.is_available(),
                        "gpu_utilization": metrics.gpu_utilization,
                    },
                )

        except Exception as e:
            return TestResult(
                test_name="cuda_optimization",
                success=False,
                metrics=PerformanceMetrics(),
                error_message=str(e),
            )

    def test_gpu_acceleration(self) -> TestResult:
        """GPU ê°€ì† í…ŒìŠ¤íŠ¸"""
        try:
            with self._performance_monitor("gpu_acceleration") as metrics:
                # GPU ì»¤ë„ í…ŒìŠ¤íŠ¸
                gpu_kernels = get_gpu_pattern_kernels()

                # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
                test_data = torch.randint(1, 46, (100, 6))  # 100ê°œ ë¡œë˜ ë²ˆí˜¸

                # ë¹ˆë„ ë¶„ì„ ì»¤ë„ í…ŒìŠ¤íŠ¸
                freq_result = gpu_kernels.frequency_analysis_kernel(test_data)

                # ê°„ê²© ë¶„ì„ ì»¤ë„ í…ŒìŠ¤íŠ¸
                gap_result = gpu_kernels.gap_analysis_kernel(test_data)

                # ì—°ì† ë²ˆí˜¸ ë¶„ì„ ì»¤ë„ í…ŒìŠ¤íŠ¸
                consecutive_result = gpu_kernels.consecutive_analysis_kernel(test_data)

                # ê²°ê³¼ ê²€ì¦
                success = (
                    freq_result.size(0) == 45  # 45ê°œ ë²ˆí˜¸
                    and gap_result.size(0) == 45
                    and consecutive_result.size(0) == 100  # 100ê°œ ê²°ê³¼
                )

                return TestResult(
                    test_name="gpu_acceleration",
                    success=success,
                    metrics=metrics,
                    details={
                        "freq_result_shape": freq_result.shape,
                        "gap_result_shape": gap_result.shape,
                        "consecutive_result_shape": consecutive_result.shape,
                    },
                )

        except Exception as e:
            return TestResult(
                test_name="gpu_acceleration",
                success=False,
                metrics=PerformanceMetrics(),
                error_message=str(e),
            )

    def test_multiprocessing(self) -> TestResult:
        """ë©€í‹°í”„ë¡œì„¸ì‹± í…ŒìŠ¤íŠ¸"""
        try:
            with self._performance_monitor("multiprocessing") as metrics:
                # í”„ë¡œì„¸ìŠ¤ í’€ í…ŒìŠ¤íŠ¸
                process_pool = get_enhanced_process_pool()

                # í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
                def test_function(data):
                    return sum(data) ** 0.5

                # í…ŒìŠ¤íŠ¸ ë°ì´í„°
                test_data = [[i] * 1000 for i in range(1, 21)]  # 20ê°œ ì²­í¬

                # ë³‘ë ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
                start_time = time.time()
                results = process_pool.parallel_map(test_function, test_data)
                parallel_time = time.time() - start_time

                # ìˆœì°¨ ì²˜ë¦¬ì™€ ë¹„êµ
                start_time = time.time()
                sequential_results = [test_function(data) for data in test_data]
                sequential_time = time.time() - start_time

                # ì„±ëŠ¥ ê°œì„  ê³„ì‚°
                speedup = sequential_time / parallel_time if parallel_time > 0 else 0
                metrics.throughput = len(test_data) / parallel_time

                # ê²°ê³¼ ê²€ì¦
                success = (
                    len(results) == len(test_data)
                    and speedup > 0.5  # ìµœì†Œ 50% ì„±ëŠ¥ (ì˜¤ë²„í—¤ë“œ ê³ ë ¤)
                )

                return TestResult(
                    test_name="multiprocessing",
                    success=success,
                    metrics=metrics,
                    details={
                        "results_count": len(results),
                        "speedup": speedup,
                        "parallel_time": parallel_time,
                        "sequential_time": sequential_time,
                    },
                )

        except Exception as e:
            return TestResult(
                test_name="multiprocessing",
                success=False,
                metrics=PerformanceMetrics(),
                error_message=str(e),
            )

    def test_pattern_analysis(self) -> TestResult:
        """íŒ¨í„´ ë¶„ì„ í…ŒìŠ¤íŠ¸"""
        try:
            with self._performance_monitor("pattern_analysis") as metrics:
                # íŒ¨í„´ ë¶„ì„ê¸° ì„í¬íŠ¸ ë° í…ŒìŠ¤íŠ¸
                from ..analysis.pattern_analyzer import PatternAnalyzer
                from ..shared.types import LotteryNumber

                # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
                test_data = [
                    LotteryNumber(numbers=[1, 2, 3, 4, 5, 6], draw_number=i)
                    for i in range(1, 101)
                ]

                # íŒ¨í„´ ë¶„ì„ê¸° ì´ˆê¸°í™”
                analyzer = PatternAnalyzer()

                # ë¶„ì„ ì‹¤í–‰
                analysis_result = analyzer.analyze(test_data)

                # ê²°ê³¼ ê²€ì¦
                success = (
                    analysis_result is not None
                    and hasattr(analysis_result, "frequency_map")
                    and len(analysis_result.frequency_map) > 0
                )

                return TestResult(
                    test_name="pattern_analysis",
                    success=success,
                    metrics=metrics,
                    details={
                        "analysis_type": type(analysis_result).__name__,
                        "frequency_map_size": (
                            len(analysis_result.frequency_map)
                            if hasattr(analysis_result, "frequency_map")
                            else 0
                        ),
                    },
                )

        except Exception as e:
            return TestResult(
                test_name="pattern_analysis",
                success=False,
                metrics=PerformanceMetrics(),
                error_message=str(e),
            )

    def test_vectorization(self) -> TestResult:
        """ë²¡í„°í™” í…ŒìŠ¤íŠ¸"""
        try:
            with self._performance_monitor("vectorization") as metrics:
                # ë²¡í„°í™” ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
                from ..analysis.enhanced_pattern_vectorizer import (
                    EnhancedPatternVectorizer,
                )

                # ë²¡í„°í™”ê¸° ì´ˆê¸°í™”
                vectorizer = EnhancedPatternVectorizer()

                # í…ŒìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼ ìƒì„±
                test_analysis = {
                    "pattern_analysis": {
                        "frequency_map": {i: i * 0.1 for i in range(1, 46)}
                    },
                    "distribution_pattern": {"entropy": 3.5},
                    "roi_features": {"avg_roi": 0.8},
                }

                # ë²¡í„°í™” ì‹¤í–‰
                vector = vectorizer.vectorize_full_analysis_enhanced(test_analysis)
                feature_names = vectorizer.get_feature_names()

                # ê²°ê³¼ ê²€ì¦
                success = (
                    vector is not None
                    and len(vector) == 168  # 168ì°¨ì›
                    and len(feature_names) == len(vector)  # ì´ë¦„ê³¼ ë²¡í„° ì°¨ì› ì¼ì¹˜
                    and not torch.isnan(torch.tensor(vector)).any()  # NaN ì—†ìŒ
                )

                return TestResult(
                    test_name="vectorization",
                    success=success,
                    metrics=metrics,
                    details={
                        "vector_dimension": len(vector),
                        "feature_names_count": len(feature_names),
                        "vector_range": (float(min(vector)), float(max(vector))),
                    },
                )

        except Exception as e:
            return TestResult(
                test_name="vectorization",
                success=False,
                metrics=PerformanceMetrics(),
                error_message=str(e),
            )

    def test_performance_benchmark(self) -> TestResult:
        """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸"""
        try:
            with self._performance_monitor("performance_benchmark") as metrics:
                # ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
                large_data = torch.randint(1, 46, (1000, 6))  # 1000ê°œ ë¡œë˜ ë²ˆí˜¸

                # GPU ì»¤ë„ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
                gpu_kernels = get_gpu_pattern_kernels()

                start_time = time.time()
                freq_result = gpu_kernels.frequency_analysis_kernel(large_data)
                gap_result = gpu_kernels.gap_analysis_kernel(large_data)
                consecutive_result = gpu_kernels.consecutive_analysis_kernel(large_data)
                processing_time = time.time() - start_time

                # ì²˜ë¦¬ëŸ‰ ê³„ì‚°
                throughput = len(large_data) / processing_time
                metrics.throughput = throughput

                # ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„± ì—¬ë¶€
                success = (
                    processing_time < self.performance_targets["max_execution_time"]
                    and throughput >= self.performance_targets["min_throughput"]
                )

                return TestResult(
                    test_name="performance_benchmark",
                    success=success,
                    metrics=metrics,
                    details={
                        "data_size": len(large_data),
                        "processing_time": processing_time,
                        "throughput": throughput,
                        "target_throughput": self.performance_targets["min_throughput"],
                    },
                )

        except Exception as e:
            return TestResult(
                test_name="performance_benchmark",
                success=False,
                metrics=PerformanceMetrics(),
                error_message=str(e),
            )

    def test_stress_test(self) -> TestResult:
        """ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸"""
        try:
            with self._performance_monitor("stress_test") as metrics:
                # ë™ì‹œ ì‘ì—… ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸
                import concurrent.futures

                def stress_worker(worker_id):
                    # GPU ì»¤ë„ ì‘ì—…
                    gpu_kernels = get_gpu_pattern_kernels()
                    test_data = torch.randint(1, 46, (50, 6))

                    results = []
                    for _ in range(10):  # ê° ì›Œì»¤ê°€ 10ë²ˆ ë°˜ë³µ
                        freq_result = gpu_kernels.frequency_analysis_kernel(test_data)
                        results.append(freq_result)

                    return len(results)

                # ë™ì‹œ ì‹¤í–‰
                with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
                    futures = [executor.submit(stress_worker, i) for i in range(4)]

                    results = []
                    for future in concurrent.futures.as_completed(futures, timeout=60):
                        try:
                            result = future.result()
                            results.append(result)
                        except Exception as e:
                            self.logger.error(f"ìŠ¤íŠ¸ë ˆìŠ¤ ì›Œì»¤ ì‹¤íŒ¨: {e}")
                            results.append(0)

                # ì„±ê³µë¥  ê³„ì‚°
                success_rate = sum(1 for r in results if r > 0) / len(results)
                metrics.error_rate = 1.0 - success_rate

                success = success_rate >= 0.8  # 80% ì´ìƒ ì„±ê³µ

                return TestResult(
                    test_name="stress_test",
                    success=success,
                    metrics=metrics,
                    details={
                        "worker_count": len(results),
                        "success_rate": success_rate,
                        "completed_tasks": sum(results),
                    },
                )

        except Exception as e:
            return TestResult(
                test_name="stress_test",
                success=False,
                metrics=PerformanceMetrics(),
                error_message=str(e),
            )

    def test_memory_leak(self) -> TestResult:
        """ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ í…ŒìŠ¤íŠ¸"""
        try:
            with self._performance_monitor("memory_leak") as metrics:
                initial_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB

                # ë°˜ë³µì ì¸ í• ë‹¹/í•´ì œ í…ŒìŠ¤íŠ¸
                for i in range(100):
                    # GPU ë©”ëª¨ë¦¬ í• ë‹¹/í•´ì œ
                    memory_pool = get_gpu_memory_pool()
                    with memory_pool.allocate(10 * 1024 * 1024) as tensor:  # 10MB
                        if tensor is not None:
                            # ê°„ë‹¨í•œ ì—°ì‚°
                            _ = tensor.sum()

                    # ì£¼ê¸°ì  ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
                    if i % 20 == 0:
                        gc.collect()
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()

                final_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
                memory_increase = final_memory - initial_memory

                # ë©”ëª¨ë¦¬ ì¦ê°€ëŸ‰ í—ˆìš© ë²”ìœ„ (100MB ì´í•˜)
                success = memory_increase < 100

                return TestResult(
                    test_name="memory_leak",
                    success=success,
                    metrics=metrics,
                    details={
                        "initial_memory_mb": initial_memory,
                        "final_memory_mb": final_memory,
                        "memory_increase_mb": memory_increase,
                    },
                )

        except Exception as e:
            return TestResult(
                test_name="memory_leak",
                success=False,
                metrics=PerformanceMetrics(),
                error_message=str(e),
            )

    def test_resource_cleanup(self) -> TestResult:
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ í…ŒìŠ¤íŠ¸"""
        try:
            with self._performance_monitor("resource_cleanup") as metrics:
                # ëª¨ë“  ë¦¬ì†ŒìŠ¤ ì •ë¦¬
                cleanup_cuda_resources()
                cleanup_all_memory_pools()
                cleanup_process_pool()
                cleanup_gpu_kernels()

                # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()

                # ì •ë¦¬ í›„ ë©”ëª¨ë¦¬ í™•ì¸
                final_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB

                success = True  # ì •ë¦¬ ì‘ì—…ì€ ì˜ˆì™¸ ì—†ì´ ì™„ë£Œë˜ë©´ ì„±ê³µ

                return TestResult(
                    test_name="resource_cleanup",
                    success=success,
                    metrics=metrics,
                    details={
                        "final_memory_mb": final_memory,
                    },
                )

        except Exception as e:
            return TestResult(
                test_name="resource_cleanup",
                success=False,
                metrics=PerformanceMetrics(),
                error_message=str(e),
            )

    @contextmanager
    def _performance_monitor(self, test_name: str):
        """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì»¨í…ìŠ¤íŠ¸"""
        # ì´ˆê¸° ìƒíƒœ
        start_time = time.time()
        initial_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB

        if torch.cuda.is_available():
            initial_gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        else:
            initial_gpu_memory = 0

        metrics = PerformanceMetrics()

        try:
            yield metrics
        finally:
            # ìµœì¢… ìƒíƒœ
            end_time = time.time()
            final_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB

            if torch.cuda.is_available():
                final_gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024  # MB
            else:
                final_gpu_memory = 0

            # ë©”íŠ¸ë¦­ ê³„ì‚°
            metrics.execution_time = end_time - start_time
            metrics.memory_usage_mb = final_memory - initial_memory
            metrics.peak_memory_mb = final_memory
            metrics.gpu_memory_mb = final_gpu_memory - initial_gpu_memory
            metrics.cpu_utilization = psutil.cpu_percent()

    def _cleanup_between_tests(self):
        """í…ŒìŠ¤íŠ¸ ê°„ ì •ë¦¬"""
        try:
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            time.sleep(0.1)  # ì ì‹œ ëŒ€ê¸°
        except Exception as e:
            self.logger.warning(f"í…ŒìŠ¤íŠ¸ ê°„ ì •ë¦¬ ì‹¤íŒ¨: {e}")

    def _generate_test_summary(self, total_time: float) -> Dict[str, Any]:
        """í…ŒìŠ¤íŠ¸ ìš”ì•½ ìƒì„±"""
        try:
            # ì„±ê³µ/ì‹¤íŒ¨ í†µê³„
            total_tests = len(self.test_results)
            successful_tests = sum(1 for result in self.test_results if result.success)
            failed_tests = total_tests - successful_tests

            # ì„±ëŠ¥ í†µê³„
            total_execution_time = sum(
                result.metrics.execution_time for result in self.test_results
            )
            avg_execution_time = (
                total_execution_time / total_tests if total_tests > 0 else 0
            )

            peak_memory = (
                max(result.metrics.peak_memory_mb for result in self.test_results)
                if self.test_results
                else 0
            )

            # ëª©í‘œ ë‹¬ì„± ì—¬ë¶€
            performance_goals_met = {
                "execution_time": total_time
                < self.performance_targets["max_execution_time"],
                "memory_usage": peak_memory
                < self.performance_targets["max_memory_usage_mb"],
                "success_rate": (
                    (successful_tests / total_tests) >= 0.9
                    if total_tests > 0
                    else False
                ),
            }

            return {
                "summary": {
                    "total_tests": total_tests,
                    "successful_tests": successful_tests,
                    "failed_tests": failed_tests,
                    "success_rate": (
                        successful_tests / total_tests if total_tests > 0 else 0
                    ),
                    "total_time": total_time,
                    "avg_execution_time": avg_execution_time,
                    "peak_memory_mb": peak_memory,
                },
                "performance_goals": {
                    "targets": self.performance_targets,
                    "achievements": performance_goals_met,
                    "overall_success": all(performance_goals_met.values()),
                },
                "detailed_results": [
                    {
                        "test_name": result.test_name,
                        "success": result.success,
                        "execution_time": result.metrics.execution_time,
                        "memory_usage_mb": result.metrics.memory_usage_mb,
                        "error": result.error_message,
                        "details": result.details,
                    }
                    for result in self.test_results
                ],
                "recommendations": self._generate_recommendations(),
            }

        except Exception as e:
            self.logger.error(f"í…ŒìŠ¤íŠ¸ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    def _generate_recommendations(self) -> List[str]:
        """ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []

        # ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ ë¶„ì„
        failed_tests = [result for result in self.test_results if not result.success]

        if failed_tests:
            recommendations.append(
                f"ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ {len(failed_tests)}ê°œë¥¼ ìš°ì„ ì ìœ¼ë¡œ ìˆ˜ì •í•˜ì„¸ìš”."
            )

        # ì„±ëŠ¥ ë¶„ì„
        slow_tests = [
            result
            for result in self.test_results
            if result.metrics.execution_time > 5.0
        ]

        if slow_tests:
            recommendations.append("ì‹¤í–‰ ì‹œê°„ì´ ê¸´ í…ŒìŠ¤íŠ¸ë“¤ì˜ ìµœì í™”ë¥¼ ê²€í† í•˜ì„¸ìš”.")

        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„
        memory_intensive_tests = [
            result
            for result in self.test_results
            if result.metrics.peak_memory_mb > 500
        ]

        if memory_intensive_tests:
            recommendations.append(
                "ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ì€ í…ŒìŠ¤íŠ¸ë“¤ì˜ ìµœì í™”ë¥¼ ê²€í† í•˜ì„¸ìš”."
            )

        if not recommendations:
            recommendations.append("ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ğŸ‰")

        return recommendations


# í¸ì˜ í•¨ìˆ˜
def run_quick_integration_test() -> Dict[str, Any]:
    """ë¹ ë¥¸ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    tester = SystemIntegrationTester()
    return tester.run_full_integration_test()


def run_performance_benchmark() -> Dict[str, Any]:
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ë§Œ ì‹¤í–‰"""
    tester = SystemIntegrationTester()
    result = tester.test_performance_benchmark()
    return {
        "benchmark_result": result,
        "success": result.success,
        "metrics": result.metrics.__dict__,
        "details": result.details,
    }
