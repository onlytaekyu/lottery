"""
ê³ ì„±ëŠ¥ ë©€í‹°í”„ë¡œì„¸ì‹± ì‹œìŠ¤í…œ - ë¡œë˜ ë¶„ì„ ìµœì í™”
âš ï¸ process_pool_manager.pyì˜ GPU ì›Œì»¤ ê´€ë¦¬ ê¸°ëŠ¥ì´ í†µí•©ë˜ì—ˆìŠµë‹ˆë‹¤.

ì´ ëª¨ë“ˆì€ ë¡œë˜ ë²ˆí˜¸ ë¶„ì„ì„ ìœ„í•œ ê³ ì„±ëŠ¥ ë©€í‹°í”„ë¡œì„¸ì‹± ì‹œìŠ¤í…œì„ ì œê³µí•©ë‹ˆë‹¤.
GPU ë©”ëª¨ë¦¬ ê¸°ë°˜ ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì • ë° ì§€ëŠ¥ì  GPU ì›Œì»¤ ê´€ë¦¬ ê¸°ëŠ¥ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
"""

import os
import time
import multiprocessing as mp
import threading
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
from typing import Dict, List, Any, Optional, Callable, Tuple
from dataclasses import dataclass, field
from queue import Queue, Empty
import psutil

try:
    import torch
    import torch.cuda
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

from ..utils.unified_logging import get_logger, log_exception

logger = get_logger(__name__)


def gpu_worker_process(task_queue: mp.Queue, result_queue: mp.Queue, gpu_id: int):
    """GPU ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ í•¨ìˆ˜ (process_pool_manager.pyì—ì„œ í†µí•©)"""
    os.environ["CUDA_VISIBLE_DEVICES"] = str(gpu_id)
    device = torch.device(f"cuda:{gpu_id}")
    logger.info(f"âœ… GPU ì›Œì»¤ ì‹œì‘ (PID: {os.getpid()}, GPU: {gpu_id})")

    while True:
        try:
            task_id, func, args, kwargs = task_queue.get()
            if func is None:
                break

            result_queue.put(("busy", gpu_id, task_id))
            result = func(*args, **kwargs, device=device)
            result_queue.put(("done", gpu_id, task_id, result))

        except Exception as e:
            log_exception(e, f"GPU ì›Œì»¤ (GPU: {gpu_id}) ì˜¤ë¥˜ ë°œìƒ")
            result_queue.put(("error", gpu_id, task_id, e))


@dataclass
class ProcessingTask:
    """ì²˜ë¦¬ ì‘ì—… ì •ì˜"""

    task_id: str
    function: Callable
    args: Tuple
    kwargs: Dict[str, Any]
    priority: int = 0
    timeout: Optional[float] = None
    created_at: float = field(default_factory=time.time)


@dataclass
class ProcessingResult:
    """ì²˜ë¦¬ ê²°ê³¼"""

    task_id: str
    result: Any
    success: bool
    error: Optional[str] = None
    execution_time: float = 0.0
    worker_id: Optional[str] = None


@dataclass
class ParallelManagerConfig:
    """ê³ ê¸‰ ë³‘ë ¬ ì²˜ë¦¬ ê´€ë¦¬ì ì„¤ì • (process_pool_manager.pyì—ì„œ í†µí•©)"""

    cpu_workers: int = min(4, psutil.cpu_count(logical=False) or 1)
    gpu_workers: int = torch.cuda.device_count() if TORCH_AVAILABLE else 0
    chunk_size: int = 100
    timeout: float = 300.0
    memory_limit_mb: int = 1024
    enable_monitoring: bool = True
    auto_restart: bool = True
    restart_threshold: int = 100  # ì‘ì—… ìˆ˜í–‰ í›„ ì¬ì‹œì‘


class EnhancedProcessPool:
    """
    ê³ ì„±ëŠ¥ í”„ë¡œì„¸ìŠ¤ í’€ ê´€ë¦¬ì (í†µí•© ë²„ì „)
    âš ï¸ process_pool_manager.pyì˜ GPU ì›Œì»¤ ê´€ë¦¬ ê¸°ëŠ¥ì´ í†µí•©ë˜ì—ˆìŠµë‹ˆë‹¤.
    """

    def __init__(
        self,
        max_workers: Optional[int] = None,
        chunk_size: int = 100,
        timeout: float = 300,
        enable_monitoring: bool = True,
        memory_limit_mb: int = 1024,
        cpu_affinity: bool = False,
        config: Optional[ParallelManagerConfig] = None,
        max_retries: int = 3,
    ):
        # ê¸°ì¡´ ì„¤ì •
        self.max_workers = max_workers or min(8, os.cpu_count() or 1)
        self.chunk_size = chunk_size
        self.timeout = timeout
        self.enable_monitoring = enable_monitoring
        self.memory_limit_mb = memory_limit_mb
        self.cpu_affinity = cpu_affinity
        self.max_retries = max_retries

        # í†µí•©ëœ ì„¤ì • (process_pool_manager.pyì—ì„œ)
        self.config = config or ParallelManagerConfig()

        self.logger = get_logger(__name__)

        # ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ í’€
        self.process_pool = None
        self.thread_pool = None

        # ê¸°ì¡´ ì‘ì—… í
        self.task_queue = Queue()
        self.result_queue = Queue()

        # í†µí•©ëœ GPU ì›Œì»¤ ê´€ë¦¬ (process_pool_manager.pyì—ì„œ)
        self.gpu_task_queue = mp.Queue()
        self.gpu_result_queue = mp.Queue()
        self.gpu_processes = []
        self.gpu_worker_status = {}  # {gpu_id: {'status': 'idle'/'busy', 'last_job_time': float}}

        # ê¸°ì¡´ ëª¨ë‹ˆí„°ë§
        self.stats = {
            "tasks_submitted": 0,
            "tasks_completed": 0,
            "tasks_failed": 0,
            "total_execution_time": 0.0,
            "peak_memory_usage": 0,
            "active_workers": 0,
            # í†µí•©ëœ GPU í†µê³„
            "gpu_tasks_submitted": 0,
            "gpu_tasks_completed": 0,
            "gpu_tasks_failed": 0,
            "gpu_workers_active": 0,
        }

        # ê¸°ì¡´ ì›Œì»¤ ìƒíƒœ ì¶”ì 
        self.worker_stats = {}
        self.active_tasks = {}

        # ì´ˆê¸°í™”
        self._initialize_pools()
        self._initialize_gpu_workers()  # GPU ì›Œì»¤ ì´ˆê¸°í™” ì¶”ê°€

        if enable_monitoring:
            self._start_monitoring()

        logger.info(f"âœ… í†µí•© ê³ ì„±ëŠ¥ í”„ë¡œì„¸ìŠ¤ í’€ ì´ˆê¸°í™” ì™„ë£Œ (CPU ì›Œì»¤: {self.max_workers}ê°œ, GPU ì›Œì»¤: {self.config.gpu_workers}ê°œ)")

    def _initialize_pools(self):
        """í”„ë¡œì„¸ìŠ¤ ë° ìŠ¤ë ˆë“œ í’€ ì´ˆê¸°í™”"""
        try:
            # í”„ë¡œì„¸ìŠ¤ í’€ ì´ˆê¸°í™”
            self.process_pool = ProcessPoolExecutor(
                max_workers=self.max_workers,
                mp_context=mp.get_context("spawn"),  # Windows í˜¸í™˜ì„±
            )

            # ìŠ¤ë ˆë“œ í’€ ì´ˆê¸°í™” (I/O ì‘ì—…ìš©)
            self.thread_pool = ThreadPoolExecutor(max_workers=min(4, self.max_workers))

            # CPU ì„ í˜¸ë„ ì„¤ì • (Linux/Unix)
            if self.cpu_affinity and hasattr(os, "sched_setaffinity"):
                try:
                    available_cpus = list(range(os.cpu_count()))
                    os.sched_setaffinity(0, available_cpus[: self.max_workers])
                    logger.debug(f"CPU ì„ í˜¸ë„ ì„¤ì •: {available_cpus[:self.max_workers]}")
                except Exception as e:
                    logger.warning(f"CPU ì„ í˜¸ë„ ì„¤ì • ì‹¤íŒ¨: {e}")

        except Exception as e:
            logger.error(f"í”„ë¡œì„¸ìŠ¤ í’€ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    def _initialize_gpu_workers(self):
        """GPU ì›Œì»¤ ì´ˆê¸°í™” (process_pool_manager.pyì—ì„œ í†µí•©)"""
        if not TORCH_AVAILABLE or self.config.gpu_workers <= 0:
            logger.info("GPU ì›Œì»¤ ì‚¬ìš© ì•ˆí•¨ (CUDA ë¯¸ì‚¬ìš© ë˜ëŠ” GPU ì—†ìŒ)")
            return

        try:
            for i in range(self.config.gpu_workers):
                proc = mp.Process(
                    target=gpu_worker_process,
                    args=(self.gpu_task_queue, self.gpu_result_queue, i),
                )
                proc.start()
                self.gpu_processes.append(proc)
                self.gpu_worker_status[i] = {
                    "status": "idle",
                    "last_job_time": time.time(),
                }

            self.stats["gpu_workers_active"] = self.config.gpu_workers
            logger.info(f"âœ… GPU ì›Œì»¤ {self.config.gpu_workers}ê°œ ì´ˆê¸°í™” ì™„ë£Œ")

        except Exception as e:
            logger.error(f"GPU ì›Œì»¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    def _get_idle_gpu_worker(self) -> Optional[int]:
        """ê°€ì¥ ì˜¤ë«ë™ì•ˆ ìœ íœ´ ìƒíƒœì˜€ë˜ GPU ì›Œì»¤ IDë¥¼ ë°˜í™˜ (process_pool_manager.pyì—ì„œ í†µí•©)"""
        idle_workers = {
            gpu_id: data
            for gpu_id, data in self.gpu_worker_status.items()
            if data["status"] == "idle"
        }
        if not idle_workers:
            return None
        # ë§ˆì§€ë§‰ ì‘ì—… ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ê°€ì¥ ì˜¤ë˜ëœ ì›Œì»¤ ì„ íƒ
        return min(
            idle_workers, key=lambda gpu_id: idle_workers[gpu_id]["last_job_time"]
        )

    def _execute_on_gpu(self, func: Callable, tasks: List[Any]) -> List[Any]:
        """GPU ì›Œì»¤ í’€ì—ì„œ ì‘ì—… ì‹¤í–‰ (process_pool_manager.pyì—ì„œ í†µí•©)"""
        num_tasks = len(tasks)
        results = [None] * num_tasks
        retries = [0] * num_tasks

        task_map = {i: task for i, task in enumerate(tasks)}
        self.stats["gpu_tasks_submitted"] += num_tasks

        while task_map:
            # ìœ íœ´ GPU ì›Œì»¤ì—ê²Œ ì‘ì—… í• ë‹¹
            idle_worker_id = self._get_idle_gpu_worker()
            if idle_worker_id is not None and task_map:
                task_id, task_args = task_map.popitem()
                self.gpu_worker_status[idle_worker_id]["status"] = "busy"
                self.gpu_task_queue.put((task_id, func, task_args, {}))

            # ê²°ê³¼ ì²˜ë¦¬
            try:
                status, gpu_id, task_id, data = self.gpu_result_queue.get(timeout=1)

                if status == "done":
                    results[task_id] = data
                    self.gpu_worker_status[gpu_id]["status"] = "idle"
                    self.gpu_worker_status[gpu_id]["last_job_time"] = time.time()
                    self.stats["gpu_tasks_completed"] += 1
                elif status == "error":
                    logger.warning(f"GPU ì‘ì—… {task_id} ì‹¤íŒ¨. ì¬ì‹œë„í•©ë‹ˆë‹¤. (ì˜¤ë¥˜: {data})")
                    self.gpu_worker_status[gpu_id]["status"] = "idle"
                    if retries[task_id] < self.max_retries:
                        retries[task_id] += 1
                        task_map[task_id] = tasks[task_id]  # ì‘ì—…ì„ ë‹¤ì‹œ íì— ë„£ìŒ
                    else:
                        logger.error(f"GPU ì‘ì—… {task_id} ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼. ìµœì¢… ì‹¤íŒ¨ ì²˜ë¦¬.")
                        results[task_id] = data  # ì˜ˆì™¸ë¥¼ ê²°ê³¼ë¡œ ì €ì¥
                        self.stats["gpu_tasks_failed"] += 1
                elif status == "busy":
                    self.gpu_worker_status[gpu_id]["status"] = "busy"

            except Empty:
                # íƒ€ì„ì•„ì›ƒ ë°œìƒ, ê³„ì† ì§„í–‰
                continue

        return results

    def _execute_on_cpu(self, func: Callable, tasks: List[Any]) -> List[Any]:
        """CPU í”„ë¡œì„¸ìŠ¤ í’€ì—ì„œ ì‘ì—… ì‹¤í–‰ (process_pool_manager.pyì—ì„œ í†µí•©)"""
        if not self.process_pool:
            raise RuntimeError("í”„ë¡œì„¸ìŠ¤ í’€ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
        futures = [self.process_pool.submit(func, *task) for task in tasks]
        results = [None] * len(tasks)
        future_map = {future: i for i, future in enumerate(futures)}

        for future in as_completed(future_map):
            index = future_map[future]
            try:
                results[index] = future.result()
                self.stats["tasks_completed"] += 1
            except Exception as e:
                logger.error(f"CPU ì‘ì—… {index} ì‹¤íŒ¨: {e}")
                results[index] = e  # ì˜ˆì™¸ë¥¼ ê²°ê³¼ë¡œ ì €ì¥
                self.stats["tasks_failed"] += 1

        return results

    def execute_parallel(
        self, func: Callable, tasks: List[Any], prefer_gpu: bool = False
    ) -> List[Any]:
        """
        ì‘ì—…ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰ (GPU ìš°ì„  ë˜ëŠ” CPU) (process_pool_manager.pyì—ì„œ í†µí•©)
        """
        if prefer_gpu and self.config.gpu_workers > 0 and TORCH_AVAILABLE:
            logger.info(f"ğŸš€ GPU ë³‘ë ¬ ì‹¤í–‰: {len(tasks)}ê°œ ì‘ì—…")
            return self._execute_on_gpu(func, tasks)
        else:
            logger.info(f"ğŸš€ CPU ë³‘ë ¬ ì‹¤í–‰: {len(tasks)}ê°œ ì‘ì—…")
            self.stats["tasks_submitted"] += len(tasks)
            return self._execute_on_cpu(func, tasks)

    def parallel_analyze_chunks(
        self,
        data_chunks: List[Any],
        analyzer_func: Callable,
        combine_func: Optional[Callable] = None,
        prefer_gpu: bool = False,
        **kwargs,
    ) -> Any:
        """
        ë°ì´í„° ì²­í¬ë¥¼ ë³‘ë ¬ë¡œ ë¶„ì„ (í†µí•© ë²„ì „)

        Args:
            data_chunks: ë¶„ì„í•  ë°ì´í„° ì²­í¬ ë¦¬ìŠ¤íŠ¸
            analyzer_func: ë¶„ì„ í•¨ìˆ˜
            combine_func: ê²°ê³¼ ê²°í•© í•¨ìˆ˜ (Noneì´ë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜)
            prefer_gpu: GPU ì‚¬ìš© ì„ í˜¸ ì—¬ë¶€
            **kwargs: ë¶„ì„ í•¨ìˆ˜ì— ì „ë‹¬í•  ì¶”ê°€ ì¸ì

        Returns:
            ë¶„ì„ ê²°ê³¼ (ê²°í•©ëœ ê²°ê³¼ ë˜ëŠ” ê²°ê³¼ ë¦¬ìŠ¤íŠ¸)
        """
        try:
            logger.info(f"ğŸš€ í†µí•© ë³‘ë ¬ ë¶„ì„ ì‹œì‘: {len(data_chunks)}ê°œ ì²­í¬ (GPU ì„ í˜¸: {prefer_gpu})")
            start_time = time.time()

            if prefer_gpu and self.config.gpu_workers > 0 and TORCH_AVAILABLE:
                # GPU ë³‘ë ¬ ì²˜ë¦¬
                results = self._execute_on_gpu(analyzer_func, data_chunks)
            else:
                # ê¸°ì¡´ CPU ë³‘ë ¬ ì²˜ë¦¬
                if not self.process_pool:
                    # í”„ë¡œì„¸ìŠ¤ í’€ì´ ì—†ìœ¼ë©´ ìˆœì°¨ ì‹¤í–‰
                    results = []
                    for chunk in data_chunks:
                        try:
                            result = analyzer_func(chunk, **kwargs)
                            results.append(result)
                        except Exception as e:
                            logger.error(f"ìˆœì°¨ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
                            results.append(None)
                else:
                    # í”„ë¡œì„¸ìŠ¤ í’€ ì‚¬ìš©
                    futures = []
                    for i, chunk in enumerate(data_chunks):
                        task_id = f"chunk_analysis_{i}_{time.time()}"

                        future = self.process_pool.submit(
                            self._execute_analysis_task, task_id, analyzer_func, chunk, kwargs
                        )

                        futures.append((task_id, future))
                        self.stats["tasks_submitted"] += 1

                    # ê²°ê³¼ ìˆ˜ì§‘
                    results = []
                    failed_tasks = []

                    for task_id, future in futures:
                        try:
                            result = future.result(timeout=self.timeout)
                            if result.success:
                                results.append(result.result)
                                self.stats["tasks_completed"] += 1
                            else:
                                failed_tasks.append((task_id, result.error))
                                self.stats["tasks_failed"] += 1

                        except Exception as e:
                            failed_tasks.append((task_id, str(e)))
                            self.stats["tasks_failed"] += 1
                            logger.error(f"ì‘ì—… {task_id} ì‹¤íŒ¨: {e}")

                    if failed_tasks:
                        logger.warning(f"ì‹¤íŒ¨í•œ ì‘ì—…: {len(failed_tasks)}ê°œ")
                        for task_id, error in failed_tasks:
                            logger.debug(f"ì‹¤íŒ¨ ì‘ì—… {task_id}: {error}")

            execution_time = time.time() - start_time
            self.stats["total_execution_time"] += execution_time

            # ê²°ê³¼ ê²°í•©
            if combine_func and callable(combine_func):
                logger.debug("ê²°ê³¼ ê²°í•© ì¤‘...")
                final_result = combine_func(results)
                logger.info(f"âœ… í†µí•© ë³‘ë ¬ ë¶„ì„ ì™„ë£Œ (ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ)")
                return final_result
            else:
                logger.info(f"âœ… í†µí•© ë³‘ë ¬ ë¶„ì„ ì™„ë£Œ (ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ, ê²°ê³¼: {len(results)}ê°œ)")
                return results

        except Exception as e:
            logger.error(f"í†µí•© ë³‘ë ¬ ë¶„ì„ ì‹¤íŒ¨: {e}")
            raise

    def parallel_map(
        self,
        func: Callable,
        iterable: List[Any],
        chunksize: Optional[int] = None,
        **kwargs,
    ) -> List[Any]:
        """
        í•¨ìˆ˜ë¥¼ iterableì— ë³‘ë ¬ë¡œ ë§¤í•‘

        Args:
            func: ì ìš©í•  í•¨ìˆ˜
            iterable: ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            chunksize: ì²­í¬ í¬ê¸°
            **kwargs: í•¨ìˆ˜ì— ì „ë‹¬í•  ì¶”ê°€ ì¸ì

        Returns:
            ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        try:
            chunk_size = chunksize or max(1, len(iterable) // (self.max_workers * 2))

            # ë°ì´í„° ì²­í‚¹
            chunks = [
                iterable[i : i + chunk_size]
                for i in range(0, len(iterable), chunk_size)
            ]

            # ì²­í¬ ì²˜ë¦¬ í•¨ìˆ˜
            def process_chunk(chunk):
                return [func(item, **kwargs) for item in chunk]

            # ê²°ê³¼ ê²°í•© í•¨ìˆ˜
            def combine_results(chunk_results):
                combined = []
                for chunk_result in chunk_results:
                    combined.extend(chunk_result)
                return combined

            return self.parallel_analyze_chunks(chunks, process_chunk, combine_results)

        except Exception as e:
            logger.error(f"ë³‘ë ¬ ë§¤í•‘ ì‹¤íŒ¨: {e}")
            raise

    def adaptive_parallel_processing(
        self,
        data: List[Any],
        func: Callable,
        min_chunk_size: int = 10,
        max_chunk_size: int = 1000,
        target_execution_time: float = 1.0,
        **kwargs,
    ) -> List[Any]:
        """
        ì ì‘í˜• ë³‘ë ¬ ì²˜ë¦¬ - ì„±ëŠ¥ì— ë”°ë¼ ì²­í¬ í¬ê¸° ìë™ ì¡°ì •

        Args:
            data: ì²˜ë¦¬í•  ë°ì´í„°
            func: ì²˜ë¦¬ í•¨ìˆ˜
            min_chunk_size: ìµœì†Œ ì²­í¬ í¬ê¸°
            max_chunk_size: ìµœëŒ€ ì²­í¬ í¬ê¸°
            target_execution_time: ëª©í‘œ ì‹¤í–‰ ì‹œê°„ (ì´ˆ)
            **kwargs: í•¨ìˆ˜ ì¸ì

        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        try:
            if len(data) <= min_chunk_size:
                # ë°ì´í„°ê°€ ì‘ìœ¼ë©´ ë‹¨ì¼ ìŠ¤ë ˆë“œë¡œ ì²˜ë¦¬
                return [func(item, **kwargs) for item in data]

            # ì´ˆê¸° ì²­í¬ í¬ê¸° ê²°ì •
            initial_chunk_size = min(
                max_chunk_size, max(min_chunk_size, len(data) // self.max_workers)
            )

            # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì‘ì€ ìƒ˜í”Œ ì²˜ë¦¬
            test_size = min(initial_chunk_size, len(data) // 10)
            if test_size > 0:
                test_data = data[:test_size]

                start_time = time.time()
                self.parallel_map(func, test_data, test_size, **kwargs)
                test_time = time.time() - start_time

                if test_time > 0:
                    # ëª©í‘œ ì‹œê°„ì— ë§ì¶° ì²­í¬ í¬ê¸° ì¡°ì •
                    optimal_chunk_size = int(
                        test_size * target_execution_time / test_time
                    )
                    optimal_chunk_size = max(
                        min_chunk_size, min(max_chunk_size, optimal_chunk_size)
                    )
                else:
                    optimal_chunk_size = initial_chunk_size
            else:
                optimal_chunk_size = initial_chunk_size

            self.logger.debug(f"ì ì‘í˜• ì²­í¬ í¬ê¸°: {optimal_chunk_size}")

            # ì „ì²´ ë°ì´í„° ì²˜ë¦¬
            return self.parallel_map(func, data, optimal_chunk_size, **kwargs)

        except Exception as e:
            self.logger.error(f"ì ì‘í˜• ë³‘ë ¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            # í´ë°±: ë‹¨ì¼ ìŠ¤ë ˆë“œ ì²˜ë¦¬
            return [func(item, **kwargs) for item in data]

    def pipeline_processing(
        self,
        data: List[Any],
        pipeline_stages: List[Callable],
        stage_configs: Optional[List[Dict[str, Any]]] = None,
    ) -> List[Any]:
        """
        íŒŒì´í”„ë¼ì¸ ë³‘ë ¬ ì²˜ë¦¬

        Args:
            data: ì…ë ¥ ë°ì´í„°
            pipeline_stages: íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ í•¨ìˆ˜ë“¤
            stage_configs: ê° ë‹¨ê³„ë³„ ì„¤ì •

        Returns:
            ìµœì¢… ì²˜ë¦¬ ê²°ê³¼
        """
        try:
            self.logger.info(f"ğŸ”„ íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì‹œì‘: {len(pipeline_stages)}ë‹¨ê³„")

            current_data = data
            stage_configs = stage_configs or [{}] * len(pipeline_stages)

            for i, (stage_func, config) in enumerate(
                zip(pipeline_stages, stage_configs)
            ):
                self.logger.debug(
                    f"íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ {i+1}/{len(pipeline_stages)} ì²˜ë¦¬ ì¤‘"
                )

                start_time = time.time()
                current_data = self.parallel_map(stage_func, current_data, **config)
                stage_time = time.time() - start_time

                self.logger.debug(f"ë‹¨ê³„ {i+1} ì™„ë£Œ: {stage_time:.2f}ì´ˆ")

            self.logger.info("âœ… íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì™„ë£Œ")
            return current_data

        except Exception as e:
            self.logger.error(f"íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise

    def _execute_analysis_task(
        self, task_id: str, func: Callable, data: Any, kwargs: Dict[str, Any]
    ) -> ProcessingResult:
        """ë¶„ì„ ì‘ì—… ì‹¤í–‰"""
        start_time = time.time()
        worker_id = f"worker_{os.getpid()}"

        try:
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
            process = psutil.Process()
            initial_memory = process.memory_info().rss / 1024 / 1024  # MB

            # ì‘ì—… ì‹¤í–‰
            result = func(data, **kwargs)

            # ìµœì¢… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
            final_memory = process.memory_info().rss / 1024 / 1024  # MB
            final_memory - initial_memory

            execution_time = time.time() - start_time

            # ë©”ëª¨ë¦¬ ì œí•œ ì²´í¬
            if final_memory > self.memory_limit_mb:
                self.logger.warning(
                    f"ë©”ëª¨ë¦¬ ì œí•œ ì´ˆê³¼: {final_memory:.1f}MB > {self.memory_limit_mb}MB"
                )

            return ProcessingResult(
                task_id=task_id,
                result=result,
                success=True,
                execution_time=execution_time,
                worker_id=worker_id,
            )

        except Exception as e:
            execution_time = time.time() - start_time
            error_msg = f"ì‘ì—… ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}"

            return ProcessingResult(
                task_id=task_id,
                result=None,
                success=False,
                error=error_msg,
                execution_time=execution_time,
                worker_id=worker_id,
            )

    def _start_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ìŠ¤ë ˆë“œ ì‹œì‘"""

        def monitor_worker():
            while True:
                try:
                    # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§
                    cpu_percent = psutil.cpu_percent(interval=1)
                    memory_percent = psutil.virtual_memory().percent

                    # í†µê³„ ì—…ë°ì´íŠ¸
                    self.stats["peak_memory_usage"] = max(
                        self.stats["peak_memory_usage"], memory_percent
                    )

                    # ë¡œê·¸ ì¶œë ¥ (5ë¶„ë§ˆë‹¤)
                    if int(time.time()) % 300 == 0:
                        self.logger.debug(
                            f"ì‹œìŠ¤í…œ ìƒíƒœ - CPU: {cpu_percent:.1f}%, "
                            f"ë©”ëª¨ë¦¬: {memory_percent:.1f}%, "
                            f"ì™„ë£Œëœ ì‘ì—…: {self.stats['tasks_completed']}"
                        )

                    time.sleep(10)  # 10ì´ˆë§ˆë‹¤ ëª¨ë‹ˆí„°ë§

                except Exception as e:
                    self.logger.error(f"ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
                    time.sleep(30)

        monitor_thread = threading.Thread(target=monitor_worker, daemon=True)
        monitor_thread.start()
        self.logger.debug("ëª¨ë‹ˆí„°ë§ ìŠ¤ë ˆë“œ ì‹œì‘")

    def get_stats(self) -> Dict[str, Any]:
        """í†µê³„ ì •ë³´ ë°˜í™˜"""
        try:
            # ì‹œìŠ¤í…œ ì •ë³´ ì¶”ê°€
            cpu_count = os.cpu_count()
            memory_info = psutil.virtual_memory()

            return {
                **self.stats,
                "max_workers": self.max_workers,
                "chunk_size": self.chunk_size,
                "timeout": self.timeout,
                "system_info": {
                    "cpu_count": cpu_count,
                    "memory_total_gb": memory_info.total / (1024**3),
                    "memory_available_gb": memory_info.available / (1024**3),
                    "memory_percent": memory_info.percent,
                },
                "efficiency": {
                    "success_rate": (
                        self.stats["tasks_completed"]
                        / max(1, self.stats["tasks_submitted"])
                    )
                    * 100,
                    "avg_execution_time": (
                        self.stats["total_execution_time"]
                        / max(1, self.stats["tasks_completed"])
                    ),
                    "throughput": (
                        self.stats["tasks_completed"]
                        / max(1, self.stats["total_execution_time"])
                    ),
                },
            }
        except Exception as e:
            self.logger.error(f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return self.stats

    def shutdown(self, wait: bool = True):
        """
        ëª¨ë“  ë¦¬ì†ŒìŠ¤ ì¢…ë£Œ (í†µí•© ë²„ì „)
        """
        try:
            # ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ í’€ ì¢…ë£Œ
            if self.process_pool:
                self.process_pool.shutdown(wait=wait)
                self.process_pool = None

            # ìŠ¤ë ˆë“œ í’€ ì¢…ë£Œ
            if self.thread_pool:
                self.thread_pool.shutdown(wait=wait)
                self.thread_pool = None

            # GPU ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ (process_pool_manager.pyì—ì„œ í†µí•©)
            if self.config.gpu_workers > 0 and self.gpu_processes:
                logger.info(f"GPU ì›Œì»¤ {len(self.gpu_processes)}ê°œ ì¢…ë£Œ ì¤‘...")
                
                # ì¢…ë£Œ ì‹ í˜¸ ì „ì†¡
                for _ in range(self.config.gpu_workers):
                    self.gpu_task_queue.put((None, None, None, None))
                
                # í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ëŒ€ê¸°
                for proc in self.gpu_processes:
                    if proc.is_alive():
                        proc.join(timeout=5)
                        if proc.is_alive():
                            logger.warning(f"GPU ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ {proc.pid} ê°•ì œ ì¢…ë£Œ")
                            proc.terminate()
                
                self.gpu_processes.clear()
                self.gpu_worker_status.clear()
                
                logger.info("âœ… GPU ì›Œì»¤ ì¢…ë£Œ ì™„ë£Œ")

            # ëª¨ë‹ˆí„°ë§ ì¤‘ì§€
            if hasattr(self, '_monitor_thread') and self._monitor_thread.is_alive():
                self._monitor_thread.join(timeout=2)

            logger.info("âœ… í†µí•© í”„ë¡œì„¸ìŠ¤ í’€ ì¢…ë£Œ ì™„ë£Œ")

        except Exception as e:
            logger.error(f"í”„ë¡œì„¸ìŠ¤ í’€ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")
            raise

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.shutdown()


# ì „ì—­ í”„ë¡œì„¸ìŠ¤ í’€ ì¸ìŠ¤í„´ìŠ¤
_process_pool = None
_pool_lock = threading.Lock()


def get_enhanced_process_pool(
    config: Optional[Dict[str, Any]] = None,
) -> EnhancedProcessPool:
    """ê³ ì„±ëŠ¥ í”„ë¡œì„¸ìŠ¤ í’€ ë°˜í™˜ (ì‹±ê¸€í†¤)"""
    global _process_pool

    with _pool_lock:
        if _process_pool is None:
            pool_config = config or {}
            _process_pool = EnhancedProcessPool(**pool_config)
        return _process_pool


def cleanup_process_pool():
    """í”„ë¡œì„¸ìŠ¤ í’€ ì •ë¦¬"""
    global _process_pool

    with _pool_lock:
        if _process_pool is not None:
            _process_pool.shutdown()
            _process_pool = None


# í¸ì˜ í•¨ìˆ˜ë“¤
def parallel_lottery_analysis(
    data_chunks: List[Any], analyzer_func: Callable, **kwargs
) -> List[Any]:
    """ë¡œë˜ ë¶„ì„ì„ ìœ„í•œ ë³‘ë ¬ ì²˜ë¦¬ í¸ì˜ í•¨ìˆ˜"""
    pool = get_enhanced_process_pool()
    return pool.parallel_analyze_chunks(data_chunks, analyzer_func, **kwargs)


class DynamicBatchSizeController:
    """GPU ë©”ëª¨ë¦¬ ê¸°ë°˜ ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì •ê¸°"""

    def __init__(
        self,
        initial_batch_size: int = 2000,
        min_batch_size: int = 500,
        max_batch_size: int = 5000,
        memory_threshold: float = 0.8,
        adjustment_factor: float = 0.7,
    ):
        self.current_batch_size = initial_batch_size
        self.min_batch_size = min_batch_size
        self.max_batch_size = max_batch_size
        self.memory_threshold = memory_threshold
        self.adjustment_factor = adjustment_factor

        # GPU ê°€ìš©ì„± í™•ì¸
        self.gpu_available = TORCH_AVAILABLE and torch.cuda.is_available()

        # ì„±ëŠ¥ í†µê³„
        self.oom_count = 0
        self.successful_batches = 0
        self.total_processing_time = 0.0
        self.last_adjustment_time = time.time()

        # ìŠ¤ë ˆë“œ ì•ˆì „ì„±
        self.lock = threading.Lock()

        if self.gpu_available:
            logger.info(
                f"âœ… GPU ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì • í™œì„±í™” - ì´ˆê¸° í¬ê¸°: {initial_batch_size}"
            )
        else:
            logger.info(f"âš ï¸ CPU ê¸°ë°˜ ë°°ì¹˜ ì²˜ë¦¬ - ê³ ì • í¬ê¸°: {initial_batch_size}")

    def get_current_batch_size(self) -> int:
        """í˜„ì¬ ìµœì  ë°°ì¹˜ í¬ê¸° ë°˜í™˜"""
        with self.lock:
            if self.gpu_available:
                try:
                    memory_usage = self._get_gpu_memory_usage()
                    if memory_usage > self.memory_threshold:
                        self._reduce_batch_size()
                    elif (
                        memory_usage < 0.5
                        and self.current_batch_size < self.max_batch_size
                    ):
                        self._increase_batch_size()
                except Exception as e:
                    logger.warning(f"GPU ë©”ëª¨ë¦¬ í™•ì¸ ì‹¤íŒ¨: {e}")

            return self.current_batch_size

    def handle_oom(self) -> int:
        """OutOfMemory ë°œìƒì‹œ ë°°ì¹˜ í¬ê¸° ê°ì†Œ"""
        with self.lock:
            self.oom_count += 1
            old_size = self.current_batch_size

            self.current_batch_size = max(
                self.min_batch_size, int(self.current_batch_size * 0.5)
            )

            logger.warning(
                f"ğŸš¨ OOM ë°œìƒ - ë°°ì¹˜ í¬ê¸° ì¡°ì •: {old_size} â†’ {self.current_batch_size}"
            )
            return self.current_batch_size

    def report_success(self, processing_time: float):
        """ì„±ê³µì ì¸ ë°°ì¹˜ ì²˜ë¦¬ ë³´ê³ """
        with self.lock:
            self.successful_batches += 1
            self.total_processing_time += processing_time

            if (
                self.successful_batches % 10 == 0
                and time.time() - self.last_adjustment_time > 60
            ):
                self._consider_batch_increase()

    def _get_gpu_memory_usage(self) -> float:
        """GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ë°˜í™˜"""
        if not self.gpu_available:
            return 0.0

        try:
            if TORCH_AVAILABLE:
                allocated = torch.cuda.memory_allocated()
                cached = torch.cuda.memory_reserved()
                total = torch.cuda.get_device_properties(0).total_memory
                return max(allocated, cached) / total
            return 0.0
        except Exception as e:
            logger.debug(f"GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  í™•ì¸ ì‹¤íŒ¨: {e}")
            return 0.0

    def _reduce_batch_size(self):
        """ë°°ì¹˜ í¬ê¸° ê°ì†Œ"""
        old_size = self.current_batch_size
        self.current_batch_size = max(
            self.min_batch_size, int(self.current_batch_size * self.adjustment_factor)
        )

        if old_size != self.current_batch_size:
            logger.info(f"ğŸ“‰ ë°°ì¹˜ í¬ê¸° ê°ì†Œ: {old_size} â†’ {self.current_batch_size}")
            self.last_adjustment_time = time.time()

    def _increase_batch_size(self):
        """ë°°ì¹˜ í¬ê¸° ì¦ê°€"""
        old_size = self.current_batch_size
        self.current_batch_size = min(
            self.max_batch_size, int(self.current_batch_size * 1.2)
        )

        if old_size != self.current_batch_size:
            logger.info(f"ğŸ“ˆ ë°°ì¹˜ í¬ê¸° ì¦ê°€: {old_size} â†’ {self.current_batch_size}")
            self.last_adjustment_time = time.time()

    def _consider_batch_increase(self):
        """ë°°ì¹˜ í¬ê¸° ì¦ê°€ ê³ ë ¤"""
        if (
            self.oom_count == 0
            and self.current_batch_size < self.max_batch_size
            and self._get_gpu_memory_usage() < 0.6
        ):
            self._increase_batch_size()

    def get_stats(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        with self.lock:
            avg_time = self.total_processing_time / max(1, self.successful_batches)

            return {
                "current_batch_size": self.current_batch_size,
                "oom_count": self.oom_count,
                "successful_batches": self.successful_batches,
                "average_processing_time": avg_time,
                "gpu_memory_usage": self._get_gpu_memory_usage(),
                "gpu_available": self.gpu_available,
            }


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_batch_controller_instance = None


def get_dynamic_batch_controller(**kwargs) -> DynamicBatchSizeController:
    """ë™ì  ë°°ì¹˜ í¬ê¸° ì»¨íŠ¸ë¡¤ëŸ¬ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _batch_controller_instance

    if _batch_controller_instance is None:
        _batch_controller_instance = DynamicBatchSizeController(**kwargs)

    return _batch_controller_instance


def adaptive_lottery_processing(
    lottery_data: List[Any], processing_func: Callable, **kwargs
) -> List[Any]:
    """ì ì‘í˜• ë¡œë˜ ë°ì´í„° ì²˜ë¦¬ í¸ì˜ í•¨ìˆ˜ (ë™ì  ë°°ì¹˜ í¬ê¸° ì§€ì›)"""
    pool = get_enhanced_process_pool()

    # ë™ì  ë°°ì¹˜ í¬ê¸° ì»¨íŠ¸ë¡¤ëŸ¬ ì‚¬ìš©
    batch_controller = get_dynamic_batch_controller()
    optimal_batch_size = batch_controller.get_current_batch_size()

    # ë°°ì¹˜ í¬ê¸°ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì •í•˜ì—¬ ì²˜ë¦¬
    try:
        start_time = time.time()
        result = pool.adaptive_parallel_processing(
            lottery_data, processing_func, max_chunk_size=optimal_batch_size, **kwargs
        )
        processing_time = time.time() - start_time
        batch_controller.report_success(processing_time)
        return result

    except RuntimeError as e:
        if "out of memory" in str(e).lower():
            batch_controller.handle_oom()
            # ì¤„ì–´ë“  ë°°ì¹˜ í¬ê¸°ë¡œ ì¬ì‹œë„
            new_batch_size = batch_controller.get_current_batch_size()
            return pool.adaptive_parallel_processing(
                lottery_data, processing_func, max_chunk_size=new_batch_size, **kwargs
            )
        raise


# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
CPUBatchProcessor = EnhancedProcessPool
