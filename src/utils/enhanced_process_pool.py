"""
ê³ ì„±ëŠ¥ ë©€í‹°í”„ë¡œì„¸ì‹± ì‹œìŠ¤í…œ - ë¡œë˜ ë¶„ì„ ìµœì í™”

ì´ ëª¨ë“ˆì€ ë¡œë˜ ë²ˆí˜¸ ë¶„ì„ì„ ìœ„í•œ ê³ ì„±ëŠ¥ ë©€í‹°í”„ë¡œì„¸ì‹± ì‹œìŠ¤í…œì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import os
import time
import multiprocessing as mp
import threading
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
from typing import Dict, List, Any, Optional, Callable, Tuple, Union
from dataclasses import dataclass, field
from queue import Queue, Empty
import pickle
import psutil
from ..utils.unified_logging import get_logger

logger = get_logger(__name__)


@dataclass
class ProcessingTask:
    """ì²˜ë¦¬ ì‘ì—… ì •ì˜"""

    task_id: str
    function: Callable
    args: Tuple
    kwargs: Dict[str, Any]
    priority: int = 0
    timeout: Optional[float] = None
    created_at: float = field(default_factory=time.time)


@dataclass
class ProcessingResult:
    """ì²˜ë¦¬ ê²°ê³¼"""

    task_id: str
    result: Any
    success: bool
    error: Optional[str] = None
    execution_time: float = 0.0
    worker_id: Optional[str] = None


class EnhancedProcessPool:
    """ê³ ì„±ëŠ¥ í”„ë¡œì„¸ìŠ¤ í’€ ê´€ë¦¬ì"""

    def __init__(
        self,
        max_workers: Optional[int] = None,
        chunk_size: int = 100,
        timeout: float = 300,
        enable_monitoring: bool = True,
        memory_limit_mb: int = 1024,
        cpu_affinity: bool = False,
    ):
        self.max_workers = max_workers or min(8, os.cpu_count() or 1)
        self.chunk_size = chunk_size
        self.timeout = timeout
        self.enable_monitoring = enable_monitoring
        self.memory_limit_mb = memory_limit_mb
        self.cpu_affinity = cpu_affinity

        self.logger = get_logger(__name__)

        # í”„ë¡œì„¸ìŠ¤ í’€
        self.process_pool = None
        self.thread_pool = None

        # ì‘ì—… í
        self.task_queue = Queue()
        self.result_queue = Queue()

        # ëª¨ë‹ˆí„°ë§
        self.stats = {
            "tasks_submitted": 0,
            "tasks_completed": 0,
            "tasks_failed": 0,
            "total_execution_time": 0.0,
            "peak_memory_usage": 0,
            "active_workers": 0,
        }

        # ì›Œì»¤ ìƒíƒœ ì¶”ì 
        self.worker_stats = {}
        self.active_tasks = {}

        # ì´ˆê¸°í™”
        self._initialize_pools()

        if enable_monitoring:
            self._start_monitoring()

        self.logger.info(
            f"âœ… ê³ ì„±ëŠ¥ í”„ë¡œì„¸ìŠ¤ í’€ ì´ˆê¸°í™” ì™„ë£Œ (ì›Œì»¤: {self.max_workers}ê°œ)"
        )

    def _initialize_pools(self):
        """í”„ë¡œì„¸ìŠ¤ ë° ìŠ¤ë ˆë“œ í’€ ì´ˆê¸°í™”"""
        try:
            # í”„ë¡œì„¸ìŠ¤ í’€ ì´ˆê¸°í™”
            self.process_pool = ProcessPoolExecutor(
                max_workers=self.max_workers,
                mp_context=mp.get_context("spawn"),  # Windows í˜¸í™˜ì„±
            )

            # ìŠ¤ë ˆë“œ í’€ ì´ˆê¸°í™” (I/O ì‘ì—…ìš©)
            self.thread_pool = ThreadPoolExecutor(max_workers=min(4, self.max_workers))

            # CPU ì„ í˜¸ë„ ì„¤ì • (Linux/Unix)
            if self.cpu_affinity and hasattr(os, "sched_setaffinity"):
                try:
                    available_cpus = list(range(os.cpu_count()))
                    os.sched_setaffinity(0, available_cpus[: self.max_workers])
                    self.logger.debug(
                        f"CPU ì„ í˜¸ë„ ì„¤ì •: {available_cpus[:self.max_workers]}"
                    )
                except Exception as e:
                    self.logger.warning(f"CPU ì„ í˜¸ë„ ì„¤ì • ì‹¤íŒ¨: {e}")

        except Exception as e:
            self.logger.error(f"í”„ë¡œì„¸ìŠ¤ í’€ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    def parallel_analyze_chunks(
        self,
        data_chunks: List[Any],
        analyzer_func: Callable,
        combine_func: Optional[Callable] = None,
        **kwargs,
    ) -> Any:
        """
        ë°ì´í„° ì²­í¬ë¥¼ ë³‘ë ¬ë¡œ ë¶„ì„

        Args:
            data_chunks: ë¶„ì„í•  ë°ì´í„° ì²­í¬ ë¦¬ìŠ¤íŠ¸
            analyzer_func: ë¶„ì„ í•¨ìˆ˜
            combine_func: ê²°ê³¼ ê²°í•© í•¨ìˆ˜ (Noneì´ë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜)
            **kwargs: ë¶„ì„ í•¨ìˆ˜ì— ì „ë‹¬í•  ì¶”ê°€ ì¸ì

        Returns:
            ë¶„ì„ ê²°ê³¼ (ê²°í•©ëœ ê²°ê³¼ ë˜ëŠ” ê²°ê³¼ ë¦¬ìŠ¤íŠ¸)
        """
        try:
            self.logger.info(f"ğŸš€ ë³‘ë ¬ ë¶„ì„ ì‹œì‘: {len(data_chunks)}ê°œ ì²­í¬")
            start_time = time.time()

            # ì‘ì—… ì œì¶œ
            futures = []
            for i, chunk in enumerate(data_chunks):
                task_id = f"chunk_analysis_{i}_{time.time()}"

                future = self.process_pool.submit(
                    self._execute_analysis_task, task_id, analyzer_func, chunk, kwargs
                )

                futures.append((task_id, future))
                self.stats["tasks_submitted"] += 1

            # ê²°ê³¼ ìˆ˜ì§‘
            results = []
            failed_tasks = []

            for task_id, future in futures:
                try:
                    result = future.result(timeout=self.timeout)
                    if result.success:
                        results.append(result.result)
                        self.stats["tasks_completed"] += 1
                    else:
                        failed_tasks.append((task_id, result.error))
                        self.stats["tasks_failed"] += 1

                except Exception as e:
                    failed_tasks.append((task_id, str(e)))
                    self.stats["tasks_failed"] += 1
                    self.logger.error(f"ì‘ì—… {task_id} ì‹¤íŒ¨: {e}")

            execution_time = time.time() - start_time
            self.stats["total_execution_time"] += execution_time

            if failed_tasks:
                self.logger.warning(f"ì‹¤íŒ¨í•œ ì‘ì—…: {len(failed_tasks)}ê°œ")
                for task_id, error in failed_tasks:
                    self.logger.debug(f"ì‹¤íŒ¨ ì‘ì—… {task_id}: {error}")

            # ê²°ê³¼ ê²°í•©
            if combine_func and results:
                combined_result = combine_func(results)
                self.logger.info(f"âœ… ë³‘ë ¬ ë¶„ì„ ì™„ë£Œ: {execution_time:.2f}ì´ˆ (ê²°í•©ë¨)")
                return combined_result
            else:
                self.logger.info(
                    f"âœ… ë³‘ë ¬ ë¶„ì„ ì™„ë£Œ: {execution_time:.2f}ì´ˆ ({len(results)}ê°œ ê²°ê³¼)"
                )
                return results

        except Exception as e:
            self.logger.error(f"ë³‘ë ¬ ë¶„ì„ ì‹¤íŒ¨: {e}")
            raise

    def parallel_map(
        self,
        func: Callable,
        iterable: List[Any],
        chunksize: Optional[int] = None,
        **kwargs,
    ) -> List[Any]:
        """
        í•¨ìˆ˜ë¥¼ iterableì— ë³‘ë ¬ë¡œ ë§¤í•‘

        Args:
            func: ì ìš©í•  í•¨ìˆ˜
            iterable: ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            chunksize: ì²­í¬ í¬ê¸°
            **kwargs: í•¨ìˆ˜ì— ì „ë‹¬í•  ì¶”ê°€ ì¸ì

        Returns:
            ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        try:
            chunk_size = chunksize or max(1, len(iterable) // (self.max_workers * 2))

            # ë°ì´í„° ì²­í‚¹
            chunks = [
                iterable[i : i + chunk_size]
                for i in range(0, len(iterable), chunk_size)
            ]

            # ì²­í¬ ì²˜ë¦¬ í•¨ìˆ˜
            def process_chunk(chunk):
                return [func(item, **kwargs) for item in chunk]

            # ê²°ê³¼ ê²°í•© í•¨ìˆ˜
            def combine_results(chunk_results):
                combined = []
                for chunk_result in chunk_results:
                    combined.extend(chunk_result)
                return combined

            return self.parallel_analyze_chunks(chunks, process_chunk, combine_results)

        except Exception as e:
            self.logger.error(f"ë³‘ë ¬ ë§¤í•‘ ì‹¤íŒ¨: {e}")
            raise

    def adaptive_parallel_processing(
        self,
        data: List[Any],
        func: Callable,
        min_chunk_size: int = 10,
        max_chunk_size: int = 1000,
        target_execution_time: float = 1.0,
        **kwargs,
    ) -> List[Any]:
        """
        ì ì‘í˜• ë³‘ë ¬ ì²˜ë¦¬ - ì„±ëŠ¥ì— ë”°ë¼ ì²­í¬ í¬ê¸° ìë™ ì¡°ì •

        Args:
            data: ì²˜ë¦¬í•  ë°ì´í„°
            func: ì²˜ë¦¬ í•¨ìˆ˜
            min_chunk_size: ìµœì†Œ ì²­í¬ í¬ê¸°
            max_chunk_size: ìµœëŒ€ ì²­í¬ í¬ê¸°
            target_execution_time: ëª©í‘œ ì‹¤í–‰ ì‹œê°„ (ì´ˆ)
            **kwargs: í•¨ìˆ˜ ì¸ì

        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        try:
            if len(data) <= min_chunk_size:
                # ë°ì´í„°ê°€ ì‘ìœ¼ë©´ ë‹¨ì¼ ìŠ¤ë ˆë“œë¡œ ì²˜ë¦¬
                return [func(item, **kwargs) for item in data]

            # ì´ˆê¸° ì²­í¬ í¬ê¸° ê²°ì •
            initial_chunk_size = min(
                max_chunk_size, max(min_chunk_size, len(data) // self.max_workers)
            )

            # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì‘ì€ ìƒ˜í”Œ ì²˜ë¦¬
            test_size = min(initial_chunk_size, len(data) // 10)
            if test_size > 0:
                test_data = data[:test_size]

                start_time = time.time()
                test_result = self.parallel_map(func, test_data, test_size, **kwargs)
                test_time = time.time() - start_time

                if test_time > 0:
                    # ëª©í‘œ ì‹œê°„ì— ë§ì¶° ì²­í¬ í¬ê¸° ì¡°ì •
                    optimal_chunk_size = int(
                        test_size * target_execution_time / test_time
                    )
                    optimal_chunk_size = max(
                        min_chunk_size, min(max_chunk_size, optimal_chunk_size)
                    )
                else:
                    optimal_chunk_size = initial_chunk_size
            else:
                optimal_chunk_size = initial_chunk_size

            self.logger.debug(f"ì ì‘í˜• ì²­í¬ í¬ê¸°: {optimal_chunk_size}")

            # ì „ì²´ ë°ì´í„° ì²˜ë¦¬
            return self.parallel_map(func, data, optimal_chunk_size, **kwargs)

        except Exception as e:
            self.logger.error(f"ì ì‘í˜• ë³‘ë ¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            # í´ë°±: ë‹¨ì¼ ìŠ¤ë ˆë“œ ì²˜ë¦¬
            return [func(item, **kwargs) for item in data]

    def pipeline_processing(
        self,
        data: List[Any],
        pipeline_stages: List[Callable],
        stage_configs: Optional[List[Dict[str, Any]]] = None,
    ) -> List[Any]:
        """
        íŒŒì´í”„ë¼ì¸ ë³‘ë ¬ ì²˜ë¦¬

        Args:
            data: ì…ë ¥ ë°ì´í„°
            pipeline_stages: íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ í•¨ìˆ˜ë“¤
            stage_configs: ê° ë‹¨ê³„ë³„ ì„¤ì •

        Returns:
            ìµœì¢… ì²˜ë¦¬ ê²°ê³¼
        """
        try:
            self.logger.info(f"ğŸ”„ íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì‹œì‘: {len(pipeline_stages)}ë‹¨ê³„")

            current_data = data
            stage_configs = stage_configs or [{}] * len(pipeline_stages)

            for i, (stage_func, config) in enumerate(
                zip(pipeline_stages, stage_configs)
            ):
                self.logger.debug(
                    f"íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ {i+1}/{len(pipeline_stages)} ì²˜ë¦¬ ì¤‘"
                )

                start_time = time.time()
                current_data = self.parallel_map(stage_func, current_data, **config)
                stage_time = time.time() - start_time

                self.logger.debug(f"ë‹¨ê³„ {i+1} ì™„ë£Œ: {stage_time:.2f}ì´ˆ")

            self.logger.info("âœ… íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì™„ë£Œ")
            return current_data

        except Exception as e:
            self.logger.error(f"íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise

    def _execute_analysis_task(
        self, task_id: str, func: Callable, data: Any, kwargs: Dict[str, Any]
    ) -> ProcessingResult:
        """ë¶„ì„ ì‘ì—… ì‹¤í–‰"""
        start_time = time.time()
        worker_id = f"worker_{os.getpid()}"

        try:
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
            process = psutil.Process()
            initial_memory = process.memory_info().rss / 1024 / 1024  # MB

            # ì‘ì—… ì‹¤í–‰
            result = func(data, **kwargs)

            # ìµœì¢… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
            final_memory = process.memory_info().rss / 1024 / 1024  # MB
            memory_used = final_memory - initial_memory

            execution_time = time.time() - start_time

            # ë©”ëª¨ë¦¬ ì œí•œ ì²´í¬
            if final_memory > self.memory_limit_mb:
                self.logger.warning(
                    f"ë©”ëª¨ë¦¬ ì œí•œ ì´ˆê³¼: {final_memory:.1f}MB > {self.memory_limit_mb}MB"
                )

            return ProcessingResult(
                task_id=task_id,
                result=result,
                success=True,
                execution_time=execution_time,
                worker_id=worker_id,
            )

        except Exception as e:
            execution_time = time.time() - start_time
            error_msg = f"ì‘ì—… ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}"

            return ProcessingResult(
                task_id=task_id,
                result=None,
                success=False,
                error=error_msg,
                execution_time=execution_time,
                worker_id=worker_id,
            )

    def _start_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ìŠ¤ë ˆë“œ ì‹œì‘"""

        def monitor_worker():
            while True:
                try:
                    # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§
                    cpu_percent = psutil.cpu_percent(interval=1)
                    memory_percent = psutil.virtual_memory().percent

                    # í†µê³„ ì—…ë°ì´íŠ¸
                    self.stats["peak_memory_usage"] = max(
                        self.stats["peak_memory_usage"], memory_percent
                    )

                    # ë¡œê·¸ ì¶œë ¥ (5ë¶„ë§ˆë‹¤)
                    if int(time.time()) % 300 == 0:
                        self.logger.debug(
                            f"ì‹œìŠ¤í…œ ìƒíƒœ - CPU: {cpu_percent:.1f}%, "
                            f"ë©”ëª¨ë¦¬: {memory_percent:.1f}%, "
                            f"ì™„ë£Œëœ ì‘ì—…: {self.stats['tasks_completed']}"
                        )

                    time.sleep(10)  # 10ì´ˆë§ˆë‹¤ ëª¨ë‹ˆí„°ë§

                except Exception as e:
                    self.logger.error(f"ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
                    time.sleep(30)

        monitor_thread = threading.Thread(target=monitor_worker, daemon=True)
        monitor_thread.start()
        self.logger.debug("ëª¨ë‹ˆí„°ë§ ìŠ¤ë ˆë“œ ì‹œì‘")

    def get_stats(self) -> Dict[str, Any]:
        """í†µê³„ ì •ë³´ ë°˜í™˜"""
        try:
            # ì‹œìŠ¤í…œ ì •ë³´ ì¶”ê°€
            cpu_count = os.cpu_count()
            memory_info = psutil.virtual_memory()

            return {
                **self.stats,
                "max_workers": self.max_workers,
                "chunk_size": self.chunk_size,
                "timeout": self.timeout,
                "system_info": {
                    "cpu_count": cpu_count,
                    "memory_total_gb": memory_info.total / (1024**3),
                    "memory_available_gb": memory_info.available / (1024**3),
                    "memory_percent": memory_info.percent,
                },
                "efficiency": {
                    "success_rate": (
                        self.stats["tasks_completed"]
                        / max(1, self.stats["tasks_submitted"])
                    )
                    * 100,
                    "avg_execution_time": (
                        self.stats["total_execution_time"]
                        / max(1, self.stats["tasks_completed"])
                    ),
                    "throughput": (
                        self.stats["tasks_completed"]
                        / max(1, self.stats["total_execution_time"])
                    ),
                },
            }
        except Exception as e:
            self.logger.error(f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return self.stats

    def shutdown(self, wait: bool = True):
        """í”„ë¡œì„¸ìŠ¤ í’€ ì¢…ë£Œ"""
        try:
            self.logger.info("í”„ë¡œì„¸ìŠ¤ í’€ ì¢…ë£Œ ì‹œì‘")

            if self.process_pool:
                self.process_pool.shutdown(wait=wait)

            if self.thread_pool:
                self.thread_pool.shutdown(wait=wait)

            self.logger.info("âœ… í”„ë¡œì„¸ìŠ¤ í’€ ì¢…ë£Œ ì™„ë£Œ")

        except Exception as e:
            self.logger.error(f"í”„ë¡œì„¸ìŠ¤ í’€ ì¢…ë£Œ ì‹¤íŒ¨: {e}")

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.shutdown()


# ì „ì—­ í”„ë¡œì„¸ìŠ¤ í’€ ì¸ìŠ¤í„´ìŠ¤
_process_pool = None
_pool_lock = threading.Lock()


def get_enhanced_process_pool(
    config: Optional[Dict[str, Any]] = None,
) -> EnhancedProcessPool:
    """ê³ ì„±ëŠ¥ í”„ë¡œì„¸ìŠ¤ í’€ ë°˜í™˜ (ì‹±ê¸€í†¤)"""
    global _process_pool

    with _pool_lock:
        if _process_pool is None:
            pool_config = config or {}
            _process_pool = EnhancedProcessPool(**pool_config)
        return _process_pool


def cleanup_process_pool():
    """í”„ë¡œì„¸ìŠ¤ í’€ ì •ë¦¬"""
    global _process_pool

    with _pool_lock:
        if _process_pool is not None:
            _process_pool.shutdown()
            _process_pool = None


# í¸ì˜ í•¨ìˆ˜ë“¤
def parallel_lottery_analysis(
    data_chunks: List[Any], analyzer_func: Callable, **kwargs
) -> List[Any]:
    """ë¡œë˜ ë¶„ì„ì„ ìœ„í•œ ë³‘ë ¬ ì²˜ë¦¬ í¸ì˜ í•¨ìˆ˜"""
    pool = get_enhanced_process_pool()
    return pool.parallel_analyze_chunks(data_chunks, analyzer_func, **kwargs)


def adaptive_lottery_processing(
    lottery_data: List[Any], processing_func: Callable, **kwargs
) -> List[Any]:
    """ì ì‘í˜• ë¡œë˜ ë°ì´í„° ì²˜ë¦¬ í¸ì˜ í•¨ìˆ˜"""
    pool = get_enhanced_process_pool()
    return pool.adaptive_parallel_processing(lottery_data, processing_func, **kwargs)
