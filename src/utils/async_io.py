"""ë¹„ë™ê¸° IO ì²˜ë¦¬ ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ íŒŒì¼ ì…ì¶œë ¥ ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ ìœ„í•œ í´ë˜ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
"""

import asyncio
import aiofiles
from typing import Any, Dict, List, Optional, Union, Tuple
from pathlib import Path
import json
import pickle
import zlib
from dataclasses import dataclass, field
import time

from .unified_logging import get_logger

logger = get_logger(__name__)


@dataclass
class AsyncIOConfig:
    """ë¹„ë™ê¸° IO ì„¤ì • í´ë˜ìŠ¤"""

    chunk_size: int = 1 << 20  # 1MB
    max_concurrent_ops: int = 4
    compression_level: int = 6
    compression_threshold: int = 1 << 20  # 1MB
    retry_count: int = 3
    retry_delay: float = 1.0
    timeout: float = 30.0
    use_compression: bool = True
    use_encryption: bool = False
    encryption_key: str = ""
    encryption_iv: str = ""
    stats: Dict[str, Any] = field(default_factory=dict)


class AsyncIOManager:
    """ë¹„ë™ê¸° IO ê´€ë¦¬ì í´ë˜ìŠ¤"""

    def __init__(self, config: Optional[AsyncIOConfig] = None):
        self.config = config or AsyncIOConfig()
        self._semaphore = asyncio.Semaphore(self.config.max_concurrent_ops)
        self._stats = {
            "read_ops": 0,
            "write_ops": 0,
            "read_bytes": 0,
            "write_bytes": 0,
            "read_time": 0.0,
            "write_time": 0.0,
            "errors": 0,
            "retries": 0,
        }

    async def read_file(self, file_path: Union[str, Path]) -> Optional[bytes]:
        """íŒŒì¼ ë¹„ë™ê¸° ì½ê¸°"""
        start_time = time.time()
        retry_count = 0

        while retry_count < self.config.retry_count:
            try:
                async with self._semaphore:
                    async with aiofiles.open(file_path, "rb") as f:
                        data = await f.read()
                        self._stats["read_ops"] += 1
                        self._stats["read_bytes"] += len(data)
                        self._stats["read_time"] += time.time() - start_time
                        return data
            except Exception as e:
                retry_count += 1
                self._stats["errors"] += 1
                self._stats["retries"] += 1
                if retry_count < self.config.retry_count:
                    await asyncio.sleep(self.config.retry_delay)
                    logger.warning(
                        f"íŒŒì¼ ì½ê¸° ì¬ì‹œë„ {retry_count}/{self.config.retry_count}: {str(e)}"
                    )
                else:
                    logger.error(f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {str(e)}")
                    return None

    def sync_read_file(self, file_path: Union[str, Path]) -> Optional[bytes]:
        """íŒŒì¼ ë™ê¸°ì‹ ì½ê¸° - ë¹„ë™ê¸° ë©”ì„œë“œì˜ ë™ê¸° ë²„ì „"""
        start_time = time.time()
        retry_count = 0

        while retry_count < self.config.retry_count:
            try:
                with open(file_path, "rb") as f:
                    data = f.read()
                    self._stats["read_ops"] += 1
                    self._stats["read_bytes"] += len(data)
                    self._stats["read_time"] += time.time() - start_time
                    return data
            except Exception as e:
                retry_count += 1
                self._stats["errors"] += 1
                self._stats["retries"] += 1
                if retry_count < self.config.retry_count:
                    time.sleep(self.config.retry_delay)
                    logger.warning(
                        f"íŒŒì¼ ì½ê¸° ì¬ì‹œë„ {retry_count}/{self.config.retry_count}: {str(e)}"
                    )
                else:
                    logger.error(f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {str(e)}")
                    return None

        return None

    async def write_file(self, file_path: Union[str, Path], data: bytes) -> bool:
        """íŒŒì¼ ë¹„ë™ê¸° ì“°ê¸°"""
        start_time = time.time()
        retry_count = 0

        # ë°ì´í„° ì••ì¶•
        if (
            self.config.use_compression
            and len(data) > self.config.compression_threshold
        ):
            data = zlib.compress(data, level=self.config.compression_level)

        while retry_count < self.config.retry_count:
            try:
                async with self._semaphore:
                    async with aiofiles.open(file_path, "wb") as f:
                        await f.write(data)
                        self._stats["write_ops"] += 1
                        self._stats["write_bytes"] += len(data)
                        self._stats["write_time"] += time.time() - start_time
                        return True
            except Exception as e:
                retry_count += 1
                self._stats["errors"] += 1
                self._stats["retries"] += 1
                if retry_count < self.config.retry_count:
                    await asyncio.sleep(self.config.retry_delay)
                    logger.warning(
                        f"íŒŒì¼ ì“°ê¸° ì¬ì‹œë„ {retry_count}/{self.config.retry_count}: {str(e)}"
                    )
                else:
                    logger.error(f"íŒŒì¼ ì“°ê¸° ì‹¤íŒ¨: {str(e)}")
                    return False
        return False

    async def read_json(self, file_path: Union[str, Path]) -> Optional[Dict[str, Any]]:
        """JSON íŒŒì¼ ë¹„ë™ê¸° ì½ê¸°"""
        data = await self.read_file(file_path)
        if data is not None:
            try:
                return json.loads(data)
            except Exception as e:
                logger.error(f"JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
                return None
        return None

    async def write_json(
        self, file_path: Union[str, Path], data: Dict[str, Any]
    ) -> bool:
        """JSON íŒŒì¼ ë¹„ë™ê¸° ì“°ê¸°"""
        try:
            json_data = json.dumps(data)
            return await self.write_file(file_path, json_data.encode())
        except Exception as e:
            logger.error(f"JSON ì§ë ¬í™” ì‹¤íŒ¨: {str(e)}")
            return False

    async def read_pickle(self, file_path: Union[str, Path]) -> Optional[Any]:
        """Pickle íŒŒì¼ ë¹„ë™ê¸° ì½ê¸°"""
        data = await self.read_file(file_path)
        if data is not None:
            try:
                return pickle.loads(data)
            except Exception as e:
                logger.error(f"Pickle íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
                return None
        return None

    async def write_pickle(self, file_path: Union[str, Path], data: Any) -> bool:
        """Pickle íŒŒì¼ ë¹„ë™ê¸° ì“°ê¸°"""
        try:
            pickle_data = pickle.dumps(data)
            return await self.write_file(file_path, pickle_data)
        except Exception as e:
            logger.error(f"Pickle ì§ë ¬í™” ì‹¤íŒ¨: {str(e)}")
            return False

    async def read_lines(self, file_path: Union[str, Path]) -> Optional[List[str]]:
        """íŒŒì¼ ë¼ì¸ë³„ ë¹„ë™ê¸° ì½ê¸°"""
        try:
            async with aiofiles.open(file_path, "r") as f:
                return await f.readlines()
        except Exception as e:
            logger.error(f"íŒŒì¼ ë¼ì¸ ì½ê¸° ì‹¤íŒ¨: {str(e)}")
            return None

    async def write_lines(self, file_path: Union[str, Path], lines: List[str]) -> bool:
        """íŒŒì¼ ë¼ì¸ë³„ ë¹„ë™ê¸° ì“°ê¸°"""
        try:
            async with aiofiles.open(file_path, "w") as f:
                await f.writelines(lines)
                return True
        except Exception as e:
            logger.error(f"íŒŒì¼ ë¼ì¸ ì“°ê¸° ì‹¤íŒ¨: {str(e)}")
            return False

    def get_stats(self) -> Dict[str, Any]:
        """IO í†µê³„ ë°˜í™˜"""
        return self._stats.copy()

    def reset_stats(self):
        """IO í†µê³„ ì´ˆê¸°í™”"""
        self._stats = {
            "read_ops": 0,
            "write_ops": 0,
            "read_bytes": 0,
            "write_bytes": 0,
            "read_time": 0.0,
            "write_time": 0.0,
            "errors": 0,
            "retries": 0,
        }

    # --- ğŸš€ ë³‘ë ¬ ë° ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬ ìµœì í™” ---

    async def read_files_in_parallel(
        self, file_paths: List[Union[str, Path]]
    ) -> List[Optional[bytes]]:
        """ì—¬ëŸ¬ íŒŒì¼ì„ ë³‘ë ¬ë¡œ ë¹„ë™ê¸° ì½ê¸°"""
        tasks = [self.read_file(fp) for fp in file_paths]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        processed_results = []
        for res in results:
            if isinstance(res, Exception):
                logger.error(f"ë³‘ë ¬ íŒŒì¼ ì½ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {res}")
                processed_results.append(None)
            else:
                processed_results.append(res)
        return processed_results

    async def write_files_in_parallel(
        self, operations: List[Tuple[Union[str, Path], bytes]]
    ) -> List[bool]:
        """ì—¬ëŸ¬ íŒŒì¼ì„ ë³‘ë ¬ë¡œ ë¹„ë™ê¸° ì“°ê¸°"""
        tasks = [self.write_file(fp, data) for fp, data in operations]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        processed_results = []
        for res in results:
            if isinstance(res, Exception):
                logger.error(f"ë³‘ë ¬ íŒŒì¼ ì“°ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {res}")
                processed_results.append(False)
            else:
                processed_results.append(res)
        return processed_results

    async def read_file_chunked(self, file_path: Union[str, Path]):
        """ëŒ€ìš©ëŸ‰ íŒŒì¼ì„ ì²­í¬ ë‹¨ìœ„ë¡œ ë¹„ë™ê¸° ì½ê¸° (ë¹„ë™ê¸° ì œë„ˆë ˆì´í„°)"""
        start_time = time.time()
        try:
            async with self._semaphore:
                async with aiofiles.open(file_path, "rb") as f:
                    while True:
                        chunk = await f.read(self.config.chunk_size)
                        if not chunk:
                            break
                        self._stats["read_ops"] += 1
                        self._stats["read_bytes"] += len(chunk)
                        yield chunk
        except Exception as e:
            logger.error(f"ì²­í¬ ë‹¨ìœ„ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
        finally:
            self._stats["read_time"] += time.time() - start_time

    async def write_file_chunked(self, file_path: Union[str, Path], data_generator):
        """ë°ì´í„° ì œë„ˆë ˆì´í„°ë¡œë¶€í„° ì²­í¬ ë‹¨ìœ„ë¡œ ë¹„ë™ê¸° ì“°ê¸°"""
        start_time = time.time()
        total_bytes = 0
        try:
            async with self._semaphore:
                async with aiofiles.open(file_path, "wb") as f:
                    async for chunk in data_generator:
                        await f.write(chunk)
                        total_bytes += len(chunk)
            self._stats["write_ops"] += 1
            self._stats["write_bytes"] += total_bytes
            return True
        except Exception as e:
            logger.error(f"ì²­í¬ ë‹¨ìœ„ íŒŒì¼ ì“°ê¸° ì‹¤íŒ¨: {e}")
            return False
        finally:
            self._stats["write_time"] += time.time() - start_time
