#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
ê³ ì„±ëŠ¥ GPU ë²¡í„° ë‚´ë³´ë‚´ê¸° ì‹œìŠ¤í…œ (v3 - í†µí•©/ê°„ì†Œí™”)

CUDA ìµœì í™” ë° ë¹„ë™ê¸° I/Oë¥¼ í™œìš©í•˜ì—¬ ë²¡í„°ë¥¼ ê³ ì†ìœ¼ë¡œ ì²˜ë¦¬í•˜ê³  ë‚´ë³´ëƒ…ë‹ˆë‹¤.
íŒŒì¼ í¬ê¸°ì™€ ë³µì¡ì„±ì„ ëŒ€í­ ì¤„ì—¬ ìœ ì§€ë³´ìˆ˜ì„±ì„ ê·¹ëŒ€í™”í–ˆìŠµë‹ˆë‹¤.
"""

import os
import json
import numpy as np
import torch
import torch.nn.functional as F
from typing import List, Dict, Any, Optional, Tuple, Union
from datetime import datetime
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import math
import gc
from pathlib import Path
import mmap
import weakref
import asyncio

from .unified_logging import get_logger
from .async_io import get_gpu_async_io_manager

logger = get_logger(__name__)

# GPU ê°€ìš©ì„± ì²´í¬
GPU_AVAILABLE = torch.cuda.is_available()
GPU_COUNT = torch.cuda.device_count() if GPU_AVAILABLE else 0

if GPU_AVAILABLE:
    logger.info(f"âœ… CUDA ë²¡í„° ì²˜ë¦¬ ì‹œìŠ¤í…œ í™œì„±í™” (GPU ìˆ˜: {GPU_COUNT})")
else:
    logger.warning("âš ï¸ GPU ì—†ìŒ - CPU ì „ìš© ë²¡í„° ì²˜ë¦¬ ëª¨ë“œ")


class GPUMemoryPool:
    """GPU ë©”ëª¨ë¦¬ í’€ (ì™„ì „ ìë™ ê´€ë¦¬)"""

    def __init__(self, max_pool_size: int = 50):
        self.pools = {}  # {shape_dtype: [tensors]}
        self.pool_lock = threading.RLock()
        self.max_pool_size = max_pool_size
        self.stats = {"hits": 0, "misses": 0, "allocations": 0}

        # ìë™ ì •ë¦¬ ìŠ¤ë ˆë“œ
        self.cleanup_thread = threading.Thread(target=self._auto_cleanup, daemon=True)
        self.cleanup_running = True
        self.cleanup_thread.start()

    def get_tensor(
        self, shape: tuple, dtype=torch.float32, device="cuda"
    ) -> torch.Tensor:
        """ë©”ëª¨ë¦¬ í’€ì—ì„œ í…ì„œ íšë“"""
        key = f"{shape}_{dtype}_{device}"

        with self.pool_lock:
            if key in self.pools and self.pools[key]:
                tensor = self.pools[key].pop()
                self.stats["hits"] += 1
                return tensor.zero_()

            # ìƒˆ í…ì„œ ìƒì„±
            try:
                if device == "cuda" and GPU_AVAILABLE:
                    tensor = torch.zeros(shape, dtype=dtype, device=device)
                else:
                    tensor = torch.zeros(shape, dtype=dtype)
                self.stats["allocations"] += 1
                self.stats["misses"] += 1
                return tensor
            except torch.cuda.OutOfMemoryError:
                # GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ CPU í´ë°±
                logger.warning("GPU ë©”ëª¨ë¦¬ ë¶€ì¡±, CPU í…ì„œ ìƒì„±")
                return torch.zeros(shape, dtype=dtype, device="cpu")

    def return_tensor(self, tensor: torch.Tensor):
        """í…ì„œë¥¼ í’€ì— ë°˜í™˜"""
        if tensor.numel() < 1000:  # ì‘ì€ í…ì„œëŠ” í’€ë§í•˜ì§€ ì•ŠìŒ
            return

        key = f"{tensor.shape}_{tensor.dtype}_{tensor.device}"

        with self.pool_lock:
            if key not in self.pools:
                self.pools[key] = []

            if len(self.pools[key]) < self.max_pool_size:
                self.pools[key].append(tensor.detach())

    def _auto_cleanup(self):
        """ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬"""
        while self.cleanup_running:
            try:
                time.sleep(30)  # 30ì´ˆë§ˆë‹¤ ì •ë¦¬
                with self.pool_lock:
                    for key in list(self.pools.keys()):
                        if len(self.pools[key]) > self.max_pool_size // 2:
                            # ì ˆë°˜ë§Œ ìœ ì§€
                            self.pools[key] = self.pools[key][: self.max_pool_size // 2]

                    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                    if GPU_AVAILABLE:
                        torch.cuda.empty_cache()

            except Exception as e:
                logger.debug(f"ë©”ëª¨ë¦¬ í’€ ì •ë¦¬ ì˜¤ë¥˜: {e}")

    def get_stats(self) -> Dict[str, Any]:
        """í’€ í†µê³„"""
        hit_rate = self.stats["hits"] / max(
            self.stats["hits"] + self.stats["misses"], 1
        )
        return {
            **self.stats,
            "hit_rate": f"{hit_rate * 100:.1f}%",
            "active_pools": len(self.pools),
            "total_cached": sum(len(pool) for pool in self.pools.values()),
        }


class GPUVectorExporter:
    """GPU ê°€ì†ì„ í™œìš©í•œ í†µí•© ë²¡í„° ë‚´ë³´ë‚´ê¸° í´ë˜ìŠ¤"""

    def __init__(self):
        self.device = torch.device("cuda" if GPU_AVAILABLE else "cpu")
        self.async_io = get_gpu_async_io_manager()
        self.max_batch_size = 1024  # í•œ ë²ˆì— ì²˜ë¦¬í•  ìµœëŒ€ ë²¡í„° ìˆ˜

        if GPU_AVAILABLE:
            logger.info(f"âœ… GPU ë²¡í„° ë‚´ë³´ë‚´ê¸° ì‹œìŠ¤í…œ ì´ˆê¸°í™” (Device: {self.device})")
        else:
            logger.warning("âš ï¸ GPU ì‚¬ìš© ë¶ˆê°€. CPU ëª¨ë“œë¡œ ë²¡í„° ë‚´ë³´ë‚´ê¸° ì‹œìŠ¤í…œ ì‹¤í–‰.")

    async def export(
        self,
        vectors: Union[np.ndarray, List[np.ndarray], torch.Tensor],
        paths: Union[str, List[str]],
        transform: Optional[str] = "normalize",
    ):
        """
        ë‹¨ì¼ ë˜ëŠ” ë‹¤ì¤‘ ë²¡í„°ë¥¼ ë¹„ë™ê¸°ì ìœ¼ë¡œ ë‚´ë³´ëƒ…ë‹ˆë‹¤.
        ë‚´ë¶€ì ìœ¼ë¡œ batch_exportë¥¼ í˜¸ì¶œí•˜ì—¬ ì¼ê´€ëœ ë¡œì§ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        """
        vectors_list = vectors if isinstance(vectors, list) else [vectors]
        paths_list = paths if isinstance(paths, list) else [paths]

        await self.batch_export(vectors_list, paths_list, transform)

    def zero_copy_export(
        self,
        vectors: Union[np.ndarray, List[np.ndarray]],
        paths: Union[str, List[str]],
        transforms: Optional[List[str]] = None,
        metadata: Optional[Dict[str, Any]] = None,
        use_compression: bool = True,
    ) -> List[bool]:
        """
        ì œë¡œ ì¹´í”¼ ë²¡í„° ë‚´ë³´ë‚´ê¸° (ë™ê¸° ë²„ì „)

        Args:
            vectors: ë‚´ë³´ë‚¼ ë²¡í„°ë“¤
            paths: ì €ì¥í•  ê²½ë¡œë“¤
            transforms: ë³€í™˜ íƒ€ì…ë“¤ (ì„ íƒì‚¬í•­)
            metadata: ë©”íƒ€ë°ì´í„° (ì„ íƒì‚¬í•­)
            use_compression: ì••ì¶• ì‚¬ìš© ì—¬ë¶€ (ì„ íƒì‚¬í•­)

        Returns:
            ê° ë²¡í„° ì €ì¥ ì„±ê³µ ì—¬ë¶€ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # ì…ë ¥ ì •ê·œí™”
            vectors_list = vectors if isinstance(vectors, list) else [vectors]
            paths_list = paths if isinstance(paths, list) else [paths]

            if len(vectors_list) != len(paths_list):
                raise ValueError("ë²¡í„° ìˆ˜ì™€ ê²½ë¡œ ìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

            # ë³€í™˜ íƒ€ì… ì„¤ì •
            if transforms is None:
                transform = "normalize"
            elif isinstance(transforms, list) and transforms:
                transform = transforms[0]  # ì²« ë²ˆì§¸ ë³€í™˜ íƒ€ì… ì‚¬ìš©
            else:
                transform = None

            # ë¹„ë™ê¸° batch_export í˜¸ì¶œ
            asyncio.run(self.batch_export(vectors_list, paths_list, transform))

            # ëª¨ë“  ì €ì¥ì´ ì„±ê³µí–ˆë‹¤ê³  ê°€ì • (ì‹¤ì œë¡œëŠ” ê° íŒŒì¼ë³„ ì„±ê³µ ì—¬ë¶€ë¥¼ ì¶”ì í•´ì•¼ í•¨)
            return [True] * len(vectors_list)

        except Exception as e:
            logger.error(f"ì œë¡œ ì¹´í”¼ ë²¡í„° ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
            return [False] * len(
                vectors_list if isinstance(vectors, list) else [vectors]
            )

    async def batch_export(
        self,
        vectors: Union[List[np.ndarray], List[torch.Tensor]],
        paths: List[str],
        transform: Optional[str] = "normalize",
    ):
        """
        ë²¡í„° ë°°ì¹˜ë¥¼ GPUë¡œ ë³‘ë ¬ ì²˜ë¦¬í•˜ê³  ë¹„ë™ê¸°ì ìœ¼ë¡œ íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤.
        """
        if len(vectors) != len(paths):
            raise ValueError("ë²¡í„°ì˜ ìˆ˜ì™€ ê²½ë¡œì˜ ìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

        if not vectors:
            return

        # ì „ì²´ ë²¡í„°ë¥¼ ìˆœíšŒí•˜ë©° ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        for i in range(0, len(vectors), self.max_batch_size):
            batch_vectors = vectors[i : i + self.max_batch_size]
            batch_paths = paths[i : i + self.max_batch_size]

            # GPU ê°€ì†ì´ ê°€ëŠ¥í•˜ë©´ GPUì—ì„œ ì²˜ë¦¬
            if GPU_AVAILABLE:
                await self._process_batch_gpu(batch_vectors, batch_paths, transform)
            else:  # GPU ì‚¬ìš© ë¶ˆê°€ ì‹œ CPUë¡œ ì²˜ë¦¬
                await self._process_batch_cpu(batch_vectors, batch_paths, transform)

    async def _process_batch_gpu(self, batch_vectors, batch_paths, transform):
        """GPUë¥¼ ì‚¬ìš©í•˜ì—¬ ë°°ì¹˜ ì²˜ë¦¬ ë° ë¹„ë™ê¸° ì €ì¥"""
        # 1. ë°ì´í„°ë¥¼ GPU í…ì„œë¡œ ë³€í™˜
        # pin_memoryë¥¼ ì‚¬ìš©í•˜ë©´ CPU->GPU ì „ì†¡ ì†ë„ë¥¼ ë†’ì¼ ìˆ˜ ìˆìŒ
        try:
            tensors = [
                torch.from_numpy(v).pin_memory().to(self.device, non_blocking=True)
                for v in batch_vectors
            ]
        except TypeError:  # ì´ë¯¸ í…ì„œì¼ ê²½ìš°
            tensors = [
                v.pin_memory().to(self.device, non_blocking=True) for v in batch_vectors
            ]

        # 2. GPUì—ì„œ ë³‘ë ¬ë¡œ ë³€í™˜ ì ìš©
        if transform == "normalize":
            # ì—¬ëŸ¬ í…ì„œë¥¼ í•œë²ˆì— ì •ê·œí™”
            transformed_tensors = [
                torch.nn.functional.normalize(t, dim=0) for t in tensors
            ]
        else:  # ë³€í™˜ ì—†ìŒ
            transformed_tensors = tensors

        # 3. ë¹„ë™ê¸°ì ìœ¼ë¡œ íŒŒì¼ ì €ì¥
        tasks = []
        for tensor, path in zip(transformed_tensors, batch_paths):
            # í…ì„œë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜ (CPUë¡œ ì´ë™) í›„ ë°”ì´íŠ¸ë¡œ ë³€í™˜
            # .cpu()ëŠ” ë™ê¸° ì—°ì‚°ì´ë¯€ë¡œ, I/O ì‘ì—… ì „ì— ìˆ˜í–‰
            data_bytes = tensor.cpu().numpy().tobytes()
            tasks.append(self.async_io.smart_write_file(Path(path), data_bytes))

        await asyncio.gather(*tasks)

    async def _process_batch_cpu(self, batch_vectors, batch_paths, transform):
        """CPUë¥¼ ì‚¬ìš©í•˜ì—¬ ë°°ì¹˜ ì²˜ë¦¬ ë° ë¹„ë™ê¸° ì €ì¥"""
        # (CPU ì²˜ë¦¬ ë¡œì§ì€ ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ êµ¬í˜„, í•„ìš”ì‹œ ë©€í‹°í”„ë¡œì„¸ì‹± ì¶”ê°€ ê°€ëŠ¥)
        transformed_vectors = []
        for v in batch_vectors:
            if transform == "normalize":
                norm = np.linalg.norm(v)
                transformed_vectors.append(v / norm if norm > 0 else v)
            else:
                transformed_vectors.append(v)

        write_tasks = [
            self.async_io.smart_write_file(Path(path), vec.tobytes())
            for vec, path in zip(transformed_vectors, batch_paths)
        ]
        await asyncio.gather(*write_tasks)

    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        logger.info("GPU ë²¡í„° ë‚´ë³´ë‚´ê¸° ì‹œìŠ¤í…œ ì •ë¦¬")


# --- ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ---
_exporter_instance: Optional[GPUVectorExporter] = None


def get_gpu_vector_exporter() -> GPUVectorExporter:
    """GPUVectorExporterì˜ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    global _exporter_instance
    if _exporter_instance is None:
        _exporter_instance = GPUVectorExporter()
    return _exporter_instance


# === ê¸€ë¡œë²Œ ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤) ===
_vector_processor: Optional[GPUVectorExporter] = None
_processor_lock = threading.Lock()


def get_vector_exporter() -> GPUVectorExporter:
    """ê¸€ë¡œë²Œ ë²¡í„° ì²˜ë¦¬ê¸° ë°˜í™˜ (ì‹±ê¸€í†¤)"""
    global _vector_processor
    if _vector_processor is None:
        with _processor_lock:
            if _vector_processor is None:
                _vector_processor = GPUVectorExporter()
    return _vector_processor


# === í¸ì˜ í•¨ìˆ˜ë“¤ ===


def gpu_accelerated_export(
    vectors: Union[np.ndarray, List[np.ndarray]],
    paths: Union[str, List[str]],
    transforms: Optional[List[str]] = None,
    metadata: Optional[Dict[str, Any]] = None,
    use_compression: bool = True,
) -> List[bool]:
    """í¸ì˜ í•¨ìˆ˜: GPU ê°€ì† ë²¡í„° ë‚´ë³´ë‚´ê¸°"""
    processor = get_vector_exporter()
    return processor.zero_copy_export(
        vectors, paths, transforms, metadata, use_compression
    )


def save_feature_vector_optimized(
    vector: np.ndarray,
    feature_names: List[str],
    base_path: str,
    transform_type: str = "normalize",
    formats: List[str] = None,
    metadata: Optional[Dict[str, Any]] = None,
) -> Dict[str, bool]:
    """ìµœì í™”ëœ íŠ¹ì„± ë²¡í„° ì €ì¥"""
    if formats is None:
        formats = ["npy", "npz"]

    paths = []
    vectors = []
    transforms = []

    base_path_obj = Path(base_path)

    for fmt in formats:
        path = str(base_path_obj.with_suffix(f".{fmt}"))
        paths.append(path)
        vectors.append(vector)
        transforms.append(transform_type)

    # ë©”íƒ€ë°ì´í„° ì¶”ê°€
    if metadata is None:
        metadata = {}
    metadata.update(
        {
            "feature_names": feature_names,
            "vector_shape": vector.shape,
            "created_at": datetime.now().isoformat(),
            "formats": formats,
        }
    )

    processor = get_vector_exporter()
    results = processor.zero_copy_export(vectors, paths, transforms, metadata)

    return dict(zip(formats, results))


def cleanup_vector_system():
    """ë²¡í„° ì‹œìŠ¤í…œ ì •ë¦¬"""
    global _vector_processor
    if _vector_processor:
        _vector_processor.cleanup()
        _vector_processor = None
    logger.info("ğŸ§¹ ë²¡í„° ì‹œìŠ¤í…œ ì •ë¦¬ ì™„ë£Œ")


# í•˜ìœ„ í˜¸í™˜ì„± ë˜í¼
def export_vector_with_filtering(*args, **kwargs):
    """í•˜ìœ„ í˜¸í™˜ì„±: í•„í„°ë§ê³¼ í•¨ê»˜ ë²¡í„° ë‚´ë³´ë‚´ê¸°"""
    return gpu_accelerated_export(*args, **kwargs)


def export_gnn_state_inputs(*args, **kwargs):
    """í•˜ìœ„ í˜¸í™˜ì„±: GNN ìƒíƒœ ì…ë ¥ ë‚´ë³´ë‚´ê¸°"""
    return gpu_accelerated_export(*args, **kwargs)


# ëª¨ë“ˆ ë¡œë“œ ì‹œ ì´ˆê¸°í™”
if __name__ != "__main__":
    logger.info("ğŸš€ CUDA ë²¡í„° ì²˜ë¦¬ ì‹œìŠ¤í…œ ë¡œë“œ ì™„ë£Œ")
